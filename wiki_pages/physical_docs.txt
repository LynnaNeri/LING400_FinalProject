
Dementia is the general name for a decline in cognitive abilities that impacts a person's ability to perform  everyday activities. This typically involves problems with memory, thinking, and behavior. Aside from memory impairment and a disruption in thought patterns, the most common symptoms include emotional problems, difficulties with language, and decreased motivation.[2]  The symptoms may be described as occurring in a continuum over several stages.[10][a] Dementia ultimately has a significant effect on the individual, caregivers, and on social relationships in general.[2] A diagnosis of dementia requires the observation of a change from a person's usual mental functioning and a greater cognitive decline than what is caused by normal aging.[12]
Several diseases and injuries to the brain such as a stroke can give rise to dementia. However, the most common cause is Alzheimer's disease, a neurodegenerative disorder.[2] The Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), has re-described dementia as either a mild or major neurocognitive disorder with varying degrees of severity and many causative subtypes. The International Classification of Diseases (ICD-11) also classes dementia as a neurocognitive disorder (NCD) with many forms or subclasses.[13] Dementia is listed as an acquired brain syndrome, marked by a decline in cognitive function, and is contrasted with neurodevelopmental disorders.[14] Dementia is also described as a spectrum of disorders with causative subtypes of dementia based on a known disorder, such as Parkinson's disease, for Parkinson's disease dementia; Huntington's disease, for Huntington's disease dementia; vascular disease, for vascular dementia; HIV infection, causing HIV dementia; frontotemporal lobar degeneration for frontotemporal dementia; or Lewy body disease for dementia with Lewy bodies, and prion diseases.[15] Subtypes of neurodegenerative dementias may also be based on the underlying pathology of misfolded proteins such as synucleinopathies, and tauopathies.[15] More than one type of dementia existing together is known as mixed dementia.[14]
Many neurocognitive disorders may be caused by another medical condition or disorder that includes brain tumours, and subdural hematoma; endocrine disorders such as hypothyroidism, and hypoglycemia; nutritional deficiencies including thiamine, and niacin; infections, immune disorders, liver or kidney failure, metabolic disorders such as Kufs disease, and some leukodystrophies, and neurological disorders such as epilepsy, and multiple sclerosis. Some of the neurocognitive deficits may sometimes show improvement with treatment of the medical condition.[16]
Diagnosis is usually based on history of the illness and cognitive testing with imaging. Blood tests may be taken to rule out other possible causes that may be reversible, such as hypothyroidism (an underactive thyroid), and to determine the dementia subtype. One commonly used cognitive test is the mini–mental state examination. The greatest risk factor for developing dementia is aging, however dementia is not a normal part of aging. Many people aged 90 and above show no signs of dementia.[17] Several risk factors for dementia, such as smoking and obesity, are preventable by lifestyle changes. Screening the general older population for the disorder is not seen to affect the outcome.[18]
Dementia is currently the seventh leading cause of death worldwide and has 10 million new cases reported every year (one every ~3 seconds).[2] There is no known cure for dementia. Acetylcholinesterase inhibitors such as donepezil are often used and may be beneficial in mild to moderate disorder. The overall benefit, however, may be minor. There are many measures that can improve the quality of life of people with dementia and their caregivers. Cognitive and behavioral interventions may be appropriate for treating associated symptoms of depression.[19]
The signs and symptoms of dementia are termed as the neuropsychiatric symptoms, also known as the behavioral and psychological symptoms of dementia.[20][21] Behavioral symptoms can include agitation, restlessness, inappropriate behavior, sexual disinhibition, and aggression, which can be verbal or physical.[22] These symptoms may result from impairments in cognitive inhibition.[23] Psychological symptoms can include depression, hallucinations (most often visual),[24] and delusions, apathy, and anxiety.[22][25] The most commonly affected areas include memory, visuospatial function affecting perception and orientation, language, attention and problem solving. The rate at which symptoms progress occurs on a continuum over several stages, and they vary across the dementia subtypes.[26][10] Most types of dementia are slowly progressive with some deterioration of the brain well established before signs of the disorder become apparent. Often there are other conditions present such as high blood pressure, or diabetes, and there can sometimes be as many as four of these comorbidities.[27]
People with dementia are also more likely to have problems with incontinence: they are three times more likely to have urinary and four times more likely to have fecal incontinence compared to people of similar ages.[28][29]
Dementia symptoms can vary widely from person to person. It affects memory, attention span, communication, reasoning, judgment, problem solving and visual perception, etc. Signs that may point to dementia include getting lost in a familiar neighborhood, using unusual words to refer to familiar objects, forgetting the name of a close family member or friend, forgetting old memories, not being able to complete tasks independently, etc.[30]
The course of dementia is often described in four stages that show a pattern of progressive cognitive and functional impairment. However, the use of numeric scales allows for more detailed descriptions. These scales include the Global Deterioration Scale for Assessment of Primary Degenerative Dementia (GDS or Reisberg Scale), the Functional Assessment Staging Test (FAST), and the Clinical Dementia Rating (CDR).[31] Using the GDS, which more accurately identifies each stage of the disease progression, a more detailed course is described in seven stages – two of which are broken down further into five and six degrees. Stage 7(f) is the final stage.[32][33]
Pre-dementia states include pre-clinical and prodromal stages. The prodromal stages includes (1) mild cognitive impairment (MCI), (2) delirium-onset, and psychiatric-onset presentations.[34]
Sensory dysfunction is claimed for this stage which may precede the first clinical signs of dementia by up to ten years.[10] Most notably the sense of smell is lost.[10][35] The loss of the sense of smell is associated with depression and loss of appetite leading to poor nutrition.[36] It is suggested that this dysfunction may come about because the olfactory epithelium is exposed to the environment. The lack of blood–brain barrier protection here means that toxic elements can enter and cause damage to the chemosensory networks.[10]
Pre-dementia states considered as prodromal are mild cognitive impairment (MCI), and mild behavioral impairment (MBI).[37][38][39]
Kynurenine is a metabolite of tryptophan that regulates microbiome signalling, immune cell response, and neuronal excitation. A disruption in the kynurenine pathway may be associated with the neuropsychiatric symptoms and cognitive prognosis in mild dementia.[40][41]
In this stage signs and symptoms may be subtle. Often, the early signs become apparent when looking back.[42] 70% of those diagnosed with MCI later progress to dementia.[12] In MCI, changes in the person's brain have been happening for a long time, but symptoms are just beginning to appear. These problems, however, are not severe enough to affect daily function. If and when they do, the diagnosis becomes dementia. They may have some memory trouble and trouble finding words, but they solve everyday problems and competently handle their life affairs.[43] During this stage, it is ideal to ensure that advance care planning has occurred to protect the wishes of the person. Advance directives which are specific to dementia exist,[44] which can be particularly helpful in addressing the decisions related to feeding which come with the progression of the illness.
Mild cognitive impairment has been relisted in both DSM-5, and ICD-11, as mild neurocognitive disorders – milder forms of the major neurocognitive disorder (dementia) subtypes.[45]
In the early stage of dementia, symptoms become noticeable to other people. In addition, the symptoms begin to interfere with daily activities, and will register a score on a mini–mental state examination (MMSE). MMSE scores are set at 24 to 30 for a normal cognitive rating and lower scores reflect severity of symptoms. The symptoms are dependent on the type of dementia. More complicated chores and tasks around the house or at work become more difficult. The person can usually still take care of themselves but may forget things like taking pills or doing laundry and may need prompting or reminders.[46]
The symptoms of early dementia usually include memory difficulty, but can also include some word-finding problems, and problems with executive functions of planning and organization.[47] Managing finances may prove difficult. Other signs might be getting lost in new places, repeating things, and personality changes.[48]
In some types of dementia, such as dementia with Lewy bodies and frontotemporal dementia, personality changes and difficulty with organization and planning may be the first signs.[49]
As dementia progresses, initial symptoms generally worsen. The rate of decline is different for each person. MMSE scores between 6–17 signal moderate dementia. For example, people with moderate Alzheimer's dementia lose almost all new information. People with dementia may be severely impaired in solving problems, and their social judgment is often impaired. They cannot usually function outside their own home, and generally should not be left alone. They may be able to do simple chores around the house but not much else, and begin to require assistance for personal care and hygiene beyond simple reminders.[12] A lack of insight into having the condition will become evident.[50][51]
People with late-stage dementia typically turn increasingly inward and need assistance with most or all of their personal care. People with dementia in the late stages usually need 24-hour supervision to ensure their personal safety, and meeting of basic needs. If left unsupervised, they may wander or fall; may not recognize common dangers such as a hot stove; or may not realize that they need to use the bathroom and become incontinent.[43] They may not want to get out of bed, or may need assistance doing so. Commonly, the person no longer recognizes familiar faces. They may have significant changes in sleeping habits or have trouble sleeping at all.[12]
Changes in eating frequently occur. Cognitive awareness is needed for eating and swallowing and progressive cognitive decline results in eating and swallowing difficulties. This can cause food to be refused, or choked on, and help with feeding will often be required.[52] For ease of feeding, food may be liquidized into a thick purée. They may also struggle to walk, particularly among those with Alzheimer's disease.[53][54][55] In some cases, there is a paradoxical lucidity immediately before death, where there is an unexpected recovery of mental clarity.[56]
Many causes of dementia are neurodegenerative, and protein misfolding is a cardinal feature of these.[57] Other common causes include vascular dementia, dementia with Lewy bodies, frontotemporal dementia, and mixed dementia (commonly Alzheimer's disease and vascular dementia).[2][b][61] Less common causes include normal pressure hydrocephalus, Parkinson's disease dementia, syphilis, HIV, and Creutzfeldt–Jakob disease.[62]
 Alzheimer's disease accounts for 60–70% of cases of dementia worldwide. The most common symptoms of Alzheimer's disease are short-term memory loss and word-finding difficulties. Trouble with visuospatial functioning (getting lost often), reasoning, judgment and insight fail. Insight refers to whether or not the person realizes they have memory problems.
The part of the brain most affected by Alzheimer's is the hippocampus. Other parts that show atrophy (shrinking) include the temporal and parietal lobes. Although this pattern of brain shrinkage suggests Alzheimer's, it is variable and a brain scan is insufficient for a diagnosis. 
Little is known about the events that occur during and that actually cause Alzheimer's disease. This is due to the fact that brain tissue from patients with the disease can only be studied after the person's death. However, it is known that one of the first aspects of the disease is a dysfunction in the gene that produces amyloid. Extracellular senile plaques (SPs), consisting of beta-amyloid (Aβ) peptides, and intracellular neurofibrillary tangles (NFTs) that are formed by hyperphosphorylated tau proteins, are two well-established pathological hallmarks of AD.[63] Amyloid causes inflammation around the senile plaques of the brain, and too much build up of this inflammation leads to changes in the brain that cannot be controlled, leading to the symptoms of Alzheimer's.[64]
Several articles have been published on a possible relationship (as a either primary cause or exacerbation of Alzheimer's disease) between general anesthesia and Alzheimer's in specifically the elderly.[65]
Vascular dementia accounts for at least 20% of dementia cases, making it the second most common type.[66] It is caused by disease or injury affecting the blood supply to the brain, typically involving a series of mini-strokes. The symptoms of this dementia depend on where in the brain the strokes occurred and whether the blood vessels affected were large or small.[12] Repeated injury can cause progressive dementia over time, while a single injury located in an area critical for cognition such as the hippocampus, or thalamus, can lead to sudden cognitive decline.[66] Elements of vascular dementia may be present in all other forms of dementia.[67]
Brain scans may show evidence of multiple strokes of different sizes in various locations. People with vascular dementia tend to have risk factors for disease of the blood vessels, such as tobacco use, high blood pressure, atrial fibrillation, high cholesterol, diabetes, or other signs of vascular disease such as a previous heart attack or angina.[68]
The prodromal symptoms of dementia with Lewy bodies (DLB) include mild cognitive impairment, and delirium onset.[69]
The symptoms of DLB are more frequent, more severe, and earlier presenting than in the other dementia subtypes.[70]
Dementia with Lewy bodies has the primary symptoms of fluctuating cognition, alertness or attention; REM sleep behavior disorder (RBD); one or more of the main features of parkinsonism, not due to medication or stroke; and repeated visual hallucinations.[71] The visual hallucinations in DLB are generally vivid hallucinations of people or animals and they often occur when someone is about to fall asleep or wake up. Other prominent symptoms include problems with planning (executive function) and difficulty with visual-spatial function,[12] and disruption in autonomic bodily functions.[72] Abnormal sleep behaviors may begin before cognitive decline is observed and are a core feature of DLB.[71] RBD is diagnosed either by sleep study recording or, when sleep studies cannot be performed, by medical history and validated questionnaires.[71]
Parkinson's disease is a Lewy body disease that often progresses to Parkinson's disease dementia following a period of dementia-free Parkinson's disease.[73]
Frontotemporal dementias (FTDs) are characterized by drastic personality changes and language difficulties. In all FTDs, the person has a relatively early social withdrawal and early lack of insight. Memory problems are not a main feature.[12][74] There are six main types of FTD. The first has major symptoms in personality and behavior. This is called behavioral variant FTD (bv-FTD) and is the most common. The hallmark feature of bv-FTD is impulsive behavior, and this can be detected in pre-dementia states.[39] In bv-FTD, the person shows a change in personal hygiene, becomes rigid in their thinking, and rarely acknowledges problems; they are socially withdrawn, and often have a drastic increase in appetite. They may become socially inappropriate. For example, they may make inappropriate sexual comments, or may begin using pornography openly. One of the most common signs is apathy, or not caring about anything. Apathy, however, is a common symptom in many dementias.[12]
Two types of FTD feature aphasia (language problems) as the main symptom. One type is called semantic variant primary progressive aphasia (SV-PPA). The main feature of this is the loss of the meaning of words. It may begin with difficulty naming things. The person eventually may lose the meaning of objects as well. For example, a drawing of a bird, dog, and an airplane in someone with FTD may all appear almost the same.[12] In a classic test for this, a patient is shown a picture of a pyramid and below it a picture of both a palm tree and a pine tree. The person is asked to say which one goes best with the pyramid. In SV-PPA the person cannot answer that question. The other type is called non-fluent agrammatic variant primary progressive aphasia (NFA-PPA). This is mainly a problem with producing speech. They have trouble finding the right words, but mostly they have a difficulty coordinating the muscles they need to speak. Eventually, someone with NFA-PPA only uses one-syllable words or may become totally mute.
A frontotemporal dementia associated with amyotrophic lateral sclerosis (ALS) known as (FTD-ALS) includes the symptoms of FTD (behavior, language and movement problems) co-occurring with amyotrophic lateral sclerosis (loss of motor neurons). Two FTD-related disorders are progressive supranuclear palsy (also classed as a Parkinson-plus syndrome),[75][76] and corticobasal degeneration.[12] These disorders are tau-associated.
Huntington's disease is a neurodegenerative disease caused by mutations in a single gene HTT, that encodes for huntingtin protein. Symptoms include cognitive impairment and this usually declines further into dementia.[77]
The first main symptoms of Huntington's disease often include:
HIV-associated dementia results as a late stage from HIV infection, and mostly affects younger people.[79] The essential features of HIV-associated dementia are disabling cognitive impairment accompanied by motor dysfunction, speech problems and behavioral change.[79] Cognitive impairment is characterised by mental slowness, trouble with memory and poor concentration. Motor symptoms include a loss of fine motor control leading to clumsiness, poor balance and tremors. Behavioral changes may include apathy, lethargy and diminished emotional responses and spontaneity. Histopathologically, it is identified by the infiltration of monocytes and macrophages into the central nervous system (CNS), gliosis, pallor of myelin sheaths, abnormalities of dendritic processes and neuronal loss.[80]
Creutzfeldt–Jakob disease is a rapidly progressive prion disease that typically causes dementia that worsens over weeks to months. Prions are disease-causing pathogens created from abnormal proteins.[81]
Alcohol-related dementia, also called alcohol-related brain damage, occurs as a result of excessive use of alcohol particularly as a substance abuse disorder. Different factors can be involved in this development including thiamine deficiency and age vulnerability.[82][83] A degree of brain damage is seen in more than 70% of those with alcohol use disorder. Brain regions affected are similar to those that are affected by aging, and also by Alzheimer's disease. Regions showing loss of volume include the frontal, temporal, and parietal lobes, as well as the cerebellum, thalamus, and hippocampus.[83] This loss can be more notable, with greater cognitive impairments seen in those aged 65 years and older.[83]
More than one type of dementia, known as mixed dementia, may exist together in about 10% of dementia cases.[2] The most common type of mixed dementia is Alzheimer's disease and vascular dementia.[84] This particular type of mixed dementia's main onsets are a mixture of old age, high blood pressure, and damage to blood vessels in the brain.[14]
Diagnosis of mixed dementia can be difficult, as often only one type will predominate. This makes the treatment of people with mixed dementia uncommon, with many people missing out on potentially helpful treatments. Mixed dementia can mean that symptoms onset earlier, and worsen more quickly since more parts of the brain will be affected.[14]
Chronic inflammatory conditions that may affect the brain and cognition include Behçet's disease, multiple sclerosis, sarcoidosis, Sjögren's syndrome, lupus, celiac disease, and non-celiac gluten sensitivity.[85][86] These types of dementias can rapidly progress, but usually have a good response to early treatment. This consists of immunomodulators or steroid administration, or in certain cases, the elimination of the causative agent.[86] A 2019 review found no association between celiac disease and dementia overall but a potential association with vascular dementia.[87] A 2018 review found a link between celiac disease or non-celiac gluten sensitivity and cognitive impairment and that celiac disease may be associated with Alzheimer's disease, vascular dementia, and frontotemporal dementia.[88] A strict gluten-free diet started early may protect against dementia associated with gluten-related disorders.[87][88]
Cases of easily reversible dementia include hypothyroidism, vitamin B12 deficiency, Lyme disease, and neurosyphilis. For Lyme disease and neurosyphilis, testing should be done if risk factors are present. Because risk factors are often difficult to determine, testing for neurosyphilis and Lyme disease, as well as other mentioned factors, may be undertaken as a matter of course where dementia is suspected.[12]: 31–32 
Many other medical and neurological conditions include dementia only late in the illness. For example, a proportion of patients with Parkinson's disease develop dementia, though widely varying figures are quoted for this proportion.[89] When dementia occurs in Parkinson's disease, the underlying cause may be dementia with Lewy bodies or Alzheimer's disease, or both.[90] Cognitive impairment also occurs in the Parkinson-plus syndromes of progressive supranuclear palsy and corticobasal degeneration (and the same underlying pathology may cause the clinical syndromes of frontotemporal lobar degeneration). Although the acute porphyrias may cause episodes of confusion and psychiatric disturbance, dementia is a rare feature of these rare diseases. Limbic-predominant age-related TDP-43 encephalopathy (LATE) is a type of dementia that primarily affects people in their 80s or 90s and in which TDP-43 protein deposits in the limbic portion of the brain.[91]
Hereditary disorders that can also cause dementia include: some metabolic disorders such as lysosomal storage disorders, leukodystrophies, and spinocerebellar ataxias.
Symptoms are similar across dementia types and it is difficult to diagnose by symptoms alone. Diagnosis may be aided by brain scanning techniques. In many cases, the diagnosis requires a brain biopsy to become final, but this is rarely recommended (though it can be performed at autopsy). In those who are getting older, general screening for cognitive impairment using cognitive testing or early diagnosis of dementia has not been shown to improve outcomes.[18][92] However, screening exams are useful in 65+ persons with memory complaints.[12]
Normally, symptoms must be present for at least six months to support a diagnosis.[93] Cognitive dysfunction of shorter duration is called delirium. Delirium can be easily confused with dementia due to similar symptoms. Delirium is characterized by a sudden onset, fluctuating course, a short duration (often lasting from hours to weeks), and is primarily related to a somatic (or medical) disturbance. In comparison, dementia has typically a long, slow onset (except in the cases of a stroke or trauma), slow decline of mental functioning, as well as a longer trajectory (from months to years).[94]
Some mental illnesses, including depression and psychosis, may produce symptoms that must be differentiated from both delirium and dementia.[95] These are differently diagnosed as pseudodementias, and any dementia evaluation needs to include a depression screening such as the Neuropsychiatric Inventory or the Geriatric Depression Scale.[96][12] Physicians used to think that people with memory complaints had depression and not dementia (because they thought that those with dementia are generally unaware of their memory problems). However, researchers have realized that many older people with memory complaints in fact have mild cognitive impairment the earliest stage of dementia. Depression should always remain high on the list of possibilities, however, for an elderly person with memory trouble. Changes in thinking, hearing and vision are associated with normal ageing and can cause problems when diagnosing dementia due to the similarities.[97] Given the challenging nature of predicting the onset of dementia and making a dementia diagnosis clinical decision making aids underpinned by machine learning and artificial intelligence have the potential to enhance clinical practice.[98]
Various brief cognitive tests (5–15 minutes) have reasonable reliability to screen for dementia, but may be affected by factors such as age, education and ethnicity.[101] Age and education have a significant influence on the diagnosis of dementia. For example, Individuals with lower education are more likely to be diagnosed with dementia than their educated counterparts.[102] While many tests have been studied,[103][104][105] presently the mini mental state examination (MMSE) is the best studied and most commonly used. The MMSE is a useful tool for helping to diagnose dementia if the results are interpreted along with an assessment of a person's personality, their ability to perform activities of daily living, and their behaviour.[4] Other cognitive tests include the abbreviated mental test score (AMTS), the, "modified mini–mental state examination" (3MS),[106] the Cognitive Abilities Screening Instrument (CASI),[107] the Trail-making test,[108] and the clock drawing test.[31] The MoCA (Montreal Cognitive Assessment) is a reliable screening test and is available online for free in 35 different languages.[12] The MoCA has also been shown somewhat better at detecting mild cognitive impairment than the MMSE.[109][38] People with hearing loss, which commonly occurs alongside dementia, score worse in the MoCA test, which could lead to a false diagnosis of dementia. Researchers have developed an adapted version of the MoCA test, which is accurate and reliable and avoids the need for people to listen and respond to questions.[110][111] The AD-8 – a screening questionnaire used to assess changes in function related to cognitive decline – is potentially useful, but is not diagnostic, is variable, and has risk of bias.[112] An integrated cognitive assessment (CognICA) is a five-minute test that is highly sensitive to the early stages of dementia, and uses an application deliverable to an iPad.[113][114] Previously in use in the UK, in 2021 CognICA was given FDA approval for its commercial use as a medical device.[114]
Another approach to screening for dementia is to ask an informant (relative or other supporter) to fill out a questionnaire about the person's everyday cognitive functioning. Informant questionnaires provide complementary information to brief cognitive tests. Probably the best known questionnaire of this sort is the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE).[115] Evidence is insufficient to determine how accurate the IQCODE is for diagnosing or predicting dementia.[116] The Alzheimer's Disease Caregiver Questionnaire is another tool. It is about 90% accurate for Alzheimer's when by a caregiver.[12] The General Practitioner Assessment Of Cognition combines both a patient assessment and an informant interview. It was specifically designed for use in the primary care setting.
Clinical neuropsychologists provide diagnostic consultation following administration of a full battery of cognitive testing, often lasting several hours, to determine functional patterns of decline associated with varying types of dementia. Tests of memory, executive function, processing speed, attention and language skills are relevant, as well as tests of emotional and psychological adjustment. These tests assist with ruling out other etiologies and determining relative cognitive decline over time or from estimates of prior cognitive abilities.[117]
Routine blood tests are usually performed to rule out treatable causes. These include tests for vitamin B12, folic acid, thyroid-stimulating hormone (TSH), C-reactive protein, full blood count, electrolytes, calcium, renal function, and liver enzymes. Abnormalities may suggest vitamin deficiency, infection, or other problems that commonly cause confusion or disorientation in the elderly.[118]
A CT scan or MRI scan is commonly performed to possibly find either normal pressure hydrocephalus, a potentially reversible cause of dementia, or connected tumor. The scans can also yield information relevant to other types of dementia, such as infarction (stroke) that would point at a vascular type of dementia. These tests do not pick up diffuse metabolic changes associated with dementia in a person who shows no gross neurological problems (such as paralysis or weakness) on a neurological exam.[119]
The functional neuroimaging modalities of SPECT and PET are more useful in assessing long-standing cognitive dysfunction, since they have shown similar ability to diagnose dementia as a clinical exam and cognitive testing.[120] The ability of SPECT to differentiate vascular dementia from Alzheimer's disease, appears superior to differentiation by clinical exam.[121]
The value of PiB-PET imaging using Pittsburgh compound B (PiB) as a radiotracer has been established in predictive diagnosis, particularly Alzheimer's disease.[122]
Risk factors for dementia include high blood pressure, hearing loss, smoking, obesity, depression, inactivity, diabetes, lower levels of education and low social contact. Over-indulgence in alcohol, lack of sleep, anemia, traumatic brain injury, and air pollution can also increase the chance of developing dementia.[7][123] Many of these risk factors, including the lower level of education, smoking, physical inactivity and diabetes, are modifiable.[124] Several of the group are known as vascular risk factors that may be possible to be reduced or eliminated.[125] Managing these risk factors can reduce the risk of dementia in individuals in their late midlife or older age. A reduction in a number of these risk factors can give a positive outcome.[126] The decreased risk achieved by adopting a healthy lifestyle is seen even in those with a high genetic risk.[127]
In addition to the above risk factors, other psychological features, including certain personality traits (high neuroticism, and low conscientiousness), low purpose in life, and high loneliness, are risk factors for Alzheimer's disease and related dementias.[128][129][130] For example, based on the English Longitudinal Study of Ageing (ELSA), research found that loneliness in older people can increase the risk of dementia by one-third. Not having a partner (being single, divorced, or widowed) can double the risk of dementia. However, having two or three closer relationships might reduce the risk by three-fifths.[131][132]
The two most modifiable risk factors for dementia are physical inactivity and lack of cognitive stimulation.[133] Physical activity, in particular aerobic exercise, is associated with a reduction in age-related brain tissue loss, and neurotoxic factors thereby preserving brain volume and neuronal integrity. Cognitive activity strengthens neural plasticity and together they help to support cognitive reserve. The neglect of these risk factors diminishes this reserve.[133]
Sensory impairments of vision and hearing are modifiable risk factors for dementia.[134] These impairments may precede the cognitive symptoms of Alzheimer's disease for example, by many years.[135] Hearing loss may lead to social isolation which negatively affects cognition.[136] Social isolation is also identified as a modifiable risk factor.[135] Age-related hearing loss in midlife is linked to cognitive impairment in late life, and is seen as a risk factor for the development of Alzheimer's disease and dementia. Such hearing loss may be caused by a central auditory processing disorder that makes the understanding of speech against background noise difficult. Age-related hearing loss is characterised by slowed central processing of auditory information.[135][137] Worldwide, mid-life hearing loss may account for around 9% of dementia cases.[138]
Frailty may increase the risk of cognitive decline, and dementia, and  the inverse also holds of cognitive impairment increasing the risk of frailty. Prevention of frailty may help to prevent cognitive decline.[135]
There are no medications that can prevent cognitive decline and dementia.[139] However blood pressure lowering medications might decrease the risk of dementia or cognitive problems by around 7.0%.[140]
Economic disadvantage has been shown to have a strong link to higher dementia prevalence,[141] which cannot yet be fully explained by other risk factors.
Limited evidence links poor oral health to cognitive decline. However, failure to perform tooth brushing and gingival inflammation can be used as dementia risk predictors.[142]
The link between Alzheimer's and gum disease is oral bacteria.[143] In the oral cavity, bacterial species include P. gingivalis, F. nucleatum, P. intermedia, and T. forsythia. Six oral treponema spirochetes have been examined in the brains of Alzheimer's patients.[144] Spirochetes are neurotropic in nature, meaning they act to destroy nerve tissue and create inflammation. Inflammatory pathogens are an indicator of Alzheimer's disease and bacteria related to gum disease have been found in the brains of patients with Alzheimer's disease.[144] The bacteria invade nerve tissue in the brain, increasing the permeability of the blood–brain barrier and promoting the onset of Alzheimer's. Individuals with a plethora of tooth plaque risk cognitive decline.[145] Poor oral hygiene can have an adverse effect on speech and nutrition, causing general and cognitive health decline.
Herpes simplex virus (HSV) has been found in more than 70% of those aged over 50. HSV persists in the peripheral nervous system and can be triggered by stress, illness or fatigue.[144] High proportions of viral-associated proteins in amyloid plaques or neurofibrillary tangles (NFTs) confirm the involvement of HSV-1 in Alzheimer's disease pathology. NFTs are known as the primary marker of Alzheimer's disease. HSV-1 produces the main components of NFTs.[146]
Diet is seen to be a modifiable risk factor for the development of dementia. Thiamine deficiency is identified to increase the risk of Alzheimer's disease in adults.[147] The role of thiamine in brain physiology is unique and essential for the normal cognitive function of older people.[148] Many dietary choices of the elderly population, including the higher intake of gluten-free products, compromise the intake of thiamine as these products are not fortified with thiamine.[149]
The Mediterranean and DASH diets are both associated with less cognitive decline. A different approach has been to incorporate elements of both of these diets into one known as the MIND diet.[150] These diets are generally low in saturated fats while providing a good source of carbohydrates, mainly those that help stabilize blood sugar and insulin levels.[151] Raised blood sugar levels over a long time, can damage nerves and cause memory problems if they are not managed.[152] Nutritional factors associated with the proposed diets for reducing dementia risk include unsaturated fatty acids, vitamin E, vitamin C, flavonoids, vitamin B, and vitamin D.[153][154] A study conducted at the University of Exeter in the United Kingdom seems to have confirmed these findings with fruits, vegetables, whole grains, and healthy fats creating an optimum diet that can help reduce the risk of dementia by roughly 25%.[155]
The MIND diet may be more protective but further studies are needed. The Mediterranean diet seems to be more protective against Alzheimer's than DASH but there are no consistent findings against dementia in general. The role of olive oil needs further study as it may be one of the most important components in reducing the risk of cognitive decline and dementia.[150][156]
In those with celiac disease or non-celiac gluten sensitivity, a strict gluten-free diet may relieve the symptoms given a mild cognitive impairment.[87][88] Once dementia is advanced no evidence suggests that a gluten-free diet is useful.[87]
Omega-3 fatty acid supplements do not appear to benefit or harm people with mild to moderate symptoms.[157] However, there is good evidence that omega-3 incorporation into the diet is of benefit in treating depression, a common symptom,[158] and potentially modifiable risk factor for dementia.[7]
There are limited options for treating dementia, with most approaches focused on managing or reducing individual symptoms. There are no treatment options available to delay the onset of dementia.[159] Acetylcholinesterase inhibitors are often used early in the disorder course; however, benefit is generally small.[8][160] More than half of people with dementia may experience psychological or behavioral symptoms including agitation, sleep problems, aggression, and/or psychosis. Treatment for these symptoms is aimed at reducing the person's distress and keeping the person safe. Treatments other than medication appear to be better for agitation and aggression.[161] Cognitive and behavioral interventions may be appropriate. Some evidence suggests that education and support for the person with dementia, as well as caregivers and family members, improves outcomes.[162] Palliative care interventions may lead to improvements in comfort in dying, but the evidence is low.[163] Exercise programs are beneficial with respect to activities of daily living, and potentially improve dementia.[164]
The effect of therapies can be evaluated for example by assessing agitation using the Cohen-Mansfield Agitation Inventory (CMAI); by assessing mood and engagement with the Menorah Park Engagement Scale (MPES);[165] and the Observed Emotion Rating Scale (OERS)[166] or by assessing indicators for depression using the Cornell Scale for Depression in Dementia (CSDD)[167] or a simplified version thereof.[168]
Often overlooked in treating and managing dementia is the role of the caregiver and what is known about how they can support multiple interventions. Findings from a 2021 systematic review of the literature found caregivers of people with dementia in nursing homes do not have sufficient tools or clinical guidance for behavioral and psychological symptoms of dementia (BPSD) along with medication use.[169] Simple measures like talking to people about their interests can improve the quality of life for care home residents living with dementia. A programme showed that such simple measures reduced residents' agitation and depression. They also needed fewer GP visits and hospital admissions, which also meant that the programme was cost-saving.[170][171]
Psychological therapies for dementia include some limited evidence for reminiscence therapy (namely, some positive effects in the areas of quality of life, cognition, communication and mood – the first three particularly in care home settings),[172] some benefit for cognitive reframing for caretakers,[173] unclear evidence for validation therapy[174] and tentative evidence for mental exercises, such as cognitive stimulation programs for people with mild to moderate dementia.[175] Offering personally tailored activities may help reduce challenging behavior and may improve quality of life.[176] It is not clear if personally tailored activities have an impact on affect or improve for the quality of life for the caregiver.[176]
Adult daycare centers as well as special care units in nursing homes often provide specialized care for dementia patients. Daycare centers offer supervision, recreation, meals, and limited health care to participants, as well as providing respite for caregivers. In addition, home care can provide one-to-one support and care in the home allowing for more individualized attention that is needed as the disorder progresses. Psychiatric nurses can make a distinctive contribution to people's mental health.[177]
Since dementia impairs normal communication due to changes in receptive and expressive language, as well as the ability to plan and problem solve, agitated behavior is often a form of communication for the person with dementia. Actively searching for a potential cause, such as pain, physical illness, or overstimulation can be helpful in reducing agitation.[178] Additionally, using an "ABC analysis of behavior" can be a useful tool for understanding behavior in people with dementia. It involves looking at the antecedents (A), behavior (B), and consequences (C) associated with an event to help define the problem and prevent further incidents that may arise if the person's needs are misunderstood.[179] The strongest evidence for non-pharmacological therapies for the management of changed behaviors in dementia is for using such approaches.[180] Low quality evidence suggests that regular (at least five sessions of) music therapy may help institutionalized residents. It may reduce depressive symptoms and improve overall behaviors. It may also supply a beneficial effect on emotional well-being and quality of life, as well as reduce anxiety.[181] In 2003, The Alzheimer's Society established 'Singing for the Brain' (SftB) a project based on pilot studies which suggested that the activity encouraged participation and facilitated the learning of new songs. The sessions combine aspects of reminiscence therapy and music.[182] Musical and interpersonal connectedness can underscore the value of the person and improve quality of life.[183]
Some London hospitals found that using color, designs, pictures and lights helped people with dementia adjust to being at the hospital. These adjustments to the layout of the dementia wings at these hospitals helped patients by preventing confusion.[184]
Life story work as part of reminiscence therapy, and video biographies have been found to address the needs of clients and their caregivers in various ways, offering the client the opportunity to leave a legacy and enhance their personhood and also benefitting youth who participate in such work. Such interventions can be more beneficial when undertaken at a relatively early stage of dementia. They may also be problematic in those who have difficulties in processing past experiences[183]
Animal-assisted therapy has been found to be helpful. Drawbacks may be that pets are not always welcomed in a communal space in the care setting. An animal may pose a risk to residents, or may be perceived to be dangerous. Certain animals may also be regarded as "unclean" or "dangerous" by some cultural groups.[183]
Occupational therapy also addresses psychological and psychosocial needs of patients with dementia through improving daily occupational performance and caregivers' competence.[185] When compensatory intervention strategies are added to their daily routine, the level of performance is enhanced and reduces the burden commonly placed on their caregivers.[185] Occupational therapists can also work with other disciplines to create a client centered intervention.[186] To manage cognitive disability, and coping with behavioral and psychological symptoms of dementia, combined occupational and behavioral therapies can support patients with dementia even further.[186]
There is no strong evidence to suggest that cognitive training is beneficial for people with Parkinson's disease, dementia, or mild cognitive impairment.[187]
Offering personally tailored activity sessions to people with dementia in long-term care homes may slightly reduce challenging behavior.[188]
No medications have been shown to prevent or cure dementia.[189] Medications may be used to treat the behavioral and cognitive symptoms, but have no effect on the underlying disease process.[12][190]
Acetylcholinesterase inhibitors, such as donepezil, may be useful for Alzheimer's disease,[191] Parkinson's disease dementia, DLB, or vascular dementia.[190] The quality of the evidence is poor[192] and the benefit is small.[8] No difference has been shown between the agents in this family.[193] In a minority of people side effects include a slow heart rate and fainting.[194] Rivastigmine is recommended for treating symptoms in Parkinson's disease dementia.[61]
Medications that have anticholinergic effects increase all-cause mortality in people with dementia, although the effect of these medications on cognitive function remains uncertain, according to a systematic review published in 2021.[195]
Before prescribing antipsychotic medication in the elderly, an assessment for an underlying cause of the behavior is needed.[196] Severe and life-threatening reactions occur in almost half of people with DLB,[72][197] and can be fatal after a single dose.[198] People with Lewy body dementias who take neuroleptics are at risk for neuroleptic malignant syndrome, a life-threatening illness.[199] Extreme caution is required in the use of antipsychotic medication in people with DLB because of their sensitivity to these agents.[71] Antipsychotic drugs are used to treat dementia only if non-drug therapies have not worked, and the person's actions threaten themselves or others.[200][201][202][203] Aggressive behavior changes are sometimes the result of other solvable problems, that could make treatment with antipsychotics unnecessary.[200] Because people with dementia can be aggressive, resistant to their treatment, and otherwise disruptive, sometimes antipsychotic drugs are considered as a therapy in response.[200] These drugs have risky adverse effects, including increasing the person's chance of stroke and death.[200] Given these adverse events and small benefit antipsychotics are avoided whenever possible.[180] Generally, stopping antipsychotics for people with dementia does not cause problems, even in those who have been on them a long time.[204]
N-methyl-D-aspartate (NMDA) receptor blockers such as memantine may be of benefit but the evidence is less conclusive than for AChEIs.[191] Due to their differing mechanisms of action memantine and acetylcholinesterase inhibitors can be used in combination however the benefit is slight.[205][206]
An extract of Ginkgo biloba known as EGb 761 has been widely used for treating mild to moderate dementia and other neuropsychiatric disorders.[207] Its use is approved throughout Europe.[208] The World Federation of Biological Psychiatry guidelines lists EGb 761 with the same weight of evidence (level B) given to acetylcholinesterase inhibitors, and mementine. EGb 761 is the only one that showed improvement of symptoms in both AD and vascular dementia. EGb 761 is seen as being able to play an important role either on its own or as an add-on particularly when other therapies prove ineffective.[207] EGb 761 is seen to be neuroprotective; it is a free radical scavenger, improves mitochondrial function, and modulates serotonin and dopamine levels. Many studies of its use in mild to moderate dementia have shown it to significantly improve cognitive function, activities of daily living, neuropsychiatric symptoms, and quality of life.[207][209] However, its use has not been shown to prevent the progression of dementia.[207]
While depression is frequently associated with dementia, the use of antidepressants such as selective serotonin reuptake inhibitors (SSRIs) do not appear to affect outcomes.[210][211] However, the SSRIs sertraline and citalopram have been demonstrated to reduce symptoms of agitation, compared to placebo.[212]
No solid evidence indicates that folate or vitamin B12 improves outcomes in those with cognitive problems.[213] Statins have no benefit in dementia.[214] Medications for other health conditions may need to be managed differently for a person who has a dementia diagnosis. It is unclear whether blood pressure medication and dementia are linked. People may experience an increase in cardiovascular-related events if these medications are withdrawn.[215]
The Medication Appropriateness Tool for Comorbid Health Conditions in Dementia (MATCH-D) criteria can help identify ways that a diagnosis of dementia changes medication management for other health conditions.[216] These criteria were developed because people with dementia live with an average of five other chronic diseases, which are often managed with medications. The systematic review that informed the criteria were published subsequently in 2018 and updated in 2022.[217]
Over 40% of people with dementia report sleep problems. Approaches to treating these sleep problems include medications and non-pharmacological approaches.[218] The use of medications to alleviate sleep disturbances that people with dementia often experience has not been well researched, even for medications that are commonly prescribed.[219] In 2012 the American Geriatrics Society recommended that benzodiazepines such as diazepam, and non-benzodiazepine hypnotics, be avoided for people with dementia due to the risks of increased cognitive impairment and falls.[220] Benzodiazepines are also known to promote delirium.[221] Additionally, little evidence supports the effectiveness of benzodiazepines in this population.[219][222] No clear evidence shows that melatonin or ramelteon improves sleep for people with dementia due to Alzheimer's,[219] but it is used to treat REM sleep behavior disorder in dementia with Lewy bodies.[72] Limited evidence suggests that a low dose of trazodone may improve sleep, however more research is needed.[219]
Non-pharmacological approaches have been suggested for treating sleep problems for those with dementia, however, there is no strong evidence or firm conclusions on the effectiveness of different types of interventions, especially for those who are living in an institutionalized setting such as a nursing home or long-term care home.[218]
As people age, they experience more health problems, and most health problems associated with aging carry a substantial burden of pain; therefore, between 25% and 50% of older adults experience persistent pain. Seniors with dementia experience the same prevalence of conditions likely to cause pain as seniors without dementia.[223] Pain is often overlooked in older adults and, when screened for, is often poorly assessed, especially among those with dementia, since they become incapable of informing others of their pain.[223][224] Beyond the issue of humane care, unrelieved pain has functional implications. Persistent pain can lead to decreased ambulation, depressed mood, sleep disturbances, impaired appetite, and exacerbation of cognitive impairment[224] and pain-related interference with activity is a factor contributing to falls in the elderly.[223][225]
Although persistent pain in people with dementia is difficult to communicate, diagnose, and treat, failure to address persistent pain has profound functional, psychosocial and quality of life implications for this vulnerable population. Health professionals often lack the skills and usually lack the time needed to recognize, accurately assess and adequately monitor pain in people with dementia.[223][226] Family members and friends can make a valuable contribution to the care of a person with dementia by learning to recognize and assess their pain. Educational resources and observational assessment tools are available.[223][227][228]
Persons with dementia may have difficulty eating. Whenever it is available as an option, the recommended response to eating problems is having a caretaker assist them.[200] A secondary option for people who cannot swallow effectively is to consider gastrostomy feeding tube placement as a way to give nutrition. However, in bringing comfort and maintaining functional status while lowering risk of aspiration pneumonia and death, assistance with oral feeding is at least as good as tube feeding.[200][229] Tube-feeding is associated with agitation, increased use of physical and chemical restraints and worsening pressure ulcers. Tube feedings may cause fluid overload, diarrhea, abdominal pain, local complications, less human interaction and may increase the risk of aspiration.[230][231]
Benefits in those with advanced dementia has not been shown.[232] The risks of using tube feeding include agitation, rejection by the person (pulling out the tube, or otherwise physical or chemical immobilization to prevent them from doing this), or developing pressure ulcers.[200] The procedure is directly related to a 1% fatality rate[233] with a 3% major complication rate.[234] The percentage of people at end of life with dementia using feeding tubes in the US has dropped from 12% in 2000 to 6% as of 2014.[235][236]
The immediate and long-term effects of modifying the thickness of fluids for swallowing difficulties in people with dementia are not well known.[237] While thickening fluids may have an immediate positive effect on swallowing and improving oral intake, the long-term impact on the health of the person with dementia should also be considered.[237]
Exercise programs may improve the ability of people with dementia to perform daily activities, but the best type of exercise is still unclear.[238] Getting more exercise can slow the development of cognitive problems such as dementia, proving to reduce the risk of Alzheimer's disease by about 50%. A balance of strength exercise, to help muscles pump blood to the brain, and balance exercises are recommended for aging people. A suggested amount of about 2+1⁄2 hours per week can reduce risks of cognitive decay as well as other health risks like falling.[239]
There is a lack of high-quality evidence to determine whether assistive technology effectively supports people with dementia to manage memory issues.[240] Some of the specific things that are used today that helps with dementia today are: clocks, communication aids, electrical appliances the use monitoring, GPS location/ tracking devices, home care robots, in-home cameras, and medication management are just to name a few.[241] Technology has the potential to be a valuable intervention for alleviating loneliness and promoting social connections, supported by available evidence. [242]
Evidence of the therapeutic values of aromatherapy and massage is unclear.[243][244] It is not clear if cannabinoids are harmful or effective for people with dementia.[245]
Given the progressive and terminal nature of dementia, palliative care can be helpful to patients and their caregivers by helping people with the disorder and their caregivers understand what to expect, deal with loss of physical and mental abilities, support the person's wishes and goals including surrogate decision making, and discuss wishes for or against CPR and life support.[246][247] Because the decline can be rapid, and because most people prefer to allow the person with dementia to make their own decisions, palliative care involvement before the late stages of dementia is recommended.[248][249] Further research is required to determine the appropriate palliative care interventions and how well they help people with advanced dementia.[163]
Person-centered care helps maintain the dignity of people with dementia.[250]
Remotely delivered interventions including support, training and information may reduce the burden for the informal caregiver and improve their depressive symptoms.[251] There is no certain evidence that they improve health-related quality of life.[251]
In several localities in Japan, digital surveillance may be made available to family members, if a dementia patient is prone to wandering and going missing.[252]
The number of cases of dementia worldwide in 2021 was estimated at 55 million, with close to 10 million new cases each year.[2] By 2050, the number of people living with dementia is estimated to be over 150 million globally.[253] Around 7% of people over aged 65 have dementia, with slightly higher rates (up to 10% of those over 65) in places with relatively high life expectancy.[254] An estimated 58% of people with dementia are living in low and middle income countries.[255][256] The prevalence of dementia differs in different world regions, ranging from 4.7% in Central Europe to 8.7% in North Africa/Middle East; the prevalence in other regions is estimated to be between 5.6 and 7.6%.[255] The number of people living with dementia is estimated to double every 20 years. In 2016 dementia resulted in about 2.4 million deaths,[9] up from 0.8 million in 1990.[257] The genetic and environmental risk factors for dementia disorders vary by ethnicity.[258][259] For instance, Alzheimer's disease among Hispanic/Latino and African American subjects exhibit lower risks associated with gene changes in the apolipoprotein E gene than do non-Hispanic white subjects.[citation needed]
The annual incidence of dementia diagnosis is nearly 10 million worldwide.[163] Almost half of new dementia cases occur in Asia, followed by Europe (25%), the Americas (18%) and Africa (8%). The incidence of dementia increases exponentially with age, doubling with every 6.3-year increase in age.[255] Dementia affects 5% of the population older than 65 and 20–40% of those older than 85.[260] Rates are slightly higher in women than men at ages 65 and greater.[260] The disease trajectory is varied and the median time from diagnosis to death depends strongly on age at diagnosis, from 6.7 years for people diagnosed aged 60–69 to 1.9 years for people diagnosed at 90 or older.[163]
Dementia impacts not only individuals with dementia, but also their carers and the wider society. Among people aged 60 years and over, dementia is ranked the 9th most burdensome condition according to the 2010 Global Burden of Disease (GBD) estimates. The global costs of dementia was around US$818 billion in 2015, a 35.4% increase from US$604 billion in 2010.[255]
About 3% of people between the ages of 65–74 have dementia, 19% between 75 and 84, and nearly half of those over 85 years of age. As more people are living longer, dementia is becoming more common.[261] For people of a specific age, however, it may be becoming less frequent in the developed world, due to a decrease in modifiable risk factors made possible by greater financial and educational resources. It is one of the most common causes of disability among the elderly but can develop before the age of 65 when it is known as early-onset dementia or presenile dementia.[262][263] Less than 1% of those with Alzheimer's have gene mutations that cause a much earlier development of the disease, around the age of 45, known as early-onset Alzheimer's disease.[264] More than 95% of people with Alzheimer's disease have the sporadic form (late onset, 80–90 years of age).[264] Worldwide the cost of dementia in 2015 was put at US$818 billion. People with dementia are often physically or chemically restrained to a greater degree than necessary, raising issues of human rights.[2][265] Social stigma is commonly perceived by those with the condition, and also by their caregivers.[92]
Until the end of the 19th century, dementia was a much broader clinical concept. It included mental illness and any type of psychosocial incapacity, including reversible conditions.[266] Dementia at this time simply referred to anyone who had lost the ability to reason, and was applied equally to psychosis, "organic" diseases like syphilis that destroy the brain, and to the dementia associated with old age, which was attributed to "hardening of the arteries".
Dementia has been referred to in medical texts since antiquity. One of the earliest known allusions to dementia is attributed to the 7th-century BC Greek philosopher Pythagoras, who divided the human lifespan into six distinct phases: 0–6 (infancy), 7–21 (adolescence), 22–49 (young adulthood), 50–62 (middle age), 63–79 (old age), and 80–death (advanced age). The last two he described as the "senium", a period of mental and physical decay, and that the final phase was when "the scene of mortal existence closes after a great length of time that very fortunately, few of the human species arrive at, where the mind is reduced to the imbecility of the first epoch of infancy".[267] In 550 BC, the Athenian statesman and poet Solon argued that the terms of a man's will might be invalidated if he exhibited loss of judgement due to advanced age. Chinese medical texts made allusions to the condition as well, and the characters for "dementia" translate literally to "foolish old person".[268]
Athenian philosophers Aristotle and Plato discussed the mental decline that can come with old age and predicted that this affects everyone who becomes old and nothing can be done to stop this decline from taking place. Plato specifically talked about how the elderly should not be in positions that require responsibility because, "There is not much acumen of the mind that once carried them in their youth, those characteristics one would call judgement, imagination, power of reasoning, and memory. They see them gradually blunted by deterioration and can hardly fulfill their function."[269]
For comparison, the Roman statesman Cicero held a view much more in line with modern-day medical wisdom that loss of mental function was not inevitable in the elderly and "affected only those old men who were weak-willed". He spoke of how those who remained mentally active and eager to learn new things could stave off dementia. However, Cicero's views on aging, although progressive, were largely ignored in a world that would be dominated for centuries by Aristotle's medical writings. Physicians during the Roman Empire, such as Galen and Celsus, simply repeated the beliefs of Aristotle while adding few new contributions to medical knowledge.
Byzantine physicians sometimes wrote of dementia. It is recorded that at least seven emperors whose lifespans exceeded 70 years displayed signs of cognitive decline. In Constantinople, special hospitals housed those diagnosed with dementia or insanity, but these did not apply to the emperors, who were above the law and whose health conditions could not be publicly acknowledged.
Otherwise, little is recorded about dementia in Western medical texts for nearly 1700 years. One of the few references was the 13th-century friar Roger Bacon, who viewed old age as divine punishment for original sin. Although he repeated existing Aristotelian beliefs that dementia was inevitable, he did make the progressive assertion that the brain was the center of memory and thought rather than the heart.
Poets, playwrights, and other writers made frequent allusions to the loss of mental function in old age. William Shakespeare notably mentions it in plays such as Hamlet and King Lear.
During the 19th century, doctors generally came to believe that elderly dementia was the result of cerebral atherosclerosis, although opinions fluctuated between the idea that it was due to blockage of the major arteries supplying the brain or small strokes within the vessels of the cerebral cortex.
In 1907, Bavarian psychiatrist Alois Alzheimer was the first to identify and describe the characteristics of progressive dementia in the brain of 51-year-old Auguste Deter.[270] Deter had begun to behave uncharacteristically, including accusing her husband of adultery, neglecting household chores, exhibiting difficulties writing and engaging in conversations, heightened insomnia, and loss of directional sense.[271] At one point, Deter was reported to have "dragged a bed sheet outside, wandered around wildly, and cried for hours at midnight."[271] Alzheimer began treating Deter when she entered a Frankfurt mental hospital on November 25, 1901.[271] During her ongoing treatment, Deter and her husband struggled to afford the cost of the medical care, and Alzheimer agreed to continue her treatment in exchange for Deter's medical records and donation of her brain upon death.[271] Deter died on April 8, 1906, after succumbing to sepsis and pneumonia.[271] Alzheimer conducted the brain biopsy using the Bielschowsky stain method, which was a new development at the time, and he observed senile plaques, neurofibrillary tangles, and atherosclerotic alteration.[270] At the time, the consensus among medical doctors had been that senile plaques were generally found in older patients, and the occurrence of neurofibrillary tangles was an entirely new observation at the time.[271] Alzheimer presented his findings at the 37th psychiatry conference of southwestern Germany in Tübingen on April 11, 1906; however, the information was poorly received by his peers.[271] By 1910, Alois Alzheimer's teacher, Emil Kraepelin, published a book in which he coined the term "Alzheimer's disease" in an attempt to acknowledge the importance of Alzheimer's discovery.[270][271]
By the 1960s, the link between Neurodegenerative Diseases and age-related cognitive decline had become more established. By the 1970s, the medical community maintained that vascular dementia was rarer than previously thought and Alzheimer's disease caused the vast majority of old age mental impairments. More recently however, it is believed that dementia is often a mixture of conditions.
In 1976, neurologist Robert Katzmann suggested a link between senile dementia and Alzheimer's disease.[272] Katzmann suggested that much of the senile dementia occurring (by definition) after the age of 65, was pathologically identical with Alzheimer's disease occurring in people under age 65 and therefore should not be treated differently.[273] Katzmann thus suggested that Alzheimer's disease, if taken to occur over age 65, is actually common, not rare, and was the fourth- or 5th-leading cause of death, even though rarely reported on death certificates in 1976.
A helpful finding was that although the incidence of Alzheimer's disease increased with age (from 5–10% of 75-year-olds to as many as 40–50% of 90-year-olds), no threshold was found by which age all persons developed it. This is shown by documented supercentenarians (people living to 110 or more) who experienced no substantial cognitive impairment. Some evidence suggests that dementia is most likely to develop between ages 80 and 84 and individuals who pass that point without being affected have a lower chance of developing it. Women account for a larger percentage of dementia cases than men, although this can be attributed to their longer overall lifespan and greater odds of attaining an age where the condition is likely to occur.[274]
Much like other diseases associated with aging, dementia was comparatively rare before the 20th century, because few people lived past 80. Conversely, syphilitic dementia was widespread in the developed world until it was largely eradicated by the use of penicillin after World War II. With significant increases in life expectancy thereafter, the number of people over 65 started rapidly climbing. While elderly persons constituted an average of 3–5% of the population prior to 1945, by 2010 many countries reached 10–14% and in Germany and Japan, this figure exceeded 20%. Public awareness of Alzheimer's Disease greatly increased in 1994 when former US president Ronald Reagan announced that he had been diagnosed with the condition.
In the 21st century, other types of dementia were differentiated from Alzheimer's disease and vascular dementias (the most common types). This differentiation is on the basis of pathological examination of brain tissues, by symptomatology, and by different patterns of brain metabolic activity in nuclear medical imaging tests such as SPECT and PETscans of the brain. The various forms have differing prognoses and differing epidemiologic risk factors. The main cause for many diseases, including Alzheimer's disease, remains unclear.[275]
Dementia in the elderly was once called senile dementia or senility, and viewed as a normal and somewhat inevitable aspect of aging.[276][277]
By 1913–20 the term dementia praecox was introduced to suggest the development of senile-type dementia at a younger age. Eventually the two terms fused, so that until 1952 physicians used the terms dementia praecox (precocious dementia) and schizophrenia interchangeably. Since then, science has determined that dementia and schizophrenia are two different disorders, though they share some similarities.[278] The term precocious dementia for a mental illness suggested that a type of mental illness like schizophrenia (including paranoia and decreased cognitive capacity) could be expected to arrive normally in all persons with greater age (see paraphrenia). After about 1920, the beginning use of dementia for what is now understood as schizophrenia and senile dementia helped limit the word's meaning to "permanent, irreversible mental deterioration". This began the change to the later use of the term. In recent studies, researchers have seen a connection between those diagnosed with schizophrenia and patients who are diagnosed with dementia, finding a positive correlation between the two diseases.[279]
The view that dementia must always be the result of a particular disease process led for a time to the proposed diagnosis of "senile dementia of the Alzheimer's type" (SDAT) in persons over the age of 65, with "Alzheimer's disease" diagnosed in persons younger than 65 who had the same pathology. Eventually, however, it was agreed that the age limit was artificial, and that Alzheimer's disease was the appropriate term for persons with that particular brain pathology, regardless of age.
After 1952, mental illnesses including schizophrenia were removed from the category of organic brain syndromes, and thus (by definition) removed from possible causes of "dementing illnesses" (dementias). At the same, however, the traditional cause of senile dementia – "hardening of the arteries" – now returned as a set of dementias of vascular cause (small strokes). These were now termed multi-infarct dementias or vascular dementias.
The societal cost of dementia is high, especially for caregivers.[280] According to a UK-based study, almost two out of three carers of people with dementia feel lonely. Most of the carers in the study were family members or friends.[281][282]
As of 2015[update], the annual cost per Alzheimer's patient in the United States was around $19,144.36. The total costs for the nation is estimated to be about $167.74 billion. By 2030, it is predicted the annual socioeconomic cost will total to about $507 billion, and by 2050 that number is expected to reach $1.89 trillion. This steady increase will be seen not just within the United States but globally. Global estimates for the costs of dementia were $957.56 billion in 2015, but by 2050 the estimated global cost is $9.12 trillion.[283]
Many countries consider the care of people living with dementia a national priority and invest in resources and education to better inform health and social service workers, unpaid caregivers, relatives and members of the wider community. Several countries have authored national plans or strategies.[284][285] These plans recognize that people can live reasonably with dementia for years, as long as the right support and timely access to a diagnosis are available. Former British Prime Minister David Cameron described dementia as a "national crisis", affecting 800,000 people in the United Kingdom.[286] In fact, dementia has become the leading cause of death for women in England.[287]
There, as with all mental disorders, people with dementia could potentially be a danger to themselves or others, they can be detained under the Mental Health Act 1983 for assessment, care and treatment. This is a last resort, and is usually avoided by people with family or friends who can ensure care.
Some hospitals in Britain work to provide enriched and friendlier care. To make the hospital wards calmer and less overwhelming to residents, staff replaced the usual nurses' station with a collection of smaller desks, similar to a reception area. The incorporation of bright lighting helps increase positive mood and allow residents to see more easily.[288]
Driving with dementia can lead to injury or death. Doctors should advise appropriate testing on when to quit driving.[289] The United Kingdom DVLA (Driver & Vehicle Licensing Agency) states that people with dementia who specifically have poor short-term memory, disorientation, or lack of insight or judgment are not allowed to drive, and in these instances the DVLA must be informed so that the driving license can be revoked. They acknowledge that in low-severity cases and those with an early diagnosis, drivers may be permitted to continue driving.
Many support networks are available to people with dementia and their families and caregivers. Charitable organizations aim to raise awareness and campaign for the rights of people living with dementia. Support and guidance are available on assessing testamentary capacity in people with dementia.[290]
In 2015, Atlantic Philanthropies announced a $177 million gift aimed at understanding and reducing dementia. The recipient was Global Brain Health Institute, a program co-led by the University of California, San Francisco and Trinity College Dublin. This donation is the largest non-capital grant Atlantic has ever made, and the biggest philanthropic donation in Irish history.[291]
In October 2020, the Caretaker's last music release, Everywhere at the End of Time, was popularized by TikTok users for its depiction of the stages of dementia.[292] Caregivers were in favor of this phenomenon; Leyland Kirby, the creator of the record, echoed this sentiment, explaining it could cause empathy among a younger public.[293]
On 2 November 2020, Scottish billionaire Sir Tom Hunter donated £1 million to dementia charities, after watching a former music teacher with dementia, Paul Harvey, playing one of his own compositions on the piano in a viral video. The donation was announced to be split between the Alzheimer's Society and Music for Dementia.[294]
Creutzfeldt–Jakob disease


Breast cancer is cancer that develops from breast tissue.[7] Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, milk rejection, fluid coming from the nipple, a newly inverted nipple, or a red or scaly patch of skin.[1] In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.[8]
Risk factors for developing breast cancer include obesity, a lack of physical exercise, alcohol consumption, hormone replacement therapy during menopause, ionizing radiation, an early age at first menstruation, having children late in life or not at all, older age, having a prior history of breast cancer, and a family history of breast cancer.[1][2][9] About 5–10% of cases are the result of an inherited genetic predisposition,[1] including BRCA mutations among others.[1] Breast cancer most commonly develops in cells from the lining of milk ducts and the lobules that supply these ducts with milk.[1] Cancers developing from the ducts are known as ductal carcinomas, while those developing from lobules are known as lobular carcinomas.[1] There are more than 18 other sub-types of breast cancer.[2] Some, such as ductal carcinoma in situ, develop from pre-invasive lesions.[2] The diagnosis of breast cancer is confirmed by taking a biopsy of the concerning tissue.[1] Once the diagnosis is made, further tests are done to determine if the cancer has spread beyond the breast and which treatments are most likely to be effective.[1]
The balance of benefits versus harms of breast cancer screening is controversial. A 2013 Cochrane review found that it was unclear if mammographic screening does more harm than good, in that a large proportion of women who test positive turn out not to have the disease.[10] A 2009 review for the US Preventive Services Task Force found evidence of benefit in those 40 to 70 years of age,[11] and the organization recommends screening every two years in women 50 to 74 years of age.[12] The medications tamoxifen or raloxifene may be used in an effort to prevent breast cancer in those who are at high risk of developing it.[2] Surgical removal of both breasts is another preventive measure in some high risk women.[2] In those who have been diagnosed with cancer, a number of treatments may be used, including surgery, radiation therapy, chemotherapy, hormonal therapy, and targeted therapy.[1] Types of surgery vary from breast-conserving surgery to mastectomy.[13][14] Breast reconstruction may take place at the time of surgery or at a later date.[14] In those in whom the cancer has spread to other parts of the body, treatments are mostly aimed at improving quality of life and comfort.[14]
Outcomes for breast cancer vary depending on the cancer type, the extent of disease, and the person's age.[14] The five-year survival rates in England and the United States are between 80 and 90%.[15][4][5] In developing countries, five-year survival rates are lower.[2] Worldwide, breast cancer is the leading type of cancer in women, accounting for 25% of all cases.[16] In 2018, it resulted in 2 million new cases and 627,000 deaths.[17] It is more common in developed countries[2] and is more than 100 times more common in women than in men.[15][18]
Breast cancer most commonly presents as a lump that feels different from the rest of the breast tissue. More than 80% of cases are discovered when a person detects such a lump with the fingertips.[19] The earliest breast cancers, however, are detected by a mammogram.[20][21] Lumps found in lymph nodes located in the armpits[19] may also indicate breast cancer.
Indications of breast cancer other than a lump may include thickening different from the other breast tissue, one breast becoming larger or lower, a nipple changing position or shape or becoming inverted, skin puckering or dimpling, a rash on or around a nipple, discharge from nipple/s, constant pain in part of the breast or armpit and swelling beneath the armpit or around the collarbone.[22] Pain ("mastodynia") is an unreliable tool in determining the presence or absence of breast cancer, but may be indicative of other breast health issues.[19][20][23]
Another symptom complex of breast cancer is Paget's disease of the breast. This syndrome presents as skin changes resembling eczema; such as redness, discoloration or mild flaking of the nipple skin. As Paget's disease of the breast advances, symptoms may include tingling, itching, increased sensitivity, burning, and pain. There may also be discharge from the nipple. Approximately half the women diagnosed with Paget's disease of the breast also have a lump in the breast.[24][25]
Inflammatory breast cancer is a rare (only seen in less than 5% of breast cancer diagnosis) yet aggressive form of breast cancer characterized by the swollen, red areas formed on the top of the breast. The visual effects of inflammatory breast cancer is a result of a blockage of lymph vessels by cancer cells. This type of breast cancer is seen in more commonly diagnosed in younger ages, obese women and African American women. As inflammatory breast cancer does not present as a lump there can sometimes be a delay in diagnosis.[26]
Mammary secretory carcinoma (MSC) is a rare form of the secretory carcinomas that occurs exclusively in the breast.[27] It usually develops in adults but in a significant percentage of cases  also affects children:[28] MSC accounts for 80% of all childhood breast cancers.[29] MSC lesions are typically slow growing, painless, small ductal breast tumours that have invaded the tissue around their ducts of origin, often spread to sentinel lymph nodes and/or axillary lymph nodes, but rarely metastasized to distant tissues.[30] These tumours typically have distinctive microscopic features and tumour cells that carry a balanced genetic translocation in which part of the NTRK3 gene is fused to part of the ETV6 gene[31] to form a fusion gene, ETV6-NTRK3. This fusion gene encodes a chimeric protein termed ETV6-NTRK3. The NTRK3 part of ETV6-NTRK3 protein has up-regulated tyrosine kinase activity that stimulates two signaling pathways, the PI3K/AKT/mTOR and MAPK/ERK pathways, which promote cell proliferation and survival and thereby may contribute to the development of MSC.[28] Conservative surgery, modified radical mastectomy, and radical mastectomy have been the most frequent procedures used to treat adults, while simple mastectomy, local excision with sentinel lymph node biopsy, and complete axillary dissection have been recommended to treat children with MSC.[32] In all cases, long-term, e.g. >20 years, follow-up examinations are recommended.[27][31] The relatively rare cases of MSC that have metastasized to distant tissues have shown little or no responses to chemotherapy and radiotherapy. Three patients with metastatic disease had good partial responses to Entrectinib, a drug that inhibits the tyrosine kinase activity of the ETV6-NTRK3 fusion protein.[33] Because of its slow growth and low rate of metastasis to distant tissues, individuals with MSC have had 20 year survival rates of 93.16%.[27]
In rare cases, what initially appears as a fibroadenoma (hard, movable non-cancerous lump) could in fact be a phyllodes tumour. Phyllodes tumours are formed within the stroma (connective tissue) of the breast and contain glandular as well as stromal tissue. Phyllodes tumours are not staged in the usual sense; they are classified on the basis of their appearance under the microscope as benign, borderline or malignant.[34]
Malignant tumours can result in metastatic tumours – secondary tumours (originating from the primary tumour) that spread beyond the site of origination. The symptoms caused by metastatic breast cancer will depend on the location of metastasis. Common sites of metastasis include bone, liver, lung, and brain.[35] When cancer has reached such an invasive state, it is categorized as a stage 4 cancer, cancers of this state are often fatal.[36] Common symptoms of stage 4 cancer include unexplained weight loss, bone and joint pain, jaundice and neurological symptoms. These symptoms are called non-specific symptoms because they could be manifestations of many other illnesses.[37] Rarely breast cancer can spread to exceedingly uncommon sites such as peripancreatic lymph nodes causing biliary obstruction leading to diagnostic difficulties.[38]
Most symptoms of breast disorders, including most lumps, do not turn out to represent underlying breast cancer. Less than 20% of lumps, for example, are cancerous,[39] and benign breast diseases such as mastitis and fibroadenoma of the breast are more common causes of breast disorder symptoms.[40]
Risk factors can be divided into two categories:
The primary risk factors for breast cancer are being female and older age.[42] Most breast cancers develop in women over 50. Women in their early 50s are twice as more likely to develop breast cancer than women in their early 40s.[43]
Other potential risk factors include genetics,[44] lack of childbearing or lack of breastfeeding,[45] higher levels of certain hormones,[46][47] certain dietary patterns, and obesity. Exposure to artificial light during sleeping might also be risk factor for the development of breast cancer.[48]
Actions to prevent breast cancer include not drinking alcoholic beverages, maintaining a healthy body composition, avoiding smoking and eating healthy food. Combining all of these (leading the healthiest possible lifestyle) would make almost a quarter of breast cancer cases worldwide preventable.[49] The remaining three-quarters of breast cancer cases cannot be prevented through lifestyle changes.[49]
Drinking alcoholic beverages increases the risk of breast cancer, even among very light drinkers (women drinking less than half of one alcoholic drink per day).[50] The risk is highest among heavy drinkers.[52] Globally, about one in 10 cases of breast cancer is caused by women drinking alcoholic beverages.[52] Drinking alcoholic beverages is among the most common modifiable risk factors.[53]
Obesity and diabetes increase the risk of breast cancer. A high body mass index (BMI) causes 7% of breast cancers while diabetes is responsible for 2%.[43][54] At the same time the correlation between obesity and breast cancer is anything but linear. Studies show that those who rapidly gain weight in adulthood are at higher risk than those who have been overweight since childhood. Likewise excess fat in the midsection seems to induce a higher risk than excess weight carried in the lower body.[55] Dietary factors that may increase risk include a high-fat diet[56] and obesity-related high cholesterol levels.[57][58]
Dietary iodine deficiency may also play a role in the development of breast cancer.[59]
Smoking tobacco appears to increase the risk of breast cancer, with the greater the amount smoked and the earlier in life that smoking began, the higher the risk.[60] In those who are long-term smokers, the relative risk is increased 35% to 50%.[60]
A lack of physical activity has been linked to about 10% of cases.[61] Sitting regularly for prolonged periods is associated with higher mortality from breast cancer. The risk is not negated by regular exercise, though it is lowered.[62]
Breast feeding reduces the risk of several types of cancers, including breast cancer.[63][64][65][66] In the 1980s, the abortion–breast cancer hypothesis posited that induced abortion increased the risk of developing breast cancer.[67] This hypothesis was the subject of extensive scientific inquiry, which concluded that neither miscarriages nor abortions are associated with a heightened risk for breast cancer.[68]
Other risk factors include radiation[69] and circadian disruptions related to shift-work[70] and routine late-night eating.[71] A number of chemicals have also been linked, including polychlorinated biphenyls, polycyclic aromatic hydrocarbons, and organic solvents[72] Although the radiation from mammography is a low dose, it is estimated that yearly screening from 40 to 80 years of age will cause approximately 225 cases of fatal breast cancer per million women screened.[73]
In general, hormone replacement therapy (HRT) to treat menopause is associated with only a small increased risk of breast cancer.[74][75][76] The level of risk also depends on the type of HRT, the duration of the treatment and the age of the person. Oestrogen-only HRT, taken by people who had a hysterectomy, comes with an extremely low level of breast cancer risk. The most commonly taken combined HRT (oestrogen and progestogen) is linked to a small risk of breast cancer. This risk is lower for women in their 50s and higher for older women. The risk increases with the duration of HRT. When HRT is taken for a year or less, there is no increased risk of breast cancer. HRT taken for more than 5 years comes with an increased risk but the risk reduces after the therapy is stopped.[75][76]
The use of hormonal birth control does not cause breast cancer for most women;[77] if it has an effect, it is small (on the order of 0.01% per user–year; comparable to the rate of maternal mortality in the United States[78]), temporary, and offset by the users' significantly reduced risk of ovarian and endometrial cancers.[78] Among those with a family history of breast cancer, use of modern oral contraceptives does not appear to affect the risk of breast cancer.[79] It is less certain whether hormonal contraceptives could increase the already high rates of breast cancer in women with mutations in the breast cancer susceptibility genes BRCA1 or BRCA2.[80]
Genetics is believed to be the primary cause of 5–10% of all cases.[81] Women whose mother was diagnosed before 50 have an increased risk of 1.7 and those whose mother was diagnosed at age 50 or after has an increased risk of 1.4.[82] In those with zero, one or two affected relatives, the risk of breast cancer before the age of 80 is 7.8%, 13.3%, and 21.1% with a subsequent mortality from the disease of 2.3%, 4.2%, and 7.6% respectively.[83] In those with a first degree relative with the disease the risk of breast cancer between the age of 40 and 50 is double that of the general population.[84]
In less than 5% of cases, genetics plays a more significant role by causing a hereditary breast–ovarian cancer syndrome.[85] This includes those who carry the BRCA1 and BRCA2 gene mutation.[85] These mutations account for up to 90% of the total genetic influence with a risk of breast cancer of 60–80% in those affected.[81] Other significant mutations include p53 (Li–Fraumeni syndrome), PTEN (Cowden syndrome), and STK11 (Peutz–Jeghers syndrome), CHEK2, ATM, BRIP1, and PALB2.[81] In 2012, researchers said that there are four genetically distinct types of the breast cancer and that in each type, hallmark genetic changes lead to many cancers.[86]
Other genetic predispositions include the density of the breast tissue and hormonal levels. Women with dense breast tissue are more likely to get tumours and are less likely to be diagnosed with breast cancer – because the dense tissue makes tumours less visible on mammograms. Furthermore, women with naturally high estrogen and progesterone levels are also at higher risk for tumour development.[87][88]
Breast changes like atypical ductal hyperplasia[89] and lobular carcinoma in situ,[90][91] found in benign breast conditions such as fibrocystic breast changes, are correlated with an increased breast cancer risk.
Diabetes mellitus might also increase the risk of breast cancer.[92] Autoimmune diseases such as lupus erythematosus seem also to increase the risk for the acquisition of breast cancer.[93]
The major causes of sporadic breast cancer are associated with hormone levels. Breast cancer is promoted by estrogen. This hormone activates the development of breast throughout puberty, menstrual cycles and pregnancy. The imbalance between estrogen and progesterone during the menstrual phases causes cell proliferation. Moreover, oxidative metabolites of estrogen can increase DNA damage and mutations. Repeated cycling and the impairment of repair process can transform a normal cell into pre-malignant and eventually malignant cell through mutation. During the premalignant stage, high proliferation of stromal cells can be activated by estrogen to support the development of breast cancer. During the ligand binding activation, the ER can regulate gene expression by interacting with estrogen response elements within the promotor of specific genes. The expression and activation of ER due to lack of estrogen can be stimulated by extracellular signals.[94] Interestingly, the ER directly binding with the several proteins, including growth factor receptors, can promote the expression of genes related to cell growth and survival.[95]
Raised prolactin levels in the blood are associated with increased risk of breast cancer.[96] A meta-analysis of observational research with over two million individuals has suggested a moderate association of antipsychotic use with breast cancer, possibly mediated by prolactin-inducing properties of specific agents.[97]
Breast cancer, like other cancers, occurs because of an interaction between an environmental (external) factor and a genetically susceptible host. Normal cells divide as many times as needed and stop. They attach to other cells and stay in place in tissues. Cells become cancerous when they lose their ability to stop dividing, to attach to other cells, to stay where they belong, and to die at the proper time.
Normal cells will self-destruct (programmed cell death) when they are no longer needed. Until then, cells are protected from programmed death by several protein clusters and pathways. One of the protective pathways is the PI3K/AKT pathway; another is the RAS/MEK/ERK pathway. Sometimes the genes along these protective pathways are mutated in a way that turns them permanently "on", rendering the cell incapable of self-destructing when it is no longer needed. This is one of the steps that causes cancer in combination with other mutations. Normally, the PTEN protein turns off the PI3K/AKT pathway when the cell is ready for programmed cell death. In some breast cancers, the gene for the PTEN protein is mutated, so the PI3K/AKT pathway is stuck in the "on" position, and the cancer cell does not self-destruct.[98]
Mutations that can lead to breast cancer have been experimentally linked to estrogen exposure.[99] Additionally, G-protein coupled estrogen receptors have been associated with various cancers of the female reproductive system including breast cancer.[100]
Abnormal growth factor signaling in the interaction between stromal cells and epithelial cells can facilitate malignant cell growth.[101][102] In breast adipose tissue, overexpression of leptin leads to increased cell proliferation and cancer.[103]
In the United States, 10 to 20 percent of women with breast cancer or ovarian cancer have a first- or second-degree relative with one of these diseases. Men with breast cancer have an even higher likelihood. The familial tendency to develop these cancers is called hereditary breast–ovarian cancer syndrome. The best known of these, the BRCA mutations, confer a lifetime risk of breast cancer of between 60 and 85 percent and a lifetime risk of ovarian cancer of between 15 and 40 percent. Some mutations associated with cancer, such as p53, BRCA1 and BRCA2, occur in mechanisms to correct errors in DNA. These mutations are either inherited or acquired after birth. Presumably, they allow further mutations, which allow uncontrolled division, lack of attachment, and metastasis to distant organs.[69][104] However, there is strong evidence of residual risk variation that goes well beyond hereditary BRCA gene mutations between carrier families. This is caused by unobserved risk factors.[105] This implicates environmental and other causes as triggers for breast cancers. The inherited mutation in BRCA1 or BRCA2 genes can interfere with repair of DNA cross links and DNA double strand breaks (known functions of the encoded protein).[106] These carcinogens cause DNA damage such as DNA cross links and double strand breaks that often require repairs by pathways containing BRCA1 and BRCA2.[107][108] However, mutations in BRCA genes account for only 2 to 3 percent of all breast cancers.[109] Levin et al. say that cancer may not be inevitable for all carriers of BRCA1 and BRCA2 mutations.[110] About half of hereditary breast–ovarian cancer syndromes involve unknown genes. Furthermore, certain latent viruses, may decrease the expression of the BRCA1 gene and increase the risk of breast tumours.[111]
GATA-3 directly controls the expression of estrogen receptor (ER) and other genes associated with epithelial differentiation, and the loss of GATA-3 leads to loss of differentiation and poor prognosis due to cancer cell invasion and metastasis.[112]
Most types of breast cancer are easy to diagnose by microscopic analysis of a sample – or biopsy – of the affected area of the breast. Also, there are types of breast cancer that require specialized lab exams.
The two most commonly used screening methods, physical examination of the breasts by a healthcare provider and mammography, can offer an approximate likelihood that a lump is cancer, and may also detect some other lesions, such as a simple cyst.[113] When these examinations are inconclusive, a healthcare provider can remove a sample of the fluid in the lump for microscopic analysis (a procedure known as fine needle aspiration, or fine needle aspiration and cytology, FNAC) to help establish the diagnosis. A needle aspiration can be performed in a healthcare provider's office or clinic. A local anesthetic may be used to numb the breast tissue to prevent pain during the procedure, but may not be necessary if the lump is not beneath the skin. A finding of clear fluid makes the lump highly unlikely to be cancerous, but bloody fluid may be sent off for inspection under a microscope for cancerous cells. Together, physical examination of the breasts, mammography, and FNAC can be used to diagnose breast cancer with a good degree of accuracy.
Other options for biopsy include a core biopsy or vacuum-assisted breast biopsy,[114] which are procedures in which a section of the breast lump is removed; or an excisional biopsy, in which the entire lump is removed. Very often the results of physical examination by a healthcare provider, mammography, and additional tests that may be performed in special circumstances (such as imaging by ultrasound or MRI) are sufficient to warrant excisional biopsy as the definitive diagnostic and primary treatment method.[115][non-primary source needed]
MRI showing breast cancer
Excised human breast tissue, showing an irregular, dense, white stellate area of cancer 2 cm in diameter, within yellow fatty tissue
High-grade invasive ductal carcinoma, with minimal tubule formation, marked pleomorphism, and prominent mitoses, 40x field
Micrograph showing a lymph node invaded by ductal breast carcinoma, with an extension of the tumor beyond the lymph node
Neuropilin-2 expression in normal breast and breast carcinoma tissue
F-18 FDG PET/CT: A breast cancer metastasis to the right scapula
Needle breast biopsy
Elastography shows stiff cancer tissue on ultrasound imaging.
Ultrasound image shows irregularly shaped mass of breast cancer.
Infiltrating (invasive) breast carcinoma
Mammograms showing a normal breast (left) and a breast with cancer (right)
Breast cancers are classified by several grading systems. Each of these influences the prognosis and can affect treatment response. Description of a breast cancer optimally includes all of these factors.
Stage T1 breast cancer
Stage T2 breast cancer
Stage T3 breast cancer
Metastatic or stage 4 breast cancer
Stage 1A breast cancer
Stage 1B breast cancer
Stage 2A breast cancer
Stage 2A breast cancer
Stage 2B breast cancer
Stage 2B breast cancer
Stage 2B breast cancer
Stage 3A breast cancer
Stage 3A breast cancer
Stage 3A breast cancer
Stage 3B breast cancer
Stage 3B breast cancer
Stage 4 breast cancer
Breast cancer screening refers to testing otherwise-healthy women for breast cancer in an attempt to achieve an earlier diagnosis under the assumption that early detection will improve outcomes. A number of screening tests have been employed including clinical and self breast exams, mammography, genetic screening, ultrasound, and magnetic resonance imaging.
A clinical or self breast exam involves feeling the breast for lumps or other abnormalities. Clinical breast exams are performed by health care providers, while self-breast exams are performed by the person themselves.[122] Evidence does not support the effectiveness of either type of breast exam, as by the time a lump is large enough to be found it is likely to have been growing for several years and thus soon be large enough to be found without an exam.[123][124] Mammographic screening for breast cancer uses X-rays to examine the breast for any uncharacteristic masses or lumps. During a screening, the breast is compressed and a technician takes photos from multiple angles. A general mammogram takes photos of the entire breast, while a diagnostic mammogram focuses on a specific lump or area of concern.[125]
A number of national bodies recommend breast cancer screening. For the average woman, the U.S. Preventive Services Task Force and American College of Physicians recommends mammography every two years in women between the ages of 50 and 74,[12][126] the Council of Europe recommends mammography between 50 and 69 with most programs using a 2-year frequency,[127] while the European Commission recommends mammography from 45 to 75 every 2 to 3 years,[128] and in Canada screening is recommended between the ages of 50 and 74 at a frequency of 2 to 3 years.[129] The American Cancer Society also endorses that women ages 40 and older receive mammograms annually.[130] These task force reports point out that in addition to unnecessary surgery and anxiety, the risks of more frequent mammograms include a small but significant increase in breast cancer induced by radiation.[131]
The Cochrane collaboration (2013) states that the best quality evidence neither demonstrates a reduction in cancer specific, nor a reduction in all cause mortality from screening mammography.[10] When less rigorous trials are added to the analysis there is a reduction in mortality due to breast cancer of 0.05% (a decrease of 1 in 2000 deaths from breast cancer over 10 years or a relative decrease of 15% from breast cancer).[10] Screening over 10 years results in a 30% increase in rates of over-diagnosis and over-treatment (3 to 14 per 1000) and more than half will have at least one falsely positive test.[10][132] This has resulted in the view that it is not clear whether mammography screening does more good or harm.[10] Cochrane states that, due to recent improvements in breast cancer treatment, and the risks of false positives from breast cancer screening leading to unnecessary treatment, "it therefore no longer seems beneficial to attend for breast cancer screening" at any age.[133] Whether MRI as a screening method has greater harms or benefits when compared to standard mammography is not known.[134][135]
Women can reduce their risk of breast cancer by maintaining a healthy weight, reducing alcohol use, increasing physical activity, and breast-feeding.[136] These modifications might prevent 38% of breast cancers in the US, 42% in the UK, 28% in Brazil, and 20% in China.[136] The benefits with moderate exercise such as brisk walking are seen at all age groups including postmenopausal women.[136][137] High levels of physical activity reduce the risk of breast cancer by about 14%.[138] Strategies that encourage regular physical activity and reduce obesity could also have other benefits, such as reduced risks of cardiovascular disease and diabetes.[41] A study that included data from 130,957 women of European ancestry found "strong evidence that greater levels of physical activity and less sedentary time are likely to reduce breast cancer risk, with results generally consistent across breast cancer subtypes".[139]
The American Cancer Society and the American Society of Clinical Oncology advised in 2016 that people should eat a diet high in vegetables, fruits, whole grains, and legumes.[140] Eating foods rich in soluble fiber contributes to reducing breast cancer risk.[141][142] High intake of citrus fruit has been associated with a 10% reduction in the risk of breast cancer.[143] Marine omega-3 polyunsaturated fatty acids appear to reduce the risk.[144] High consumption of soy-based foods may reduce risk.[145]
Removal of both breasts before any cancer has been diagnosed or any suspicious lump or other lesion has appeared (a procedure known as "prophylactic bilateral mastectomy" or "risk reducing mastectomy") may be considered in women with BRCA1 and BRCA2 mutations, which are associated with a substantially heightened risk for an eventual diagnosis of breast cancer.[146][147] Evidence is not strong enough to support this procedure in anyone but women at the highest risk.[148] BRCA testing is recommended in those with a high family risk after genetic counseling. It is not recommended routinely.[149] This is because there are many forms of changes in BRCA genes, ranging from harmless polymorphisms to obviously dangerous frameshift mutations.[149] The effect of most of the identifiable changes in the genes is uncertain. Testing in an average-risk person is particularly likely to return one of these indeterminate, useless results. Removing the second breast in a person who has breast cancer (contralateral risk‐reducing mastectomy or CRRM) may reduce the risk of cancer in the second breast, however, it is unclear if removing the second breast in those who have breast cancer improves survival.[148] An increasing number women who test positive for faulty BRCA1 or BRCA2 genes choose to have risk-reducing surgery. At the same time the average waiting time for undergoing the procedure is two-years which is much longer than recommended.[150][151]
The selective estrogen receptor modulators reduce the risk of breast cancer but increase the risk of thromboembolism and endometrial cancer.[152] There is no overall change in the risk of death.[152][153] They are thus not recommended for the prevention of breast cancer in women at average risk but it is recommended they be offered for those at high risk and over the age of 35.[154] The benefit of breast cancer reduction continues for at least five years after stopping a course of treatment with these medications.[155] Aromatase inhibitors (such as exemestane and anastrozole) may be more effective than selective estrogen receptor modulators (such as tamoxifen) at reducing breast cancer risk and they are not associated with an increased risk of endometrial cancer and thromboembolism.[156]
The management of breast cancer depends on various factors, including the stage of the cancer and the person's age. Treatments are more aggressive when the cancer is more advanced or there is a higher risk of recurrence of the cancer following treatment.
Breast cancer is usually treated with surgery, which may be followed by chemotherapy or radiation therapy, or both. A multidisciplinary approach is preferable.[157] Hormone receptor-positive cancers are often treated with hormone-blocking therapy over courses of several years. Monoclonal antibodies, or other immune-modulating treatments, may be administered in certain cases of metastatic and other advanced stages of breast cancer, although this range of treatment is still being studied.[158]
Surgery involves the physical removal of the tumour, typically along with some of the surrounding tissue. One or more lymph nodes may be biopsied during the surgery; increasingly the lymph node sampling is performed by a sentinel lymph node biopsy.
Standard surgeries include:
Once the tumour has been removed, if the person desires, breast reconstruction surgery, a type of plastic surgery, may then be performed to improve the aesthetic appearance of the treated site.
Alternatively, women use breast prostheses to simulate a breast under clothing, or choose a flat chest. Nipple prosthesis can be used at any time following the mastectomy.
Medications used after and in addition to surgery are called adjuvant therapy. Chemotherapy or other types of therapy prior to surgery are called neoadjuvant therapy. Aspirin may reduce mortality from breast cancer when used with other treatments.[159][160]
There are currently three main groups of medications used for adjuvant breast cancer treatment: hormone-blocking agents, chemotherapy, and monoclonal antibodies.
Some breast cancers require estrogen to continue growing. They can be identified by the presence of estrogen receptors (ER+) and progesterone receptors (PR+) on their surface (sometimes referred to together as hormone receptors). These ER+ cancers can be treated with drugs that either block the receptors, e.g. tamoxifen, or alternatively block the production of estrogen with an aromatase inhibitor, e.g. anastrozole[161] or letrozole. The use of tamoxifen is recommended for 10 years.[162] Tamoxifen increases the risk of postmenopausal bleeding, endometrial polyps, hyperplasia, and endometrial cancer; using tamoxifen with an intrauterine system releasing levonorgestrel might increase vaginal bleeding after one to two years, but reduces somewhat endometrial polyps and hyperplasia, but not necessarily endometrial cancer.[163] Letrozole is recommended for five years.
Aromatase inhibitors are only suitable for women after menopause; however, in this group, they appear better than tamoxifen.[164] This is because the active aromatase in postmenopausal women is different from the prevalent form in premenopausal women, and therefore these agents are ineffective in inhibiting the predominant aromatase of premenopausal women.[165] Aromatase inhibitors should not be given to premenopausal women with intact ovarian function (unless they are also on treatment to stop their ovaries from working).[166] CDK inhibitors can be used in combination with endocrine or aromatase therapy.[167]
Chemotherapy is predominantly used for cases of breast cancer in stages 2–4, and is particularly beneficial in estrogen receptor-negative (ER-) disease. The chemotherapy medications are administered in combinations, usually for periods of 3–6 months. One of the most common regimens, known as "AC", combines cyclophosphamide with doxorubicin. Sometimes a taxane drug, such as docetaxel, is added, and the regime is then known as "CAT". Another common treatment is cyclophosphamide, methotrexate, and fluorouracil (or "CMF"). Most chemotherapy medications work by destroying fast-growing and/or fast-replicating cancer cells, either by causing DNA damage upon replication or by other mechanisms. However, the medications also damage fast-growing normal cells, which may cause serious side effects. Damage to the heart muscle is the most dangerous complication of doxorubicin, for example.[citation needed]
Trastuzumab, a monoclonal antibody to HER2, has improved the five-year disease free survival of stage 1–3 HER2-positive breast cancers to about 87% (overall survival 95%).[168] Between 25% and 30% of breast cancers overexpress the HER2 gene or its protein product,[169] and overexpression of HER2 in breast cancer is associated with increased disease recurrence and worse prognosis. Trastuzumab, however, is very expensive, and its use may cause serious side effects (approximately 2% of people who receive it develop significant heart damage).[170] Another antibody pertuzumab prevents HER2 dimerization and is recommended together with trastuzumab and chemotherapy in severe disease.[171][172]
Elacestrant (Orserdu) was approved for medical use in the United States in January 2023.[173][174]
Capivasertib (Truqap) was approved for medical use in the United States in November 2023.[175][176][177]
Radiotherapy is given after surgery to the region of the tumour bed and regional lymph nodes, to destroy microscopic tumour cells that may have escaped surgery. When given intraoperatively as targeted intraoperative radiotherapy, it may also have a beneficial effect on tumour microenvironment.[178][179] Radiation therapy can be delivered as external beam radiotherapy or as brachytherapy (internal radiotherapy). Conventionally radiotherapy is given after the operation for breast cancer. Radiation can also be given at the time of operation on the breast cancer. Radiation can reduce the risk of recurrence by 50–66% (1/2 – 2/3 reduction of risk) when delivered in the correct dose[180] and is considered essential when breast cancer is treated by removing only the lump (Lumpectomy or Wide local excision). In early breast cancer, partial breast irradiation does not give the same cancer control in the breast as treating the whole breast and may cause worse side effects.[181]
Care after primary breast cancer treatment, otherwise called 'follow-up care', can be intensive involving regular laboratory tests in asymptomatic people to try to achieve earlier detection of possible metastases. A review has found that follow-up programs involving regular physical examinations and yearly mammography alone are as effective as more intensive programs consisting of laboratory tests in terms of early detection of recurrence, overall survival and quality of life.[182]
Multidisciplinary rehabilitation programmes, often including exercise, education and psychological help, may produce short-term improvements in functional ability, psychosocial adjustment and social participation in people with breast cancer.[183]
Upper limb problems such as shoulder and arm pain, weakness and restricted movement are a common side effect after radiotherapy or breast cancer surgery.[184] According to research in the UK, an exercise programme started 7–10 days after surgery can reduce upper limb problems.[185][186]
The stage of the breast cancer is the most important component of traditional classification methods of breast cancer, because it has a greater effect on the prognosis than the other considerations. Staging takes into consideration size, local involvement, lymph node status and whether metastatic disease is present. The higher the stage at diagnosis, the poorer the prognosis. The stage is raised by the invasiveness of disease to lymph nodes, chest wall, skin or beyond, and the aggressiveness of the cancer cells. The stage is lowered by the presence of cancer-free zones and close-to-normal cell behaviour (grading). Size is not a factor in staging unless the cancer is invasive. For example, ductal carcinoma in situ (DCIS) involving the entire breast will still be stage zero and consequently an excellent prognosis.
The breast cancer grade is assessed by comparison of the breast cancer cells to normal breast cells. The closer to normal the cancer cells are, the slower their growth and the better the prognosis. If cells are not well differentiated, they will appear immature, will divide more rapidly, and will tend to spread. Well differentiated is given a grade of 1, moderate is grade 2, while poor or undifferentiated is given a higher grade of 3 or 4 (depending upon the scale used). The most widely used grading system is the Nottingham scheme.[189]
Younger women with an age of less than 40 years or women over 80 years tend to have a poorer prognosis than post-menopausal women due to several factors. Their breasts may change with their menstrual cycles, they may be nursing infants, and they may be unaware of changes in their breasts. Therefore, younger women are usually at a more advanced stage when diagnosed. There may also be biological factors contributing to a higher risk of disease recurrence for younger women with breast cancer.[190]
Not all people with breast cancer experience their illness in the same manner. Factors such as age can have a significant impact on the way a person copes with a breast cancer diagnosis. Premenopausal women with estrogen-receptor positive breast cancer must confront the issues of early menopause induced by many of the chemotherapy regimens used to treat their breast cancer, especially those that use hormones to counteract ovarian function.[191]
In women with non-metastatic breast cancer, psychological interventions such as cognitive behavioral therapy can have positive effects on outcomes such as anxiety, depression and mood disturbance, and can also improve the quality of life.[192] Physical activity interventions may also have beneficial effects on health related quality of life, anxiety, fitness and physical activity in women with breast cancer following adjuvant therapy.[193]
With nearly 3 million BC survivors in the USA,[194] the 5-year survival rate for patients has increased to over 90% thanks to advancements in breast cancer treatment and earlier detection.[195]  Cardiovascular diseases (CVD) are becoming more widely acknowledged as a significant cause of morbidity and mortality as breast cancer patients live longer.[196]
The recent meta-analysis compared the risk of developing cardiovascular disease in breast cancer patients to the risk in a general matched cancer-free population, and also estimated the incidence of cardiovascular events in BC patients.[197] After analysing data from 26 studies (836,301 patients), the authors found that breast cancer survivors demonstrated a higher risk for cardiovascular death within five years of cancer diagnosis (HR = 1.09; 95% CI: 1.07, 1.11), HF within ten years (HR = 1.21; 95% CI: 1.1, 1.33), and AF within three years (HR = 1.13; 95% CI: 1.05, 1.21).[197]
The epidemiological data from 2,111,882 breast cancer patients revealed that the pooled incidence rates for cardiovascular death was 1.73 (95% CI 1.18, 2.53), 4.44 (95% CI 3.33, 5.92) for heart failure, 4.29 (95% CI 3.09, 5.94) for coronary artery disease, 1.98 (95% CI 1.24, 3.16) for myocardial infarction, 4.33 (95% CI 2.97, 6.30) for stroke of any type, and 2.64 (95% CI 2.97, 6.30) for ischemic stroke per 1000 person-years of follow-up.[197] The study clearly highlighted the crucial need for clinicians to thoroughly examine the cardiovascular risk factor profile of breast cancer survivors and monitor their cardiovascular health.[197]
Breast cancer is the most-common invasive cancer in women,[199] accounting for 30% of cancer cases in women.[200] Along with lung cancer, breast cancer is the most commonly diagnosed cancer, with 2.09 million cases each in 2018.[201] Breast cancer affects 1 in 7 (14%) of women worldwide.[202] (The most common form of cancer is non-invasive non-melanoma skin cancer; non-invasive cancers are generally easily cured, cause very few deaths, and are routinely excluded from cancer statistics.) In 2008, breast cancer caused 458,503 deaths worldwide (13.7% of cancer deaths in women and 6.0% of all cancer deaths for men and women together).[203] Lung cancer, the second most-common cause of cancer-related deaths in women, caused 12.8% of cancer deaths in women (18.2% of all cancer deaths for men and women together).[203]
The incidence of breast cancer varies greatly around the world: it is lowest in less-developed countries and greatest in the more-developed countries. In the twelve world regions, the annual age-standardized incidence rates per 100,000 women are as follows: 18 in Eastern Asia, 22 in South Central Asia and sub-Saharan Africa, 26 in South-Eastern Asia, 26, 28 in North Africa and Western Asia, 42 in South and Central America, 42, 49 in Eastern Europe, 56 in Southern Europe, 73 in Northern Europe, 74 in Oceania, 78 in Western Europe, and 90 in North America.[204] Metastatic breast cancer affects between 19% (United States) and 50% (parts of Africa) of women with breast cancer.[205]
The number of cases worldwide has significantly increased since the 1970s, a phenomenon partly attributed to the modern lifestyles.[206][207] Breast cancer is strongly related to age with only 5% of all breast cancers occurring in women under 40 years old.[208] There were more than 41,000 newly diagnosed cases of breast cancer registered in England in 2011, around 80% of these cases were in women age 50 or older.[209] Based on U.S. statistics in 2015 there were 2.8 million women affected by breast cancer.[199] In the United States, the age-adjusted incidence of breast cancer per 100,000 women rose from around 102 cases per year in the 1970s to around 141 in the late-1990s, and has since fallen, holding steady around 125 since 2003. However, age-adjusted deaths from breast cancer per 100,000 women only rose slightly from 31.4 in 1975 to 33.2 in 1989 and have since declined steadily to 20.5 in 2014.[210]
Multiple primary tumours can arise in different sites (as opposed to a single tumour spreading). These tumours can occur in both breasts (bilateral tumours), in different quadrants of a single breast (multi-centric cancer), or separate tumours within a single breast quadrant (multi-focal cancer).[211][212]
Incidence of multi-centric and multi-focal breast cancers (MMBC) is increasing, partly due to improving mammography technology.[213] Incidence of MMBC is reported between 9 and 75% in high income countries, depending on criteria used.[213] For instance, China reported that only 2% of patients are defined as MMBC.[213] The reason for this difference is due to lack of uniformity in diagnosis.[213] Therefore, standardised method and criteria should be made in order to define the incidence of MMBC correctly.[213]
Mutations in tumour suppressor genes such as BRCA1 and BRCA2, the PI3K/AKT/mTOR pathway and PTEN can be related to formation of multiple primary breast cancers.[214] Diagnosis occurs via the same modalities as other breast cancers.
Mastectomy is the standard surgical treatment for multi centric breast cancer patients.[215]  Double lumpectomies, also labelled as breast conservative therapy (BCT), is an alternative and preferred surgical treatment to mastectomy for early stage multi centric breast cancer patients.[216] The procedure of double lumpectomies involves the surgical ablation of the cancerous tumour foci and the surrounding breast tissues in different quadrant of the same breast.[216] The benefits of double lumpectomies are the avoidance of breast reconstruction surgery and minimal breast scarring. However, it is not preferred for patients with more than two tumours within the same breast due to difficulty in removing all cancer cells.[217] Patients with multiple primary breast tumours may receive treatments such as chemotherapy, radiotherapy and breast reconstruction surgery[217][218] for the same indications as other breast cancer patients.
Because of its visibility, breast cancer was the form of cancer most often described in ancient documents.[219]: 9–13  Because autopsies were rare, cancers of the internal organs were essentially invisible to ancient medicine. Breast cancer, however, could be felt through the skin, and in its advanced state often developed into fungating lesions: the tumour would become necrotic (die from the inside, causing the tumour to appear to break up) and ulcerate through the skin, weeping fetid, dark fluid.[219]: 9–13 
The oldest discovered evidence of breast cancer is from Egypt and dates back 4200 years, to the Sixth Dynasty.[220] The study of a woman's remains from the necropolis of Qubbet el-Hawa showed the typical destructive damage due to metastatic spread.[220] The Edwin Smith Papyrus describes eight cases of tumours or ulcers of the breast that were treated by cauterization. The writing says about the disease, "There is no treatment."[221] For centuries, physicians described similar cases in their practices, with the same conclusion. Ancient medicine, from the time of the Greeks through the 17th century, was based on humoralism, and thus believed that breast cancer was generally caused by imbalances in the fundamental fluids that controlled the body, especially an excess of black bile.[219]: 32  Alternatively it was seen as divine punishment.[222]
Mastectomy for breast cancer was performed at least as early as AD 548, when it was proposed by the court physician Aetios of Amida to Theodora.[219]: 9–13  It was not until doctors achieved greater understanding of the circulatory system in the 17th century that they could link breast cancer's spread to the lymph nodes in the armpit. In the early 18th century the French surgeon Jean Louis Petit performed total mastectomies that included removing the axillary lymph nodes, as he recognized that this reduced recurrence.[223] Petit's work built on the methods of the surgeon Bernard Peyrilhe, who in the 17th century additionally removed the pectoral muscle underlying the breast, as he judged that this greatly improved the prognosis.[224] But poor results and the considerable risk to the patient meant that physicians did not share the opinion of surgeons such as Nicolaes Tulp, who in the 17th century proclaimed "the sole remedy is a timely operation". The eminent surgeon Richard Wiseman documented in the mid 17th century that following 12 mastectomies, two patients died during the operation, eight patients died shortly after the operation from progressive cancer and only two of the 12 patients were cured.[225]: 6  Physicians were conservative in the treatment they prescribed in the early stages of breast cancer. Patients were treated with a mixture of detox purges, blood letting and traditional remedies that were supposed to lower acidity, such as the alkaline arsenic.[226]: 24 
When in 1664 Anne of Austria was diagnosed with breast cancer, the initial treatment involved compresses saturated with hemlock juice. When the lumps increased the King's physician commenced a treatment with arsenic ointments.[226]: 25  The royal patient died 1666 in atrocious pain.[226]: 26  Each failing treatment for breast cancer led to the search for new treatments, spurring a market in remedies that were advertised and sold by quacks, herbalists, chemists and apothecaries.[227] The lack of anesthesia and antiseptics made mastectomy a painful and dangerous ordeal.[225] In the 18th century, a wide variety of anatomical discoveries were accompanied by new theories about the cause and growth of breast cancer. The investigative surgeon John Hunter claimed that neural fluid generated breast cancer. Other surgeons proposed that milk within the mammary ducts led to cancerous growths. Theories about trauma to the breast as cause for malignant changes in breast tissue were advanced. The discovery of breast lumps and swellings fueled controversies about hard tumours and whether lumps were benign stages of cancer. Medical opinion about necessary immediate treatment varied.[225]: 5  The surgeon Benjamin Bell advocated removal of the entire breast, even when only a portion was affected.[228]
Breast cancer was uncommon until the 19th century, when improvements in sanitation and control of deadly infectious diseases resulted in dramatic increases in lifespan. Previously, most women had died too young to have developed breast cancer.[229] In 1878, an article in Scientific American described historical treatment by pressure intended to induce local ischemia in cases when surgical removal were not possible.[230] William Stewart Halsted started performing radical mastectomies in 1882, helped greatly by advances in general surgical technology, such as aseptic technique and anesthesia. The Halsted radical mastectomy often involved removing both breasts, associated lymph nodes, and the underlying chest muscles. This often led to long-term pain and disability, but was seen as necessary to prevent the cancer from recurring.[219]: 102–106  Before the advent of the Halsted radical mastectomy, 20-year survival rates were only 10%; Halsted's surgery raised that rate to 50%.[219]: 1 
Breast cancer staging systems were developed in the 1920s and 1930s to determining the extent to which a cancer has developed by growing and spreading.[219]: 102–106  The first case-controlled study on breast cancer epidemiology was done by Janet Lane-Claypon, who published a comparative study in 1926 of 500 breast cancer cases and 500 controls of the same background and lifestyle for the British Ministry of Health.[231] Radical mastectomies remained the standard of care in the USA until the 1970s, but in Europe, breast-sparing procedures, often followed by radiation therapy, were generally adopted in the 1950s.[219]: 102–106  In 1955 George Crile Jr. published Cancer and Common Sense arguing that cancer patients needed to understand available treatment options. Crile became a close friend of the environmentalist Rachel Carson, who had undergone a Halsted radical mastectomy in 1960 to treat her malign breast cancer.[232]: 39–40  The US oncologist Jerome Urban promoted superradical mastectomies, taking even more tissue, until 1963, when the ten-year survival rates proved equal to the less-damaging radical mastectomy.[219]: 102–106  Carson died in 1964 and Crile went on to published a wide variety of articles, both in the popular press and in medical journals, challenging the widespread used of the Halsted radical mastectomy. In 1973 Crile published What Women Should Know About the Breast Cancer Controversy. When in 1974 Betty Ford was diagnosed with breast cancer, the options for treating breast cancer were openly discussed in the press.[232]: 58  During the 1970s, a new understanding of metastasis led to perceiving cancer as a systemic illness as well as a localized one, and more sparing procedures were developed that proved equally effective.[233]
In the 1980s and 1990s, thousands of women who had successfully completed standard treatment then demanded and received high-dose bone marrow transplants, thinking this would lead to better long-term survival. However, it proved completely ineffective, and 15–20% of women died because of the brutal treatment.[234]: 200–203  The 1995 reports from the Nurses' Health Study and the 2002 conclusions of the Women's Health Initiative trial conclusively proved that hormone replacement therapy significantly increased the incidence of breast cancer.[234]
Before the 20th century, breast cancer was feared and discussed in hushed tones, as if it were shameful. As little could be safely done with primitive surgical techniques, women tended to suffer silently rather than seeking care.[citation needed] When surgery advanced, and long-term survival rates improved, women began raising awareness of the disease and the possibility of successful treatment. The "Women's Field Army", run by the American Society for the Control of Cancer (later the American Cancer Society) during the 1930s and 1940s was one of the first organized campaigns. In 1952, the first peer-to-peer support group, called "Reach to Recovery", began providing post-mastectomy, in-hospital visits from women who had survived breast cancer.[234]: 37–38 
The breast cancer movement of the 1980s and 1990s developed out of the larger feminist movements and women's health movement of the 20th century.[234]: 4  This series of political and educational campaigns, partly inspired by the politically and socially effective AIDS awareness campaigns, resulted in the widespread acceptance of second opinions before surgery, less invasive surgical procedures, support groups, and other advances in care.[235]
A pink ribbon is the most prominent symbol of breast cancer awareness. Pink ribbons, which can be made inexpensively, are sometimes sold as fundraisers, much like poppies on Remembrance Day. They may be worn to honor those who have been diagnosed with breast cancer, or to identify products that the manufacturer would like to sell to consumers that are interested in breast cancer.[234]: 27–72  In the 1990s breast cancer awareness campaigns were launched by US based corporations. As part of these cause related marketing campaigns corporations donated to a variety of breast cancer initiatives for every pink ribbon product that was purchased.[236]>: 132–133  The Wall Street Journal noted "that the strong emotions provoked by breast cancer translate to a company's bottom line". While many US corporations donated to existing breast cancer initiatives others such as Avon established their own breast cancer foundations on the back of pink ribbon products.[236]: 135–136 
Wearing or displaying a pink ribbon has been criticized by the opponents of this practice as a kind of slacktivism, because it has no practical positive effect. It has also been criticized as hypocrisy, because some people wear the pink ribbon to show good will towards women with breast cancer, but then oppose these women's practical goals, like patient rights and anti-pollution legislation.[234]: 366–368 [237] Critics say that the feel-good nature of pink ribbons and pink consumption distracts society from the lack of progress on preventing and curing breast cancer.[234]: 365–366  It is also criticized for reinforcing gender stereotypes and objectifying women and their breasts.<[234]: 372–374  In 2002 Breast Cancer Action launched the "Think Before You Pink" campaign against pinkwashing to target businesses that have co-opted the pink campaign to promote products that cause breast cancer, such as alcoholic beverages.[238]
In her 2006 book Pink Ribbons, Inc.: Breast Cancer and the Politics of Philanthropy Samantha King claimed that breast cancer has been transformed from a serious disease and individual tragedy to a market-driven industry of survivorship and corporate sales pitch.[239] In 2010 Gayle Sulik argued that the primary purposes or goals of breast cancer culture are to maintain breast cancer's dominance as the pre-eminent women's health issue, to promote the appearance that society is doing something effective about breast cancer, and to sustain and expand the social, political, and financial power of breast cancer activists.[234]: 57  In the same year Barbara Ehrenreich published an opinion piece in Harper's Magazine, lamenting that in breast cancer culture, breast cancer therapy is viewed as a rite of passage rather than a disease. To fit into this mold, the woman with breast cancer needs to normalize and feminize her appearance, and minimize the disruption that her health issues cause anyone else. Anger, sadness, and negativity must be silenced. As with most cultural models, people who conform to the model are given social status, in this case as cancer survivors. Women who reject the model are shunned, punished and shamed. The culture is criticized for treating adult women like little girls, as evidenced by "baby" toys such as pink teddy bears given to adult women.[240]
In 2009 the US science journalist Christie Aschwanden criticized that the emphasis on breast cancer screening may be harming women by subjecting them to unnecessary radiation, biopsies, and surgery. One-third of diagnosed breast cancers might recede on their own.[241] Screening mammography efficiently finds non-life-threatening, asymptomatic breast cancers and precancers, even while overlooking serious cancers. According to the cancer researcher H. Gilbert Welch, screening mammography has taken the "brain-dead approach that says the best test is the one that finds the most cancers" rather than the one that finds dangerous cancers.[241]
In 2002 it was noted that as a result of breast cancer's high visibility, the statistical results can be misinterpreted, such as the claim that one in eight women will be diagnosed with breast cancer during their lives – a claim that depends on the unrealistic assumption that no woman will die of any other disease before the age of 95.[219]: 199–200  By 2010 the breast cancer survival rate in Europe was 91% at one years and 65% at five years. In the USA the five-year survival rate for localized breast cancer was 96.8%, while in cases of metastases it was only 20.6%. Because the prognosis for breast cancer was at this stage relatively favorable, compared to the prognosis for other cancers, breast cancer as cause of death among women was 13.9% of all cancer deaths. The second most common cause of death from cancer in women was lung cancer, the most common cancer worldwide for men and women. The improved survival rate made breast cancer the most prevalent cancer in the world. In 2010 an estimated 3.6 million women worldwide have had a breast cancer diagnosis in the past five years, while only 1.4 million male or female survivors from lung cancer were alive.[242]
There are ethnic disparities in the mortality rates for breast cancer as well as in breast cancer treatment. Breast cancer is the most prevalent cancer affecting women of every ethnic group in the United States. Breast cancer incidence among black women aged 45 and older is higher than that of white women in the same age group. White women aged 60–84 have higher incidence rates of breast cancer than Black women. Despite this, Black women at every age are more likely to succumb to breast cancer.[243]
Breast cancer treatment has improved greatly in recent years, but black women are still less likely to obtain treatment compared to white women.[243] Risk factors such as socioeconomic status, late-stage, or breast cancer at diagnosis, genetic differences in tumour subtypes, differences in health care access all contribute to these disparities. Socioeconomic determinants affecting the disparity in breast cancer illness include poverty, culture, as well as social injustice. In Hispanic women, the incidence of breast cancer is lower than in non-Hispanic women but is often diagnosed at a later stage than white women with larger tumors.
Black women are usually diagnosed with breast cancer at a younger age than white women. The median age of diagnosis for Black women is 59, in comparison to 62 in White women. The incidence of breast cancer in Black women has increased by 0.4% per year since 1975 and 1.5% per year among Asian/Pacific Islander women since 1992. Incidence rates were stable for non-Hispanic White, Hispanics, and Native women. The five-year survival rate is noted to be 81% in Black women and 92% in White women. Chinese and Japanese women have the highest survival rates.[243]
Poverty is a major driver for disparities related to breast cancer. Low-income women are less likely to undergo breast cancer screening and thus are more likely to have a late-stage diagnosis.[243] Ensuring women of all ethnic groups receive equitable health care including breast screening, can positively affect these disparities.[244]
Pregnancy at an early age decreases the risk of developing breast cancer later in life.[245] The risk of breast cancer also declines with the number of children a woman has.[245] Breast cancer then becomes more common in the 5 or 10 years following pregnancy but then becomes less common than among the general population.[246] These cancers are known as postpartum breast cancer and have worse outcomes including an increased risk of distant spread of disease and mortality.[247] Other cancers found during or shortly after pregnancy appear at approximately the same rate as other cancers in women of a similar age.[248]
Diagnosing new cancer in a pregnant woman is difficult, in part because any symptoms are commonly assumed to be a normal discomfort associated with pregnancy.[248] As a result, cancer is typically discovered at a somewhat later stage than average in many pregnant or recently pregnant women. Some imaging procedures, such as MRIs (magnetic resonance imaging), CT scans, ultrasounds, and mammograms with fetal shielding are considered safe during pregnancy; some others, such as PET scans are not.[248]
Treatment is generally the same as for non-pregnant women.[248] However, radiation is normally avoided during pregnancy, especially if the fetal dose might exceed 100 cGy. In some cases, some or all treatments are postponed until after birth if the cancer is diagnosed late in the pregnancy. Early deliveries to speed the start of treatment are not uncommon. Surgery is generally considered safe during pregnancy, but some other treatments, especially certain chemotherapy drugs given during the first trimester, increase the risk of birth defects and pregnancy loss (spontaneous abortions and stillbirths).[248] Elective abortions are not required and do not improve the likelihood of the mother surviving or being cured.[248]
Radiation treatments may interfere with the mother's ability to breastfeed her baby because it reduces the ability of that breast to produce milk and increases the risk of mastitis. Also, when chemotherapy is being given after birth, many of the drugs pass through breast milk to the baby, which could harm the baby.[248]
Regarding future pregnancy among breast cancer survivors, there is often fear of cancer recurrence.[249] On the other hand, many still regard pregnancy and parenthood to represent normality, happiness and life fulfillment.[249]
In breast cancer survivors, non-hormonal birth control methods such as the copper intrauterine device (IUD) should be used as first-line options.[250] Progestogen-based methods such as depot medroxyprogesterone acetate, IUD with progestogen or progestogen only pills have a poorly investigated but possible increased risk of cancer recurrence, but may be used if positive effects outweigh this possible risk.[251]
In breast cancer survivors, it is recommended to first consider non-hormonal options for menopausal effects, such as bisphosphonates or selective estrogen receptor modulators (SERMs) for osteoporosis, and vaginal estrogen for local symptoms. Observational studies of systemic hormone replacement therapy after breast cancer are generally reassuring. If hormone replacement is necessary after breast cancer, estrogen-only therapy or estrogen therapy with an intrauterine device with progestogen may be safer options than combined systemic therapy.[252]
Treatments are being evaluated in clinical trials. This includes individual drugs, combinations of drugs, and surgical and radiation techniques Investigations include new types of targeted therapy,[253] cancer vaccines, oncolytic virotherapy,[254] gene therapy[255][256] and immunotherapy.[257]
The latest research is reported annually at scientific meetings such as that of the American Society of Clinical Oncology, San Antonio Breast Cancer Symposium,[258] and the St. Gallen Oncology Conference in St. Gallen, Switzerland.[259] These studies are reviewed by professional societies and other organizations, and formulated into guidelines for specific treatment groups and risk category.
Fenretinide, a retinoid, is also being studied as a way to reduce the risk of breast cancer.[260][261] In particular, combinations of ribociclib plus endocrine therapy have been the subject of clinical trials.[262]
A 2019 review found moderate certainty evidence that giving people antibiotics before breast cancer surgery helped to prevent surgical site infection (SSI). Further study is required to determine the most effective antibiotic protocol and use in women undergoing immediate breast reconstruction.[263]
As of 2014 cryoablation is being studied to see if it could be a substitute for a lumpectomy in small cancers.[264] There is tentative evidence in those with tumours less than 2 centimeters.[265] It may also be used in those in who surgery is not possible.[265] Another review states that cryoablation looks promising for early breast cancer of small size.[266]
Part of the current knowledge on breast carcinomas is based on in vivo and in vitro studies performed with cell lines derived from breast cancers. These provide an unlimited source of homogenous self-replicating material, free of contaminating stromal cells, and often easily cultured in simple standard media. The first breast cancer cell line described, BT-20, was established in 1958. Since then, and despite sustained work in this area, the number of permanent lines obtained has been strikingly low (about 100). Indeed, attempts to culture breast cancer cell lines from primary tumours have been largely unsuccessful. This poor efficiency was often due to technical difficulties associated with the extraction of viable tumour cells from their surrounding stroma. Most of the available breast cancer cell lines issued from metastatic tumours, mainly from pleural effusions. Effusions provided generally large numbers of dissociated, viable tumour cells with little or no contamination by fibroblasts and other tumour stroma cells.
Many of the currently used BCC lines were established in the late 1970s. A very few of them, namely MCF-7, T-47D, MDA-MB-231 and SK-BR-3, account for more than two-thirds of all abstracts reporting studies on mentioned breast cancer cell lines, as concluded from a Medline-based survey.
Clinically, the most useful metabolic markers in breast cancer are the estrogen and progesterone receptors that are used to predict response to hormone therapy. New or potentially new markers for breast cancer include BRCA1 and BRCA2[267] to identify people at high risk of developing breast cancer, HER-2,[medical citation needed] and SCD1, for predicting response to therapeutic regimens, and urokinase plasminogen activator, PA1-1 and SCD1 for assessing prognosis.[medical citation needed]


A myocardial infarction (MI), commonly known as a heart attack, occurs when blood flow decreases or stops in one of the coronary arteries of the heart, causing infarction (tissue death) to the heart muscle.[1] The most common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck or jaw.[1] Often it occurs in the center or left side of the chest and lasts for more than a few minutes.[1] The discomfort may occasionally feel like heartburn.[1] Other symptoms may include shortness of breath, nausea, feeling faint, a cold sweat, feeling tired, and decreased level of consciousness.[1] About 30% of people have atypical symptoms.[8] Women more often present without chest pain and instead have neck pain, arm pain or feel tired.[11] Among those over 75 years old, about 5% have had an MI with little or no history of symptoms.[12] An MI may cause heart failure, an irregular heartbeat, cardiogenic shock or cardiac arrest.[3][4]
Most MIs occur due to coronary artery disease.[3] Risk factors include high blood pressure, smoking, diabetes, lack of exercise, obesity, high blood cholesterol, poor diet, and excessive alcohol intake.[5][6] The complete blockage of a coronary artery caused by a rupture of an atherosclerotic plaque is usually the underlying mechanism of an MI.[3] MIs is produced less caused by coronary artery spasms, which may be due to cocaine, significant emotional stress (often known as Takotsubo syndrome or broken heart syndrome) and extreme cold, among others.[13][14] Many tests are helpful to help with diagnosis, including electrocardiograms (ECGs), blood tests and coronary angiography.[7] An ECG, which is a recording of the heart's electrical activity, may confirm an ST elevation MI (STEMI), if ST elevation is present.[8][15] Commonly used blood tests include troponin and less often creatine kinase MB.[7]
Treatment of an MI is time-critical.[16] Aspirin is an appropriate immediate treatment for a suspected MI.[9] Nitroglycerin or opioids may be used to help with chest pain; however, they do not improve overall outcomes.[8][9] Supplemental oxygen is recommended in those with low oxygen levels or shortness of breath.[9] In a STEMI, treatments attempt to restore blood flow to the heart and include percutaneous coronary intervention (PCI), where the arteries are pushed open and may be stented, or thrombolysis, where the blockage is removed using medications.[8] People who have a non-ST elevation myocardial infarction (NSTEMI) are often managed with the blood thinner heparin, with the additional use of PCI in those at high risk.[9] In people with blockages of multiple coronary arteries and diabetes, coronary artery bypass surgery (CABG) may be recommended rather than angioplasty.[17] After an MI, lifestyle modifications, along with long-term treatment with aspirin, beta blockers and statins, are typically recommended.[8]
Worldwide, about 15.9 million myocardial infarctions occurred in 2015.[10] More than 3 million people had an ST elevation MI, and more than 4 million had an NSTEMI.[18] STEMIs occur about twice as often in men as women.[19] About one million people have an MI each year in the United States.[3] In the developed world, the risk of death in those who have had a STEMI is about 10%.[8] Rates of MI for a given age have decreased globally between 1990 and 2010.[20] In 2011, an MI was one of the top five most expensive conditions during inpatient hospitalizations in the US, with a cost of about $11.5 billion for 612,000 hospital stays.[21]
Myocardial infarction (MI) refers to tissue death (infarction) of the heart muscle (myocardium) caused by ischemia, the lack of oxygen delivery to myocardial tissue. It is a type of acute coronary syndrome, which describes a sudden or short-term change in symptoms related to blood flow to the heart.[22] Unlike the other type of acute coronary syndrome, unstable angina, a myocardial infarction occurs when there is cell death, which can be estimated by measuring by a blood test for biomarkers (the cardiac protein troponin).[23] When there is evidence of an MI, it may be classified as an ST elevation myocardial infarction (STEMI) or Non-ST elevation myocardial infarction (NSTEMI) based on the results of an ECG.[24]
The phrase "heart attack" is often used non-specifically to refer to myocardial infarction. An MI is different from—but can cause—cardiac arrest, where the heart is not contracting at all or so poorly that all vital organs cease to function, thus leading to death.[25] It is also distinct from heart failure, in which the pumping action of the heart is impaired. However, an MI may lead to heart failure.[26]
Chest pain that may or may not radiate to other parts of the body is the most typical and significant symptom of myocardial infarction. It might be accompanied by other symptoms such as sweating.[27]
Chest pain is one of the most common symptoms of acute myocardial infarction and is often described as a sensation of tightness, pressure, or squeezing. Pain radiates most often to the left arm, but may also radiate to the lower jaw, neck, right arm, back, and upper abdomen.[28][29] The pain most suggestive of an acute MI, with the highest likelihood ratio, is pain radiating to the right arm and shoulder.[30][29] Similarly, chest pain similar to a previous heart attack is also suggestive.[31] The pain associated with MI is usually diffuse, does not change with position, and lasts for more than 20 minutes.[24] It might be described as pressure, tightness, knifelike, tearing, burning sensation (all these are also manifested during other diseases). It could be felt as an unexplained anxiety, and pain might be absent altogether.[29] Levine's sign, in which a person localizes the chest pain by clenching one or both fists over their sternum, has classically been thought to be predictive of cardiac chest pain, although a prospective observational study showed it had a poor positive predictive value.[32]
Typically, chest pain because of ischemia, be it unstable angina or myocardial infarction, lessens with the use of nitroglycerin, but nitroglycerin may also relieve chest pain arising from non-cardiac causes.[33]
Chest pain may be accompanied by sweating, nausea or vomiting, and fainting,[24][30] and these symptoms may also occur without any pain at all.[28] Dizziness or lightheadedness is common and occurs due to reduction in oxygen and blood to the brain. In women, the most common symptoms of myocardial infarction include shortness of breath, weakness, and fatigue.[34] Women are more likely to have unusual or unexplained tiredness and nausea or vomiting as symptoms.[35] Women having heart attacks are more likely to have palpitations, back pain, labored breath, vomiting, and left arm pain than men, although the studies showing these differences had high variability.[36] Women are less likely to report chest pain during a heart attack and more likely to report nausea, jaw pain, neck pain, cough, and fatigue, although these findings are inconsistent across studies.[37] Women with heart attacks also had more indigestion, dizziness, loss of appetite, and loss of consciousness.[38] Shortness of breath is a common, and sometimes the only symptom, occurring when damage to the heart limits the output of the left ventricle, with breathlessness arising either from low oxygen in the blood, or pulmonary edema.[28][39] Other less common symptoms include weakness, light-headedness, palpitations, and abnormalities in heart rate or blood pressure.[16] These symptoms are likely induced by a massive surge of catecholamines from the sympathetic nervous system, which occurs in response to pain and, where present, low blood pressure.[40] Loss of consciousness can occur in myocardial infarctions due to inadequate blood flow to the brain and cardiogenic shock, and sudden death, frequently due to the development of ventricular fibrillation. [41] When the brain was without oxygen for too long due to a myocardial infarction, coma can occur. Cardiac arrest, and atypical symptoms such as palpitations, occur more frequently in women, the elderly, those with diabetes, in people who have just had surgery, and in critically ill patients.[24]
"Silent" myocardial infarctions can happen without any symptoms at all.[12] These cases can be discovered later on electrocardiograms, using blood enzyme tests, or at autopsy after a person has died. Such silent myocardial infarctions represent between 22 and 64% of all infarctions,[12] and are more common in the elderly,[12] in those with diabetes mellitus[16] and after heart transplantation. In people with diabetes, differences in pain threshold, autonomic neuropathy, and psychological factors have been cited as possible explanations for the lack of symptoms.[42] In heart transplantation, the donor heart is not fully innervated by the nervous system of the recipient.[43]
The most prominent risk factors for myocardial infarction are older age, actively smoking, high blood pressure, diabetes mellitus, and total cholesterol and high-density lipoprotein levels.[44] Many risk factors of myocardial infarction are shared with coronary artery disease, the primary cause of myocardial infarction,[16] with other risk factors including male sex, low levels of physical activity, a past family history, obesity, and alcohol use.[16] Risk factors for myocardial disease are often included in risk factor stratification scores, such as the Framingham Risk Score.[19] At any given age, men are more at risk than women for the development of cardiovascular disease.[45] High levels of blood cholesterol is a known risk factor, particularly high low-density lipoprotein, low high-density lipoprotein, and high triglycerides.[46]
Many risk factors for myocardial infarction are potentially modifiable, with the most important being tobacco smoking (including secondhand smoke).[16] Smoking appears to be the cause of about 36% and obesity the cause of 20% of coronary artery disease.[47] Lack of physical activity has been linked to 7–12% of cases.[47][48] Less common causes include stress-related causes such as job stress, which accounts for about 3% of cases,[47] and chronic high stress levels.[49]
There is varying evidence about the importance of saturated fat in the development of myocardial infarctions. Eating polyunsaturated fat instead of saturated fats has been shown in studies to be associated with a decreased risk of myocardial infarction,[50] while other studies find little evidence that reducing dietary saturated fat or increasing polyunsaturated fat intake affects heart attack risk.[51][52] Dietary cholesterol does not appear to have a significant effect on blood cholesterol and thus recommendations about its consumption may not be needed.[53] Trans fats do appear to increase risk.[51] Acute and prolonged intake of high quantities of alcoholic drinks (3–4 or more daily) increases the risk of a heart attack.[54]
Family history of ischemic heart disease or MI, particularly if one has a male first-degree relative (father, brother) who had a myocardial infarction before age 55 years, or a female first-degree relative (mother, sister) less than age 65 increases a person's risk of MI.[45]
Genome-wide association studies have found 27 genetic variants that are associated with an increased risk of myocardial infarction.[55] The strongest association of MI has been found with chromosome 9 on the short arm p at locus 21, which contains genes CDKN2A and 2B, although the single nucleotide polymorphisms that are implicated are within a non-coding region.[55] The majority of these variants are in regions that have not been previously implicated in coronary artery disease. The following genes have an association with MI: PCSK9, SORT1, MIA3, WDR12, MRAS, PHACTR1, LPA, TCF21, MTHFDSL, ZC3HC1, CDKN2A, 2B, ABO, PDGF0, APOA5, MNF1ASM283, COL4A1, HHIPC1, SMAD3, ADAMTS7, RAS1, SMG6, SNF8, LDLR, SLC5A3, MRPS6, KCNE2.[55]
The risk of having a myocardial infarction increases with older age, low physical activity, and low socioeconomic status.[45] Heart attacks appear to occur more commonly in the morning hours, especially between 6AM and noon.[56] Evidence suggests that heart attacks are at least three times more likely to occur in the morning than in the late evening.[57] Shift work is also associated with a higher risk of MI.[58] One analysis has found an increase in heart attacks immediately following the start of daylight saving time.[59] A 2021 WHO study concluded that working 55+ hours a week raises the risk of stroke by 35% and the risk of dying from heart conditions by 17%, when compared to a 35-40 hour week.[60]
Women who use combined oral contraceptive pills have a modestly increased risk of myocardial infarction, especially in the presence of other risk factors.[61] The use of non-steroidal anti inflammatory drugs (NSAIDs), even for as short as a week, increases risk.[62]
Endometriosis in women under the age of 40 is an identified risk factor.[63]
Air pollution is also an important modifiable risk. Short-term exposure to air pollution such as carbon monoxide, nitrogen dioxide, and sulfur dioxide (but not ozone) have been associated with MI and other acute cardiovascular events.[64] For sudden cardiac deaths, every increment of 30 units in Pollutant Standards Index correlated with an 8% increased risk of out-of-hospital cardiac arrest on the day of exposure.[65] Extremes of temperature are also associated.[66]
A number of acute and chronic infections including Chlamydophila pneumoniae, influenza, Helicobacter pylori, and Porphyromonas gingivalis among others have been linked to atherosclerosis and myocardial infarction.[67] As of 2013, there is no evidence of benefit from antibiotics or vaccination, however, calling the association into question.[67][68] Myocardial infarction can also occur as a late consequence of Kawasaki disease.[69]
Calcium deposits in the coronary arteries can be detected with CT scans. Calcium seen in coronary arteries can provide predictive information beyond that of classical risk factors.[70] High blood levels of the amino acid homocysteine is associated with premature atherosclerosis;[71] whether elevated homocysteine in the normal range is causal is controversial.[72]
In people without evident coronary artery disease, possible causes for the myocardial infarction are coronary spasm or coronary artery dissection.[73]
The most common cause of a myocardial infarction is the rupture of an atherosclerotic plaque on an artery supplying heart muscle.[41][74] Plaques can become unstable, rupture, and additionally promote the formation of a blood clot that blocks the artery; this can occur in minutes. Blockage of an artery can lead to tissue death in tissue being supplied by that artery.[75] Atherosclerotic plaques are often present for decades before they result in symptoms.[75]
The gradual buildup of cholesterol and fibrous tissue in plaques in the wall of the coronary arteries or other arteries, typically over decades, is termed atherosclerosis.[76] Atherosclerosis is characterized by progressive inflammation of the walls of the arteries.[75] Inflammatory cells, particularly macrophages, move into affected arterial walls. Over time, they become laden with cholesterol products, particularly LDL, and become foam cells. A cholesterol core forms as foam cells die. In response to growth factors secreted by macrophages, smooth muscle and other cells move into the plaque and act to stabilize it. A stable plaque may have a thick fibrous cap with calcification. If there is ongoing inflammation, the cap may be thin or ulcerate. Exposed to the pressure associated with blood flow, plaques, especially those with a thin lining, may rupture and trigger the formation of a blood clot (thrombus).[75] The cholesterol crystals have been associated with plaque rupture through mechanical injury and inflammation.[77]
Atherosclerotic disease is not the only cause of myocardial infarction, but it may exacerbate or contribute to other causes. A myocardial infarction may result from a heart with a limited blood supply subject to increased oxygen demands, such as in fever, a fast heart rate, hyperthyroidism, too few red blood cells in the bloodstream, or low blood pressure. Damage or failure of procedures such as percutaneous coronary intervention or coronary artery bypass grafts may cause a myocardial infarction. Spasm of coronary arteries, such as Prinzmetal's angina may cause blockage.[24][28]
If impaired blood flow to the heart lasts long enough, it triggers a process called the ischemic cascade; the heart cells in the territory of the blocked coronary artery die (infarction), chiefly through necrosis, and do not grow back. A collagen scar forms in their place.[75] When an artery is blocked, cells lack oxygen, needed to produce ATP in mitochondria. ATP is required for the maintenance of electrolyte balance, particularly through the Na/K ATPase. This leads to an ischemic cascade of intracellular changes, necrosis and apoptosis of affected cells.[78]
Cells in the area with the worst blood supply, just below the inner surface of the heart (endocardium), are most susceptible to damage.[79][80] Ischemia first affects this region, the subendocardial region, and tissue begins to die within 15–30 minutes of loss of blood supply.[81] The dead tissue is surrounded by a zone of potentially reversible ischemia that progresses to become a full-thickness transmural infarct.[78][81] The initial "wave" of infarction can take place over 3–4 hours.[75][78] These changes are seen on gross pathology and cannot be predicted by the presence or absence of Q waves on an ECG.[80] The position, size and extent of an infarct depends on the affected artery, totality of the blockage, duration of the blockage, the presence of collateral blood vessels, oxygen demand, and success of interventional procedures.[28][74]
Tissue death and myocardial scarring alter the normal conduction pathways of the heart and weaken affected areas. The size and location put a person at risk of abnormal heart rhythms (arrhythmias) or heart block, aneurysm of the heart ventricles, inflammation of the heart wall following infarction, and rupture of the heart wall that can have catastrophic consequences.[74][82]
Injury to the myocardium also occurs during re-perfusion. This might manifest as ventricular arrhythmia. The re-perfusion injury is a consequence of the calcium and sodium uptake from the cardiac cells and the release of oxygen radicals during reperfusion. No-reflow phenomenon—when blood is still unable to be distributed to the affected myocardium despite clearing the occlusion—also contributes to myocardial injury. Topical endothelial swelling is one of many factors contributing to this phenomenon.[83]
A myocardial infarction, according to current consensus, is defined by elevated cardiac biomarkers with a rising or falling trend and at least one of the following:[84]
A myocardial infarction is usually clinically classified as an ST-elevation MI (STEMI) or a non-ST elevation MI (NSTEMI). These are based on ST elevation, a portion of a heartbeat graphically recorded on an ECG.[24] STEMIs make up about 25–40% of myocardial infarctions.[19] A more explicit classification system, based on international consensus in 2012, also exists. This classifies myocardial infarctions into five types:[24]
There are many different biomarkers used to determine the presence of cardiac muscle damage. Troponins, measured through a blood test, are considered to be the best,[19] and are preferred because they have greater sensitivity and specificity for measuring injury to the heart muscle than other tests.[74] A rise in troponin occurs within 2–3 hours of injury to the heart muscle, and peaks within 1–2 days. The level of the troponin, as well as a change over time, are useful in measuring and diagnosing or excluding myocardial infarctions, and the diagnostic accuracy of troponin testing is improving over time.[74] One high-sensitivity cardiac troponin can rule out a heart attack as long as the ECG is normal.[85][86]
Other tests, such as CK-MB or myoglobin, are discouraged.[87] CK-MB is not as specific as troponins for acute myocardial injury, and may be elevated with past cardiac surgery, inflammation or electrical cardioversion; it rises within 4–8 hours and returns to normal within 2–3 days.[28] Copeptin may be useful to rule out MI rapidly when used along with troponin.[88]
Electrocardiograms (ECGs) are a series of leads placed on a person's chest that measure electrical activity associated with contraction of the heart muscle.[89] The taking of an ECG is an important part of the workup of an AMI,[24] and ECGs are often not just taken once but may be repeated over minutes to hours, or in response to changes in signs or symptoms.[24]
ECG readouts product a waveform with different labelled features.[89] In addition to a rise in biomarkers, a rise in the ST segment, changes in the shape or flipping of T waves, new Q waves, or a new left bundle branch block can be used to diagnose an AMI.[24] In addition, ST elevation can be used to diagnose an ST segment myocardial infarction (STEMI). A rise must be new in V2 and V3 ≥2 mm (0,2 mV) for males or ≥1.5 mm (0.15 mV) for females or ≥1 mm (0.1 mV) in two other adjacent chest or limb leads.[19][24] ST elevation is associated with infarction, and may be preceded by changes indicating ischemia, such as ST depression or inversion of the T waves.[89] Abnormalities can help differentiate the location of an infarct, based on the leads that are affected by changes.[16] Early STEMIs may be preceded by peaked T waves.[19] Other ECG abnormalities relating to complications of acute myocardial infarctions may also be evident, such as atrial or ventricular fibrillation.[90]
Noninvasive imaging plays an important role in the diagnosis and characterisation of myocardial infarction.[24] Tests such as chest X-rays can be used to explore and exclude alternate causes of a person's symptoms.[24] Echocardiography may assist in modifying clinical suspicion of ongoing myocardial infarction in patients that can't be ruled out or ruled in following initial ECG and Troponin testing.[91] Myocardial perfusion imaging has no role in the acute diagnostic algorithm; however, it can confirm a clinical suspicion of Chronic Coronary Syndrome when the patient's history, physical examination (including cardiac examination) ECG, and cardiac biomarkers suggest coronary artery disease.[92]
Echocardiography, an ultrasound scan of the heart, is able to visualize the heart, its size, shape, and any abnormal motion of the heart walls as they beat that may indicate a myocardial infarction. The flow of blood can be imaged, and contrast dyes may be given to improve image.[24] Other scans using radioactive contrast include SPECT CT-scans using thallium, sestamibi (MIBI scans) or tetrofosmin; or a PET scan using Fludeoxyglucose or rubidium-82.[24] These nuclear medicine scans can visualize the perfusion of heart muscle.[24] SPECT may also be used to determine viability of tissue, and whether areas of ischemia are inducible.[24][93]
Medical societies and professional guidelines recommend that the physician confirm a person is at high risk for Chronic Coronary Syndrome before conducting diagnostic non-invasive imaging tests to make a diagnosis,[92][94][91] as such tests are unlikely to change management and result in increased costs.[92] Patients who have a normal ECG and who are able to exercise, for example, most likely do not merit routine imaging.[92]
Poor movement of the heart due to an MI as seen on ultrasound[95]
Pulmonary edema due to an MI as seen on ultrasound[95]
There are many causes of chest pain, which can originate from the heart, lungs, gastrointestinal tract, aorta, and other muscles, bones and nerves surrounding the chest.[96] In addition to myocardial infarction, other causes include angina, insufficient blood supply (ischemia) to the heart muscles without evidence of cell death, gastroesophageal reflux disease; pulmonary embolism, tumors of the lungs, pneumonia, rib fracture, costochondritis, heart failure and other musculoskeletal injuries.[96][24] Rarer severe differential diagnoses include aortic dissection, esophageal rupture, tension pneumothorax, and pericardial effusion causing cardiac tamponade.[97] The chest pain in an MI may mimic heartburn.[41] Causes of sudden-onset breathlessness generally involve the lungs or heart – including pulmonary edema, pneumonia, allergic reactions and asthma, and pulmonary embolus, acute respiratory distress syndrome and metabolic acidosis.[96] There are many different causes of fatigue, and myocardial infarction is not a common cause.[98]
There is a large crossover between the lifestyle and activity recommendations to prevent a myocardial infarction, and those that may be adopted as secondary prevention after an initial myocardial infarction,[74] because of shared risk factors and an aim to reduce atherosclerosis affecting heart vessels.[28] The influenza vaccine also appear to protect against myocardial infarction with a benefit of 15 to 45%.[99]
Physical activity can reduce the risk of cardiovascular disease, and people at risk are advised to engage in 150 minutes of moderate or 75 minutes of vigorous intensity aerobic exercise a week.[100] Keeping a healthy weight, drinking alcohol within the recommended limits, and quitting smoking reduce the risk of cardiovascular disease.[100]
Substituting unsaturated fats such as olive oil and rapeseed oil instead of saturated fats may reduce the risk of myocardial infarction,[50] although there is not universal agreement.[51] Dietary modifications are recommended by some national authorities, with recommendations including increasing the intake of wholegrain starch, reducing sugar intake (particularly of refined sugar), consuming five portions of fruit and vegetables daily, consuming two or more portions of fish per week, and consuming 4–5 portions of unsalted nuts, seeds, or legumes per week.[100] The dietary pattern with the greatest support is the Mediterranean diet.[101] Vitamins and mineral supplements are of no proven benefit,[102] and neither are plant stanols or sterols.[100]
Public health measures may also act at a population level to reduce the risk of myocardial infarction, for example by reducing unhealthy diets (excessive salt, saturated fat, and trans-fat) including food labeling and marketing requirements as well as requirements for catering and restaurants and stimulating physical activity. This may be part of regional cardiovascular disease prevention programs or through the health impact assessment of regional and local plans and policies.[103]
Most guidelines recommend combining different preventive strategies. A 2015 Cochrane Review found some evidence that such an approach might help with blood pressure, body mass index and waist circumference. However, there was insufficient evidence to show an effect on mortality or actual cardio-vascular events.[104]
Statins, drugs that act to lower blood cholesterol, decrease the incidence and mortality rates of myocardial infarctions.[105] They are often recommended in those at an elevated risk of cardiovascular diseases.[100]
Aspirin has been studied extensively in people considered at increased risk of myocardial infarction. Based on numerous studies in different groups (e.g. people with or without diabetes), there does not appear to be a benefit strong enough to outweigh the risk of excessive bleeding.[106][107] Nevertheless, many clinical practice guidelines continue to recommend aspirin for primary prevention,[108] and some researchers feel that those with very high cardiovascular risk but low risk of bleeding should continue to receive aspirin.[109]
There is a large crossover between the lifestyle and activity recommendations to prevent a myocardial infarction, and those that may be adopted as secondary prevention after an initial myocardial infarct.[74] Recommendations include stopping smoking, a gradual return to exercise, eating a healthy diet, low in saturated fat and low in cholesterol, drinking alcohol within recommended limits, exercising, and trying to achieve a healthy weight.[74][110] Exercise is both safe and effective even if people have had stents or heart failure,[111] and is recommended to start gradually after 1–2 weeks.[74] Counselling should be provided relating to medications used, and for warning signs of depression.[74] Previous studies suggested a benefit from omega-3 fatty acid supplementation but this has not been confirmed.[110]
Following a heart attack, nitrates, when taken for two days, and ACE-inhibitors decrease the risk of death.[112] Other medications include:
Aspirin is continued indefinitely, as well as another antiplatelet agent such as clopidogrel or ticagrelor ("dual antiplatelet therapy" or DAPT) for up to twelve months.[110] If someone has another medical condition that requires anticoagulation (e.g. with warfarin) this may need to be adjusted based on risk of further cardiac events as well as bleeding risk.[110] In those who have had a stent, more than 12 months of clopidogrel plus aspirin does not affect the risk of death.[113]
Beta blocker therapy such as metoprolol or carvedilol is recommended to be started within 24 hours, provided there is no acute heart failure or heart block.[19][87] The dose should be increased to the highest tolerated.[110] Contrary to most guidelines, the use of beta blockers does not appear to affect the risk of death,[114][115] possibly because other treatments for MI have improved. When beta blocker medication is given within the first 24–72 hours of a STEMI no lives are saved. However, 1 in 200 people were prevented from a repeat heart attack, and another 1 in 200 from having an abnormal heart rhythm. Additionally, for 1 in 91 the medication causes a temporary decrease in the heart's ability to pump blood.[116]
ACE inhibitor therapy should be started within 24 hours and continued indefinitely at the highest tolerated dose. This is provided there is no evidence of worsening kidney failure, high potassium, low blood pressure, or known narrowing of the renal arteries.[74] Those who cannot tolerate ACE inhibitors may be treated with an angiotensin II receptor antagonist.[110]
Statin therapy has been shown to reduce mortality and subsequent cardiac events and should be commenced to lower LDL cholesterol. Other medications, such as ezetimibe, may also be added with this goal in mind.[74]
Aldosterone antagonists (spironolactone or eplerenone) may be used if there is evidence of left ventricular dysfunction after an MI, ideally after beginning treatment with an ACE inhibitor.[110][117]
A defibrillator, an electric device connected to the heart and surgically inserted under the skin, may be recommended. This is particularly if there are any ongoing signs of heart failure, with a low left ventricular ejection fraction and a New York Heart Association grade II or III after 40 days of the infarction.[74] Defibrillators detect potentially fatal arrhythmia and deliver an electrical shock to the person to depolarize a critical mass of the heart muscle.[118]
A myocardial infarction requires immediate medical attention. Treatment aims to preserve as much heart muscle as possible, and to prevent further complications.[28] Treatment depends on whether the myocardial infarction is a STEMI or NSTEMI.[74] Treatment in general aims to unblock blood vessels, reduce blood clot enlargement, reduce ischemia, and modify risk factors with the aim of preventing future MIs.[28] In addition, the main treatment for myocardial infarctions with ECG evidence of ST elevation (STEMI) include thrombolysis or percutaneous coronary intervention, although PCI is also ideally conducted within 1–3 days for NSTEMI.[74] In addition to clinical judgement, risk stratification may be used to guide treatment, such as with the TIMI and GRACE scoring systems.[16][74][119]
The pain associated with myocardial infarction is often treated with nitroglycerin, a vasodilator, or opioid medications such as morphine.[28] Nitroglycerin (given under the tongue or injected into a vein) may improve blood supply to the heart.[28] It is an important part of therapy for its pain relief effects, though there is no proven benefit to mortality.[28][120] Morphine or other opioid medications may also be used, and are effective for the pain associated with STEMI.[28] There is poor evidence that morphine shows any benefit to overall outcomes, and there is some evidence of potential harm.[121][122]
Aspirin, an antiplatelet drug, is given as a loading dose to reduce the clot size and reduce further clotting in the affected artery.[28][74] It is known to decrease mortality associated with acute myocardial infarction by at least 50%.[74] P2Y12 inhibitors such as clopidogrel, prasugrel and ticagrelor are given concurrently, also as a loading dose, with the dose depending on whether further surgical management or fibrinolysis is planned.[74] Prasugrel and ticagrelor are recommended in European and American guidelines, as they are active more quickly and consistently than clopidogrel.[74] P2Y12 inhibitors are recommended in both NSTEMI and STEMI, including in PCI, with evidence also to suggest improved mortality.[74] Heparins, particularly in the unfractionated form, act at several points in the clotting cascade, help to prevent the enlargement of a clot, and are also given in myocardial infarction, owing to evidence suggesting improved mortality rates.[74] In very high-risk scenarios, inhibitors of the platelet glycoprotein αIIbβ3a receptor such as eptifibatide or tirofiban may be used.[74]
There is varying evidence on the mortality benefits in NSTEMI. A 2014 review of P2Y12 inhibitors such as clopidogrel found they do not change the risk of death when given to people with a suspected NSTEMI prior to PCI,[123] nor do heparins change the risk of death.[124] They do decrease the risk of having a further myocardial infarction.[74][124]
Primary percutaneous coronary intervention (PCI) is the treatment of choice for STEMI if it can be performed in a timely manner, ideally within 90–120 minutes of contact with a medical provider.[74][125] Some recommend it is also done in NSTEMI within 1–3 days, particularly when considered high-risk.[74] A 2017 review, however, did not find a difference between early versus later PCI in NSTEMI.[126]
PCI involves small probes, inserted through peripheral blood vessels such as the femoral artery or radial artery into the blood vessels of the heart. The probes are then used to identify and clear blockages using small balloons, which are dragged through the blocked segment, dragging away the clot, or the insertion of stents.[28][74] Coronary artery bypass grafting is only considered when the affected area of heart muscle is large, and PCI is unsuitable, for example with difficult cardiac anatomy.[127] After PCI, people are generally placed on aspirin indefinitely and on dual antiplatelet therapy (generally aspirin and clopidogrel) for at least a year.[19][74][128]
If PCI cannot be performed within 90 to 120 minutes in STEMI then fibrinolysis, preferably within 30 minutes of arrival to hospital, is recommended.[74][129] If a person has had symptoms for 12 to 24 hours evidence for effectiveness of thrombolysis is less and if they have had symptoms for more than 24 hours it is not recommended.[130] Thrombolysis involves the administration of medication that activates the enzymes that normally dissolve blood clots. These medications include tissue plasminogen activator, reteplase, streptokinase, and tenecteplase.[28] Thrombolysis is not recommended in a number of situations, particularly when associated with a high risk of bleeding or the potential for problematic bleeding, such as active bleeding, past strokes or bleeds into the brain, or severe hypertension. Situations in which thrombolysis may be considered, but with caution, include recent surgery, use of anticoagulants, pregnancy, and proclivity to bleeding.[28] Major risks of thrombolysis are major bleeding and intracranial bleeding.[28] Pre-hospital thrombolysis reduces time to thrombolytic treatment, based on studies conducted in higher income countries; however, it is unclear whether this has an impact on mortality rates.[131]
In the past, high flow oxygen was recommended for everyone with a possible myocardial infarction.[87] More recently, no evidence was found for routine use in those with normal oxygen levels and there is potential harm from the intervention.[132][133][134][135][136] Therefore, oxygen is currently only recommended if oxygen levels are found to be low or if someone is in respiratory distress.[28][87]
If despite thrombolysis there is significant cardiogenic shock, continued severe chest pain, or less than a 50% improvement in ST elevation on the ECG recording after 90 minutes, then rescue PCI is indicated emergently.[137][138]
Those who have had cardiac arrest may benefit from targeted temperature management with evaluation for implementation of hypothermia protocols. Furthermore, those with cardiac arrest, and ST elevation at any time, should usually have angiography.[19] Aldosterone antagonists appear to be useful in people who have had an STEMI and do not have heart failure.[139]
Cardiac rehabilitation benefits many who have experienced myocardial infarction,[74] even if there has been substantial heart damage and resultant left ventricular failure. It should start soon after discharge from the hospital. The program may include lifestyle advice, exercise, social support, as well as recommendations about driving, flying, sports participation, stress management, and sexual intercourse.[110] Returning to sexual activity after myocardial infarction is a major concern for most patients, and is an important area to be discussed in the provision of holistic care.[140][141]
In the short-term, exercise-based cardiovascular rehabilitation programs may reduce the risk of a myocardial infarction, reduces a large number of hospitalizations from all causes, reduces hospital costs, improves health-related quality of life, and has a small effect on all-cause mortality.[142] Longer-term studies indicate that exercise-based cardiovascular rehabilitation programs may reduce cardiovascular mortality and myocardial infarction.
The prognosis after myocardial infarction varies greatly depending on the extent and location of the affected heart muscle, and the development and management of complications.[16] Prognosis is worse with older age and social isolation.[16] Anterior infarcts, persistent ventricular tachycardia or fibrillation, development of heart blocks, and left ventricular impairment are all associated with poorer prognosis.[16] Without treatment, about a quarter of those affected by MI die within minutes and about forty percent within the first month.[16] Morbidity and mortality from myocardial infarction has, however, improved over the years due to earlier and better treatment:[30] in those who have a STEMI in the United States, between 5 and 6 percent die before leaving the hospital and 7 to 18 percent die within a year.[19]
It is unusual for babies to experience a myocardial infarction, but when they do, about half die.[143] In the short-term, neonatal survivors seem to have a normal quality of life.[143]
Complications may occur immediately following the myocardial infarction or may take time to develop. Disturbances of heart rhythms, including atrial fibrillation, ventricular tachycardia and fibrillation and heart block can arise as a result of ischemia, cardiac scarring, and infarct location.[16][74] Stroke is also a risk, either as a result of clots transmitted from the heart during PCI, as a result of bleeding following anticoagulation, or as a result of disturbances in the heart's ability to pump effectively as a result of the infarction.[74] Regurgitation of blood through the mitral valve is possible, particularly if the infarction causes dysfunction of the papillary muscle.[74] Cardiogenic shock as a result of the heart being unable to adequately pump blood may develop, dependent on infarct size, and is most likely to occur within the days following an acute myocardial infarction. Cardiogenic shock is the largest cause of in-hospital mortality.[30][74] Rupture of the ventricular dividing wall or left ventricular wall may occur within the initial weeks.[74] Dressler's syndrome, a reaction following larger infarcts and a cause of pericarditis is also possible.[74]
Heart failure may develop as a long-term consequence, with an impaired ability of heart muscle to pump, scarring, and an increase in the size of the existing muscle. Aneurysm of the left ventricle myocardium develops in about 10% of MI and is itself a risk factor for heart failure, ventricular arrhythmia, and the development of clots.[16]
Risk factors for complications and death include age, hemodynamic parameters (such as heart failure, cardiac arrest on admission, systolic blood pressure, or Killip class of two or greater), ST-segment deviation, diabetes, serum creatinine, peripheral vascular disease, and elevation of cardiac markers.[144][145][146]
Myocardial infarction is a common presentation of coronary artery disease. The World Health Organization estimated in 2004, that 12.2% of worldwide deaths were from ischemic heart disease;[147] with it being the leading cause of death in high- or middle-income countries and second only to lower respiratory infections in lower-income countries.[147] Worldwide, more than 3 million people have STEMIs and 4 million have NSTEMIs a year.[18] STEMIs occur about twice as often in men as women.[19]
Rates of death from ischemic heart disease (IHD) have slowed or declined in most high-income countries, although cardiovascular disease still accounted for one in three of all deaths in the US in 2008.[148] For example, rates of death from cardiovascular disease have decreased almost a third between 2001 and 2011 in the United States.[149]
In contrast, IHD is becoming a more common cause of death in the developing world. For example, in India, IHD had become the leading cause of death by 2004, accounting for 1.46 million deaths (14% of total deaths) and deaths due to IHD were expected to double during 1985–2015.[150] Globally, disability adjusted life years (DALYs) lost to ischemic heart disease are predicted to account for 5.5% of total DALYs in 2030, making it the second-most-important cause of disability (after unipolar depressive disorder), as well as the leading cause of death by this date.[147]
Social determinants such as neighborhood disadvantage, immigration status, lack of social support, social isolation, access to health services play an important role in myocardial infarction risk and survival.[151][152][153][154] Studies have shown that low socioeconomic status is associated with an increased risk of poorer survival. There are well-documented disparities in myocardial infarction survival by socioeconomic status, race, education, and census-tract-level poverty.[155]
Race: In the U.S. African Americans have a greater burden of myocardial infarction and other cardiovascular events. On a population level, there is a higher overall prevalence of risk factors that are unrecognized and therefore not treated, which places these individuals at a greater likelihood of experiencing adverse outcomes and therefore potentially higher morbidity and mortality.[156] Similarly, South Asians (including South Asians that have migrated to other countries around the world) experience higher rates of acute myocardial infarctions at younger ages, which can be largely explained by a higher prevalence of risk factors at younger ages.[157]
Socioeconomic status: Among individuals who live in the low-socioeconomic (SES) areas, which is close to 25% of the US population, myocardial infarctions (MIs) occurred twice as often compared with people who lived in higher SES areas.[158]
Immigration status: In 2018 many lawfully present immigrants who are eligible for coverage remain uninsured because immigrant families face a range of enrollment barriers, including fear, confusion about eligibility policies, difficulty navigating the enrollment process, and language and literacy challenges. Uninsured undocumented immigrants are ineligible for coverage options due to their immigration status.[159]
Health care access: Lack of health insurance and financial concerns about accessing care were associated with delays in seeking emergency care for acute myocardial infarction which can have significant, adverse consequences on patient outcomes.[160]
Education: Researchers found that compared to people with graduate degrees, those with lower educational attainment appeared to have a higher risk of heart attack, dying from a cardiovascular event, and overall death.[161]
Depictions of heart attacks in popular media often include collapsing or loss of consciousness which are not common symptoms; these depictions contribute to widespread misunderstanding about the symptoms of myocardial infarctions, which in turn contributes to people not getting care when they should.[162]
At common law, in general, a myocardial infarction is a disease but may sometimes be an injury. This can create coverage issues in the administration of no-fault insurance schemes such as workers' compensation. In general, a heart attack is not covered;[163] however, it may be a work-related injury if it results, for example, from unusual emotional stress or unusual exertion.[164] In addition, in some jurisdictions, heart attacks had by persons in particular occupations such as police officers may be classified as line-of-duty injuries by statute or policy. In some countries or states, a person having had an MI may be prevented from participating in activity that puts other people's lives at risk, for example driving a car or flying an airplane.[165]
Polycystic ovary syndrome, or polycystic ovarian syndrome (PCOS), is the most common endocrine disorder in women of reproductive age.[14] The syndrome is named after cysts which form on the ovaries of some people with this condition, though this is not a universal symptom, and not the underlying cause of the disorder.[15][16]
Women with PCOS may experience irregular menstrual periods, heavy periods, excess hair, acne, pelvic pain, difficulty getting pregnant, and patches of thick, darker, velvety skin.[3] The primary characteristics of this syndrome include: hyperandrogenism, anovulation, insulin resistance, and neuroendocrine disruption.[17]
A review of international evidence found that the prevalence of PCOS could be as high as 26% among some populations, though ranges between 4% and 18% are reported for general populations.[18][19][20]
The exact cause of PCOS remains uncertain, and treatment involves management of symptoms using medication.[19]
Two definitions are commonly used:
In 2003 a consensus workshop sponsored by ESHRE/ASRM in Rotterdam indicated PCOS to be present if any two out of three criteria are met, in the absence of other entities that might cause these findings:[22][23][24]
The Rotterdam definition is wider, including many more women, the most notable ones being women without androgen excess. Critics say that findings obtained from the study of women with androgen excess cannot necessarily be extrapolated to women without androgen excess.[25][26]
Signs and symptoms of PCOS include irregular or no menstrual periods, heavy periods, excess body and facial hair, acne, pelvic pain, difficulty getting pregnant, and patches of thick, darker, velvety skin,[3] ovarian cysts, enlarged ovaries, excess androgen, and weight gain.[27][28]
Associated conditions include type 2 diabetes, obesity, obstructive sleep apnea, heart disease, mood disorders, and endometrial cancer.[4]
Common signs and symptoms of PCOS include the following:
Women with PCOS tend to have central obesity, but studies are conflicting as to whether visceral and subcutaneous abdominal fat is increased, unchanged, or decreased in women with PCOS relative to non-PCOS women with the same body mass index.[37] In any case, androgens, such as testosterone, androstanolone (dihydrotestosterone), and nandrolone decanoate have been found to increase visceral fat deposition in both female animals and women.[38]
Although 80% of PCOS presents in women with obesity, 20% of women diagnosed with the disease are non-obese or "lean" women.[39] However, obese women that have PCOS have a higher risk of adverse outcomes, such as hypertension, insulin resistance, metabolic syndrome, and endometrial hyperplasia.[40]
Even though most women with PCOS are overweight or obese, it is important to acknowledge that non-overweight women can also be diagnosed with PCOS. Up to 30% of women diagnosed with PCOS maintain a normal weight before and after diagnosis. "Lean" women still face the various symptoms of PCOS with the added challenges of having their symptoms properly addressed and recognized. Lean women often go undiagnosed for years, and usually are diagnosed after struggles to conceive.[41] Lean women are likely to have a missed diagnosis of diabetes and cardiovascular disease. These women also have an increased risk of developing insulin resistance, despite not being overweight. Lean women are often taken less seriously with their diagnosis of PCOS, and also face challenges finding appropriate treatment options. This is because most treatment options are limited to approaches of losing weight and healthy dieting.[42]
Testosterone levels are usually elevated in women with PCOS.[43][44] In a 2020 systematic review and meta-analysis of sexual dysfunction related to PCOS which included 5,366 women with PCOS from 21 studies, testosterone levels were analyzed and were found to be 2.34 nmol/L (67 ng/dL) in women with PCOS and 1.57 nmol/L (45 ng/dL) in women without PCOS.[44] In a 1995 study of 1,741 women with PCOS, mean testosterone levels were 2.6 (1.1–4.8) nmol/L (75 (32–140) ng/dL).[45] In a 1998 study which reviewed many studies and subjected them to meta-analysis, testosterone levels in women with PCOS were 62 to 71 ng/dL (2.2–2.5 nmol/L) and testosterone levels in women without PCOS were about 32 ng/dL (1.1 nmol/L).[46] In a 2010 study of 596 women with PCOS which used liquid chromatography–mass spectrometry (LC–MS) to quantify testosterone, median levels of testosterone were 41 and 47 ng/dL (with 25th–75th percentiles of 34–65 ng/dL and 27–58 ng/dL and ranges of 12–184 ng/dL and 1–205 ng/dL) via two different labs.[47] If testosterone levels are above 100 to 200 ng/dL, per different sources, other possible causes of hyperandrogenism, such as congenital adrenal hyperplasia or an androgen-secreting tumor, may be present and should be excluded.[45][48][43]. In women with PCOS, the ratio of Luteinizing Hormone (LH) to Follicle-Stimulating Hormone (FSH) is typically elevated, ranging from 2 to 3, whereas in healthy women, it typically stays within the range of 1 to 2. This imbalance is driven by an increase in luteinizing hormone levels and a decrease in follicle-stimulating hormone levels.[49]
Warning signs may include a change in appearance. But there are also manifestations of mental health problems, such as anxiety, depression, and eating disorders.[27][medical citation needed]
A diagnosis of PCOS suggests an increased risk of the following:
The risk of ovarian cancer and breast cancer is not significantly increased overall.[50]
PCOS is a heterogeneous disorder of uncertain cause.[63][64] There is some evidence that it is a genetic disease. Such evidence includes the familial clustering of cases, greater concordance in monozygotic compared with dizygotic twins and heritability of endocrine and metabolic features of PCOS.[7][63][64] There is some evidence that exposure to higher than typical levels of androgens and the anti-Müllerian hormone (AMH) in utero increases the risk of developing PCOS in later life.[65]
It may be caused by a combination of genetic and environmental factors.[6][7][66] Risk factors include obesity, a lack of physical exercise, and a family history of someone with the condition.[8] Diagnosis is based on two of the following three findings: anovulation, high androgen levels, and ovarian cysts.[4] Cysts may be detectable by ultrasound.[9] Other conditions that produce similar symptoms include adrenal hyperplasia, hypothyroidism, and high blood levels of prolactin.[9]
The genetic component appears to be inherited in an autosomal dominant fashion with high genetic penetrance but variable expressivity in females; this means that each child has a 50% chance of inheriting the predisposing genetic variant(s) from a parent, and, if a daughter receives the variant(s), the daughter will have the disease to some extent.[64][67][68][69] The genetic variant(s) can be inherited from either the father or the mother, and can be passed along to both sons (who may be asymptomatic carriers or may have symptoms such as early baldness and/or excessive hair) and daughters, who will show signs of PCOS.[67][69] The phenotype appears to manifest itself at least partially via heightened androgen levels secreted by ovarian follicle theca cells from women with the allele.[68] The exact gene affected has not yet been identified.[7][64][70] In rare instances, single-gene mutations can give rise to the phenotype of the syndrome.[71] Current understanding of the pathogenesis of the syndrome suggests, however, that it is a complex multigenic disorder.[72]
Due to the scarcity of large-scale screening studies, the prevalence of endometrial abnormalities in PCOS remains unknown, though women with the condition may be at increased risk for endometrial hyperplasia and carcinoma as well as menstrual dysfunction and infertility.
The severity of PCOS symptoms appears to be largely determined by factors such as obesity.[7][22][73] PCOS has some aspects of a metabolic disorder, since its symptoms are partly reversible. Even though considered as a gynecological problem, PCOS consists of 28 clinical symptoms.[74]
Even though the name suggests that the ovaries are central to disease pathology, cysts are a symptom instead of the cause of the disease. Some symptoms of PCOS will persist even if both ovaries are removed; the disease can appear even if cysts are absent. Since its first description by Stein and Leventhal in 1935, the criteria of diagnosis, symptoms, and causative factors are subject to debate. Gynecologists often see it as a gynecological problem, with the ovaries being the primary organ affected. However, recent insights show a multisystem disorder, with the primary problem lies in hormonal regulation in the hypothalamus, with the involvement of many organs. The term PCOS is used due to the fact that there is a wide spectrum of symptoms possible. It is common to have polycystic ovaries without having PCOS; approximately 20% of European women  have polycystic ovaries, but most of those women do not have PCOS.[15]
PCOS may be related to or worsened by exposures[clarification needed] during the prenatal period,[75][76][77] epigenetic factors, environmental impacts (especially industrial endocrine disruptors, such as bisphenol A and certain drugs)[78][79][80] and the increasing rates of obesity.[79]
Endocrine disruptors are defined as chemicals that can interfere with the endocrine system by mimicking hormones such as estrogen. According to the NIH (National Institute of Health), examples of endocrine disruptors can include dioxins and triclosan. Endocrine disruptors can cause adverse health impacts in animals. [81] Additional research is needed to assess the role that endocrine disruptors may play in disrupting reproductive health in women and possibly triggering or exacerbating PCOS and its related symptoms.[82]
Polycystic ovaries develop when the ovaries are stimulated to produce excessive amounts of androgenic hormones, in particular testosterone, by either one or a combination of the following (almost certainly combined with genetic susceptibility):[68]
A majority of women with PCOS have insulin resistance and/or are obese, which is a strong risk factor for insulin resistance, although insulin resistance is a common finding among women with PCOS in normal-weight women as well.[10][22][31] Elevated insulin levels contribute to or cause the abnormalities seen in the hypothalamic–pituitary–ovarian axis that lead to PCOS. Hyperinsulinemia increases GnRH pulse frequency,[83] which in turn results in an increase in the LH/FSH ratio[83][84] increased ovarian androgen production; decreased follicular maturation; and decreased SHBG binding.[83] Furthermore, excessive insulin increases the activity of 17α-hydroxylase, which catalyzes the conversion of progesterone to androstenedione, which is in turn converted to testosterone. The combined effects of hyperinsulinemia contribute to an increased risk of PCOS.[83]
Adipose (fat) tissue possesses aromatase, an enzyme that converts androstenedione to estrone and testosterone to estradiol. The excess of adipose tissue in obese women creates the paradox of having both excess androgens (which are responsible for hirsutism and virilization) and excess estrogens (which inhibit FSH via negative feedback).[85]
The syndrome acquired its most widely used name due to the common sign on ultrasound examination of multiple (poly) ovarian cysts. These "cysts" are in fact immature ovarian follicles. The follicles have developed from primordial follicles, but this development has stopped ("arrested") at an early stage, due to the disturbed ovarian function. The follicles may be oriented along the ovarian periphery, appearing as a 'string of pearls' on ultrasound examination.[86]
PCOS may be associated with chronic inflammation,[87] with several investigators correlating inflammatory mediators with anovulation and other PCOS symptoms.[88][89] Similarly, there seems to be a relation between PCOS and an increased level of oxidative stress.[90]
Not every person with PCOS has polycystic ovaries (PCO), nor does everyone with ovarian cysts have PCOS; although a pelvic ultrasound is a major diagnostic tool, it is not the only one.[91] The diagnosis is fairly straightforward using the Rotterdam criteria, even when the syndrome is associated with a wide range of symptoms.[92]
Transvaginal ultrasound scan of polycystic ovary
Polycystic ovary as seen on sonography
Other causes of irregular or absent menstruation and hirsutism, such as hypothyroidism, congenital adrenal hyperplasia (21-hydroxylase deficiency), Cushing's syndrome, hyperprolactinemia, androgen-secreting neoplasms, and other pituitary or adrenal disorders, should be investigated.[22][24][93]
Some other blood tests are suggestive but not diagnostic. The ratio of LH (luteinizing hormone) to FSH (follicle-stimulating hormone), when measured in international units, is elevated in women with PCOS. Common cut-offs to designate abnormally high LH/FSH ratios are 2:1[100] or 3:1[93] as tested on day 3 of the menstrual cycle. The pattern is not very sensitive; a ratio of 2:1 or higher was present in less than 50% of women with PCOS in one study.[100] There are often low levels of sex hormone-binding globulin,[93] in particular among obese or overweight women.[101]
Anti-Müllerian hormone (AMH) is increased in PCOS, and may become part of its diagnostic criteria.[102][103][104]
PCOS has no cure.[5] Treatment may involve lifestyle changes such as weight loss and exercise.[10][11]
Recent research suggests that daily exercise including both aerobic and strength activities can improve hormone imbalances.[107]
Birth control pills may help with improving the regularity of periods, excess hair growth, and acne.[12] Combined oral contraceptives are especially effective, and used as the first-line of treatment to reduce acne and hirsutism, and regulate menstrual cycle. This is especially the case of adolescents.[107]
Metformin and anti-androgens may also help.[12] Other typical acne treatments and hair removal techniques may be used.[12] Efforts to improve fertility include weight loss, metformin, and ovulation induction using clomiphene or letrozole.[108] In vitro fertilization is used by some in whom other measures are not effective.[108]
Certain cosmetic procedures may also help alleviate symptoms in some cases. For example, the use of laser hair removal, electrolysis, or general waxing, plucking and shaving are all effective methods for reducing hirsutism.[35]
The primary treatments for PCOS include lifestyle changes and use of medications.[109]
Goals of treatment may be considered under four categories:[citation needed]
In each of these areas, there is considerable debate as to the optimal treatment. One of the major factors underlying the debate is the lack of large-scale clinical trials comparing different treatments. Smaller trials tend to be less reliable and hence may produce conflicting results. General interventions that help to reduce weight or insulin resistance can be beneficial for all these aims, because they address what is believed to be the underlying cause.[110] As PCOS appears to cause significant emotional distress, appropriate support may be useful.[111]
Where PCOS is associated with overweight or obesity, successful weight loss is the most effective method of restoring normal ovulation/menstruation. The American Association of Clinical Endocrinologists guidelines recommend a goal of achieving 10–15% weight loss or more, which improves insulin resistance and all[clarification needed] hormonal disorders.[112] Still, many women find it very difficult to achieve and sustain significant weight loss. Insulin resistance itself can cause increased food cravings and lower energy levels, which can make it difficult to lose weight on a regular weight-loss diet. A scientific review in 2013 found similar improvements in weight, body composition and pregnancy rate, menstrual regularity, ovulation, hyperandrogenism, insulin resistance, lipids, and quality of life to occur with weight loss, independent of diet composition.[113] Still, a low GI diet, in which a significant portion of total carbohydrates is obtained from fruit, vegetables, and whole-grain sources, has resulted in greater menstrual regularity than a macronutrient-matched healthy diet.[113]
Reducing intake of food groups that cause inflammation, such as dairy, sugars and simple carbohydrates, can be beneficial.[35]
A mediterranean diet is often very effective due to its anti-inflammatory and anti-oxidative properties.[107]
Vitamin D deficiency may play some role in the development of the metabolic syndrome, and treatment of any such deficiency is indicated.[114][115] However, a systematic review of 2015 found no evidence that vitamin D supplementation reduced or mitigated metabolic and hormonal dysregulations in PCOS.[116] As of 2012, interventions using dietary supplements to correct metabolic deficiencies in people with PCOS had been tested in small, uncontrolled and nonrandomized clinical trials; the resulting data are insufficient to recommend their use.[117]
Medications for PCOS include oral contraceptives and metformin. The oral contraceptives increase sex hormone binding globulin production, which increases binding of free testosterone. This reduces the symptoms of hirsutism caused by high testosterone and regulates return to normal menstrual periods.[114] Anti-androgens such as finasteride, flutamide, spironolactone, and bicalutamide do not show advantages over oral contraceptives, but could be an option for people who do not tolerate them.[118] Finasteride is the only oral medication for the treatment of androgenic alopecia, that is FDA approved.[35]
Metformin is a medication commonly used in type 2 diabetes mellitus to reduce insulin resistance, and is used off label (in the UK, US, AU and EU) to treat insulin resistance seen in PCOS. In many cases, metformin also supports ovarian function and return to normal ovulation.[114][119] A newer insulin resistance medication class, the thiazolidinediones (glitazones), have shown equivalent efficacy to metformin, but metformin has a more favorable side effect profile.[120][121] The United Kingdom's National Institute for Health and Clinical Excellence recommended in 2004 that women with PCOS and a body mass index above 25 be given metformin when other therapy has failed to produce results.[122][123] Metformin may not be effective in every type of PCOS, and therefore there is some disagreement about whether it should be used as a general first line therapy.[124] In addition to this, metformin is associated with several unpleasant side effects: including abdominal pain, metallic taste in the mouth, diarrhoea and vomiting.[125] Metformin is thought to be safe to use during pregnancy (pregnancy category B in the US).[126] A review in 2014 concluded that the use of metformin does not increase the risk of major birth defects in women treated with metformin during the first trimester.[127] Liraglutide may reduce weight and waist circumference in people with PCOS more than other medications.[128] The use of statins in the management of underlying metabolic syndrome remains unclear.[109]
It can be difficult to become pregnant with PCOS because it causes irregular ovulation. Medications to induce fertility when trying to conceive include the ovulation inducer clomiphene or pulsatile leuprorelin.  Evidence from randomised controlled trials suggests that in terms of live birth, metformin may be better than placebo, and metform plus clomiphene may be better than clomiphene alone, but that in both cases women may be more likely to experience gastrointestinal side effects with metformin.[129]
Not all women with PCOS have difficulty becoming pregnant. But some women with PCOS may have difficulty getting pregnant since their body does not produce the hormones necessary for regular ovulation.[130] PCOS might also increase the risk of miscarriage or premature delivery. However, it is possible to have a normal pregnancy. Including medical care and a healthy lifestyle to follow.[citation needed]
For those that do, anovulation or infrequent ovulation is a common cause and PCOS is the main cause of anovulatory infertility.[131] Other factors include changed levels of gonadotropins, hyperandrogenemia, and hyperinsulinemia.[132] Like women without PCOS, women with PCOS that are ovulating may be infertile due to other causes, such as tubal blockages due to a history of sexually transmitted diseases.[133]
For overweight anovulatory women with PCOS, weight loss and diet adjustments, especially to reduce the intake of simple carbohydrates, are associated with resumption of natural ovulation.[134] Digital health interventions have been shown to be particularly effective in providing combined therapy to manage PCOS through both lifestyle changes and medication.[citation needed]
Femara is an alternative medicine that raises FSH levels and promote the development of the follicle.[35]
For those women that after weight loss still are anovulatory or for anovulatory lean women, then ovulation induction using the medications letrozole or clomiphene citrate are the principal treatments used to promote ovulation.[135][136][137] Clomiphene can cause mood swings and abdominal cramping for some.[35]
Previously, the anti-diabetes medication metformin was recommended treatment for anovulation, but it appears less effective than letrozole or clomiphene.[138][139]
For women not responsive to letrozole or clomiphene and diet and lifestyle modification, there are options available including assisted reproductive technology procedures such as controlled ovarian hyperstimulation with follicle-stimulating hormone (FSH) injections followed by in vitro fertilisation (IVF).[140]
Though surgery is not commonly performed, the polycystic ovaries can be treated with a laparoscopic procedure called "ovarian drilling" (puncture of 4–10 small follicles with electrocautery, laser, or biopsy needles), which often results in either resumption of spontaneous ovulations[114] or ovulations after adjuvant treatment with clomiphene or FSH.[141] (Ovarian wedge resection is no longer used as much due to complications such as adhesions and the presence of frequently effective medications.) There are, however, concerns about the long-term effects of ovarian drilling on ovarian function.[114]
Although women with PCOS are far more likely to have depression than women without, the evidence for anti-depressant use in women with PCOS remains inconclusive.[142] However, the pathophysiology of depression and mental stress during PCOS is linked to various changes including psychological changes such as high activity of pro-inflammatory markers and immune system during stress.[143]
PCOS is associated with other mental health related conditions besides depression such as anxiety, bipolar disorder, and obsessive–compulsive disorder.[33]
When appropriate (e.g., in women of child-bearing age who require contraception), a standard contraceptive pill is frequently effective in reducing hirsutism.[114] Progestogens such as norgestrel and levonorgestrel should be avoided due to their androgenic effects.[114] Metformin combined with an oral contraceptive may be more effective than either metformin or the oral contraceptive on its own.[144]
In the case of taking medication for acne, Kelly Morrow-Baez PHD, in her exposition titled Thriving with PCOS, informs that it "takes time for medications to adjust hormone levels, and once those hormone levels are adjusted, it takes more time still for pores to be unclogged of overproduced oil and for any bacterial infections under the skin to clear up before you will see discernible results." (p.138) [35]
Other medications with anti-androgen effects include flutamide,[145] and spironolactone,[114] which can give some improvement in hirsutism. Metformin can reduce hirsutism, perhaps by reducing insulin resistance, and is often used if there are other features such as insulin resistance, diabetes, or obesity that should also benefit from metformin. Eflornithine (Vaniqa) is a medication that is applied to the skin in cream form, and acts directly on the hair follicles to inhibit hair growth. It is usually applied to the face.[114] 5-alpha reductase inhibitors (such as finasteride and dutasteride) may also be used;[146] they work by blocking the conversion of testosterone to dihydrotestosterone (the latter of which responsible for most hair growth alterations and androgenic acne).
Although these agents have shown significant efficacy in clinical trials (for oral contraceptives, in 60–100% of individuals[114]), the reduction in hair growth may not be enough to eliminate the social embarrassment of hirsutism, or the inconvenience of plucking or shaving. Individuals vary in their response to different therapies. It is usually worth trying other medications if one does not work, but medications do not work well for all individuals.[147]
If fertility is not the primary aim, then menstruation can usually be regulated with a contraceptive pill.[114] The purpose of regulating menstruation, in essence, is for the woman's convenience, and perhaps her sense of well-being; there is no medical requirement for regular periods, as long as they occur sufficiently often.[148]
If a regular menstrual cycle is not desired, then therapy for an irregular cycle is not necessarily required. Most experts say that, if a menstrual bleed occurs at least every three months, then the endometrium (womb lining) is being shed sufficiently often to prevent an increased risk of endometrial abnormalities or cancer.[149] If menstruation occurs less often or not at all, some form of progestogen replacement is recommended.[146]
A 2017 review concluded that while both myo-inositol and D-chiro-inositols may regulate menstrual cycles and improve ovulation, there is a lack of evidence regarding effects on the probability of pregnancy.[150][151] A 2012 and 2017 review have found myo-inositol supplementation appears to be effective in improving several of the hormonal disturbances of PCOS.[152][153] Myo-inositol reduces the amount of gonadotropins and the length of controlled ovarian hyperstimulation in women undergoing in vitro fertilization.[154] A 2011 review found not enough evidence to conclude any beneficial effect from D-chiro-inositol.[155] There is insufficient evidence to support the use of acupuncture, current studies are inconclusive and there's a need for additional randomized controlled trials.[156][157]
PCOS is the most common endocrine disorder among women between the ages of 18 and 44.[22] It affects approximately 2% to 20% of this age group depending on how it is defined.[8][13] When someone is infertile due to lack of ovulation, PCOS is the most common cause and could guide to patients' diagnosis.[4] The earliest known description of what is now recognized as PCOS dates from 1721 in Italy.[158]
The prevalence of PCOS depends on the choice of diagnostic criteria. The World Health Organization estimates that it affects 116 million women worldwide as of 2010 (3.4% of women).[159] Another estimate indicates that 7% of women of reproductive age are affected.[160] Another study using the Rotterdam criteria found that about 18% of women had PCOS, and that 70% of them were previously undiagnosed.[22] Prevalence also varies across countries due to lack of large-scale scientific studies; India, for example, has a purported rate of 1 in 5 women having PCOS.[161]
There are few studies that have investigated the racial differences in cardiometabolic factors in women with PCOS. There is also limited data on the racial differences in the risk of metabolic syndrome and cardiovascular disease in adolescents and young adults with PCOS.[162] The first study to comprehensively examine racial differences discovered notable racial differences in risk factors for cardiovascular disease. African American women were found to be significantly more obese, with a significantly higher prevalence of metabolic syndrome compared to white adult women with PCOS.[163] It is important for the further research of racial differences among women with PCOS, to ensure that every woman that is affected by PCOS has the available resources for management.[164][165]
Ultrasonographic findings of polycystic ovaries are found in 8–25% of women non-affected by the syndrome.[166][167][168][169] 14% women on oral contraceptives are found to have polycystic ovaries.[167] Ovarian cysts are also a common side effect of levonorgestrel-releasing intrauterine devices (IUDs).[170]
There are few studies that have investigated the racial differences in cardiometabolic factors in women with PCOS.[171]
The condition was first described in 1935 by American gynecologists Irving F. Stein, Sr. and Michael L. Leventhal, from whom its original name of Stein–Leventhal syndrome is taken.[91][21] Stein and Leventhal first described PCOS as an endocrine disorder in the United States, and since then, it has become recognized as one of the most common causes of oligo ovulatory infertility among women.[50]
The earliest published description of a person with what is now recognized as PCOS was in 1721 in Italy.[158] Cyst-related changes to the ovaries were described in 1844.[158]
Other names for this syndrome include polycystic ovarian syndrome, polycystic ovary disease, functional ovarian hyperandrogenism, ovarian hyperthecosis, sclerocystic ovary syndrome, and Stein–Leventhal syndrome. The eponymous last option is the original name; it is now used, if at all, only for the subset of women with all the symptoms of amenorrhea with infertility, hirsutism, and enlarged polycystic ovaries.[91]
Most common names for this disease derive from a typical finding on medical images, called a polycystic ovary. A polycystic ovary has an abnormally large number of developing eggs visible near its surface, looking like many small cysts.[91]
In 2005, 4 million cases of PCOS were reported in the US, costing $4.36 billion in healthcare costs.[172] In 2016 out of the National Institute Health's research budget of $32.3 billion for that year, 0.1% was spent on PCOS research.[173] Among those aged between 14 and 44, PCOS is conservatively estimated to cost $4.37 billion per year.[23]
As opposed to women in the general population, women with PCOS experience higher rates of depression and anxiety. International guidelines and Indian guidelines suggest psychosocial factors should be considered in women with PCOS, as well as screenings for depression and anxiety.[174] Globally, this aspect has been increasingly focused on because it reflects the true impact of PCOS on the lives of patients. Research shows that PCOS adversely impacts a patient's quality of life.[174]
A number of celebrities and public figures have spoken about their experiences with PCOS, including:

Ovarian cancer is a cancerous tumor of an ovary.[10] It may originate from the ovary itself or more commonly from communicating nearby structures such as fallopian tubes or the inner lining of the abdomen.[3][11] The ovary is made up of three different cell types including epithelial cells, germ cells, and stromal cells.[12] When these cells become abnormal, they have the ability to divide and form tumors. These cells can also invade or spread to other parts of the body.[13] When this process begins, there may be no or only vague symptoms.[1] Symptoms become more noticeable as the cancer progresses.[1][14] These symptoms may include bloating, vaginal bleeding, pelvic pain, abdominal swelling, constipation, and loss of appetite, among others.[1] Common areas to which the cancer may spread include the lining of the abdomen, lymph nodes, lungs, and liver.[15]
The risk of ovarian cancer increases with age. Most cases of ovarian cancer develop after menopause.[16] It is also more common in women who have ovulated more over their lifetime.[17] This includes those who have never had children, those who began ovulation at a younger age and those who reach menopause at an older age.[5] Other risk factors include hormone therapy after menopause, fertility medication, and obesity.[4][6] Factors that decrease risk include hormonal birth control, tubal ligation, pregnancy, and breast feeding.[6] About 10% of cases are related to inherited genetic risk; women with mutations in the genes BRCA1 or BRCA2 have about a 50% chance of developing the disease.[5] Some family cancer syndromes such as hereditary nonpolyposis colon cancer and Peutz-Jeghers syndrome also increase the risk of developing ovarian cancer.[16] Epithelial ovarian carcinoma is the most common type of ovarian cancer, comprising more than 95% of cases.[5] There are five main subtypes of ovarian carcinoma, of which high-grade serous carcinoma (HGSC) is the most common.[5] Less common types of ovarian cancer include germ cell tumors[18] and sex cord stromal tumors.[5] A diagnosis of ovarian cancer is confirmed through a biopsy of tissue, usually removed during surgery.[1]
Screening is not recommended in women who are at average risk, as evidence does not support a reduction in death and the high rate of false positive tests may lead to unneeded surgery, which is accompanied by its own risks.[19] Those at very high risk may have their ovaries removed as a preventive measure.[4] If caught and treated in an early stage, ovarian cancer is often curable.[1] Treatment usually includes some combination of surgery, radiation therapy, and chemotherapy.[1] Outcomes depend on the extent of the disease, the subtype of cancer present, and other medical conditions.[5][20] The overall five-year survival rate in the United States is 49%.[7] Outcomes are worse in the developing world.[5]
In 2020, new cases occurred in approximately 313,000 women.[21] In 2019 it resulted in 13,445 deaths in the United States.[22] Death from ovarian cancer increased globally between 1990 and 2017 by 84.2%.[23] Ovarian cancer is the second-most common gynecologic cancer in the United States. It causes more deaths than any other cancer of the female reproductive system.[24] Among women it ranks fifth in cancer-related deaths.[25] The typical age of diagnosis is 63.[2] Death from ovarian cancer is more common in North America and Europe than in Africa and Asia.[5] In the United States, it is more common in White and Hispanic women than Black or American Indian women.[22]
Early signs and symptoms of ovarian cancer may be absent or subtle. In most cases, symptoms exist for several months before being recognized and diagnosed.[26][27] Symptoms can often be misdiagnosed as irritable bowel syndrome.[28] The early stages of ovarian cancer tend to be painless which makes it difficult to detect it early on. Symptoms can vary based on the subtype.[26] Ovarian borderline tumors, also known as low malignant potential (LMP) ovarian tumors, do not cause an increase in CA125 levels and are not identifiable with an ultrasound. The typical symptoms of an LMP tumor can include abdominal distension or pelvic pain. Particularly large masses tend to be benign or borderline.[29][26]
The most typical symptoms of ovarian cancer include bloating, abdominal or pelvic pain or discomfort, back pain, irregular menstruation or postmenopausal vaginal bleeding, pain or bleeding after or during sexual intercourse, loss of appetite, fatigue, diarrhea, indigestion, heartburn, constipation, nausea, feeling full, and possibly urinary symptoms (including frequent urination and urgent urination).[27]
Later symptoms of ovarian cancer are due to the growing mass causing pain by pressing on other abdominopelvic organs or from metastases.[26][30][31] Because of the anatomic location of the ovaries deep in the pelvis, most masses are large and advanced at the time of diagnosis.[32] The growing mass may cause pain if ovarian torsion develops. If these symptoms start to occur more often or more severely than usual, especially after no significant history of such symptoms, ovarian cancer is considered.[26][29] Metastases may cause a Sister Mary Joseph nodule.[31] Rarely, teratomas can cause growing teratoma syndrome or peritoneal gliomatosis.[31]  Some experience menometrorrhagia and abnormal vaginal bleeding after menopause in most cases. Other common symptoms include hirsutism, abdominal pain, virilization, and an adnexal mass.[33]
In adolescents or children with ovarian tumors, symptoms can include severe abdominal pain, irritation of the peritoneum, or bleeding.[34] Sex cord stromal tumors produce hormones which can lead to the premature development of secondary sex characteristics. Sex cord-stromal tumors in prepubertal children may be manifested by signs of early puberty; abdominal pain and distension are also common. Adolescents with sex cord-stromal tumors may experience amenorrhea. As the cancer becomes more advanced, it can cause an accumulation of fluid in the abdomen and lead to distension. If the malignancy has not been diagnosed by the time it causes ascites, it is typically diagnosed shortly thereafter.[26] Advanced cancers can also cause abdominal masses, lymph node masses, or pleural effusion.[31]
There are many known risk factors that may increase a woman's risk of developing ovarian cancer. The risk of developing ovarian cancer is related to the amount of time a woman spends ovulating.[35] Factors that increase the number of ovulatory cycles a woman undergoes may increase the risk of developing ovarian cancer.[35] During ovulation, cells are stimulated to divide. If this division is abnormally regulated, tumors may form which can be malignant. Early menarche and late menopause increase the number of ovulatory cycles a woman undergoes in her lifetime and so increases the risk of developing ovarian cancer.[29][35][36] Since ovulation is suppressed during pregnancy, not having children also increases the risk of ovarian cancer.[36] Therefore, women who have not borne children are at twice the risk of ovarian cancer than those who have.[26] Both obesity and hormone replacement therapy also raise the risk.[26]
The risk of developing ovarian cancer is less for women who have fewer menstrual cycles, no menstrual cycles, breast feeding, take oral contraceptives, have multiple pregnancies, and have a pregnancy at an early age.  The risk of developing ovarian cancer is reduced in women who have had tubal ligation (colloquially known as having one's "tubes tied"), both ovaries removed, or hysterectomy (an operation in which the uterus is removed).[27] Age is also a risk factor.[26][20] Non-genetic factors such as diabetes mellitus, high body mass index, tobacco, and alcohol use are also risk factors for ovarian cancer.[23]
The use of fertility medication may contribute to ovarian borderline tumor formation, but the link between the two is disputed and difficult to study.[28] Fertility drugs may be associated with a higher risk of borderline tumors.[31] Those who have been treated for infertility but remain nulliparous are at higher risk for epithelial ovarian cancer due to hormonal exposure that may lead to proliferation of cells. However, those who are successfully treated for infertility and subsequently give birth are at no higher risk. This may be due to shedding of precancerous cells during pregnancy, but the cause remains unclear.[29] The risk factor may instead be infertility itself, not the treatment.[35]
Hormonal conditions such as polycystic ovary syndrome and endometriosis are associated with ovarian cancer, but the link is not completely confirmed.[28] Postmenopausal hormone replacement therapy (HRT) with estrogen likely increases the risk of ovarian cancer. The association has not been confirmed in a large-scale study,[29][37] but notable studies including the Million Women Study have supported this link. Postmenopausal HRT with combined estrogen and progesterone may increase contemporaneous risk if used for over 5 years, but this risk returns to normal after cessation of therapy.[35] Estrogen HRT with or without progestins increases the risk of endometrioid and serous tumors but lowers the risk of mucinous tumors. Higher doses of estrogen increase this risk.[31] Endometriosis is another risk factor for ovarian cancer,[35] as is pain with menstruation. Endometriosis is associated with clear-cell and endometrioid subtypes, low-grade serous tumors, stage I and II tumors, grade 1 tumors, and lower mortality.[31]
Before menopause, obesity can increase a person's risk of ovarian cancer, but this risk is not present after menopause. This risk is also relevant in those who are both obese and have never used HRT. A similar association with ovarian cancer appears in taller women.[35]
A family history of ovarian cancer is a risk factor for ovarian cancer. Women with hereditary nonpolyposis colon cancer (Lynch syndrome), and those with BRCA-1 and BRCA-2 genetic abnormalities are at increased risk.
The major genetic risk factor for ovarian cancer is a mutation in BRCA1 or BRCA2 genes, or in DNA mismatch repair genes, which is present in 10% of ovarian cancer cases. Only one allele needs to be mutated to place a person at high risk. The gene can be inherited through either the maternal or paternal line, but has variable penetrance.[26][29] Though mutations in these genes are usually associated with increased risk of breast cancer, they also carry a substantial lifetime risk of ovarian cancer, a risk that peaks in a person's 40s and 50s. The lowest risk cited is 30% and the highest 60%.[28][26][29] Mutations in BRCA1 have a lifetime risk of developing ovarian cancer of 15–45%.[31] Mutations in BRCA2 are less risky than those with BRCA1, with a lifetime risk of 10% (lowest risk cited) to 40% (highest risk cited).[26][31] On average, BRCA-associated cancers develop 15 years before their sporadic counterparts because people who inherit the mutations on one copy of their gene only need one mutation to start the process of carcinogenesis, whereas people with two normal genes would need to acquire two mutations.[29]
In the United States, five of 100 women with a first-degree relative with ovarian cancer will eventually get ovarian cancer themselves, placing those with affected family members at triple the risk of women with unaffected family members. Seven of 100 women with two or more relatives with ovarian cancer will eventually get ovarian cancer.[29][38] In general, 5–10% of ovarian cancer cases have a genetic cause.[29] BRCA mutations are associated with high-grade serous nonmucinous epithelial ovarian cancer.[31]
A strong family history of endometrial cancer, colon cancer, or other gastrointestinal cancers may indicate the presence of a syndrome known as hereditary nonpolyposis colorectal cancer (also known as Lynch syndrome), which confers a higher risk for developing a number of cancers, including ovarian cancer. Lynch syndrome is caused by mutations in mismatch repair genes, including MSH2, MLH1, MLH6, PMS1, and PMS2.[26] The risk of ovarian cancer for an individual with Lynch syndrome is between 10 and 12 percent.[26][29] Women of Icelandic descent, European Jewish descent/Ashkenazi Jewish descent, and Hungarian descent are at higher risk for epithelial ovarian cancer.[29] Estrogen receptor beta gene (ESR2) seems to be a key to pathogenesis and response to therapy.[39] Other genes that have been associated with ovarian cancer are BRIP1, MSH6, RAD51C and RAD51D.[40] CDH1, CHEK2, PALB2 and RAD50 have also been associated with ovarian cancer.[41]
Several rare genetic disorders are associated with specific subtypes of ovarian cancer. Peutz–Jeghers syndrome, a rare genetic disorder, also predisposes women to sex cord tumour with annular tubules.[28][26] Ollier disease and Maffucci syndrome are associated with granulosa cell tumors in children and may also be associated with Sertoli-Leydig tumors. Benign fibromas are associated with nevoid basal cell carcinoma syndrome.[26]
Alcohol consumption does not appear to be related to ovarian cancer.[31][42]
The American Cancer Society recommends a healthy eating pattern that includes plenty of fruits, vegetables, whole grains, and a diet that avoids or limits red and processed meats and processed sugar.[43] High consumption of total, saturated and trans-fats increases ovarian cancer risk.[44] A 2021 umbrella review found that coffee, egg, and fat intake significantly increases the risk of ovarian cancer.[45] There is mixed evidence from studies on ovarian cancer risk and consumption of dairy products.[46][47]
Industrialized nations, with the exception of Japan, have high rates of epithelial ovarian cancer, which may be due to diet in those countries. White women are at a 30–40% higher risk for ovarian cancer when compared to Black women and Hispanic women, likely due to socioeconomic factors; white women tend to have fewer children and different rates of gynecologic surgeries that affect risk for ovarian cancer.[29]
Tentative evidence suggests that talc, pesticides, and herbicides increase the risk of ovarian cancer.[48] The American Cancer Society notes that as of now, no study has been able to accurately link any single chemical in the environment, or in the human diet, directly to mutations that cause ovarian cancer.[49]
Other factors that have been investigated, such as smoking, low levels of vitamin D in the blood,[50] presence of inclusion ovarian cysts, and infection with human papilloma virus (the cause of some cases of cervical cancer), have been disproven as risk factors for ovarian cancer.[28][31] The carcinogenicity of perineal talc is controversial, because it can act as an irritant if it travels through the reproductive tract to the ovaries.[31][29][35] Case-control studies have shown that use of perineal talc does increase the risk of ovarian cancer, but using talc more often does not create a greater risk.[31] Use of talc elsewhere on the body is unrelated to ovarian cancer.[35] Sitting regularly for prolonged periods is associated with higher mortality from epithelial ovarian cancer. The risk is not negated by regular exercise, though it is lowered.[51]
Increased age (up to the 70s) is a risk factor for epithelial ovarian cancer because more mutations in cells can accumulate and eventually cause cancer. Those over 80 are at slightly lower risk.[29]
Smoking tobacco is associated with a higher risk of mucinous ovarian cancer; after smoking cessation, the risk eventually returns to normal. Higher levels of C-reactive protein are associated with a higher risk of developing ovarian cancer.[31]
Suppression of ovulation, which would otherwise cause damage to the ovarian epithelium and, consequently, inflammation, is generally protective. This effect can be achieved by having children, taking combined oral contraceptives, and breast feeding, all of which are protective factors.[26] A longer period of breastfeeding correlates with a larger decrease in the risk of ovarian cancer.[35] Each birth decreases risk of ovarian cancer more, and this effect is seen with up to five births. Combined oral contraceptives reduce the risk of ovarian cancer by up to 50%, and the protective effect of combined oral contraceptives can last 25–30 years after they are discontinued.[29][35] Regular use of aspirin or acetaminophen (paracetamol) may be associated with a lower risk of ovarian cancer; other NSAIDs do not seem to have a similar protective effect.[31]
Tubal ligation is protective because carcinogens are unable to reach the ovary and fimbriae via the vagina, uterus, and Fallopian tubes.[26] Tubal ligation is also protective in women with the BRCA1 mutation, but not the BRCA2 mutation.[31] Hysterectomy reduces the risk, and removal of both Fallopian tubes and ovaries (bilateral salpingo-oophorectomy) dramatically reduces the risk of not only ovarian cancer but breast cancer as well.[28] This is still a topic of research, as the link between hysterectomy and lower ovarian cancer risk is controversial. The reasons that hysterectomy may be protective have not been elucidated as of 2015.[35]
A diet that includes large amounts of carotene, fiber, and vitamins with low amounts of fat—specifically, a diet with non-starchy vegetables (e.g. broccoli and onions) may be protective.[29] Dietary fiber is associated with a significant reduced risk of ovarian cancer.[52] A 2021 review found that green leafy vegetables, allium vegetables, fiber, flavanoids and green tea intake can significantly reduce ovarian cancer risk.[53]
Ovarian cancer forms when errors in normal ovarian cell growth occur. Usually, when cells grow old or get damaged, they die, and new cells take their place. Cancer starts when new cells form unneeded, and old or damaged cells do not die as they should. The buildup of extra cells often forms a mass of tissue called an ovarian tumor or growth. These abnormal cancer cells have many genetic abnormalities that cause them to grow excessively.[55] When an ovary releases an egg, the egg follicle bursts open and becomes the corpus luteum. This structure needs to be repaired by dividing cells in the ovary.[35] Continuous ovulation for a long time means more repair of the ovary by dividing cells, which can acquire mutations in each division.[29]
Overall, the most common gene mutations in ovarian cancer occur in NF1, BRCA1, BRCA2, and CDK12. Type I ovarian cancers, which tend to be less aggressive, tend to have microsatellite instability in several genes, including both oncogenes (most notably BRAF  and KRAS) and tumor suppressors (most notably PTEN).[28] The most common mutations in Type I cancers are KRAS, BRAF, ERBB2, PTEN, PIK3CA, and ARID1A.[31] Type II cancers, the more aggressive type, have different genes mutated, including p53, BRCA1, and BRCA2.[28] Low-grade cancers tend to have mutations in KRAS, whereas cancers of any grade that develop from low malignant potential tumors tend to have mutations in p53.[29] Type I cancers tend to develop from precursor lesions, whereas Type II cancers can develop from a serous tubal intraepithelial carcinoma.[31] Serous cancers that have BRCA mutations also inevitably have p53 mutations, indicating that the removal of both functional genes is important for cancer to develop.[29]
In 50% of high-grade serous cancers, homologous recombination DNA repair is dysfunctional, as are the notch and FOXM1 signaling pathways. They also almost always have p53 mutations. Other than this, mutations in high-grade serous carcinoma are hard to characterize beyond their high degree of genomic instability. BRCA1 and BRCA2 are essential for homologous recombination DNA repair, and germline mutations in these genes are found in about 15% of women with ovarian cancer.[28] The most common mutations in BRCA1 and BRCA2 are the frameshift mutations that originated in a small founding population of Ashkenazi Jews.[29]
Almost 100% of rare mucinous carcinomas have mutations in KRAS and amplifications of ERBB2 (also known as Her2/neu).[28] Overall, 20% of ovarian cancers have mutations in Her2/neu.[26]
Serous carcinomas may develop from serous tubal intraepithelial carcinoma, rather than developing spontaneously from ovarian tissue. Other carcinomas develop from cortical inclusion cysts, which are groups of epithelial ovarian cells inside the stroma.[29]
Diagnosis of ovarian cancer starts with a physical examination (including a pelvic examination), a blood test (for CA-125 and sometimes other markers), and transvaginal ultrasound.[26][56] Sometimes a rectovaginal examination is used to help plan a surgery.[29] The diagnosis must be confirmed with surgery to inspect the abdominal cavity, take biopsies (tissue samples for microscopic analysis), and look for cancer cells in the abdominal fluid. This helps to determine if an ovarian mass is benign or malignant.[26]
Ovarian cancer's early stages (I/II) are difficult to diagnose because most symptoms are nonspecific and thus of little use in diagnosis; as a result, it is rarely diagnosed until it spreads and advances to later stages (III/IV).[57] Additionally, symptoms of ovarian cancer may appear similar to irritable bowel syndrome. In women in whom pregnancy is a possibility, BHCG level can be measured during the diagnosis process. Serum alpha-fetoprotein, neuron-specific enolase, and lactate dehydrogenase can be measured in young girls and adolescents with suspected ovarian tumors as younger women with ovarian cancer are more likely to have malignant germ cell tumors.[26][31]
A physical examination, including a pelvic examination, and a pelvic ultrasound (transvaginal or otherwise) are both essential for diagnosis: physical examination may reveal increased abdominal girth and/or ascites (fluid within the abdominal cavity), while pelvic examination may reveal an ovarian or abdominal mass.[28] An adnexal mass is a significant finding that often indicates ovarian cancer, especially if it is fixed, nodular, irregular, solid, and/or bilateral. 13–21% of adnexal masses are caused by malignancy; however, there are other benign causes of adnexal masses, including ovarian follicular cyst, leiomyoma, endometriosis, ectopic pregnancy, hydrosalpinx, tuboovarian abscess, ovarian torsion, dermoid cyst, cystadenoma (serous or mucinous), diverticular or appendiceal abscess, nerve sheath tumor, pelvic kidney, ureteral or bladder diverticulum, benign cystic mesothelioma of the peritoneum, peritoneal tuberculosis, or paraovarian cyst. Ovaries that can be felt are also a sign of ovarian cancer in postmenopausal women. Other parts of a physical examination for suspected ovarian cancer can include a breast examination and a digital rectal exam. Palpation of the supraclavicular, axillary, and inguinal lymph nodes may reveal lymphadenopathy, which can be indicative of metastasis. Another indicator may be the presence of a pleural effusion, which can be noted on auscultation.[31]
When an ovarian malignancy is included in a list of diagnostic possibilities, a limited number of laboratory tests are indicated. A complete blood count and serum electrolyte test is usually obtained;[58] when an ovarian cancer is present, these tests often show a high number of platelets (20–25% of patients) and low blood sodium levels due to chemical signals secreted by the tumor.[29] A positive test for inhibin A and inhibin B can indicate a granulosa cell tumor.[31]
A blood test for a marker molecule called CA-125 is useful in differential diagnosis and in follow up of the disease, but it by itself has not been shown to be an effective method to screen for early-stage ovarian cancer due to its unacceptable low sensitivity and specificity.[58] CA-125 levels in premenopausal women over 200 U/mL may indicate ovarian cancer, as may any elevation in CA-125 above 35 U/mL in post-menopausal women. CA-125 levels are not accurate in early stage ovarian cancer, as half of stage I ovarian cancer patients have a normal CA-125 level.[31][29] CA-125 may also be elevated in benign (non-cancerous) conditions, including endometriosis, pregnancy, uterine fibroids, menstruation, ovarian cysts, systemic lupus erythematosus, liver disease, inflammatory bowel disease, pelvic inflammatory disease, and leiomyoma.[31][59] HE4 is another candidate for ovarian cancer testing, though it has not been extensively tested. Other tumor markers for ovarian cancer include CA19-9, CA72-4, CA15-3, immunosuppressive acidic protein, haptoglobin-alpha, OVX1, mesothelin, lysophosphatidic acid, osteopontin, and fibroblast growth factor 23.[31]
Use of blood test panels may help in diagnosis.[31][58] The OVA1 panel includes CA-125, beta-2 microglobulin, transferrin, apolipoprotein A1, and transthyretin. OVA1 above 5.0 in premenopausal women and 4.4 in postmenopausal women indicates a high risk for cancer.[29] A different set of laboratory tests is used for detecting sex cord-stromal tumors. High levels of testosterone or dehydroepiandrosterone sulfate, combined with other symptoms and high levels of inhibin A and inhibin B can be indicative of an SCST of any type.[33]
Current research is looking at ways to consider tumor marker proteomics in combination with other indicators of disease (i.e. radiology and/or symptoms) to improve diagnostic accuracy. The challenge in such an approach is that the disparate prevalence of ovarian cancer means that even testing with very high sensitivity and specificity will still lead to a number of false positive results, which in turn may lead to issues such as performing surgical procedures in which cancer is not found intraoperatively.[citation needed] Genomics approaches have not yet been developed for ovarian cancer.[31]
CT scanning is preferred to assess the extent of the tumor in the abdominopelvic cavity, though magnetic resonance imaging can also be used.[28] CT scanning can also be useful for finding omental caking or differentiating fluid from solid tumor in the abdomen, especially in low malignant potential tumors. However, it may not detect smaller tumors. Sometimes, a chest x-ray is used to detect metastases in the chest or pleural effusion. Another test for metastatic disease, though it is infrequently used, is a barium enema, which can show if the rectosigmoid colon is involved in the disease. Positron emission tomography, bone scans, and paracentesis are of limited use; in fact, paracentesis can cause metastases to form at the needle insertion site and may not provide useful results.[29] However, paracentesis can be used in cases where there is no pelvic mass and ascites is still present.[29] A physician suspecting ovarian cancer may also perform mammography or an endometrial biopsy (in the case of abnormal bleeding) to assess the possibility of breast malignancies and endometrial malignancy, respectively. Vaginal ultrasonography is often the first-line imaging study performed when an adnexal mass is found. Several characteristics of an adnexal mass indicate ovarian malignancy; they usually are solid, irregular, multilocular, and/or large; and they typically have papillary features, central vessels, and/or irregular internal septations.[31] However, SCST has no definitive characteristics on radiographic study.[33]
To definitively diagnose ovarian cancer, a surgical procedure to inspect the abdomen is required. This can be an open procedure (laparotomy, incision through the abdominal wall) or keyhole surgery (laparoscopy). During this procedure, suspicious tissue is removed and sent for microscopic analysis. Usually, this includes a unilateral salpingo-oophorectomy, removal of a single affected ovary and Fallopian tube. Fluid from the abdominal cavity can also be analyzed for cancerous cells. If cancer is found, this procedure can also be used to determine the extent of its spread (which is a form of tumor staging).[26]
Pafolacianine is indicated for use in adults with ovarian cancer to help identify cancerous lesions during surgery.[60] It is a diagnostic agent that is administered in the form of an intravenous injection prior to surgery.[60]
A widely recognized method of estimating the risk of malignant ovarian cancer is the risk of malignancy index (RMI), calculated based on an initial workup.[28][61] An RMI score of over 200 or 250 is generally felt to indicate high risk for ovarian cancer.[28][31]
The RMI is calculated as:
Two methods can be used to determine the ultrasound score and menopausal score, with the resultant scores being referred to as RMI 1 and RMI 2, respectively, depending on what method is used.
Ultrasound abnormalities:
Another method for quantifying risk of ovarian cancer is the Risk of Ovarian Cancer Algorithm (ROCA), which observes levels over time and determines if they are increasing rapidly enough to warrant transvaginal ultrasound.[29] The Risk of Ovarian Malignancy algorithm uses CA-125 levels and HE4 levels to calculate the risk of ovarian cancer; it may be more effective than RMI. The IOTA models can be used to estimate the probability that an adnexal tumor is malignant.[63] They include LR2 risk model, The Simple Rules risk (SRrisk) calculation and Assessment of Different Neoplasias in the Adnexa (ADNEX) model that can be used to assess risk of malignancy in an adnexal mass, based on its characteristics and risk factors. The QCancer (Ovary) algorithm is used to predict likelihood of ovarian cancer from risk factors.[31]
Ovarian cancers are classified according to the microscopic appearance of their structures (histology or histopathology). Histology dictates many aspects of clinical treatment, management, and prognosis. The gross pathology of ovarian cancers is very similar regardless of histologic type: ovarian tumors have solid and cystic masses.[29] According to SEER, the types of ovarian cancers in women age 20 and over are:[64]
subdivision
Ovarian cancers are histologically and genetically divided into type I or type II. Type I cancers are of low histological grade and include endometrioid, mucinous, and clear-cell carcinomas. Type II cancers are of higher histological grade and include serous carcinoma and carcinosarcoma.[28]
Epithelial ovarian cancer typically presents at an advanced stage and is derived from the malignant transformation of the epithelium of the ovarian surface, peritoneum, or fallopian tube.[65] It is the most common cause of gynecologic cancer death.[65] There are various types of epithelial ovarian cancer, including serous tumor, endometrioid tumor, clear-cell tumor, mucinous tumor, and undifferentiated or unclassified tumors.[66] Annually worldwide, 230,000 women will be diagnosed and 150,000 will die.[67] It has a 46% 5 year survival rate after diagnosis because of the advanced stage of the disease at the time of diagnosis.[67] Typically, around 75% of patients are diagnosed as having an advanced stage of the disease because of the asymptomatic nature of its presentation.[67] There is a genomic predisposition to epithelial ovarian cancer and the BRCA1 and BRCA2 genes have been found to be the causative genes in 65-75% of hereditary epithelial ovarian cancer.[67]
Serous ovarian cancer is the most common type of epithelial ovarian cancer and it accounts for about two-thirds of cases of epithelial ovarian cancer.[28] Low-grade serous carcinoma is less aggressive than high-grade serous carcinomas, though it does not typically respond well to chemotherapy or hormonal treatments.[28] Serous carcinomas are thought to begin in the Fallopian tube.[68][69] High grade serous carcinoma accounts for 75% of all epithelial ovarian cancer.[67] About 15–20% of high grade serous carcinoma have germline BRCA1 and BRCA2 mutations.[67] Histologically, the growth pattern of high grade serous carcinoma is heterogenous and has some papillary or solid growth patterns.[67] The tumor cells are atypical with large, irregular nuclei.[67] It has a high proliferation rate.[67] 50% of the time, serous carcinomas are bilateral, and in 85% of cases, they have spread beyond the ovary at the time of diagnosis.[70]
Serous Tubal Intraepithelial Carcinoma (STIC) is now recognized to be the precursor lesion of most so-called ovarian high-grade serous carcinomas.[70]
STIC is characterised by
Small-cell ovarian carcinoma is rare and aggressive, with two main subtypes: hypercalcemic and pulmonary.[71] This rare malignancy most commonly affects young women under the age of 40 years old with a range between 14 months and 58 years.[71] The mean age of diagnosis of 24 years.[71] Approximately two-thirds of patients will present with paraneoplastic hypercalcemia meaning they have high blood calcium levels for an unknown reason.[71][72]  The tumor secretes Parathyroid hormone related protein which acts similarly to PTH and binds PTH receptors in the bone and kidney causing hypercalcemia.[71] Recent research has found an inactivating germline and somatic mutation of SMARCA4 gene.[71][73] The hypercalcemic subtype is very aggressive and has an overall survival rate of 16% with a recurrence rate of 65% in patients who receive treatment.[71] Patients who have spread of the disease to other parts of the body tend to die 2 years after the diagnosis.[71] Extra-ovarian spread is involved in 50% of cases and lymph node spread is present in 55% of cases.[72] The most common initial presentation is a rapidly growing unilateral pelvic mass with a mean size of 15 cm.[71] Histologically, it is characterized by many sheets of small, round, tightly packed cells with clusters, nests, and cords.[71][72] Immunohistochemistry is typically positive for vimentin, cytokeratin, CD10, p53, and WT-1.[71][73]
Small cell ovarian carcinoma of the pulmonary subtype presents differently from the hypercalcemic subtype.[71] Typically, pulmonary small cell ovarian cancer usually affects both ovaries of older women and looks like oat-cell carcinoma of the lung.[29] The average age of disease onset is 59 years old and approximately 45% of cases are bilateral for the pulmonary subtype.[71] Additionally, several hormones can be elevated in the pulmonary subtype including serotonin, somatostatin, insulin, gastrin, and calcitonin.[71]
Primary peritoneal carcinomas develop from the peritoneum, a membrane that covers the abdominal cavity that has the same embryonic origin as the ovary. They are often discussed and classified with ovarian cancers when they affect the ovary.[68][74] They can develop even after the ovaries have been removed and may appear similar to mesothelioma.[29]
Ovarian clear-cell carcinoma is a rare subtype of epithelial ovarian cancer. Those diagnosed with ovarian clear-cell carcinoma are typically younger at the age of diagnosis and diagnosed at earlier stages than other subtypes of epithelial ovarian cancer.[75][76] The highest incidence of clear-cell carcinoma of the ovary have been observed among young Asian women, especially those of Korean, Taiwanese, and Japanese background.[75][76] Endometriosis has been linked to being the main risk factor for the development of clear-cell carcinoma of the ovary and has been found to be present in 50% of women diagnosed with clear-cell carcinoma of the ovary.[75] The development of clots in the legs such as deep vein thromboembolism or in the lungs with pulmonary embolism is reported to be 40% higher in patients with clear-cell carcinoma than other epithelial ovarian cancer subtypes.[76] Mutations in molecular pathways such as ARID1A, PIK3, and PIK3CA have been found to be linked to clear-cell carcinoma.[75][76] They typically present as a large, unilateral mass, with a mean size between 13 and 15 cm.[75] 90% of cases are unilateral.[75] Ovarian clear-cell carcinoma does not typically respond well to chemotherapy due to intrinsic chemoresistance, therefore treatment is typically with aggressive cytoreductive surgery and platinum-based chemotherapy.[28][75]
Clear-cell adenocarcinomas are histopathologically similar to other clear-cell carcinomas, with clear cells and hobnail cells. They represent approximately 5–10% of epithelial ovarian cancers and are associated with endometriosis in the pelvic cavity. They are typically early-stage and therefore curable by surgery, but advanced clear-cell adenocarcinomas (approximately 20%) have a poor prognosis and are often resistant to platinum chemotherapy.[29]
Endometrioid adenocarcinomas make up approximately 13-15% of all ovarian cancers.[77] Because they are typically low-grade, endometrioid adenocarcinomas have a good prognosis.[77] The median age of diagnosis is around 53 years of age.[77] These tumors frequently co-occur with endometriosis or endometrial cancer.[29][77] Cancer antigen 125 levels are typically elevated and a family history of a first degree relative with endometrioid ovarian cancer is associated with increased risk of developing endometrioid ovarian cancer.[77] The average tumor size is larger than 10 cm.[77]
Mixed müllerian tumors make up less than 1% of ovarian cancer. They have epithelial and mesenchymal cells visible and tend to have a poor prognosis.[29]
Mucinous tumors include mucinous adenocarcinoma and mucinous cystadenocarcinoma.[29]
Mucinous adenocarcinomas make up 5–10% of epithelial ovarian cancers. Histologically, they are similar to intestinal or cervical adenocarcinomas and are often actually metastases of appendiceal or colon cancers. Advanced mucinous adenocarcinomas have a poor prognosis, generally worse than serous tumors, and are often resistant to platinum chemotherapy, though they are rare.[29]
Pseudomyxoma peritonei refers to a collection of encapsulated mucous or gelatinous material in the abdominopelvic cavity, which is very rarely caused by a primary mucinous ovarian tumor. More commonly, it is associated with ovarian metastases of intestinal cancer.[29]
Undifferentiated cancers - those where the cell type cannot be determined - make up about 10% of epithelial ovarian cancers and have a comparatively poor prognosis.[29][68] When examined under the microscope, these tumors have very abnormal cells that are arranged in clumps or sheets. Usually there are recognizable clumps of serous cells inside the tumor.[29]
Malignant Brenner tumors are rare. Histologically, they have dense fibrous stroma with areas of transitional epithelium and some squamous differentiation. To be classified as a malignant Brenner tumor, it must have Brenner tumor foci and transitional cell carcinoma. The transitional cell carcinoma component is typically poorly differentiated and resembles urinary tract cancer.[29]
Transitional cell carcinomas represent less than 5% of ovarian cancers. Histologically, they appear similar to bladder carcinoma. The prognosis is intermediate - better than most epithelial cancers but worse than malignant Brenner tumors.[29]
Sex cord-stromal tumor, including estrogen-producing granulosa cell tumor, the benign thecoma, and virilizing Sertoli-Leydig cell tumor or arrhenoblastoma, accounts for 7% of ovarian cancers. They occur most frequently in women between 50 and 69 years of age but can occur in women of any age, including young girls. They are not typically aggressive and are usually unilateral;[26] they are therefore usually treated with surgery alone. Sex cord-stromal tumors are the main hormone-producing ovarian tumors.[33]
Several different cells from the mesenchyme can give rise to sex-cord or stromal tumors. These include fibroblasts and endocrine cells. The symptoms of a sex-cord or stromal ovarian tumor can differ from other types of ovarian cancer. Common signs and symptoms include ovarian torsion, hemorrhage from or rupture of the tumor, an abdominal mass, and hormonal disruption. In children, isosexual precocious pseudopuberty may occur with granulosa cell tumors since they produce estrogen. These tumors cause abnormalities in menstruation (excessive bleeding, infrequent menstruation, or no menstruation) or postmenopausal bleeding. Because these tumors produce estrogen, they can cause or occur at the same time as endometrial cancer or breast cancer. Other sex-cord/stromal tumors present with distinct symptoms. Sertoli-Leydig cell tumors cause virilization and excessive hair growth due to the production of testosterone and androstenedione, which can also cause Cushing's syndrome in rare cases. Also, sex-cord stromal tumors occur that do not cause a hormonal imbalance, including benign fibromas, which cause ascites and hydrothorax.[26] With germ cell tumors, sex cord-stromal tumors are the most common ovarian cancer diagnosed in women under 20.[33]
Granulosa cell tumors are the most common sex-cord stromal tumors, making up 70% of cases, and are divided into two histologic subtypes: adult granulosa cell tumors, which develop in women over 50, and juvenile granulosa tumors, which develop before puberty or before the age of 30. Both develop in the ovarian follicle from a population of cells that surrounds germinal cells.[33]
Adult granulosa cell tumors are characterized by later onset (30+ years, 50 on average). These tumors produce high levels of estrogen, which causes its characteristic symptoms: menometrorrhagia; endometrial hyperplasia; tender, enlarged breasts; postmenopausal bleeding; and secondary amenorrhea. The mass of the tumor can cause other symptoms, including abdominal pain and distension, or symptoms similar to an ectopic pregnancy if the tumor bleeds and ruptures.[33]
Sertoli-Leydig tumors are most common in women before the age of 30, and particularly common before puberty.[33]
Sclerosing stromal tumors typically occur in girls before puberty or women before the age of 30.[33]
Germ cell tumors of the ovary develop from the ovarian germ cells.[68] Germ cell tumor accounts for about 30% of ovarian tumors, but only 5% of ovarian cancers, because most germ-cell tumors are teratomas and most teratomas are benign. Malignant teratomas tend to occur in older women, when one of the germ layers in the tumor develops into a squamous cell carcinoma.[26] Germ-cell tumors tend to occur in young women (20s–30s) and girls, making up 70% of the ovarian cancer seen in that age group.[34] Germ-cell tumors can include dysgerminomas, teratomas, yolk sac tumors/endodermal sinus tumors, and choriocarcinomas, when they arise in the ovary. Some germ-cell tumors have an isochromosome 12, where one arm of chromosome 12 is deleted and replaced with a duplicate of the other.[26] Most germ-cell cancers have a better prognosis than other subtypes and are more sensitive to chemotherapy. They are more likely to be stage I at diagnosis.[33] Overall, they metastasize more frequently than epithelial ovarian cancers. In addition, the cancer markers used vary with tumor type: choriocarcinomas are monitored with beta-HCG and endodermal sinus tumors with alpha-fetoprotein.[26]
Germ-cell tumors are typically discovered when they become large, palpable masses. However, like sex cord tumors, they can cause ovarian torsion or hemorrhage and, in children, isosexual precocious puberty. They frequently metastasize to nearby lymph nodes, especially para-aortic and pelvic lymph nodes.[26] The most common symptom of germ cell tumors is subacute abdominal pain caused by the tumor bleeding, necrotizing, or stretching the ovarian capsule. If the tumor ruptures, causes significant bleeding, or torses the ovary, it can cause acute abdominal pain, which occurs in less than 10% of those with germ-cell tumors. They can also secrete hormones which change the menstrual cycle. In 25% of germ-cell tumors, the cancer is discovered during a routine examination and does not cause symptoms.[33]
Diagnosing germ cell tumors may be difficult because the normal menstrual cycle and puberty can cause pain and pelvic symptoms, and a young woman may even believe these symptoms to be those of pregnancy, and not seek treatment due to the stigma of teen pregnancy. Blood tests for alpha-fetoprotein, karyotype, human chorionic gonadotropin, and liver function are used to diagnose germ cell tumor and potential co-occurring gonadal dysgenesis. A germ cell tumor may be initially mistaken for a benign ovarian cyst.[33]
Dysgerminoma accounts for 35% of ovarian cancer in young women and is the most likely germ cell tumor to metastasize to the lymph nodes; nodal metastases occur in 25–30% of cases.[34][33] These tumors may have mutations in the KIT gene, a mutation known for its role in gastrointestinal stromal tumor. People with an XY karyotype and ovaries (gonadal dysgenesis) or an X,0 karyotype and ovaries (Turner syndrome) who develop a unilateral dysgerminoma are at risk for a gonadoblastoma in the other ovary, and in this case, both ovaries are usually removed when a unilateral dysgerminoma is discovered to avoid the risk of another malignant tumor. Gonadoblastomas in people with Swyer or Turner syndrome become malignant in approximately 40% of cases. However, in general, dysgerminomas are bilateral 10–20% of the time.[26][33]
They are composed of cells that cannot differentiate further and develop directly from germ cells or from gonadoblastomas. Dysgerminomas contain syncytiotrophoblasts in approximately 5% of cases, and can therefore cause elevated hCG levels. On gross appearance, dysgerminomas are typically pink to tan-colored, have multiple lobes, and are solid. Microscopically, they appear identical to seminomas and very close to embryonic primordial germ cells, having large, polyhedral, rounded clear cells. The nuclei are uniform and round or square with prominent nucleoli and the cytoplasm has high levels of glycogen. Inflammation is another prominent histologic feature of dysgerminomas.[33]
Choriocarcinoma can occur as a primary ovarian tumor developing from a germ cell, though it is usually a gestational disease that metastasizes to the ovary. Primary ovarian choriocarcinoma has a poor prognosis and can occur without a pregnancy. They produce high levels of hCG and can cause early puberty in children or menometrorrhagia (irregular, heavy menstruation) after menarche.[33]
Immature, or solid, teratomas are the most common type of ovarian germ cell tumor, making up 40–50% of cases. Teratomas are characterized by the presence of disorganized tissues arising from all three embryonic germ layers: ectoderm, mesoderm, and endoderm; immature teratomas also have undifferentiated stem cells that make them more malignant than mature teratomas (dermoid cysts). The different tissues are visible on gross pathology and often include bone, cartilage, hair, mucus, or sebum, but these tissues are not visible from the outside, which appears to be a solid mass with lobes and cysts. Histologically, they have large amounts of neuroectoderm organized into sheets and tubules along with glia; the amount of neural tissue determines the histologic grade. Immature teratomas usually only affect one ovary (10% co-occur with dermoid cysts) and usually metastasize throughout the peritoneum. They can also cause mature teratoma implants to grow throughout the abdomen in a disease called growing teratoma syndrome; these are usually benign but will continue to grow during chemotherapy, and often necessitate further surgery. Unlike mature teratomas, immature teratomas form many adhesions, making them less likely to cause ovarian torsion. There is no specific marker for immature teratomas, but carcinoembryonic antigen (CEA), CA-125, CA19-9, or AFP can sometimes indicate an immature teratoma.[33]
Stage I teratomas make up the majority (75%) of cases and have the best prognosis, with 98% of patients surviving 5 years; if a Stage I tumor is also grade 1, it can be treated with unilateral surgery only. Stage II though IV tumors make up the remaining quarter of cases and have a worse prognosis,  with 73–88% of patients surviving 5 years.[33]
Mature teratomas, or dermoid cysts, are rare tumors consisting of mostly benign tissue that develop after menopause. The tumors consist of disorganized tissue with nodules of malignant tissue, which can be of various types. The most common malignancy is squamous cell carcinoma, but adenocarcinoma, basal-cell carcinoma, carcinoid tumor, neuroectodermal tumor, malignant melanoma, sarcoma, sebaceous tumor, and struma ovarii can also be part of the dermoid cyst. They are treated with surgery and adjuvant platinum chemotherapy or radiation.[33]
Yolk sac tumors, formerly called endodermal sinus tumors, make up approximately 10–20% of ovarian germ cell malignancies, and have the worst prognosis of all ovarian germ cell tumors. They occur both before menarche (in one-third of cases) and after menarche (the remaining two-thirds of cases). Half of the people with yolk sac tumors are diagnosed in stage I. Typically, they are unilateral until metastasis, which occurs within the peritoneal cavity and via the bloodstream to the lungs. Yolk sac tumors grow quickly and recur easily, and are not easily treatable once they have recurred. Stage I yolk sac tumors are highly treatable, with a 5-year disease-free survival rate of 93%, but stage II-IV tumors are less treatable, with survival rates of 64–91%.[33]
Their gross appearance is solid, friable, and yellow, with necrotic and hemorrhagic areas. They also often contain cysts that can degenerate or rupture. Histologically, yolk sac tumors are characterized by the presence of Schiller-Duval bodies (which are pathognomonic for yolk sac tumors) and a reticular pattern. Yolk sac tumors commonly secrete alpha-fetoprotein and can be immunohistochemically stained for its presence; the level of alpha-fetoprotein in the blood is a useful marker of recurrence.[33]
Embryonal carcinomas, a rare tumor type usually found in mixed tumors, develop directly from germ cells but are not terminally differentiated; in rare cases, they may develop in dysgenetic gonads. They can develop further into a variety of other neoplasms, including choriocarcinoma, yolk sac tumor, and teratoma. They occur in younger people, with an average age at diagnosis of 14, and secrete both alpha-fetoprotein (in 75% of cases) and hCG.[33]
Histologically, embryonal carcinoma appears similar to the embryonic disc, made up of epithelial, anaplastic cells in disorganized sheets, with gland-like spaces and papillary structures.[33]
Polyembryomas, the most immature form of teratoma and very rare ovarian tumors, are histologically characterized by having several embryo-like bodies with structures resembling a germ disk, yolk sac, and amniotic sac. Syncytiotrophoblast giant cells also occur in polyembryomas.[33]
Primary ovarian squamous cell carcinomas are rare and have a poor prognosis when advanced. More typically, ovarian squamous cell carcinomas are cervical metastases, areas of differentiation in an endometrioid tumor, or derived from a mature teratoma.[29]
Mixed tumors contain elements of more than one of the above classes of tumor histology. To be classed as a mixed tumor, the minor type must make up more than 10% of the tumor.[31] Though mixed carcinomas can have any combination of cell types, mixed ovarian cancers are typically serous/endometrioid or clear-cell/endometrioid.[29] Mixed germ cell tumors make up approximately 25–30% of all germ cell ovarian cancers, with combinations of dysgerminoma, yolk sac tumor, and/or immature teratoma. The prognosis and treatment vary based on the component cell types.[33]
Ovarian cancer can also be a secondary cancer, the result of metastasis from a primary cancer elsewhere in the body.[26] About 5-30% of ovarian cancers are due to metastases, while the rest are primary cancers.[78] Common primary cancers are breast cancer, colon cancer, appendiceal cancer, and stomach cancer (primary gastric cancers that metastasize to the ovary are called Krukenberg tumors).[26] Krukenberg tumors have signet ring cells and mucinous cells.[29] Endometrial cancer and lymphomas can also metastasize to the ovary.[79]
Ovarian borderline tumors, sometimes called low malignant potential (LMP) ovarian tumors, have some benign and some malignant features.[29] LMP tumors make up approximately 10%-15% of all ovarian tumors.[31][68] They develop earlier than epithelial ovarian cancer, around the age of 40–49. They typically do not have extensive invasion; 10% of LMP tumors have areas of stromal microinvasion (<3mm, <5% of tumor). LMP tumors have other abnormal features, including increased mitosis, changes in cell size or nucleus size, abnormal nuclei, cell stratification, and small projections on cells (papillary projections). Serous and/or mucinous characteristics can be seen on histological examination, and serous histology makes up the overwhelming majority of advanced LMP tumors. More than 80% of LMP tumors are Stage I; 15% are stage II and III and less than 5% are stage IV.[29] Implants of LMP tumors are often non-invasive.[68]
Ovarian cancer is staged using the FIGO staging system and uses information obtained after surgery, which can include a total abdominal hysterectomy via midline laparotomy, removal of (usually) both ovaries and Fallopian tubes, (usually) the omentum, pelvic (peritoneal) washings, assessment of retroperitoneal lymph nodes (including the pelvic and para-aortic lymph nodes), appendectomy in suspected mucinous tumors, and pelvic/peritoneal biopsies for cytopathology.[28][26][31][80] Around 30% of ovarian cancers that appear confined to the ovary have metastasized microscopically, which is why even stage-I cancers must be staged completely.[26] 22% of cancers presumed to be stage I are observed to have lymphatic metastases.[31] The AJCC stage is the same as the FIGO stage. The AJCC staging system describes the extent of the primary tumor (T), the absence or presence of metastasis to nearby lymph nodes (N), and the absence or presence of distant metastasis (M).[81] The most common stage at diagnosis is stage IIIc, with over 70% of diagnoses.[26]
Stage 1 ovarian cancer
Stage 2 ovarian cancer
Stage 3 ovarian cancer
Stage 4 ovarian cancer
The AJCC/TNM staging system indicates where the tumor has developed, spread to lymph nodes, and metastasis.[31]
The AJCC/TNM stages can be correlated with the FIGO stages:[31]
Grade 1 tumors have well differentiated cells (look very similar to the normal tissue) and are the ones with the best prognosis. Grade 2 tumors are also called moderately well-differentiated and they are made up of cells that resemble the normal tissue. Grade 3 tumors have the worst prognosis and their cells are abnormal, referred to as poorly differentiated.[82]
Metastasis in ovarian cancer is very common in the abdomen and occurs via exfoliation, where cancer cells burst through the ovarian capsule and are able to move freely throughout the peritoneal cavity. Ovarian cancer metastases usually grow on the surface of organs rather than the inside; they are also common on the omentum and the peritoneal lining. Cancer cells can also travel through the lymphatic system and metastasize to lymph nodes connected to the ovaries via blood vessels; i.e. the lymph nodes along the infundibulopelvic ligament, the broad ligament, and the round ligament. The most commonly affected groups include the paraaortic, hypogastric, external iliac, obturator, and inguinal lymph nodes. Usually, ovarian cancer does not metastasize to the liver, lung, brain, or kidneys unless it is a recurrent disease; this differentiates ovarian cancer from many other forms of cancer.[29]
Women with strong genetic risk for ovarian cancer may consider the surgical removal of their ovaries as a preventive measure. This is often done after completion of childbearing years. This reduces the chances of developing both breast cancer (by around 50%) and ovarian cancer (by about 96%) in women at high risk. Women with BRCA gene mutations usually also have their Fallopian tubes removed at the same time (salpingo-oophorectomy), since they also have an increased risk of Fallopian tube cancer. However, these statistics may overestimate the risk reduction because of how they have been studied.[26][83]
Women with a significant family history for ovarian cancer are often referred to a genetic counselor to see if testing for BRCA mutations would be beneficial.[29] The use of oral contraceptives, the absence of 'periods' during the menstrual cycle, and tubal ligation reduce the risk.[84]
There may an association of developing ovarian cancer and ovarian stimulation during infertility treatments. Endometriosis has been linked to ovarian cancers. Human papillomavirus infection, smoking, and talc have not been identified as increasing the risk for developing ovarian cancer.[28]
There is no simple and reliable way to test for ovarian cancer in women who do not have any signs or symptoms. Screening is not recommended in women who are at average risk, as evidence does not support a reduction in death and the high rate of false positive tests may lead to unneeded surgery, which is accompanied by its own risks.[19] Women with high risk of ovarian cancer that are currently identified based on family history and genetic testing may benefit from screening.[85] The Pap test does not screen for ovarian cancer.[27]
Ovarian cancer is usually only palpable in advanced stages.[29] This high risk group has benefited with earlier detection.[28][26][83]  Screening is not recommended using CA-125 measurements, HE4 levels, ultrasound, or adnexal palpation in women who are at average risk. Currently there is no national screening programme in the UK for ovarian cancer. CA125 and transvaginal ultrasound can be utilised but there is minimal evidence to suggest this decreases mortality . More recently, the Risk of Ovarian Cancer Algorithm (ROMA) has been shown to detect earlier cancers using CA125 and age but again does not provide a robust measure to decrease mortality at present.[86]
Ovarian cancer has low prevalence, even in the high-risk group of women from the ages of 50 to 60 (about one in 2000), and screening of women with average risk is more likely to give ambiguous results than detect a problem that requires treatment. Because ambiguous results are more likely than detection of a treatable problem, and because the usual response to ambiguous results is invasive interventions, in women of average risk, the potential harms of having screening without an indication outweigh the potential benefits. The purpose of screening is to diagnose ovarian cancer at an early stage when it is more likely to be treated successfully.[26][83]
Screening with transvaginal ultrasound, pelvic examination, and CA-125 levels can be used instead of preventive surgery in women who have BRCA1 or BRCA2 mutations. This strategy has shown some success.[29]
Screening for CA125, a chemical released by ovarian tumours, with follow-up using ultrasound, was shown to be ineffective in reducing mortality in a large-scale UK study.[87]
There have been some screening trials that have used age, family history of ovarian cancer, and mutation status to identify target populations for screening.[85]
Once it is determined that ovarian, fallopian tube or primary peritoneal cancer is present, treatment is scheduled by a gynecologic oncologist (a physician trained to treat cancers of a woman's reproductive system). Gynecologic oncologists can perform surgery on and give chemotherapy to women with ovarian cancer. A treatment plan is developed.[88]
Treatment usually involves surgery and chemotherapy, and sometimes radiotherapy, regardless of the subtype of ovarian cancer.[68][89] Surgical treatment may be sufficient for well-differentiated malignant tumors and confined to the ovary. Addition of chemotherapy may be required for more aggressive tumors confined to the ovary. For patients with advanced disease, a combination of surgical reduction with a combination chemotherapy regimen is standard.  Since 1980, platinum-based drugs have had an important role in treating ovarian cancer.[citation needed] Borderline tumors, even following spread outside of the ovary, are managed well with surgery, and chemotherapy is not seen as useful.[90] Second-look surgery and maintenance chemotherapy have not been shown to provide benefit.[29]
Surgery has been the standard of care for decades and may be necessary for obtaining a specimen for diagnosis.  The surgery depends upon the extent of nearby invasion of other tissues by the cancer when it is diagnosed. This extent of the cancer is described by assigning it a stage, the presumed type, and the grade of cancer. The gynecological surgeon may remove one (unilateral oophorectomy) or both ovaries (bilateral oophorectomy). The Fallopian tubes (salpingectomy), uterus (hysterectomy), and the omentum (omentectomy) may also be removed. Typically, all of these organs are removed.[91]
For those who test positive for faulty BRCA1 or BRCA2 genes having a risk-reducing surgery is an option. An increasing number of women choose this. At the same time the average waiting time for undergoing the procedure is two-years which is much longer than recommended.[92][93]
For low-grade, unilateral stage-IA cancers, only the involved ovary (which must be unruptured) and Fallopian tube will be removed. This can be done especially in young people who wish to preserve their fertility. However, a risk of microscopic metastases exists and staging must be completed.[28] If any metastases are found, a second surgery to remove the remaining ovary and uterus is needed.[90] Tranexamic acid can be administered prior to surgery to reduce the need for blood transfusions due to blood loss during the surgery.[31]
If a tumor in a premenopausal woman is determined to be a low malignant potential tumor during surgery, and it is clearly stage I cancer, only the affected ovary is removed. For postmenopausal women with low malignant potential tumors, hysterectomy with bilateral salpingo-oophorectomy is still the preferred option. During staging, the appendix can be examined or removed. This is particularly important with mucinous tumors.[29] In children or adolescents with ovarian cancer, surgeons typically attempt to preserve one ovary to allow for the completion of puberty, but if the cancer has spread, this is not always possible. Dysgerminomas, in particular, tend to affect both ovaries: 8–15% of dysgerminomas are present in both ovaries.[34] People with low-grade (well-differentiated) tumors are typically treated only with surgery,[26] which is often curative.[68] In general, germ cell tumors can be treated with unilateral surgery unless the cancer is widespread or fertility is not a factor.[33] In women with surgically staged advanced epithelial ovarian cancer (stages III and IV), studies suggest all attempts should be made to reach complete cytoreduction (surgical efforts to remove the bulk of the tumor).[94]
In advanced cancers, where complete removal is not an option, as much tumor as possible is removed in a procedure called debulking surgery. This surgery is not always successful, and is less likely to be successful in women with extensive metastases in the peritoneum, stage- IV disease, cancer in the transverse fissure of the liver, mesentery, or diaphragm, and large areas of ascites. Debulking surgery has usually only been done once[28] but a recent study has shown a longer overall survival in recurrent ovarian cancer when surgery combined with chemotherapy was performed compared to treatment with chemotherapy alone.[95] Computed tomography (abdominal CT) is often used to assess if primary debulking surgery is possible, but low certainty evidence also suggests fluorodeoxyglucose‐18 (FDG) PET/CT and MRI may be useful as an addition for assessing macroscopic incomplete debulking.[96] More complete debulking is associated with better outcomes: women with no macroscopic evidence of disease after debulking have a median survival of 39 months, as opposed to 17 months with less complete surgery.[26] By removing metastases, many cells that are resistant to chemotherapy are removed, and any clumps of cells that have died are also removed. This allows chemotherapy to better reach the remaining cancer cells, which are more likely to be fast-growing and therefore chemosensitive.[29]
Interval debulking surgery is another protocol used, where neoadjuvant chemotherapy is given, debulking surgery is performed, and chemotherapy is finished after debulking.[90] Though no definitive studies have been completed, it is shown to be approximately equivalent to primary debulking surgery in terms of survival and shows slightly lower morbidity.[29] Previous studies have shown different results from primary debulking versus interval debulking. The ongoing TRUST study may clarify selection criteria for each surgical approach.[97]
There are several different surgical procedures that can be employed to treat ovarian cancer. For stage I and II cancer, laparoscopic (keyhole) surgery can be used, but metastases may not be found. For advanced cancer, laparoscopy is not used, since debulking metastases requires access to the entire peritoneal cavity. Depending on the extent of the cancer, procedures may include a bilateral salpingo-oophorectomy, biopsies throughout the peritoneum and abdominal lymphatic system, omentectomy, splenectomy, bowel resection, diaphragm stripping or resection, appendectomy, or even a posterior pelvic exenteration.[29]
To fully stage ovarian cancer, lymphadenectomy can be included in the surgery, but a significant survival benefit to this practice may not happen.[28] This is particularly important in germ cell tumors because they frequently metastasize to nearby lymph nodes.[26]
If ovarian cancer recurs, secondary surgery is sometimes a treatment option. This depends on how easily the tumor can be removed, how much fluid has accumulated in the abdomen, and overall health.[28] Effectivenes of this surgery depends on surgical technique, completeness of cytoreduction, and extent of disease.[98] It also can be helpful in people who had their first surgery done by a generalist and in epithelial ovarian cancer.[31] Secondary surgery can be effective in dysgerminomas and immature teratomas.[33] Evidence suggests surgery in recurrent epithelial ovarian cancer may be associated with prolonging life in some women with platinum-sensitive disease.[99]
The major side effect of oophorectomy in younger women is early menopause, which can cause osteoporosis. After surgery, hormone replacement therapy can be considered, especially in younger women. This therapy can consist of a combination of estrogen and progesterone, or estrogen alone. Estrogen alone is safe after hysterectomy; when the uterus is still present, unopposed estrogen dramatically raises the risk of endometrial cancer.[28] Estrogen therapy after surgery does not change survival rates.[31] People having ovarian cancer surgery are typically hospitalized afterwards for 3–4 days and spend around a month recovering at home.[100] Surgery outcomes are best at hospitals that do a large number of ovarian cancer surgeries.[29]
It is unclear if laparoscopy or laparotomy is better or worse for FIGO stage I ovarian cancer.[101] There is also no apparent difference between total abdominal hysterectomy and supracervical hysterectomy for advanced cancers. Approximately 2.8% of people having a first surgery for advanced ovarian cancer die within two weeks of the surgery (2.8% perioperative mortality rate).[31] More aggressive surgeries are associated with better outcomes in advanced (stage III or IV) ovarian cancer.[29]
Chemotherapy has been a general standard of care for ovarian cancer for decades, although with variable protocols. Chemotherapy is used after surgery to treat any residual disease, if appropriate.  In some cases, there may be reason to perform chemotherapy first, followed by surgery. This is called "neoadjuvant chemotherapy", and is common when a tumor cannot be completely removed or optimally debulked via surgery. Though it has not been shown to increase survival, it can reduce the risk of complications after surgery. If a unilateral salpingo-oophorectomy or other surgery is performed, additional chemotherapy, called "adjuvant chemotherapy", can be given.[28][31] Adjuvant chemotherapy is used in stage 1 cancer typically if the tumor is of a high histologic grade (grade 3) or the highest substage (stage 1c), provided the cancer has been optimally staged during surgery.[31][90] Bevacizumab may be used as an adjuvant chemotherapy if the tumor is not completely removed during surgery or if the cancer is stage IV; it can extend progression-free survival but has not been shown to extend overall survival.[31] Chemotherapy is curative in approximately 20% of advanced ovarian cancers;[29] it is more often curative with malignant germ cell tumors than epithelial tumors.[33] Adjuvant chemotherapy has been found to improve survival and reduce the risk of ovarian cancer recurring compared to no adjuvant therapy in women with early stage epithelial ovarian cancer.[102]
Chemotherapy in ovarian cancer typically consists of platins, a group of platinum-based drugs, combined with non-platins.[103] Platinum-based drugs have been used since 1980.  Common therapies can include paclitaxel, cisplatin, topotecan, doxorubicin, epirubicin, and gemcitabine. Carboplatin is typically given in combination with either paclitaxel or docetaxel; the typical combination is carboplatin with paclitaxel.[28][31] Carboplatin is superior to cisplatin in that it is less toxic and has fewer side effects, generally allowing for an improved quality of life in comparison, though both are similarly effective.[31] Three-drug regimens have not been found to be more effective,[28] and platins alone or nonplatins alone are less effective than platins and nonplatins in combination.[31] There is a small benefit in platinum‐based chemotherapy compared with non‐platinum therapy.[104] Platinum combinations can offer improved survival over single platinum. In people with relapsed ovarian cancer, evidence suggests topotecan has a similar effect on overall survival as paclitaxel and topotecan plus thalidomide, whilst it is superior to treosulfan and not as effective as pegylated liposomal doxorubicin in platinum-sensitive people.[105]
Chemotherapy can be given intravenously or in the peritoneal cavity.[26] Though intraperitoneal chemotherapy is associated with longer progression-free survival and overall survival, it also causes more adverse side effects than intravenous chemotherapy.[31] It is mainly used when the cancer has been optimally debulked. Intraperitoneal chemotherapy can be highly effective because ovarian cancer mainly spreads inside the peritoneal cavity, and higher doses of the drugs can reach the tumors this way.[29]
Chemotherapy can cause anemia; intravenous iron has been found to be more effective than oral iron supplements in reducing the need for blood transfusions.[31] Typical cycles of treatment involve one treatment every 3 weeks, repeated for 6 weeks or more.[106] Fewer than 6 weeks (cycles) of treatment is less effective than 6 weeks or more.[31] Germ-cell malignancies are treated differently than other ovarian cancers — a regimen of bleomycin, etoposide, and cisplatin (BEP) is used with 5 days of chemotherapy administered every 3 weeks for 3 to 4 cycles.[26][33] Chemotherapy for germ cell tumors has not been shown to cause amenorrhea, infertility, birth defects, or miscarriage.[33] Maintenance chemotherapy has not been shown to be effective.[31]
In people with BRCA mutations, platinum chemotherapy is more effective.[28] Germ-cell tumors and malignant sex-cord/stromal tumors are treated with chemotherapy, though dysgerminomas and sex-cord tumors are not typically very responsive.[26][34]
If ovarian cancer recurs, it is considered partially platinum-sensitive or platinum-resistant, based on the time since the last recurrence treated with platins: partially platinum-sensitive cancers recurred 6–12 months after last treatment, and platinum-resistant cancers have an interval of less than 6 months. Second-line chemotherapy can be given after the cancer becomes symptomatic because no difference in survival is seen between treating asymptomatic (elevated CA-125) and symptomatic recurrences.[medical citation needed]
For platinum-sensitive tumors, platins are the drugs of choice for second-line chemotherapy, in combination with other cytotoxic agents. Regimens include carboplatin combined with pegylated liposomal doxorubicin, gemcitabine, or paclitaxel.[26] Carboplatin-doublet therapy can be combined with paclitaxel for increased efficacy in some cases. Another potential adjuvant therapy for platinum-sensitive recurrences is olaparib, which may improve progression-free survival but has not been shown to improve overall survival.[31] (Olaparib, a PARP inhibitor, was approved by the US FDA for use in BRCA-associated ovarian cancer that had previously been treated with chemotherapy.[107][108]) For recurrent germ cell tumors, an additional 4 cycles of BEP chemotherapy is the first-line treatment for those who have been treated with surgery or platins.
If the tumor is determined to be platinum-resistant, vincristine, dactinomycin, and cyclophosphamide (VAC) or some combination of paclitaxel, gemcitabine, and oxaliplatin may be used as a second-line therapy.[33]
For platinum-resistant tumors, there are no high-efficacy chemotherapy options. Single-drug regimens (doxorubicin or topotecan) do not have high response rates,[28] but single-drug regimens of topotecan, pegylated liposomal doxorubicin, or gemcitabine are used in some cases.[26][31] Topotecan cannot be used in people with an intestinal blockage. Paclitaxel used alone is another possible regimen, or it may be combined with liposomal doxorubicin, gemcitabine, cisplatin, topotecan, etoposide, or cyclophosphamide.[106] ( See also Palliative care below.)
Novel agents are being developed to inhibit the development of new blood vessels (angiogenesis) for women with ovarian cancer who develop resistance to chemotherapy drugs. As of 2023 there would appear to be a role for these treatments but due to the additional treatment burden and cost of maintenance treatments the risk versus benefits require careful consideration.[109]
Novocure sponsored a phase-2 trial proving efficacy of tumor treating fields in recurrent platinum-resistant ovarian carcinoma, in conjunction with weekly paclitaxel chemotherapy.[46]
Dysgerminomas are most effectively treated with radiation,[34] though this can cause infertility and is being phased out in favor of chemotherapy.[26] Radiation therapy does not improve survival in people with well-differentiated tumors.[26]
In stage 1c and 2 cancers, radiation therapy is used after surgery if there is the possibility of residual disease in the pelvis but the abdomen is cancer-free. Radiotherapy can also be used in palliative care of advanced cancers. A typical course of radiotherapy for ovarian cancer is 5 days a week for 3–4 weeks. Common side effects of radiotherapy include diarrhea, constipation, and frequent urination.[110]
Despite the fact that 60% of ovarian tumors have estrogen receptors, ovarian cancer is only rarely responsive to hormonal treatments. A Cochrane review found a lack of evidence about the effects of tamoxifen in people with relapsed ovarian cancer.[111] Estrogen alone does not have an effect on the cancer, and tamoxifen and letrozole are rarely effective.[28] "Some women with borderline malignancy ovarian cancer and stromal ovarian cancer may receive hormonal therapy."[91]
Immunotherapy is a topic of current research in ovarian cancer. In some cases, the antibody drug bevacizumab, though still a topic of active research, is used to treat advanced cancer along with chemotherapy.[90] It has been approved for this use in the European Union.[112]
Specific follow-up depends on, for example, the type and stage of ovarian cancer, the treatment, and the presence of any symptoms. Usually, a check-up appointment is made about every 2 to 3 months initially, followed by twice per year for up to 5 years.[113] For epithelial ovarian cancers, the most common test upon follow-up is CA-125 level. However, treatment based only on elevated CA-125 levels and not any symptoms can increase side effects without any prolongation of life, so the implication of the outcome of a CA-125 test can be discussed before taking it.[114] The recommendation as of 2014 is recurrent cancer may be present if the CA-125 level is twice normal.[28] Treating a recurrence detected by CA-125 does not improve survival.[31]
For women with germ-cell tumors, follow-up tests generally include alpha-fetoprotein (AFP) and/or human chorionic gonadotropin. For women with stromal cancers, tests for hormones like estrogen, testosterone, and inhibin are sometimes helpful.[114] Inhibin can also be useful for monitoring the progress of sex-cord tumors, along with Müllerian inhibiting substance. AFP can also be used to monitor Sertoli-Leydig tumors.[26] In dysgerminomas, lactate dehydrogenase and its two isozymes (LDH-1 and LDH-2) are used to test for recurrence.[33]
Women with ovarian cancer may not need routine surveillance imaging to monitor the cancer unless new symptoms appear or tumor markers begin rising.[115] Imaging without these indications is discouraged because it is unlikely to detect a recurrence, improve survival, and because it has its own costs and side effects.[115] However, CT imaging can be used if desired, though this is not common.[28] If a tumor is easily imaged, imaging may be used to monitor the progress of treatment.[116]
Palliative care focuses on relieving symptoms and increasing or maintaining quality of life. This type of treatment's purpose is not to cure the cancer but to make the woman more comfortable while living with cancer that can not be cured. It has been recommended as part of the treatment plan for any person with advanced ovarian cancer or patients with significant symptoms.[117] In platinum-refractory and platinum-resistant cases, other palliative chemotherapy is the main treatment.[29][91]
Palliative care can entail treatment of symptoms and complications of the cancer, including pain, nausea, constipation, ascites, bowel obstruction, edema, pleural effusion, and mucositis.  Especially if the cancer advances and becomes incurable, treatment of symptoms becomes one of the main goals of therapy.  Palliative care can also entail helping with decision-making such as if or when hospice care is appropriate, and the preferred place for the patient at end of life care.[31]
Bowel obstruction can be treated with palliative surgery (colostomy, ileostomy, or internal bypass) or medicine, but surgery has been shown to increase survival time.[28][31] Palliative surgery may result in short bowel syndrome, enterocutaneous fistula, or re-obstruction; or may not be possible due to the extent of obstruction.[29] Other treatments of complications can include total parenteral nutrition, a low-residue diet, palliative gastrostomy, and adequate pain control.[28] Bowel obstruction can also be treated with octreotide when palliative surgery is not an option. Cancer can also block the ureters, which can be relieved by a nephrostomy or a ureteric stent. Ascites can be relieved by repeated paracentesis or placement of a drain to increase comfort.[118] Pleural effusions can be treated in a similar manner, with repeated thoracentesis, pleurodesis, or placement of a drain.[29]
Radiation therapy can be used as part of the palliative care of advanced ovarian cancer, since it can help to shrink tumors that are causing symptoms.[91] Palliative radiotherapy typically lasts for only a few treatments, a much shorter course of therapy than non-palliative radiotherapy.[110] It is also used for palliation of chemotherapy-resistant germ cell tumors.[33]
Ovarian cancer has a significant effect on quality of life, psychological health and well-being. Interventions are available to help with the needs and social support. Many ovarian cancer survivors report a good quality of life and optimism. Others reported a "spiritual change" that helped them find meaning during their experience. Others have described their loss of faith after their diagnosis with ovarian cancer. Those who have gone through treatment sometimes experience social isolation but benefit from having relationships with other survivors. Frustration and guilt have been described by some who have expressed their inability to care for their family.[119]
Self-esteem and body image changes can occur due to hair loss, removal of ovaries and other reproductive structures, and scars. There is some improvement after hair grows in. Sexual issues can develop. The removal of ovaries results in surgically induced menopause that can result in painful intercourse, vaginal dryness, loss of sexual desire and being tired. Though prognosis is better for younger survivors, the impact on sexuality can still be substantial.[119]
Anxiety, depression and distress is present in those surviving ovarian cancer at higher rates than in the general population.[119][120] The same psychosocial problems can develop in family members. Emotional effects can include a fear of death, sadness, memory problems and difficulty in concentrating. When optimism was adopted by those at the beginning of their treatment, they were less likely to develop distress. Those who have fear of the cancer recurring may have difficulty in expressing joy even when disease-free. The more treatments that a woman undergoes, the more likely the loss of hope is expressed. Women often can cope and reduce negative psychosocial effects by a number of strategies. Activities such as traveling, spending additional time with family and friends, ignoring statistics, journaling and increasing involvement in spiritually-based events are adaptive.[119]
Women with ovarian cancer may also experience difficulties with their diet and are at risk of malnutrition.[121]
Ovarian cancer usually has a relatively poor prognosis. It is disproportionately deadly because it lacks any clear early detection or screening test, meaning most cases are not diagnosed until they have reached advanced stages.[115][28]
Ovarian cancer metastasizes early in its development, often before it has been diagnosed. High-grade tumors metastasize more readily than low-grade tumors. Typically, tumor cells begin to metastasize by growing in the peritoneal cavity.[26] More than 60% of women presenting with ovarian cancer have stage-III or stage-IV cancer, when it has already spread beyond the ovaries. Ovarian cancers shed cells into the naturally occurring fluid within the abdominal cavity. These cells can then implant on other abdominal (peritoneal) structures, including the uterus, urinary bladder, bowel, lining of the bowel wall, and omentum, forming new tumor growths before cancer is even suspected.
The five-year survival rate for all stages of ovarian cancer is 46%; the one-year survival rate is 72% and the ten-year survival rate is 35%.[123] For cases where a diagnosis is made early in the disease, when the cancer is still confined to the primary site, the five-year survival rate is 92.7%.[124] About 70% of women with advanced disease respond to initial treatment, most of whom attain complete remission, but half of these women experience a recurrence 1–4 years after treatment.[26] Brain metastasis is more common in stage III/IV cancer but can still occur in cancers staged at I/II. People with brain metastases survive a median of 8.2 months, though surgery, chemotherapy, and whole brain radiation therapy can improve survival.[31]
Ovarian cancer survival varies significantly with subtype. Dysgerminomas have a very favorable prognosis. In early stages, they have a five-year survival rate of 96.9%.[34] Around two-thirds of dysgerminomas are diagnosed at stage I.[33] Stage-III dysgerminomas have a five-year survival of 61%; when treated with BEP chemotherapy after incomplete surgical removal, dysgerminomas have a 95% two-year survival rate. Sex-cord-stromal malignancies also have a favorable prognosis; because they are slow-growing, even those with metastatic disease can survive a decade or more.[26] Low malignant potential tumors usually only have a bad prognosis when there are invasive tumor implants found in the peritoneal cavity.[29]
Complications of ovarian cancer can include spread of the cancer to other organs, progressive function loss of various organs, ascites, and intestinal obstructions, which can be fatal. Intestinal obstructions in multiple sites are the most common proximate cause of death.[28] Intestinal obstruction in ovarian cancer can either be a true obstruction, where tumor blocks the intestinal lumen, or a pseudo-obstruction, when tumor prevents normal peristalsis.[125] Continuous accumulation of ascites can be treated by placing a drain that can be self-drained.[28]
There are a number of prognostic factors in ovarian cancer. Positive prognostic factors – those indicating better chances of survival – include no residual disease after surgery (stage III/IV), complete macroscopic resection (stage IV), BRCA2 mutations, young age (under 45 years), nonserous type, low histologic grade, early stage, co-occurrence with endometrial cancer, and low CA-125 levels. There is conflicting evidence for BRCA1 as a prognostic factor. Conversely, negative prognostic factors – those that indicate a worse chance of survival – include rupture of the ovarian capsule during surgery, older age (over 45 years), mucinous type, stage IV, high histologic grade, clear-cell type, upper abdominal involvement, high CA-125 levels, the presence of tumor cells in the blood, and elevated cyclooxygenase-2.[31]
Expression of various mRNAs can also be prognostic for ovarian cancer. High levels of Drosha and Dicer are associated with improved survival, whereas high levels of let-7b, HIF1A, EphA1, and poly(ADP-ribose) polymerase are associated with worse survival. Cancers that are positive for WT1 carry a worse prognosis; estrogen-receptor positive cancers have a better prognosis.[31]
Overall five-year survival rates for all types of ovarian cancer are presented below by stage and histologic grade:[26]
The survival rates given below are for the different types of ovarian cancer, according to American Cancer Society.[122] They come from the National Cancer Institute, SEER, and are based on patients diagnosed from 2004 to 2010.
Ovarian cancer frequently recurs after treatment. Overall, in a 5-year period, 20% of stage I and II cancers recur. Most recurrences are in the abdomen.[29] If a recurrence occurs in advanced disease, it typically occurs within 18 months of initial treatment (18 months progression-free survival). Recurrences can be treated, but the disease-free interval tends to shorten and chemoresistance increases with each recurrence.[28] When a dysgerminoma recurs, it is most likely to recur within a year of diagnosis, and other malignant germ cell tumors recur within 2 years 90% of the time. Germ cell tumors other than dysgerminomas have a poor prognosis when they relapse, with a 10% long-term survival rate.[33] Low malignant potential tumors rarely relapse, even when fertility-sparing surgery is the treatment of choice. 15% of LMP tumors relapse after unilateral surgery in the previously unaffected ovary, and they are typically easily treated with surgery. More advanced tumors may take up to 20 years to relapse, if they relapse at all, and are only treated with surgery unless the tumor has changed its histological characteristics or grown very quickly. In these cases, and when there is significant ascites, chemotherapy may also be used. Relapse is usually indicated by rising CA-125 levels and then progresses to symptomatic relapse within 2–6 months.[29] Recurrent sex cord-stromal tumors are typically unresponsive to treatment but not aggressive.[33]
It is the most deadly gynecologic cancer.[29]
Globally, in 2018, the incidence of ovarian cancer was 6.6 per 100,000 and mortality was 3.9.[128] Globally, about 160,000 people died from ovarian cancer in 2010. This was an increase from 113,000 in 1990.[129] The number of new cases per year in Europe is approximately 5–15 per 100,000 women.[31] In Europe, Lithuania, Latvia, Ireland, Slovakia, and the Czech Republic have the highest incidences of ovarian cancer, whereas Portugal and Cyprus have the lowest incidences.[31] In 2008, the five-year survival rate was 44%. This has increased since 1977 when the survival rate was 36%.[119]
In 2022, in the United States, an estimated 19,880 new cases were diagnosed and 12,810 women died of ovarian cancer.[131] The 5-year relative survival rate is 49.7%.[132] Around 57% cases have metastasized at the time of diagnosis.[132]
In 2014, over 220,000 diagnoses of epithelial ovarian cancer were made yearly.[28] The overall lifetime risk in the US is around 1.6%[26][31] In the US, ovarian cancer affects 1.3–1.4% and is the cause of death of about 1% of women.[29][133] In the United States, it is also the fifth-most common cancer in women but the fourth-most common cause of cancer death.[31] This decrease made it the ninth-most common cancer in women.[29]
The risks from developing specific types of ovarian cancer varies. Germ cell tumors and sex cord-stromal tumors are less common than epithelial tumors. The number of new cases a year in the US is 0.4 per 100,000 women and 0.2 per 100,000 women, respectively. In young people, sex-cord stromal tumors and germ cell tumors total 1% of overall ovarian cancer.[33] Ovarian cancer represents approximately 4% of cancers diagnosed in women.[31]
It is the 5th-most common cancer in UK women (around 7,100 were diagnosed in 2011) and the 5th-most common cause of cancer death in women (around 4,300 died in 2012).[134][31][35] The incidence rate over the whole UK population is 21.6 per 100,000.
As of 2014, the UK saw approximately 7,000–7,100 yearly diagnoses with 4,200 deaths.[28][35] A 2022 article from The Times put the estimate at 7,500 new cases yearly in Britain.[135] Early symptoms are often mistaken for common conditions such as cystitis or irritable bowel syndrome, and about 40 per cent of UK women wrongly believe that cervical screening detects ovarian cancer, an increase from 30 per cent in 2016.[135] Ashkenazi Jewish women carry mutated BRCA alleles five times more often than the rest of the population, giving them a higher risk developing ovarian cancer.[28]
Black women have twice the risk for sex cord-stromal tumors compared to non-Black women.[33] The highest prevalence is in Caucasian and Hispanic women, followed by African-American and Asian women.[128] The highest mortality from ovarian cancer is in African-American women.[128]
In the US, the incidence rate in women over 50 is approximately 33 per 100,000.[136] The rate of ovarian cancer between 1993 and 2008 decreased in women of the 40–49 age cohort and in the 50–64 age cohort.[28] Ovarian cancer is most commonly diagnosed after menopause,[35] between the ages of 60 and 64. Ninety percent of ovarian cancer occurs in women over the age of 45 and 80% in women over 50.[31] Older women are more likely to present with advanced ovarian cancer.[20]
Malignant germ cell tumors are the type of ovarian cancer most likely to occur during pregnancy. They are typically diagnosed when an adnexal mass is found on examination (in 1–2% of all pregnancies), a tumor is seen on ultrasound, or the parent's level of alpha-fetoprotein is elevated. Dermoid cysts and dysgerminomas are the most common germ cell tumors during pregnancy. Germ cell tumors diagnosed during pregnancy are unlikely to have metastasized and can be treated by surgery and, in some cases, chemotherapy, which carries the risk of birth defects. Yolk sac tumors and immature teratomas grow particularly quickly and are usually treated with chemotherapy even during pregnancy; however, dysgerminomas that have been optimally debulked may be treated after childbirth.[33]
Ovarian tumors have been reported in equine mares. Reported tumor types include teratoma,[137][138] cystadenocarcinoma,[139] and particularly granulosa cell tumor.[140][141][142][143][144][excessive citations]
Screening by hysteroscopy to obtain cell samples obtained for histological examination is being developed. This is similar to the current pap smear that is used to detect cervical cancer.[145] The UK Collaborative Trial of Ovarian Cancer Screening is testing a screening technique that combines CA-125 blood tests with transvaginal ultrasound.[28] Other studies suggest that this screening procedure may be effective.[112] Although results published in 2015 were not conclusive, there was some evidence that screening may save lives in the long-term.[146] As a result, the trial has been extended and will publish definitive results at the end of 2019. One major problem with screening is no clear progression of the disease from stage I (noninvasive) to stage III (invasive) is seen, and it may not be possible to find cancers before they reach stage III. Another problem is that screening methods tend to find too many suspicious lesions, most of which are not cancer, but malignancy can only be assessed with surgery.[28] The ROCA method combined with transvaginal ultrasonography is being researched in high-risk women to determine if it is a viable screening method. It is also being investigated in normal-risk women as it has shown promise in the wider population.[29] Studies are also in progress to determine if screening helps detect cancer earlier in people with BRCA mutations.[112]
Research into various prognostic factors for ovarian cancer is also going on. Recent research shows that thrombocytosis predicts lower survival and higher stage cancer.[28] Ongoing research is also investigating the benefits of surgery for recurrent ovarian cancer.[112]
While an active area of research, as of 2018 there is no good evidence that immunotherapy is effective for ovarian cancer.[147] However, trials of the antibody and VEGF inhibitor bevacizumab, which can slow the growth of new blood vessels in the cancer, have shown promising results, especially in combination with pazopanib, which also slows the process of blood vessel growth. Bevacizumab has been particularly effective in preliminary studies on stage-III and -IV cancer[28] and has been cited as having at least a 15% response rate.[26] It is being investigated particularly in mucinous ovarian cancers.[112]
mTOR inhibitors were a highly investigated potential treatment in the 2000s and 2010s, but the side effects of these drugs (particularly hyperglycemia and hyperlipidemia) were not well tolerated and the survival benefit not confirmed. PI3 kinase inhibitors have been of interest, but they tend to be highly toxic and cause diarrhea. Another investigated drug is selumetinib, a MAPK inhibitor. It improved survival, but did not correlate with any mutations found in tumors.[28]
Bevacizumab can also be combined with platinum chemotherapy, a combination that has had positive preliminary results in PFS, but equivocal results regarding overall survival. One disadvantage to these treatments is the side effect profile, which includes high blood pressure and proteinuria. The drug can also exacerbate bowel disease, leading to fistulae or bowel perforation. Vintafolide, which consists of an antifolate conjugated with vinblastine, is also in clinical trials; it may prove beneficial because folate receptors are overexpressed in many ovarian cancers.[28] Another potential immunotherapy is trastuzumab, which is active against tumors positive for Her2/neu mutations.[26] Other angiogenesis inhibitors are also being investigated as potential ovarian cancer treatments. Combretastatin and pazopanib are being researched in combination for recurrent ovarian cancer. Trebananib and tasquinimod are other angiogenesis inhibitors being investigated. The monoclonal antibody farletuzumab is being researched as an adjuvant to traditional chemotherapy. Another type of immunotherapy involves vaccines, including TroVax.[112]
An alternative to BEP chemotherapy, a regimen of 3 cycles of carboplatin and etoposide, is a current topic of research for germ cell malignancies.[33]
Intraperitoneal chemotherapy has also been under investigation during the 2000s and 2010s for its potential to deliver higher doses of cytotoxic agent to tumors. Preliminary trials with cisplatin and paclitaxel have shown it is not well tolerated, but does improve survival, and more tolerable regimens are being researched.[28] Cisplatin and paclitaxel are both being researched as intraperitoneal chemotherapy agents. A specific chemotherapy regimen for rare clear-cell cancers is also under investigation: irinotecan combined with cisplatin.[112]
PARP inhibitors have also shown promise in early trials, particularly in people with BRCA gene mutations, since the BRCA protein interacts with the PARP pathway. It is also being studied in recurrent ovarian cancer in general, where preliminary studies have shown longer PFS. Specifically, olaparib has shown greater survival compared to doxorubicin, though this treatment is still being investigated. It is not clear yet which biomarkers are predictive of responsiveness to PARP inhibitors.[28] Rucaparib is another PARP inhibitor being researched in BRCA-positive and BRCA-negative recurrent advanced ovarian cancer. Niraparib is a PARP inhibitor being tested in BRCA-positive recurrent ovarian cancer.[112]
Tyrosine kinase inhibitors are another investigational drug class that may have applications in ovarian cancer. Angiogenesis inhibitors in the receptor tyrosine kinase inhibitor group, including pazopanib, cediranib, and nintedanib, have also been shown to increase progression free survival (PFS), but their benefit for overall survival has not been investigated as of 2015.[28] Preliminary research showed that cediranib combined with platins in recurrent ovarian cancer increased the time to second recurrence by 3–4 months and increased survival by 3 months.[112] MK-1775 is a tyrosine kinase inhibitor that is being used in combination with paclitaxel and carboplatin in platinum-sensitive cancers with p53 mutations. Nintedanib is being researched as a potential therapy in combination with cyclophosphamide for people with recurrences.[112]
Histone deacetylase inhibitors (HDACi) are another area of research.
Hormone therapies are a topic of current research in ovarian cancer, particularly, the value of certain medications used to treat breast cancer. These include tamoxifen, letrozole, and anastrozole. Preliminary studies have showed a benefit for tamoxifen in a small number of people with advanced ovarian cancer. Letrozole may help to slow or stop growth of estrogen receptor positive ovarian cancer. Anastrozole is being investigated in postmenopausal people with estrogen receptor-positive cancer.[112]
Research into mitigating side effects of ovarian cancer treatment is also ongoing. Radiation fibrosis, the formation of scar tissue in an area treated with radiation, may be relieved with hyperbaric oxygen therapy, but research has not been completed in this area. Treatment of ovarian cancer may also cause people to experience psychiatric difficulties, including depression. Research is ongoing to determine how counseling and psychotherapy can help people who have ovarian cancer during treatment.[112]
There are some indications that pelvic inflammatory disease may be associated with ovarian cancer, especially in non-western countries. It may be due to the inflammatory process present with pelvic inflammatory disease.[148]
Clinical trials are monitored and funded by US governmental organizations to test treatment options to see if they are safe and effective. These include NIH Clinical Research Trials and You (National Institutes of Health),[149] Learn About Clinical Trials (National Cancer Institute),[150] Search for Clinical Trials (National Cancer Institute),[151] ClinicalTrials.gov (National Institutes of Health).[152][88] Clinical trials are also conducted in Canada.[153]

Medicine is the science[1] and practice[2] of caring for a patient, managing the diagnosis, prognosis, prevention, treatment, palliation of their injury or disease, and promoting their health. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.[3]
Medicine has been practiced since prehistoric times, and for most of this time it was an art (an area of creativity and skill), frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). For example, while stitching technique for sutures is an art learned through practice, knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.
Prescientific forms of medicine, now known as traditional medicine or folk medicine, remain commonly used in the absence of scientific medicine and are thus called alternative medicine. Alternative treatments outside of scientific medicine with ethical, safety and efficacy concerns are termed quackery.
Medicine (UK: /ˈmɛdsɪn/ ⓘ, US: /ˈmɛdɪsɪn/ ⓘ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease.[4][5] The word "medicine" is derived from Latin medicus, meaning "a physician".[6][7]
Medical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners.[8]
In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.[9]
In modern clinical practice, physicians and physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins with an interaction with an examination of the patient's medical history and medical record, followed by a medical interview[10] and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions.[11] Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue.
The components of the medical interview[10] and encounter are:
The physical examination is the examination of the patient for medical signs of disease that are objective and observable, in contrast to symptoms that are volunteered by the patient and are not necessarily objectively observable.[12] The healthcare provider uses sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order, although auscultation occurs prior to percussion and palpation for abdominal assessments.[13]
The clinical examination involves the study of:[14]
It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.
The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services.[15]
The medical decision-making (MDM) process includes the analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.
On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, lab or imaging results, or specialist consultations.
Contemporary medicine is, in general, conducted within health care systems. Legal, credentialing, and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have a significant impact on the way medical care is provided.
From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals, and the Catholic Church today remains the largest non-government provider of medical services in the world.[16] Advanced industrial countries (with the exception of the United States)[17][18] and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system or compulsory private or cooperative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most commonly a combination of all three.
Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those who can afford to pay for it, have self-insured it (either directly or as part of an employment contract), or may be covered by care financed directly by the government or tribe.
Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice of patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for its lack of openness,[19] new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.
The health professionals who provide care in medicine comprise multiple professions, such as medics, nurses, physiotherapists, and psychologists. These professions will have their own ethical standards, professional education, and bodies. The medical profession has been conceptualized from a sociological perspective.[20]
Provision of medical care is classified into primary, secondary, and tertiary care categories.[21]
Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care.[22] These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.
Secondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient.[23] Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, emergency departments, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.
Tertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.
Modern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.
In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.[24]
Separation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is independent from the pharmacist who provides the prescription drug. In the Western world there are centuries of tradition for separating pharmacists from physicians. In Asian countries, it is traditional for physicians to also provide drugs.[25]
Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, medical physicists, surgeons, surgeon's assistant, surgical technologist.
The scope and sciences underpinning human medicine overlap many other fields. A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.
Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.
The main branches of medicine are:
In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which has its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.
Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery". "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).
Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.
Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.
Surgical subspecialties include those a physician may specialize in after undergoing general surgery residency training as well as several surgical fields with separate residency training. Surgical subspecialties that one may pursue following general surgery residency training: [26]
Other surgical specialties within medicine with their own individual residency training:
Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases.[27] According to some sources, an emphasis on internal structures is implied.[28] In North America, specialists in internal medicine are commonly called "internists". Elsewhere, especially in Commonwealth nations, such specialists are often called physicians.[29] These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.
Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.[30]
In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.
There are many subspecialities (or subdisciplines) of internal medicine:
Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.
The following are some major medical specialties that do not directly fit into any of the above-mentioned groups:
Some interdisciplinary sub-specialties of medicine include:
Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.
Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs.  A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine.[32]
In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.
In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification.
The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.
Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.
Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:
Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.
Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.
Early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.
In Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine.[33]
In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery.[34] The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.[35]
In India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery.[36][dubious  – discuss][37] Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.[38][39]
In Greece, the ancient Greek physician Hippocrates, the "father of modern medicine",[40][41] laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence".[42][43] The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.
Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.[44]
The concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire.[45]
Although the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe.[46]
After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine".[47] He wrote The Canon of Medicine which became a standard medical text at many medieval European universities,[48] considered one of the most famous books in the history of medicine.[49] Others include Abulcasis,[50] Avenzoar,[51] Ibn al-Nafis,[52] and Averroes.[53] Persian physician Rhazes[54] was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine.[55] Some volumes of Rhazes's work Al-Mansuri, namely "On Surgery" and "A General Book on Therapy", became part of the medical curriculum in European universities.[56] Additionally, he has been described as a doctor's doctor,[57] the father of pediatrics,[54][58] and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light.[58] The Persian Bimaristan hospitals were an early example of public hospitals.[59][60]
In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.[61]
However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East.[62] In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.
The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the "traditional authority" approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.
Andreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy.[63] Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology.[64] Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris"[65] in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry".[66]
Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.
Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of variolation originated in ancient China),[67] Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.
The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.
From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.
Others that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States).
As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids.[68] Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids,[69] taxol, hyoscine, etc.).[70] Vaccines were discovered by Edward Jenner and Louis Pasteur.
The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.
Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making.
Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.[71]
Evidence-based medicine, prevention of medical error (and other "iatrogenesis"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations.[72]
Globally, many developing countries lack access to care and access to medicines.[73] As of 2015[update], most wealthy developed countries provide health care to all citizens, with a few exceptions such as the United States where lack of health insurance coverage may limit access.[74]


Leukemia (also spelled leukaemia and pronounced /luːˈkiːmiːə/[1] loo-KEE-mee-ə) is a group of blood cancers that usually begin in the bone marrow and result in high numbers of abnormal blood cells.[9] These blood cells are not fully developed and are called blasts or leukemia cells.[2] Symptoms may include bleeding and bruising, bone pain, fatigue, fever, and an increased risk of infections.[2] These symptoms occur due to a lack of normal blood cells.[2] Diagnosis is typically made by blood tests or bone marrow biopsy.[2]
The exact cause of leukemia is unknown.[5] A combination of genetic factors and environmental (non-inherited) factors are believed to play a role.[5] Risk factors include smoking, ionizing radiation, petrochemicals (such as benzene), prior chemotherapy, and Down syndrome.[5][3] People with a family history of leukemia are also at higher risk.[3] There are four main types of leukemia—acute lymphoblastic leukemia (ALL), acute myeloid leukemia (AML), chronic lymphocytic leukemia (CLL) and chronic myeloid leukemia (CML)—as well as a number of less common types.[3][10] Leukemias and lymphomas both belong to a broader group of tumors that affect the blood, bone marrow, and lymphoid system, known as tumors of the hematopoietic and lymphoid tissues.[11][12]
Treatment may involve some combination of chemotherapy, radiation therapy, targeted therapy, and bone marrow transplant, in addition to supportive care and palliative care as needed.[3][6] Certain types of leukemia may be managed with watchful waiting.[3] The success of treatment depends on the type of leukemia and the age of the person. Outcomes have improved in the developed world.[10] Five-year survival rate is 65% in the United States.[4] In children under 15 in first-world countries, the five-year survival rate is greater than 60% or even 90%, depending on the type of leukemia.[13]   In children with acute leukemia who are cancer-free after five years, the cancer is unlikely to return.[13]
In 2015, leukemia was present in 2.3 million people worldwide and caused 353,500 deaths.[7][8] In 2012, it had newly developed in 352,000 people.[10] It is the most common type of cancer in children, with three-quarters of leukemia cases in children being the acute lymphoblastic type.[3] However, over 90% of all leukemias are diagnosed in adults, with CLL and AML being most common in adults.[3][14] It occurs more commonly in the developed world.[10]
Clinically and pathologically, leukemia is subdivided into a variety of large groups. The first division is between its acute and chronic forms:[15]
Additionally, the diseases are subdivided according to which kind of blood cell is affected. This divides leukemias into lymphoblastic or lymphocytic leukemias and myeloid or myelogenous leukemias:[15]
Combining these two classifications provides a total of four main categories. Within each of these main categories, there are typically several subcategories. Finally, some rarer types are usually considered to be outside of this classification scheme.[15][16]
The most common symptoms in children are easy bruising, pale skin, fever, and an enlarged spleen or liver.[34]
Damage to the bone marrow, by way of displacing the normal bone marrow cells with higher numbers of immature white blood cells, results in a lack of blood platelets, which are important in the blood clotting process. This means people with leukemia may easily become bruised, bleed excessively, or develop pinprick bleeds (petechiae).[35]
White blood cells, which are involved in fighting pathogens, may be suppressed or dysfunctional. This could cause the person's immune system to be unable to fight off a simple infection or to start attacking other body cells. Because leukemia prevents the immune system from working normally, some people experience frequent infection, ranging from infected tonsils, sores in the mouth, or diarrhea to life-threatening pneumonia or opportunistic infections.[36]
Finally, the red blood cell deficiency leads to anemia, which may cause dyspnea and pallor.[37]
Some people experience other symptoms, such as fevers, chills, night sweats, weakness in the limbs, feeling fatigued and other common flu-like symptoms. Some people experience nausea or a feeling of fullness due to an enlarged liver and spleen; this can result in unintentional weight loss. Blasts affected by the disease may come together and become swollen in the liver or in the lymph nodes causing pain and leading to nausea.[38]
If the leukemic cells invade the central nervous system, then neurological symptoms (notably headaches) can occur. Uncommon neurological symptoms like migraines, seizures, or coma can occur as a result of brain stem pressure. All symptoms associated with leukemia can be attributed to other diseases. Consequently, leukemia is always diagnosed through medical tests.
The word leukemia, which means 'white blood', is derived from the characteristic high white blood cell count that presents in most affected people before treatment. The high number of white blood cells is apparent when a blood sample is viewed under a microscope, with the extra white blood cells frequently being immature or dysfunctional. The excessive number of cells can also interfere with the level of other cells, causing further harmful imbalance in the blood count.[39]
Some people diagnosed with leukemia do not have high white blood cell counts visible during a regular blood count. This less-common condition is called aleukemia. The bone marrow still contains cancerous white blood cells that disrupt the normal production of blood cells, but they remain in the marrow instead of entering the bloodstream, where they would be visible in a blood test. For a person with aleukemia, the white blood cell counts in the bloodstream can be normal or low. Aleukemia can occur in any of the four major types of leukemia, and is particularly common in hairy cell leukemia.[40]
Studies in 2009 and 2010 have shown a positive correlation between exposure to formaldehyde and the development of leukemia, particularly myeloid leukemia.[41][42] The different leukemias likely have different causes.[43]
Leukemia, like other cancers, results from mutations in the DNA. Certain mutations can trigger leukemia by activating oncogenes or deactivating tumor suppressor genes, and thereby disrupting the regulation of cell death, differentiation or division. These mutations may occur spontaneously or as a result of exposure to radiation or carcinogenic substances.[44]
Among adults, the known causes are natural and artificial ionizing radiation and petrochemicals, notably benzene and alkylating chemotherapy agents for previous malignancies.[45][46][47] Use of tobacco is associated with a small increase in the risk of developing acute myeloid leukemia in adults.[45] Cohort and case-control studies have linked exposure to some petrochemicals and hair dyes to the development of some forms of leukemia. Diet has very limited or no effect, although eating more vegetables may confer a small protective benefit.[48]
Viruses have also been linked to some forms of leukemia.  For example, human T-lymphotropic virus (HTLV-1) causes adult T-cell leukemia.[49]
A few cases of maternal-fetal transmission (a baby acquires leukemia because its mother had leukemia during the pregnancy) have been reported.[45] Children born to mothers who use fertility drugs to induce ovulation are more than twice as likely to develop leukemia during their childhoods than other children.[50]
In a recent systematic review and meta-analysis of any type of leukemia in neonates using phototherapy, typically to treat neonatal jaundice, a statistically significant association was detected between using phototherapy and myeloid leukemia. However, it is still questionable whether phototherapy is genuinely the cause of cancer or simply a result of the same underlying factors that gave rise to cancer. [51]
Large doses of Sr-90 emission from nuclear reactor accidents, nicknamed bone seeker increases the risk of bone cancer and leukemia in animals and is presumed to do so in people.[52]
Some people have a genetic predisposition towards developing leukemia. This predisposition is demonstrated by family histories and twin studies.[45] The affected people may have a single gene or multiple genes in common. In some cases, families tend to develop the same kinds of leukemia as other members; in other families, affected people may develop different forms of leukemia or related blood cancers.[45]
In addition to these genetic issues, people with chromosomal abnormalities or certain other genetic conditions have a greater risk of leukemia.[46] For example, people with Down syndrome have a significantly increased risk of developing forms of acute leukemia (especially acute myeloid leukemia), and Fanconi anemia is a risk factor for developing acute myeloid leukemia.[45] Mutation in SPRED1 gene has been associated with a predisposition to childhood leukemia.[53]
Chronic myelogenous leukemia is associated with a genetic abnormality called the Philadelphia translocation; 95% of people with CML carry the Philadelphia mutation, although this is not exclusive to CML and can be observed in people with other types of leukemia.[54][55][56][57]
Whether or not non-ionizing radiation causes leukemia has been studied for several decades. The International Agency for Research on Cancer expert working group undertook a detailed review of all data on static and extremely low frequency electromagnetic energy, which occurs naturally and in association with the generation, transmission, and use of electrical power.[58] They concluded that there is limited evidence that high levels of ELF magnetic (but not electric) fields might cause some cases of childhood leukemia.[58] No evidence for a relationship to leukemia or another form of malignancy in adults has been demonstrated.[58] Since exposure to such levels of ELFs is relatively uncommon, the World Health Organization concludes that ELF exposure, if later proven to be causative, would account for just 100 to 2400 cases worldwide each year, representing 0.2 to 4.9% of the total incidence of childhood leukemia for that year (about 0.03 to 0.9% of all leukemias).[59]
Diagnosis is usually based on repeated complete blood counts and a bone marrow examination following observations of the symptoms.  Sometimes, blood tests may not show that a person has leukemia, especially in the early stages of the disease or during remission.  A lymph node biopsy can be performed to diagnose certain types of leukemia in certain situations.[60]
Following diagnosis, blood chemistry tests can be used to determine the degree of liver and kidney damage or the effects of chemotherapy on the person. When concerns arise about other damages due to leukemia, doctors may use an X-ray, MRI, or ultrasound.  These can potentially show leukemia's effects on such body parts as bones (X-ray), the brain (MRI), or the kidneys, spleen, and liver (ultrasound).  CT scans can be used to check lymph nodes in the chest, though this is uncommon.[61]
Despite the use of these methods to diagnose whether or not a person has leukemia, many people have not been diagnosed because many of the symptoms are vague, non-specific, and can refer to other diseases.  For this reason, the American Cancer Society estimates that at least one-fifth of the people with leukemia have not yet been diagnosed.[40]
Most forms of leukemia are treated with pharmaceutical medication, typically combined into a multi-drug chemotherapy regimen. Some are also treated with radiation therapy. In some cases, a bone marrow transplant is effective.
Management of ALL is directed towards control of bone marrow and systemic (whole-body) disease. Additionally, treatment must prevent leukemic cells from spreading to other sites, particularly the central nervous system (CNS) e.g. monthly lumbar punctures.[clarification needed] In general, ALL treatment is divided into several phases:
Hematologists base CLL treatment on both the stage and symptoms of the individual person. A large group of people with CLL have low-grade disease, which does not benefit from treatment. Individuals with CLL-related complications or more advanced disease often benefit from treatment. In general, the indications for treatment are:
Most CLL cases are incurable by present treatments, so treatment is directed towards suppressing the disease for many years, rather than curing it. The primary chemotherapeutic plan is combination chemotherapy with chlorambucil or cyclophosphamide, plus a corticosteroid such as prednisone or prednisolone. The use of a corticosteroid has the additional benefit of suppressing some related autoimmune diseases, such as immunohemolytic anemia or immune-mediated thrombocytopenia. In resistant cases, single-agent treatments with nucleoside drugs such as fludarabine,[64] pentostatin, or cladribine may be successful. Younger and healthier people may choose allogeneic or autologous bone marrow transplantation in the hope of a permanent cure.[65]
Many different anti-cancer drugs are effective for the treatment of AML. Treatments vary somewhat according to the age of the person and according to the specific subtype of AML. Overall, the strategy is to control bone marrow and systemic (whole-body) disease, while offering specific treatment for the central nervous system (CNS), if involved.[66]
In general, most oncologists rely on combinations of drugs for the initial, induction phase of chemotherapy. Such combination chemotherapy usually offers the benefits of early remission and a lower risk of disease resistance. Consolidation and maintenance treatments are intended to prevent disease recurrence. Consolidation treatment often entails a repetition of induction chemotherapy or the intensification of chemotherapy with additional drugs. By contrast, maintenance treatment involves drug doses that are lower than those administered during the induction phase.[67]
There are many possible treatments for CML, but the standard of care for newly diagnosed people is imatinib (Gleevec) therapy.[68] Compared to most anti-cancer drugs, it has relatively few side effects and can be taken orally at home. With this drug, more than 90% of people will be able to keep the disease in check for at least five years,[68] so that CML becomes a chronic, manageable condition.
In a more advanced, uncontrolled state, when the person cannot tolerate imatinib, or if the person wishes to attempt a permanent cure, then an allogeneic bone marrow transplantation may be performed. This procedure involves high-dose chemotherapy and radiation followed by infusion of bone marrow from a compatible donor. Approximately 30% of people die from this procedure.[68]
Decision to treat
People with hairy cell leukemia who are symptom-free typically do not receive immediate treatment. Treatment is generally considered necessary when the person shows signs and symptoms such as low blood cell counts (e.g., infection-fighting neutrophil count below 1.0 K/µL), frequent infections, unexplained bruises, anemia, or fatigue that is significant enough to disrupt the person's everyday life.[69]
Typical treatment approach
People who need treatment usually receive either one week of cladribine, given daily by intravenous infusion or a simple injection under the skin, or six months of pentostatin, given every four weeks by intravenous infusion. In most cases, one round of treatment will produce a prolonged remission.[70]
Other treatments include rituximab infusion or self-injection with Interferon-alpha. In limited cases, the person may benefit from splenectomy (removal of the spleen). These treatments are not typically given as the first treatment because their success rates are lower than cladribine or pentostatin.[71]
Most people with T-cell prolymphocytic leukemia, a rare and aggressive leukemia with a median survival of less than one year, require immediate treatment.[72]
T-cell prolymphocytic leukemia is difficult to treat, and it does not respond to most available chemotherapeutic drugs.[72] Many different treatments have been attempted, with limited success in certain people: purine analogues (pentostatin, fludarabine, cladribine), chlorambucil, and various forms of combination chemotherapy (cyclophosphamide, doxorubicin, vincristine, prednisone CHOP, cyclophosphamide, vincristine, prednisone [COP], vincristine, doxorubicin, prednisone, etoposide, cyclophosphamide, bleomycin VAPEC-B). Alemtuzumab (Campath), a monoclonal antibody that attacks white blood cells, has been used in treatment with greater success than previous options.[72]
Some people who successfully respond to treatment also undergo stem cell transplantation to consolidate the response.[72]
Treatment for juvenile myelomonocytic leukemia can include splenectomy, chemotherapy, and bone marrow transplantation.[73]
The success of treatment depends on the type of leukemia and the age of the person. Outcomes have improved in the developed world.[10] The average five-year survival rate is 65% in the United States.[4] In children under 15, the five-year survival rate is greater (60 to 85%), depending on the type of leukemia.[13] In children with acute leukemia who are cancer-free after five years, the cancer is unlikely to return.[13]
Outcomes depend on whether it is acute or chronic, the specific abnormal white blood cell type, the presence and severity of anemia or thrombocytopenia, the degree of tissue abnormality, the presence of metastasis and lymph node and bone marrow infiltration, the availability of therapies and the skills of the health care team. Treatment outcomes may be better when people are treated at larger centers with greater experience.[74]
In 2010, globally, approximately 281,500 people died of leukemia.[75] In 2000, approximately 256,000 children and adults around the world developed a form of leukemia, and 209,000 died from it.[76] This represents about 3% of the almost seven million deaths due to cancer that year, and about 0.35% of all deaths from any cause.[76] Of the sixteen separate sites the body compared, leukemia was the 12th most common class of neoplastic disease and the 11th most common cause of cancer-related death.[76] Leukemia occurs more commonly in the developed world.[77]
About 245,000 people in the United States are affected with some form of leukemia, including those that have achieved remission or cure. Rates from 1975 to 2011 have increased by 0.7% per year among children.[78] Approximately 44,270 new cases of leukemia were diagnosed in the year 2008 in the US.[79] This represents 2.9% of all cancers (excluding simple basal cell and squamous cell skin cancers) in the United States, and 30.4% of all blood cancers.[80]
Among children with some form of cancer, about a third have a type of leukemia, most commonly acute lymphoblastic leukemia.[79] A type of leukemia is the second most common form of cancer in infants (under the age of 12 months) and the most common form of cancer in older children.[81] Boys are somewhat more likely to develop leukemia than girls, and white American children are almost twice as likely to develop leukemia than black American children.[81] Only about 3% cancer diagnoses among adults are for leukemias, but because cancer is much more common among adults, more than 90% of all leukemias are diagnosed in adults.[79]
Race is a risk factor in the United States.  Hispanics, especially those under the age of 20, are at the highest risk for leukemia, while whites, Native Americans, Asian Americans, and Alaska Natives are at higher risk than African Americans.[82]
More men than women are diagnosed with leukemia and die from the disease. Around 30 percent more men than women have leukemia.[83]
In Australia, leukemia is the eleventh most common cancer.[84] In 2014–2018, Australians diagnosed with leukaemia had a 64% chance (65% for males and 64% for females) of surviving for five years compared to the rest of the Australian population–there was a 21% increase in survival rates between 1989–1993.[84]
Overall, leukemia is the eleventh most common cancer in the UK (around 8,600 people were diagnosed with the disease in 2011), and it is the ninth most common cause of cancer death (around 4,800 people died in 2012).[85]
Leukemia was first described by anatomist and surgeon Alfred-Armand-Louis-Marie Velpeau in 1827. A more complete description was given by pathologist Rudolf Virchow in 1845. Around ten years after Virchow's findings, pathologist Franz Ernst Christian Neumann found that the bone marrow of a deceased person with leukemia was colored "dirty green-yellow" as opposed to the normal red. This finding allowed Neumann to conclude that a bone marrow problem was responsible for the abnormal blood of people with leukemia.[86]
By 1900, leukemia was viewed as a family of diseases as opposed to a single disease. By 1947, Boston pathologist Sidney Farber believed from past experiments that aminopterin, a folic acid mimic, could potentially cure leukemia in children. The majority of the children with ALL who were tested showed signs of improvement in their bone marrow, but none of them were actually cured. Nevertheless, this result did lead to further experiments.[87]
In 1962, researchers Emil J. Freireich, Jr. and Emil Frei III used combination chemotherapy to attempt to cure leukemia. The tests were successful with some people surviving long after the tests.[88]
Observing an abnormally large number of white blood cells in a blood sample from a person, Virchow called the condition Leukämie in German, which he formed from the two Greek words leukos (λευκός), meaning 'white', and haima (αἷμα), meaning 'blood'.[89] It was formerly also called leucemia.[90]
According to Susan Sontag, leukemia was often romanticized in 20th-century fiction, portrayed as a joy-ending, clean disease whose fair, innocent and gentle victims die young or at the wrong time. As such, it was the cultural successor to tuberculosis, which held this cultural position until it was discovered to be an infectious disease.[91] The 1970 romance novel Love Story is an example of this romanticization of leukemia.[92]
In the United States, around $5.4 billion is spent on treatment a year.[93]
Significant research into the causes, prevalence, diagnosis, treatment, and prognosis of leukemia is being performed. Hundreds of clinical trials are being planned or conducted at any given time.[94] Studies may focus on effective means of treatment, better ways of treating the disease, improving the quality of life for people, or appropriate care in remission or after cures.[95]
In general, there are two types of leukemia research: clinical or translational research and basic research. Clinical/translational research focuses on studying the disease in a defined and generally immediately applicable way, such as testing a new drug in people.  By contrast, basic science research studies the disease process at a distance, such as seeing whether a suspected carcinogen can cause leukemic changes in isolated cells in the laboratory or how the DNA changes inside leukemia cells as the disease progresses.  The results from basic research studies are generally less immediately useful to people with the disease.[96]
Treatment through gene therapy is currently being pursued. One such approach used genetically modified T cells, known as chimeric antigen receptor T cells (CAR-T cells), to attack cancer cells. In 2011, a year after treatment, two of the three people with advanced chronic lymphocytic leukemia were reported to be cancer-free[97] and in 2013, three of five subjects who had acute lymphocytic leukemia were reported to be in remission for five months to two years.[98] Subsequent studies with a variety of CAR-T types continue to be promising.[99] As of 2018, two CAR-T therapies have been approved by the Food and Drug Administration. CAR-T treatment has significant side effects,[100] and loss of the antigen targeted by the CAR-T cells is a common mechanism for relapse.[99] The stem cells that cause different types of leukemia are also being researched.[101]
Leukemia is rarely associated with pregnancy, affecting only about 1 in 10,000 pregnant women.[102] How it is handled depends primarily on the type of leukemia. Nearly all leukemias appearing in pregnant women are acute leukemias.[103] Acute leukemias normally require prompt, aggressive treatment, despite significant risks of pregnancy loss and birth defects, especially if chemotherapy is given during the developmentally sensitive first trimester.[102] Chronic myelogenous leukemia can be treated with relative safety at any time during pregnancy with Interferon-alpha hormones.[102] Treatment for chronic lymphocytic leukemias, which are rare in pregnant women, can often be postponed until after the end of the pregnancy.[102][103]
aggressive: Sézary disease

Allergies, also known as allergic diseases, are various conditions caused by hypersensitivity of the immune system to typically harmless substances in the environment.[11] These diseases include hay fever, food allergies, atopic dermatitis, allergic asthma, and anaphylaxis.[1] Symptoms may include red eyes, an itchy rash, sneezing, coughing, a runny nose, shortness of breath, or swelling.[12] Note that food intolerances and food poisoning are separate conditions.[3][4]
Common allergens include pollen and certain foods.[11] Metals and other substances may also cause such problems.[11] Food, insect stings, and medications are common causes of severe reactions.[2] Their development is due to both genetic and environmental factors.[2] The underlying mechanism involves immunoglobulin E antibodies (IgE), part of the body's immune system, binding to an allergen and then to a receptor on mast cells or basophils where it triggers the release of inflammatory chemicals such as histamine.[13] Diagnosis is typically based on a person's medical history.[3] Further testing of the skin or blood may be useful in certain cases.[3] Positive tests, however, may not necessarily mean there is a significant allergy to the substance in question.[14]
Early exposure of children to potential allergens may be protective.[5] Treatments for allergies include avoidance of known allergens and the use of medications such as steroids and antihistamines.[6] In severe reactions, injectable adrenaline (epinephrine) is recommended.[7] Allergen immunotherapy, which gradually exposes people to larger and larger amounts of allergen, is useful for some types of allergies such as hay fever and reactions to insect bites.[6] Its use in food allergies is unclear.[6]
Allergies are common.[10] In the developed world, about 20% of people are affected by allergic rhinitis,[15] about 6% of people have at least one food allergy,[3][5] and about 20% have or have had atopic dermatitis at some point in time.[16] Depending on the country, about 1–18% of people have asthma.[17][18] Anaphylaxis occurs in between 0.05–2% of people.[19] Rates of many allergic diseases appear to be increasing.[7][20][21] The word "allergy" was first used by Clemens von Pirquet in 1906.[2]
Many allergens such as dust or pollen are airborne particles. In these cases, symptoms arise in areas in contact with air, such as the eyes, nose, and lungs. For instance, allergic rhinitis, also known as hay fever, causes irritation of the nose, sneezing, itching, and redness of the eyes.[22] Inhaled allergens can also lead to increased production of mucus in the lungs, shortness of breath, coughing, and wheezing.[23]
Aside from these ambient allergens, allergic reactions can result from foods, insect stings, and reactions to medications like aspirin and antibiotics such as penicillin. Symptoms of food allergy include abdominal pain, bloating, vomiting, diarrhea, itchy skin, and hives. Food allergies rarely cause respiratory (asthmatic) reactions, or rhinitis.[24] Insect stings, food, antibiotics, and certain medicines may produce a systemic allergic response that is also called anaphylaxis; multiple organ systems can be affected, including the digestive system, the respiratory system, and the circulatory system.[25][26][27] Depending on the severity, anaphylaxis can include skin reactions, bronchoconstriction, swelling, low blood pressure, coma, and death. This type of reaction can be triggered suddenly, or the onset can be delayed. The nature of anaphylaxis is such that the reaction can seem to be subsiding but may recur throughout a period of time.[27]
Substances that come into contact with the skin, such as latex, are also common causes of allergic reactions, known as contact dermatitis or eczema.[28] Skin allergies frequently cause rashes, or swelling and inflammation within the skin, in what is known as a "weal and flare" reaction characteristic of hives and angioedema.[29]
With insect stings, a large local reaction may occur in the form of an area of skin redness greater than 10 cm in size that can last one to two days.[30] This reaction may also occur after immunotherapy.[31]
Risk factors for allergies can be placed in two broad categories, namely host and environmental factors.[32] Host factors include heredity, sex, race, and age, with heredity being by far the most significant. However, there has been a recent increase in the incidence of allergic disorders that cannot be explained by genetic factors alone. Four major environmental candidates are alterations in exposure to infectious diseases during early childhood, environmental pollution, allergen levels, and dietary changes.[33]
Dust mite allergy, also known as house dust allergy, is a sensitization and allergic reaction to the droppings of house dust mites. The allergy is common[34][35] and can trigger allergic reactions such as asthma, eczema, or itching. It is the manifestation of parasitosis. The mite's gut contains potent digestive enzymes (notably peptidase 1) that persist in their feces and are major inducers of allergic reactions such as wheezing. The mite's exoskeleton can also contribute to allergic reactions. Unlike scabies mites or skin follicle mites, house dust mites do not burrow under the skin and are not parasitic.[36]
A wide variety of foods can cause allergic reactions, but 90% of allergic responses to foods are caused by cow's milk, soy, eggs, wheat, peanuts, tree nuts, fish, and shellfish.[37] Other food allergies, affecting less than 1 person per 10,000 population, may be considered "rare".[38] The use of hydrolyzed milk baby formula versus standard milk baby formula does not appear to affect the risk.[39]
The most common food allergy in the US population is a sensitivity to crustacea.[38] Although peanut allergies are notorious for their severity, peanut allergies are not the most common food allergy in adults or children. Severe or life-threatening reactions may be triggered by other allergens and are more common when combined with asthma.[37]
Rates of allergies differ between adults and children. Children can sometimes outgrow peanut allergies. Egg allergies affect one to two percent of children but are outgrown by about two-thirds of children by the age of 5.[40] The sensitivity is usually to proteins in the white, rather than the yolk.[41]
Milk-protein allergies are most common in children.[42] Approximately 60% of milk-protein reactions are immunoglobulin E-mediated, with the remaining usually attributable to inflammation of the colon.[43] Some people are unable to tolerate milk from goats or sheep as well as from cows, and many are also unable to tolerate dairy products such as cheese. Roughly 10% of children with a milk allergy will have a reaction to beef. Beef contains small amounts of proteins that are present in greater abundance in cow's milk.[44] Lactose intolerance, a common reaction to milk, is not a form of allergy at all, but due to the absence of an enzyme in the digestive tract.[citation needed]
Those with tree nut allergies may be allergic to one or to many tree nuts, including pecans, pistachios, pine nuts, and walnuts.[41] In addition, seeds, including sesame seeds and poppy seeds, contain oils in which protein is present, which may elicit an allergic reaction.[41]
Allergens can be transferred from one food to another through genetic engineering; however genetic modification can also remove allergens. Little research has been done on the natural variation of allergen concentrations in unmodified crops.[45][46]
Latex can trigger an IgE-mediated cutaneous, respiratory, and systemic reaction. The prevalence of latex allergy in the general population is believed to be less than one percent. In a hospital study, 1 in 800 surgical patients (0.125 percent) reported latex sensitivity, although the sensitivity among healthcare workers is higher, between seven and ten percent. Researchers attribute this higher level to the exposure of healthcare workers to areas with significant airborne latex allergens, such as operating rooms, intensive-care units, and dental suites. These latex-rich environments may sensitize healthcare workers who regularly inhale allergenic proteins.[47]
The most prevalent response to latex is an allergic contact dermatitis, a delayed hypersensitive reaction appearing as dry, crusted lesions. This reaction usually lasts 48–96 hours. Sweating or rubbing the area under the glove aggravates the lesions, possibly leading to ulcerations.[47] Anaphylactic reactions occur most often in sensitive patients who have been exposed to a surgeon's latex gloves during abdominal surgery, but other mucosal exposures, such as dental procedures, can also produce systemic reactions.[47]
Latex and banana sensitivity may cross-react.  Furthermore, those with latex allergy may also have sensitivities to avocado, kiwifruit, and chestnut.[48] These people often have perioral itching and local urticaria. Only occasionally have these food-induced allergies induced systemic responses. Researchers suspect that the cross-reactivity of latex with banana, avocado, kiwifruit, and chestnut occurs because latex proteins are structurally homologous with some other plant proteins.[47]
About 10% of people report that they are allergic to penicillin; however, of that 10%, 90% turn out not to be.[49] Serious allergies only occur in about 0.03%.[49]
Typically, insects which generate allergic responses are either stinging insects (wasps, bees, hornets and ants) or biting insects (mosquitoes, ticks). Stinging insects inject venom into their victims, whilst biting insects normally introduce anti-coagulants.[citation needed]
Another non-food protein reaction, urushiol-induced contact dermatitis, originates after contact with poison ivy, eastern poison oak, western poison oak, or poison sumac. Urushiol, which is not itself a protein, acts as a hapten and chemically reacts with, binds to, and changes the shape of integral membrane proteins on exposed skin cells. The immune system does not recognize the affected cells as normal parts of the body, causing a T-cell-mediated immune response.[50] Of these poisonous plants, sumac is the most virulent.[51][52] The resulting dermatological response to the reaction between urushiol and membrane proteins includes redness, swelling, papules, vesicles, blisters, and streaking.[53]
Estimates vary on the population fraction that will have an immune system response. Approximately 25% of the population will have a strong allergic response to urushiol. In general, approximately 80–90% of adults will develop a rash if they are exposed to 0.0050 mg (7.7×10−5 gr) of purified urushiol, but some people are so sensitive that it takes only a molecular trace on the skin to initiate an allergic reaction.[54]
Allergic diseases are strongly familial: identical twins are likely to have the same allergic diseases about 70% of the time; the same allergy occurs about 40% of the time in non-identical twins.[55] Allergic parents are more likely to have allergic children,[56] and those children's allergies are likely to be more severe than those in children of non-allergic parents. Some allergies, however, are not consistent along genealogies; parents who are allergic to peanuts may have children who are allergic to ragweed. The likelihood of developing allergies is inherited and related to an irregularity in the immune system, but the specific allergen is not.[56]
The risk of allergic sensitization and the development of allergies varies with age, with young children most at risk.[57] Several studies have shown that IgE levels are highest in childhood and fall rapidly between the ages of 10 and 30 years.[57] The peak prevalence of hay fever is highest in children and young adults and the incidence of asthma is highest in children under 10.[58]
Ethnicity may play a role in some allergies; however, racial factors have been difficult to separate from environmental influences and changes due to migration.[56] It has been suggested that different genetic loci are responsible for asthma, to be specific, in people of European, Hispanic, Asian, and African origins.[59]
Allergic diseases are caused by inappropriate immunological responses to harmless antigens driven by a TH2-mediated immune response. Many bacteria and viruses elicit a TH1-mediated immune response, which down-regulates TH2 responses. The first proposed mechanism of action of the hygiene hypothesis was that insufficient stimulation of the TH1 arm of the immune system leads to an overactive TH2 arm, which in turn leads to allergic disease.[60] In other words, individuals living in too sterile an environment are not exposed to enough pathogens to keep the immune system busy. Since our bodies evolved to deal with a certain level of such pathogens, when they are not exposed to this level, the immune system will attack harmless antigens, and thus normally benign microbial objects—like pollen—will trigger an immune response.[61]
The hygiene hypothesis was developed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were, it is presumed, exposed to more infectious agents through their siblings, than in children from families with only one child. The hygiene hypothesis has been extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of allergic disorders. It is used to explain the increase in allergic diseases that have been seen since industrialization, and the higher incidence of allergic diseases in more developed countries. The hygiene hypothesis has now expanded to include exposure to symbiotic bacteria and parasites as important modulators of immune system development, along with infectious agents.[citation needed]
Epidemiological data support the hygiene hypothesis. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world, and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world.[62] Longitudinal studies in the third world demonstrate an increase in immunological disorders as a country grows more affluent and, it is presumed, cleaner.[63] The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases.[64] The use of antibacterial cleaning products has also been associated with higher incidence of asthma, as has birth by Caesarean section rather than vaginal birth.[65][66]
Chronic stress can aggravate allergic conditions. This has been attributed to a T helper 2 (TH2)-predominant response driven by suppression of interleukin 12 by both the autonomic nervous system and the hypothalamic–pituitary–adrenal axis. Stress management in highly susceptible individuals may improve symptoms.[67]
Allergic diseases are more common in industrialized countries than in countries that are more traditional or agricultural, and there is a higher rate of allergic disease in urban populations versus rural populations, although these differences are becoming less defined.[68] Historically, the trees planted in urban areas were predominantly male to prevent litter from seeds and fruits, but the high ratio of male trees causes high pollen counts, a phenomenon that horticulturist Tom Ogren has called "botanical sexism".[69]
Alterations in exposure to microorganisms is another plausible explanation, at present, for the increase in atopic allergy.[33] Endotoxin exposure reduces release of inflammatory cytokines such as TNF-α, IFNγ, interleukin-10, and interleukin-12 from white blood cells (leukocytes) that circulate in the blood.[70] Certain microbe-sensing proteins, known as Toll-like receptors, found on the surface of cells in the body are also thought to be involved in these processes.[71]
Parasitic worms and similar parasites are present in untreated drinking water in developing countries, and were present in the water of developed countries until the routine chlorination and purification of drinking water supplies.[72] Recent research has shown that some common parasites, such as intestinal worms (e.g., hookworms), secrete chemicals into the gut wall (and, hence, the bloodstream) that suppress the immune system and prevent the body from attacking the parasite.[73] This gives rise to a new slant on the hygiene hypothesis theory—that co-evolution of humans and parasites has led to an immune system that functions correctly only in the presence of the parasites. Without them, the immune system becomes unbalanced and oversensitive.[74] In particular, research suggests that allergies may coincide with the delayed establishment of gut flora in infants.[75] However, the research to support this theory is conflicting, with some studies performed in China and Ethiopia showing an increase in allergy in people infected with intestinal worms.[68] Clinical trials have been initiated to test the effectiveness of certain worms in treating some allergies.[76] It may be that the term 'parasite' could turn out to be inappropriate, and in fact a hitherto unsuspected symbiosis is at work.[76] For more information on this topic, see Helminthic therapy.
In the initial stages of allergy, a type I hypersensitivity reaction against an allergen encountered for the first time and presented by a professional antigen-presenting cell causes a response in a type of immune cell called a TH2 lymphocyte, a subset of T cells that produce a cytokine called interleukin-4 (IL-4). These TH2 cells interact with other lymphocytes called B cells, whose role is production of antibodies. Coupled with signals provided by IL-4, this interaction stimulates the B cell to begin production of a large amount of a particular type of antibody known as IgE. Secreted IgE circulates in the blood and binds to an IgE-specific receptor (a kind of Fc receptor called FcεRI) on the surface of other kinds of immune cells called mast cells and basophils, which are both involved in the acute inflammatory response. The IgE-coated cells, at this stage, are sensitized to the allergen.[33]
If later exposure to the same allergen occurs, the allergen can bind to the IgE molecules held on the surface of the mast cells or basophils. Cross-linking of the IgE and Fc receptors occurs when more than one IgE-receptor complex interacts with the same allergenic molecule and activates the sensitized cell. Activated mast cells and basophils undergo a process called degranulation, during which they release histamine and other inflammatory chemical mediators (cytokines, interleukins, leukotrienes, and prostaglandins) from their granules into the surrounding tissue causing several systemic effects, such as vasodilation, mucous secretion, nerve stimulation, and smooth muscle contraction. This results in rhinorrhea, itchiness, dyspnea, and anaphylaxis. Depending on the individual, allergen, and mode of introduction, the symptoms can be system-wide (classical anaphylaxis) or localized to specific body systems. Asthma is localized to the respiratory system and eczema is localized to the dermis.[33]
After the chemical mediators of the acute response subside, late-phase responses can often occur. This is due to the migration of other leukocytes such as neutrophils, lymphocytes, eosinophils, and macrophages to the initial site. The reaction is usually seen 2–24 hours after the original reaction.[77] Cytokines from mast cells may play a role in the persistence of long-term effects. Late-phase responses seen in asthma are slightly different from those seen in other allergic responses, although they are still caused by release of mediators from eosinophils and are still dependent on activity of TH2 cells.[78]
Although allergic contact dermatitis is termed an "allergic" reaction (which usually refers to type I hypersensitivity), its pathophysiology involves a reaction that more correctly corresponds to a type IV hypersensitivity reaction.[79] In type IV hypersensitivity, there is activation of certain types of T cells (CD8+) that destroy target cells on contact, as well as activated macrophages that produce hydrolytic enzymes.[citation needed]
Effective management of allergic diseases relies on the ability to make an accurate diagnosis.[80] Allergy testing can help confirm or rule out allergies.[81][82] Correct diagnosis, counseling, and avoidance advice based on valid allergy test results reduce the incidence of symptoms and need for medications, and improve quality of life.[81] To assess the presence of allergen-specific IgE antibodies, two different methods can be used: a skin prick test, or an allergy blood test. Both methods are recommended, and they have similar diagnostic value.[82][83]
Skin prick tests and blood tests are equally cost-effective, and health economic evidence shows that both tests were cost-effective compared with no test.[81] Early and more accurate diagnoses save cost due to reduced consultations, referrals to secondary care, misdiagnosis, and emergency admissions.[84]
Allergy undergoes dynamic changes over time. Regular allergy testing of relevant allergens provides information on if and how patient management can be changed to improve health and quality of life. Annual testing is often the practice for determining whether allergy to milk, egg, soy, and wheat have been outgrown, and the testing interval is extended to 2–3 years for allergy to peanut, tree nuts, fish, and crustacean shellfish.[82] Results of follow-up testing can guide decision-making regarding whether and when it is safe to introduce or re-introduce allergenic food into the diet.[85]
Skin testing is also known as "puncture testing" and "prick testing" due to the series of tiny punctures or pricks made into the patient's skin. Tiny amounts of suspected allergens and/or their extracts (e.g., pollen, grass, mite proteins, peanut extract) are introduced to sites on the skin marked with pen or dye (the ink/dye should be carefully selected, lest it cause an allergic response itself). A small plastic or metal device is used to puncture or prick the skin. Sometimes, the allergens are injected "intradermally" into the patient's skin, with a needle and syringe. Common areas for testing include the inside forearm and the back.
If the patient is allergic to the substance, then a visible inflammatory reaction will usually occur within 30 minutes. This response will range from slight reddening of the skin to a full-blown hive (called "wheal and flare") in more sensitive patients similar to a mosquito bite. Interpretation of the results of the skin prick test is normally done by allergists on a scale of severity, with +/− meaning borderline reactivity, and 4+ being a large reaction. Increasingly, allergists are measuring and recording the diameter of the wheal and flare reaction. Interpretation by well-trained allergists is often guided by relevant literature.[86] Some patients may believe they have determined their own allergic sensitivity from observation, but a skin test has been shown to be much better than patient observation to detect allergy.[87]
If a serious life-threatening anaphylactic reaction has brought a patient in for evaluation, some allergists will prefer an initial blood test prior to performing the skin prick test. Skin tests may not be an option if the patient has widespread skin disease or has taken antihistamines in the last several days.
Patch testing is a method used to determine if a specific substance causes allergic inflammation of the skin. It tests for delayed reactions. It is used to help ascertain the cause of skin contact allergy or contact dermatitis. Adhesive patches, usually treated with several common allergic chemicals or skin sensitizers, are applied to the back. The skin is then examined for possible local reactions at least twice, usually at 48 hours after application of the patch, and again two or three days later.
An allergy blood test is quick and simple and can be ordered by a licensed health care provider (e.g., an allergy specialist) or general practitioner. Unlike skin-prick testing, a blood test can be performed irrespective of age, skin condition, medication, symptom, disease activity, and pregnancy. Adults and children of any age can get an allergy blood test. For babies and very young children, a single needle stick for allergy blood testing is often gentler than several skin pricks.
An allergy blood test is available through most laboratories. A sample of the patient's blood is sent to a laboratory for analysis, and the results are sent back a few days later. Multiple allergens can be detected with a single blood sample. Allergy blood tests are very safe since the person is not exposed to any allergens during the testing procedure.
The test measures the concentration of specific IgE antibodies in the blood. Quantitative IgE test results increase the possibility of ranking how different substances may affect symptoms. A rule of thumb is that the higher the IgE antibody value, the greater the likelihood of symptoms. Allergens found at low levels that today do not result in symptoms cannot help predict future symptom development. The quantitative allergy blood result can help determine what a patient is allergic to, help predict and follow the disease development, estimate the risk of a severe reaction, and explain cross-reactivity.[88][89]
A low total IgE level is not adequate to rule out sensitization to commonly inhaled allergens.[90] Statistical methods, such as ROC curves, predictive value calculations, and likelihood ratios have been used to examine the relationship of various testing methods to each other. These methods have shown that patients with a high total IgE have a high probability of allergic sensitization, but further investigation with allergy tests for specific IgE antibodies for a carefully chosen of allergens is often warranted.
Laboratory methods to measure specific IgE antibodies for allergy testing include enzyme-linked immunosorbent assay (ELISA, or EIA),[91] radioallergosorbent test (RAST)[91] and fluorescent enzyme immunoassay (FEIA).[92]
Challenge testing: Challenge testing is when tiny amounts of a suspected allergen are introduced to the body orally, through inhalation, or via other routes. Except for testing food and medication allergies, challenges are rarely performed. When this type of testing is chosen, it must be closely supervised by an allergist.
Elimination/challenge tests: This testing method is used most often with foods or medicines. A patient with a suspected allergen is instructed to modify his diet to totally avoid that allergen for a set time. If the patient experiences significant improvement, he may then be "challenged" by reintroducing the allergen, to see if symptoms are reproduced.
Unreliable tests: There are other types of allergy testing methods that are unreliable, including applied kinesiology (allergy testing through muscle relaxation), cytotoxicity testing, urine autoinjection, skin titration (Rinkel method), and provocative and neutralization (subcutaneous) testing or sublingual provocation.[93]
Before a diagnosis of allergic disease can be confirmed, other plausible causes of the presenting symptoms should be considered.[94] Vasomotor rhinitis, for example, is one of many illnesses that share symptoms with allergic rhinitis, underscoring the need for professional differential diagnosis.[95] Once a diagnosis of asthma, rhinitis, anaphylaxis, or other allergic disease has been made, there are several methods for discovering the causative agent of that allergy.
Giving peanut products early may decrease the risk of allergies while only breastfeeding during at least the first few months of life may decrease the risk of dermatitis.[96][97] There is no good evidence that a mother's diet during pregnancy or breastfeeding affects the risk of allergies,[96] nor is there evidence that delayed introduction of certain foods is useful.[96] Early exposure to potential allergens may actually be protective.[5]
Fish oil supplementation during pregnancy is associated with a lower risk.[97] Probiotic supplements during pregnancy or infancy may help to prevent atopic dermatitis.[98][99]
Management of allergies typically involves avoiding the allergy trigger and taking medications to improve the symptoms.[6] Allergen immunotherapy may be useful for some types of allergies.[6]
Several medications may be used to block the action of allergic mediators, or to prevent activation of cells and degranulation processes. These include antihistamines, glucocorticoids, epinephrine (adrenaline), mast cell stabilizers, and antileukotriene agents are common treatments of allergic diseases.[100] Anticholinergics, decongestants, and other compounds thought to impair eosinophil chemotaxis are also commonly used. Although rare, the severity of anaphylaxis often requires epinephrine injection, and where medical care is unavailable, a device known as an epinephrine autoinjector may be used.[27]
Allergen immunotherapy is useful for environmental allergies, allergies to insect bites, and asthma.[6][101] Its benefit for food allergies is unclear and thus not recommended.[6] Immunotherapy involves exposing people to larger and larger amounts of allergen in an effort to change the immune system's response.[6]
Meta-analyses have found that injections of allergens under the skin is effective in the treatment in allergic rhinitis in children[102][103] and in asthma.[101] The benefits may last for years after treatment is stopped.[104] It is generally safe and effective for allergic rhinitis and conjunctivitis, allergic forms of asthma, and stinging insects.[105]
To a lesser extent, the evidence also supports the use of sublingual immunotherapy for rhinitis and asthma.[104] For seasonal allergies the benefit is small.[106] In this form the allergen is given under the tongue and people often prefer it to injections.[104] Immunotherapy is not recommended as a stand-alone treatment for asthma.[104]
An experimental treatment, enzyme potentiated desensitization (EPD), has been tried for decades but is not generally accepted as effective.[107] EPD uses dilutions of allergen and an enzyme, beta-glucuronidase, to which T-regulatory lymphocytes are supposed to respond by favoring desensitization, or down-regulation, rather than sensitization. EPD has also been tried for the treatment of autoimmune diseases, but evidence does not show effectiveness.[107]
A review found no effectiveness of homeopathic treatments and no difference compared with placebo. The authors concluded that based on rigorous clinical trials of all types of homeopathy for childhood and adolescence ailments, there is no convincing evidence that supports the use of homeopathic treatments.[108]
According to the National Center for Complementary and Integrative Health, U.S., the evidence is relatively strong that saline nasal irrigation and butterbur are effective, when compared to other alternative medicine treatments, for which the scientific evidence is weak, negative, or nonexistent, such as honey, acupuncture, omega 3's, probiotics, astragalus, capsaicin, grape seed extract, Pycnogenol, quercetin, spirulina, stinging nettle, tinospora, or guduchi.
[109][110]
The allergic diseases—hay fever and asthma—have increased in the Western world over the past 2–3 decades.[111] Increases in allergic asthma and other atopic disorders in industrialized nations, it is estimated, began in the 1960s and 1970s, with further increases occurring during the 1980s and 1990s,[112] although some suggest that a steady rise in sensitization has been occurring since the 1920s.[113] The number of new cases per year of atopy in developing countries has, in general, remained much lower.[112]
Although genetic factors govern susceptibility to atopic disease, increases in atopy have occurred within too short a period to be explained by a genetic change in the population, thus pointing to environmental or lifestyle changes.[112] Several hypotheses have been identified to explain this increased rate. Increased exposure to perennial allergens may be due to housing changes and increased time spent indoors, and a decreased activation of a common immune control mechanism may be caused by changes in cleanliness or hygiene, and exacerbated by dietary changes, obesity, and decline in physical exercise.[111] The hygiene hypothesis maintains[125] that high living standards and hygienic conditions exposes children to fewer infections. It is thought that reduced bacterial and viral infections early in life direct the maturing immune system away from TH1 type responses, leading to unrestrained TH2 responses that allow for an increase in allergy.[74][126]
Changes in rates and types of infection alone, however, have been unable to explain the observed increase in allergic disease, and recent evidence has focused attention on the importance of the gastrointestinal microbial environment. Evidence has shown that exposure to food and fecal-oral pathogens, such as hepatitis A, Toxoplasma gondii, and Helicobacter pylori (which also tend to be more prevalent in developing countries), can reduce the overall risk of atopy by more than 60%,[127] and an increased rate of parasitic infections has been associated with a decreased prevalence of asthma.[128] It is speculated that these infections exert their effect by critically altering TH1/TH2 regulation.[129] Important elements of newer hygiene hypotheses also include exposure to endotoxins, exposure to pets and growing up on a farm.[129]
Some symptoms attributable to allergic diseases are mentioned in ancient sources.[130] Particularly, three members of the Roman Julio-Claudian dynasty (Augustus, Claudius and Britannicus) are suspected to have a family history of atopy.[130][131] The concept of "allergy" was originally introduced in 1906 by the Viennese pediatrician Clemens von Pirquet, after he noticed that patients who had received injections of horse serum or smallpox vaccine usually had quicker, more severe reactions to second injections.[132] Pirquet called this phenomenon "allergy" from the Ancient Greek words ἄλλος allos meaning "other" and ἔργον ergon meaning "work".[133]
All forms of hypersensitivity used to be classified as allergies, and all were thought to be caused by an improper activation of the immune system. Later, it became clear that several different disease mechanisms were implicated, with a common link to a disordered activation of the immune system. In 1963, a new classification scheme was designed by Philip Gell and Robin Coombs that described four types of hypersensitivity reactions, known as Type I to Type IV hypersensitivity.[134] With this new classification, the word allergy, sometimes clarified as a true allergy, was restricted to type I hypersensitivities (also called immediate hypersensitivity), which are characterized as rapidly developing reactions involving IgE antibodies.[135]
A major breakthrough in understanding the mechanisms of allergy was the discovery of the antibody class labeled immunoglobulin E (IgE). IgE was simultaneously discovered in 1966–67 by two independent groups:[136] Ishizaka's team at the Children's Asthma Research Institute and Hospital in Denver, USA,[137] and by Gunnar Johansson and Hans Bennich in Uppsala, Sweden.[138] Their joint paper was published in April 1969.[139]
Radiometric assays include the radioallergosorbent test (RAST test) method, which uses IgE-binding (anti-IgE) antibodies labeled with radioactive isotopes for quantifying the levels of IgE antibody in the blood.[140] Other, newer methods use colorimetric or fluorescence-labeled technology in the place of radioactive isotopes.[citation needed]
The RAST methodology was invented and marketed in 1974 by Pharmacia Diagnostics AB, Uppsala, Sweden, and the acronym RAST is actually a brand name. In 1989, Pharmacia Diagnostics AB replaced it with a superior test named the ImmunoCAP Specific IgE blood test, which uses the newer fluorescence-labeled technology.[citation needed]
American College of Allergy Asthma and Immunology (ACAAI) and the American Academy of Allergy Asthma and Immunology (AAAAI) issued the Joint Task Force Report "Pearls and pitfalls of allergy diagnostic testing" in 2008, and is firm in its statement that the term RAST is now obsolete:
The term RAST became a colloquialism for all varieties of (in vitro allergy) tests.  This is unfortunate because it is well recognized that there are well-performing tests and some that do not perform so well, yet they are all called RASTs, making it difficult to distinguish which is which. For these reasons, it is now recommended that use of RAST as a generic descriptor of these tests be abandoned.[14]The updated version, the ImmunoCAP Specific IgE blood test, is the only specific IgE assay to receive Food and Drug Administration approval to quantitatively report to its detection limit of 0.1kU/L.[citation needed]
An allergist is a physician specially trained to manage and treat allergies, asthma, and the other allergic diseases.
In the United States physicians holding certification by the American Board of Allergy and Immunology (ABAI) have successfully completed an accredited educational program and evaluation process, including a proctored examination to demonstrate knowledge, skills, and experience in patient care in allergy and immunology.[141] Becoming an allergist/immunologist requires completion of at least nine years of training. After completing medical school and graduating with a medical degree, a physician will undergo three years of training in internal medicine (to become an internist) or pediatrics (to become a pediatrician). Once physicians have finished training in one of these specialties, they must pass the exam of either the American Board of Pediatrics (ABP), the American Osteopathic Board of Pediatrics (AOBP), the American Board of Internal Medicine (ABIM), or the American Osteopathic Board of Internal Medicine (AOBIM). Internists or pediatricians wishing to focus on the sub-specialty of allergy-immunology then complete at least an additional two years of study, called a fellowship, in an allergy/immunology training program. Allergist/immunologists listed as ABAI-certified have successfully passed the certifying examination of the ABAI following their fellowship.[142]
In the United Kingdom, allergy is a subspecialty of general medicine or pediatrics. After obtaining postgraduate exams (MRCP or MRCPCH), a doctor works for several years as a specialist registrar before qualifying for the General Medical Council specialist register. Allergy services may also be delivered by immunologists. A 2003 Royal College of Physicians report presented a case for improvement of what were felt to be inadequate allergy services in the UK.[143] In 2006, the House of Lords convened a subcommittee. It concluded likewise in 2007 that allergy services were insufficient to deal with what the Lords referred to as an "allergy epidemic" and its social cost; it made several recommendations.[144]
Low-allergen foods are being developed, as are improvements in skin prick test predictions; evaluation of the atopy patch test, wasp sting outcomes predictions, a rapidly disintegrating epinephrine tablet, and anti-IL-5 for eosinophilic diseases.[145]

Type 1 diabetes (T1D), formerly known as juvenile diabetes, is an autoimmune disease that originates when cells that make insulin (beta cells) are destroyed by the immune system.[4] Insulin is a hormone required for the cells to use blood sugar for energy and it helps regulate glucose levels in the bloodstream.[2] Before treatment this results in high blood sugar levels in the body.[1] The common symptoms of this elevated blood sugar are frequent urination, increased thirst, increased hunger, weight loss, and other serious complications.[4][10] Additional symptoms may include blurry vision, tiredness, and slow wound healing.[2] Symptoms typically develop over a short period of time, often a matter of weeks if not months.[1]
The cause of type 1 diabetes is unknown,[4] but it is believed to involve a combination of genetic and environmental factors.[1] The underlying mechanism involves an autoimmune destruction of the insulin-producing beta cells in the pancreas.[2] Diabetes is diagnosed by testing the level of sugar or glycated hemoglobin (HbA1C) in the blood.[6][8] Type 1 diabetes can typically be distinguished from type 2 by testing for the presence of autoantibodies[6] and/or declining levels/absence of C-peptide. 
There is no known way to prevent type 1 diabetes.[4] Treatment with insulin is required for survival.[1] Insulin therapy is usually given by injection just under the skin but can also be delivered by an insulin pump.[11] A diabetic diet and exercise are important parts of management.[2] If left untreated, diabetes can cause many complications.[4] Complications of relatively rapid onset include diabetic ketoacidosis and nonketotic hyperosmolar coma.[6] Long-term complications include heart disease, stroke, kidney failure, foot ulcers and damage to the eyes.[4] Furthermore, since insulin lowers blood sugar levels, complications may arise from low blood sugar if more insulin is taken than necessary.[6]
Type 1 diabetes makes up an estimated 5–10% of all diabetes cases.[9] The number of people affected globally is unknown, although it is estimated that about 80,000 children develop the disease each year.[6] Within the United States the number of people affected is estimated at one to three million.[6][12] Rates of disease vary widely, with approximately one new case per 100,000 per year in East Asia and Latin America and around 30 new cases per 100,000 per year in Scandinavia and Kuwait.[13][14] It typically begins in children and young adults.[1]
Type 1 diabetes begins suddenly, typically in childhood or adolescence.[15] The major sign of type 1 diabetes is very high blood sugar, which typically manifests in children as a few days to weeks of polyuria (increased urination), polydipsia (increased thirst), and weight loss.[16][17] Children may also experience increased appetite, blurred vision, bedwetting, recurrent skin infections, candidiasis of the perineum, irritability, and performance issues at school.[16][17] Adults with type 1 diabetes tend to have more varied symptoms that come on over months rather than days to weeks.[18][17]
Prolonged lack of insulin can also result in diabetic ketoacidosis, characterized by persistent fatigue, dry or flushed skin, abdominal pain, nausea or vomiting, confusion, trouble breathing, and a fruity breath odor.[18][19] Blood and urine tests reveal unusually high glucose and ketones in the blood and urine.[20] Untreated ketoacidosis can rapidly progress to loss of consciousness, coma, and death.[20] The percentage of children whose type 1 diabetes begins with an episode of diabetic ketoacidosis varies widely by geography, as low as 15% in parts of Europe and North America, and as high as 80% in the developing world.[20]
Type 1 diabetes is caused by the destruction of β-cells—the only cells in the body that produce insulin—and the consequent progressive insulin deficiency. Without insulin, the body is unable to respond effectively to increases in blood sugar. Due to this, people with diabetes have persistent hyperglycemia.[21] In 70–90% of cases, β-cells are destroyed by one's own immune system, for reasons that are not entirely clear.[21] The best-studied components of this autoimmune response are β-cell-targeted antibodies that begin to develop in the months or years before symptoms arise.[21] Typically someone will first develop antibodies against insulin or the protein GAD65, followed eventually by antibodies against the proteins IA-2, IA-2β, and/or ZNT8. People with more of these antibodies, and who develop them earlier in life, are at higher risk for developing symptomatic type 1 diabetes.[22] The trigger for the development of these antibodies remains unclear.[23] A number of explanatory theories have been put forward, and the cause may involve genetic susceptibility, a diabetogenic trigger, and/or exposure to an antigen.[24] The remaining 10–30% of type 1 diabetics have β-cell destruction but no sign of autoimmunity; this is called idiopathic type 1 diabetes and its cause is unknown.[21]
Various environmental risks have been studied in an attempt to understand what triggers β-cell autoimmunity. Many aspects of environment and life history are associated with slight increases in type 1 diabetes risk, however the connection between each risk and diabetes often remains unclear.[citation needed] Type 1 diabetes risk is slightly higher for children whose mothers are obese or older than 35, or for children born by caesarean section.[25] Similarly, a child's weight gain in the first year of life, total weight, and BMI are associated with slightly increased type 1 diabetes risk.[25] Some dietary habits have also been associated with type 1 diabetes risk, namely consumption of cow's milk and dietary sugar intake.[25] Animal studies and some large human studies have found small associations between type 1 diabetes risk and intake of gluten or dietary fiber; however, other large human studies have found no such association.[25] Many potential environmental triggers have been investigated in large human studies and found to be unassociated with type 1 diabetes risk including duration of breastfeeding, time of introduction of cow milk into the diet, vitamin D consumption, blood levels of active vitamin D, and maternal intake of omega-3 fatty acids.[25][26]
A longstanding hypothesis for an environmental trigger is that some viral infection early in life contributes to type 1 diabetes development. Much of this work has focused on enteroviruses, with some studies finding slight associations with type 1 diabetes, and others finding none.[27] Large human studies have searched for, but not yet found an association between type 1 diabetes and various other viral infections, including infections of the mother during pregnancy.[27] Conversely, some have postulated that reduced exposure to pathogens in the developed world increases the risk of autoimmune diseases, often called the hygiene hypothesis. Various studies of hygiene-related factors—including household crowding, daycare attendance, population density, childhood vaccinations, antihelminth medication, and antibiotic usage during early life or pregnancy—show no association with type 1 diabetes.[28]
Type 1 diabetes is partially caused by genetics, and family members of type 1 diabetics have a higher risk of developing the disease themselves. In the general population, the risk of developing type 1 diabetes is around 1 in 250. For someone whose parent has type 1 diabetes, the risk rises to 1–9%. If a sibling has type 1 diabetes, the risk is 6–7%. If someone's identical twin has type 1 diabetes, they have a 30–70% risk of developing it themselves.[29]
About half of the disease's heritability is due to variations in three HLA class II genes involved in antigen presentation: HLA-DRB1, HLA-DQA1, and HLA-DQB1.[29] The variation patterns associated with increased risk of type 1 diabetes are called HLA-DR3 and HLA-DR4-HLA-DQ8, and are common in people of European descent. A pattern associated with reduced risk of type 1 diabetes is called HLA-DR15-HLA-DQ6.[29] Large genome-wide association studies have identified dozens of other genes associated with type 1 diabetes risk, mostly genes involved in the immune system.[29]
Some medicines can reduce insulin production or damage β cells, resulting in disease that resembles type 1 diabetes. The antiviral drug didanosine triggers pancreas inflammation in 5 to 10% of those who take it, sometimes causing lasting β-cell damage.[30] Similarly, up to 5% of those who take the anti-protozoal drug pentamidine experience β-cell destruction and diabetes.[30] Several other drugs cause diabetes by reversibly reducing insulin secretion, namely statins (which may also damage β cells), the post-transplant immunosuppressants cyclosporin A and tacrolimus, the leukemia drug L-asparaginase, and the antibiotic gatifloxicin.[30][31] Pyrinuron (Vacor), a rodenticide introduced in the United States in 1976, selectively destroys pancreatic beta cells, resulting in type 1 diabetes after accidental poisoning.[32] Pyrinuron was withdrawn from the U.S. market in 1979.[33] Cancer Immunotherapy Drugs have also destroyed pancreatic beta cells, such as Opdivo among others.[34]
Diabetes is typically diagnosed by a blood test showing unusually high blood sugar. The World Health Organization defines diabetes as blood sugar levels at or above 7.0 mmol/L (126 mg/dL) after fasting for at least eight hours, or a glucose level at or above 11.1 mmol/L (200 mg/dL) two hours after an oral glucose tolerance test.[35] The American Diabetes Association additionally recommends a diagnosis of diabetes for anyone with symptoms of hyperglycemia and blood sugar at any time at or above 11.1 mmol/L, or glycated hemoglobin (hemoglobin A1C) levels at or above 48 mmol/mol.[36]
Once a diagnosis of diabetes is established, type 1 diabetes is distinguished from other types by a blood test for the presence of autoantibodies that target various components of the beta cell.[37] The most commonly available tests detect antibodies against glutamic acid decarboxylase, the beta cell cytoplasm, or insulin, each of which are targeted by antibodies in around 80% of type 1 diabetics.[37] Some healthcare providers also have access to tests for antibodies targeting the beta cell proteins IA-2 and ZnT8; these antibodies are present in around 58% and 80% of type 1 diabetics respectively.[37] Some also test for C-peptide, a byproduct of insulin synthesis. Very low C-peptide levels are suggestive of type 1 diabetes.[37]
The mainstay of type 1 diabetes treatment is the regular injection of insulin to manage hyperglycemia.[38] Injections of insulin via subcutaneous injection using either a syringe or an insulin pump are necessary multiple times per day, adjusting dosages to account for food intake, blood glucose levels and physical activity.[38] The goal of treatment is to maintain blood sugar in a normal range—80–130 mg/dL before a meal; <180 mg/dL after—as often as possible.[39] To achieve this, people with diabetes often monitor their blood glucose levels at home. Around 83% of type 1 diabetics monitor their blood glucose by capillary blood testing: pricking the finger to draw a drop of blood, and determining blood glucose with a glucose meter.[40] The American Diabetes Association recommends testing blood glucose around 6–10 times per day: before each meal, before exercise, at bedtime, occasionally after a meal, and any time someone feels the symptoms of hypoglycemia.[40] Around 17% of people with type 1 diabetes use a continuous glucose monitor, a device with a sensor under the skin that constantly measures glucose levels and communicates those levels to an external device.[40] Continuous glucose monitoring is associated with better blood sugar control than capillary blood testing alone; however, continuous glucose monitoring tends to be substantially more expensive.[40] Healthcare providers can also monitor someone's hemoglobin A1C levels which reflect the average blood sugar over the last three months.[41] The American Diabetes Association recommends a goal of keeping hemoglobin A1C levels under 7% for most adults and 7.5% for children.[41][42]
The goal of insulin therapy is to mimic normal pancreatic insulin secretion: low levels of insulin constantly present to support basic metabolism, plus the two-phase secretion of additional insulin in response to high blood sugar, then an extended phase of continued insulin secretion.[43] This is accomplished by combining different insulin preparations that act with differing speeds and durations. The standard of care for type 1 diabetes is a bolus of rapid-acting insulin 10–15 minutes before each meal or snacks, and as-needed to correct hyperglycemia.[43] In addition, constant low levels of insulin are achieved with one or two daily doses of long-acting insulin, or by steady infusion of low insulin levels by an insulin pump.[43] The exact dose of insulin appropriate for each injection depends on the content of the meal/snack, and the individual person's sensitivity to insulin, and is therefore typically calculated by the individual with diabetes or a family member by hand or assistive device (calculator, chart, mobile app, etc.).[43] People unable to manage these intensive insulin regimens are sometimes prescribed alternate plans relying on mixtures of rapid- or short-acting and intermediate-acting insulin, which are administered at fixed times along with meals of pre-planned times and carbohydrate composition.[43]
A non-insulin medication approved by the U.S. Food and Drug Administration for treating type 1 diabetes is the amylin analog pramlintide, which replaces the beta-cell hormone amylin. Addition of pramlintide to mealtime insulin injections reduces the boost in blood sugar after a meal, improving blood sugar control.[44]  Occasionally, metformin, GLP-1 receptor agonists, Dipeptidyl peptidase-4 inhibitors, or SGLT2 inhibitor are prescribed off-label to people with type 1 diabetes, although fewer than 5% of type 1 diabetics use these drugs.[38]
Besides insulin, the major way type 1 diabetics control their blood sugar is by learning how various foods impact their blood sugar levels. This is primarily done by tracking their intake of carbohydrates, the type of food with the greatest impact on blood sugar.[45] In general, people with type 1 diabetes are advised to follow an individualized eating plan rather than a pre-decided one.[46] There are camps for children to teach them how and when to use or monitor their insulin without parental help.[47] As psychological stress may have a negative effect on diabetes, a number of measures have been recommended including: exercising, taking up a new hobby, or joining a charity, among others.[48]
Regular exercise is important for maintaining general health, though the effect of exercise on blood sugar can be challenging to predict.[49] Exogenous insulin can drive down blood sugar, leaving those with diabetes at risk of hypoglycemia during and immediately after exercise, then again seven to eleven hours after exercise (called the "lag effect").[49] Conversely, high-intensity exercise can result in a shortage of insulin, and consequent hyperglycemia.[49] The risk of hypoglycemia can be managed by beginning exercise when blood sugar is relatively high (above 100 mg/dL), ingesting carbohydrates during or shortly after exercise, and reducing the amount of injected insulin within two hours of the planned exercise.[49] Similarly, the risk of exercise-induced hyperglycemia can be managed by avoiding exercise when insulin levels are very low, when blood sugar is extremely high (above 350 mg/dL), or when one feels unwell.[49]
In some cases, people can receive transplants of the pancreas or isolated islet cells to restore insulin production and alleviate diabetic symptoms. Transplantation of the whole pancreas is rare, due in part to the few available donor organs, and to the need for lifelong immunosuppressive therapy to prevent transplant rejection.[50][51] The American Diabetes Association recommends pancreas transplant only in people who also require a kidney transplant, or who struggle to perform regular insulin therapy and experience repeated severe side effects of poor blood sugar control.[51] Most pancreas transplants are done simultaneously with a kidney transplant, with both organs from the same donor.[52] The transplanted pancreas continues to function for at least five years in around three quarters of recipients, allowing them to stop taking insulin.[53]
Transplantations of islets alone have become increasingly common.[54] Pancreatic islets are isolated from a donor pancreas, then injected into the recipient's portal vein from which they implant onto the recipient's liver.[55] In nearly half of recipients, the islet transplant continues to work well enough that they still do not need exogenous insulin five years after transplantation.[56] If a transplant fails, recipients can receive subsequent injections of islets from additional donors into the portal vein.[55] Like with whole pancreas transplantation, islet transplantation requires lifelong immunosuppression and depends on the limited supply of donor organs; it is therefore similarly limited to people with severe poorly controlled diabetes and those who have had or are scheduled for a kidney transplant.[54][57]
Donislecel (Lantidra) allogeneic (donor) pancreatic islet cellular therapy was approved for medical use in the United States in June 2023.[58]
Type 1 diabetes is a result of the destruction of pancreatic beta cells, although what triggers that destruction remains unclear.[59] People with type 1 diabetes tend to have more CD8+ T-cells and B-cells that specifically target islet antigens than those without type 1 diabetes, suggesting a role for the adaptive immune system in beta cell destruction.[59][60] Type 1 diabetics also tend to have reduced regulatory T cell function, which may exacerbate autoimmunity.[59] Destruction of beta cells results in inflammation of the islet of Langerhans, called insulitis. These inflamed islets tend to contain CD8+ T-cells and – to a lesser extent – CD4+ T cells.[59] Abnormalities in the pancreas or the beta cells themselves may also contribute to beta-cell destruction. The pancreases of people with type 1 diabetes tend to be smaller, lighter, and have abnormal blood vessels, nerve innervations, and extracellular matrix organization.[61] In addition, beta cells from people with type 1 diabetes sometimes overexpress HLA class I molecules (responsible for signaling to the immune system) and have increased endoplasmic reticulum stress and issues with synthesizing and folding new proteins, any of which could contribute to their demise.[61]
The mechanism by which the beta cells actually die likely involves both necroptosis and apoptosis induced or exacerbated by CD8+ T-cells and macrophages.[62] Necroptosis can be triggered by activated T cells – which secrete toxic granzymes and perforin – or indirectly as a result of reduced blood flow or the generation of reactive oxygen species.[62] As some beta cells die, they may release cellular components that amplify the immune response, exacerbating inflammation and cell death.[62] Pancreases from people with type 1 diabetes also have signs of beta cell apoptosis, linked to activation of the janus kinase and TYK2 pathways.[62]
Partial ablation of beta-cell function is enough to cause diabetes; at diagnosis, people with type 1 diabetes often still have detectable beta-cell function. Once insulin therapy is started, many people experience a resurgence in beta-cell function, and can go some time with little-to-no insulin treatment – called the "honeymoon phase".[61] This eventually fades as beta-cells continue to be destroyed, and insulin treatment is required again.[61] Beta-cell destruction is not always complete, as 30–80% of type 1 diabetics produce small amounts of insulin years or decades after diagnosis.[61]
Onset of autoimmune diabetes is accompanied by impaired ability to regulate the hormone glucagon,[63] which acts in antagonism with insulin to regulate blood sugar and metabolism. Progressive beta cell destruction leads to dysfunction in the neighboring alpha cells which secrete glucagon, exacerbating excursions away from euglycemia in both directions; overproduction of glucagon after meals causes sharper hyperglycemia, and failure to stimulate glucagon upon hypoglycemia prevents a glucagon-mediated rescue of glucose levels.[64]
Onset of type 1 diabetes is followed by an increase in glucagon secretion after meals. Increases have been measured up to 37% during the first year of diagnosis, while c-peptide levels (indicative of islet-derived insulin), decline by up to 45%.[65] Insulin production will continue to fall as the immune system destroys beta cells, and islet-derived insulin will continue to be replaced by therapeutic exogenous insulin. Simultaneously, there is measurable alpha cell hypertrophy and hyperplasia in the early stage of the disease, leading to expanded alpha cell mass. This, together with failing beta cell insulin secretion, begins to account for rising glucagon levels that contribute to hyperglycemia.[64] Some researchers believe glucagon dysregulation to be the primary cause of early stage hyperglycemia.[66] Leading hypotheses for the cause of postprandial hyperglucagonemia suggest that exogenous insulin therapy is inadequate to replace the lost intraislet signalling to alpha cells previously mediated by beta cell-derived pulsatile insulin secretion.[67][68] Under this working hypothesis intensive insulin therapy has attempted to mimic natural insulin secretion profiles in exogenous insulin infusion therapies.[69]
In young people with type 1 diabetes, unexplained deaths could be due to nighttime hypoglycemia triggering abnormal heart rhythms or cardiac autonomic neuropathy, damage to nerves that control the function of the heart.
Glucagon secretion is normally increased upon falling glucose levels, but normal glucagon response to hypoglycemia is blunted in type 1 diabetics.[70][71] Beta cell glucose sensing and subsequent suppression of administered insulin secretion is absent, leading to islet hyperinsulinemia which inhibits glucagon release.[70][72]
Autonomic inputs to alpha cells are much more important for glucagon stimulation in the moderate to severe ranges of hypoglycemia, yet the autonomic response is blunted in a number of ways.  Recurrent hypoglycemia leads to metabolic adjustments in the glucose sensing areas of the brain, shifting the threshold for counter regulatory activation of the sympathetic nervous system to lower glucose concentration.[72] This is known as hypoglycemic unawareness. Subsequent hypoglycemia is met with impairment in sending of counter regulatory signals to the islets and adrenal cortex. This accounts for the lack of glucagon stimulation and epinephrine release that would normally stimulate and enhance glucose release and production from the liver, rescuing the diabetic from severe hypoglycemia, coma, and death.  Numerous hypotheses have been produced in the search for a cellular mechanism of hypoglycemic unawareness, and a consensus has yet to be reached.[73] The major hypotheses are summarized in the following table:[74][72][73]
In addition, autoimmune diabetes is characterized by a loss of islet specific sympathetic innervation.[75] This loss constitutes an 80–90% reduction of islet sympathetic nerve endings, happens early in the progression of the disease, and is persistent though the life of the patient.[76] It is linked to the autoimmune aspect of type 1 diabetics and fails to occur in type 2 diabetics. Early in the autoimmune event, the axon pruning is activated in islet sympathetic nerves. Increased BDNF and ROS that result from insulitis and beta cell death stimulate the p75 neurotrophin receptor (p75NTR), which acts to prune off axons. Axons are normally protected from pruning by activation of tropomyosin receptor kinase A (Trk A) receptors by NGF, which in islets is primarily produced by beta cells. Progressive autoimmune beta cell destruction, therefore, causes both the activation of pruning factors and the loss of protective factors to the islet sympathetic nerves. This unique form of neuropathy is a hallmark of type 1 diabetes, and plays a part in the loss of glucagon rescue of severe hypoglycemia.[75]
The most pressing complication of type 1 diabetes are the always present risks of poor blood sugar control: severe hypoglycemia and diabetic ketoacidosis. Hypoglycemia – typically blood sugar below 70 mg/dL – triggers the release of epinephrine, and can cause people to feel shaky, anxious, or irritable.[77] People with hypoglycemia may also experience hunger, nausea, sweats, chills, dizziness, and a fast heartbeat.[77] Some feel lightheaded, sleepy, or weak.[77] Severe hypoglycemia can develop rapidly, causing confusion, coordination problems, loss of consciousness, and seizure.[77][78] On average, people with type 1 diabetes experience a hypoglycemia event that requires assistance of another 16–20 times in 100 person-years, and an event leading to unconsciousness or seizure 2–8 times per 100 person-years.[78] The American Diabetes Association recommends treating hypoglycemia by the "15-15 rule": eat 15 grams of carbohydrates, then wait 15 minutes before checking blood sugar; repeat until blood sugar is at least 70 mg/dL.[77] Severe hypoglycemia that impairs someone's ability to eat is typically treated with injectable glucagon, which triggers glucose release from the liver into the bloodstream.[77] People with repeated bouts of hypoglycemia can develop hypoglycemia unawareness, where the blood sugar threshold at which they experience symptoms of hypoglycemia decreases, increasing their risk of severe hypoglycemic events.[79] Rates of severe hypoglycemia have generally declined due to the advent of rapid-acting and long-acting insulin products in the 1990s and early 2000s;[43] however, acute hypoglycemic still causes 4–10% of type 1 diabetes-related deaths.[78]
The other persistent risk is diabetic ketoacidosis – a state where lack of insulin results in cells burning fat rather than sugar, producing toxic ketones as a byproduct.[19] Ketoacidosis symptoms can develop rapidly, with frequent urination, excessive thirst, nausea, vomiting, and severe abdominal pain all common.[80] More severe ketoacidosis can result in labored breathing, and loss of consciousness due to cerebral edema.[80] People with type 1 diabetes experience diabetic ketoacidosis 1–5 times per 100 person-years, the majority of which result in hospitalization.[81] 13–19% of type 1 diabetes-related deaths are caused by ketoacidosis,[78] making ketoacidosis the leading cause of death in people with type 1 diabetes less than 58 years old.[81]
In addition to the acute complications of diabetes, long-term hyperglycemia results in damage to the small blood vessels throughout the body. This damage tends to manifest particularly in the eyes, nerves, and kidneys causing diabetic retinopathy, diabetic neuropathy, and diabetic nephropathy respectively.[79] In the eyes, prolonged high blood sugar causes the blood vessels in the retina to become fragile.[82]
People with type 1 diabetes also have increased risk of cardiovascular disease, which is estimated to shorten the life of the average type 1 diabetic by 8–13 years.[83] Cardiovascular disease[84] as well as neuropathy[85] may have an autoimmune basis, as well. Women with type 1 DM have a 40% higher risk of death as compared to men with type 1 DM.[86]
About 12 percent of people with type 1 diabetes have clinical depression.[87] About 6 percent of people with type 1 diabetes also have celiac disease, but in most cases there are no digestive symptoms[7][88] or are mistakenly attributed to poor control of diabetes, gastroparesis or diabetic neuropathy.[88] In most cases, celiac disease is diagnosed after onset of type 1 diabetes. The association of celiac disease with type 1 diabetes increases the risk of complications, such as retinopathy and mortality. This association can be explained by shared genetic factors, and inflammation or nutritional deficiencies caused by untreated celiac disease, even if type 1 diabetes is diagnosed first.[7]
People with diabetes show an increased rate of urinary tract infection.[89] The reason is bladder dysfunction is more common in people with diabetes than people without diabetes due to diabetes nephropathy. When present, nephropathy can cause a decrease in bladder sensation, which in turn, can cause increased residual urine, a risk factor for urinary tract infections.[90]
Sexual dysfunction in people with diabetes is often a result of physical factors such as nerve damage and poor circulation, and psychological factors such as stress and/or depression caused by the demands of the disease.[91] The most common sexual issues in males with diabetes are problems with erections and ejaculation: "With diabetes, blood vessels supplying the penis's erectile tissue can get hard and narrow, preventing the adequate blood supply needed for a firm erection. The nerve damage caused by poor blood glucose control can also cause ejaculate to go into the bladder instead of through the penis during ejaculation, called retrograde ejaculation. When this happens, semen leaves the body in the urine." Another cause of erectile dysfunction is reactive oxygen species created as a result of the disease. Antioxidants can be used to help combat this.[92] Sexual problems are common in women who have diabetes,[91] including reduced sensation in the genitals, dryness, difficulty/inability to orgasm, pain during sex, and decreased libido. Diabetes sometimes decreases estrogen levels in females, which can affect vaginal lubrication. Less is known about the correlation between diabetes and sexual dysfunction in females than in males.[91]
Oral contraceptive pills can cause blood sugar imbalances in women who have diabetes. Dosage changes can help address that, at the risk of side effects and complications.[91]
Women with type 1 diabetes show a higher than normal rate of polycystic ovarian syndrome (PCOS).[93] The reason may be that the ovaries are exposed to high insulin concentrations since women with type 1 diabetes can have frequent hyperglycemia.[94]
People with type 1 diabetes are at an increased risk for developing several autoimmune disorders, particularly thyroid problems – around 20% of people with type 1 diabetes have hypothyroidism or hyperthyroidism, typically caused by Hashimoto thyroiditis or Graves' disease respectiveley.[95][78] Celiac disease affects 2–8% of people with type 1 diabetes, and is more common in those who were younger at diabetes diagnosis, and in white people.[95] Type 1 diabetics are also at increased risk of rheumatoid arthritis, lupus, autoimmune gastritis, pernicious anemia, vitiligo, and Addison's disease.[78] Conversely, complex autoimmune syndromes caused by mutations in the immunity-related genes AIRE (causing autoimmune polyglandular syndrome), FoxP3 (causing IPEX syndrome), or STAT3 include type 1 diabetes in their effects.[96]
There is no way to prevent type 1 diabetes;[97] however, the development of diabetes symptoms can be delayed in some people who are at high risk of developing the disease. In 2022 the FDA approved an intravenous injection of teplizumab to delay the progression of type 1 diabetes in those older than eight who have already developed diabetes-related autoantibodies and problems with blood sugar control. In that population, the anti-CD3 monoclonal antibody teplizumab can delay the development of type 1 diabetes symptoms by around two years.[98]
In addition to anti-CD3 antibodies, several other immunosuppressive agents have been trialled with the aim of preventing beta cell destruction. Large trials of cyclosporine treatment suggested that cyclosporine could improve insulin secretion in those recently diagnosed with type 1 diabetes; however, people who stopped taking cyclosporine rapidly stopped making insulin, and cyclosporine's kidney toxicity and increased risk of cancer prevented people from using it long-term.[99] Several other immunosuppressive agents – prednisone, azathioprine, anti-thymocyte globulin, mycophenolate, and antibodies against CD20 and IL2 receptor α – have been the subject of research, but none have provided lasting protection from development of type 1 diabetes.[99] There have also been clinical trials attempting to induce immune tolerance by vaccination with insulin, GAD65, and various short peptides targeted by immune cells during type 1 diabetes; none have yet delayed or prevented development of disease.[100]
Several trials have attempted dietary interventions with the hope of reducing the autoimmunity that leads to type 1 diabetes. Trials that withheld cow's milk or gave infants formula free of bovine insulin decreased the development of β-cell-targeted antibodies, but did not prevent the development of type 1 diabetes.[101] Similarly, trials that gave high-risk individuals injected insulin, oral insulin, or nicotinamide did not prevent diabetes development.[101]
Type 1 diabetes makes up an estimated 10–15% of all diabetes cases[22]  or 11–22 million cases worldwide.[102] Symptoms can begin at any age, but onset is most common in children, with diagnoses slightly more common in 5 to 7 year olds, and much more common around the age of puberty.[103][15]  In contrast to most autoimmune diseases, type 1 diabetes is slightly more common in males than in females.[103]
In 2006, type 1 diabetes affected 440,000 children under 14 years of age and was the primary cause of diabetes in those less than 15 years of age.[104][22]
Rates vary widely by country and region. Incidence is highest in Scandinavia, at 30–60 new cases per 100,000 children per year, intermediate in the U.S. and Southern Europe at 10–20 cases per 100,000 per year, and lowest in China, much of Asia, and South America at 1–3 cases per 100,000 per year.[26]
In the United States, type 1 and 2 diabetes affected about 208,000 youths under the age of 20 in 2015. Over 18,000 youths are diagnosed with Type 1 diabetes every year. Every year about 234,051 Americans die due to diabetes (type I or II) or diabetes-related complications, with 69,071 having it as the primary cause of death.[105]
In Australia, about one million people have been diagnosed with diabetes and of this figure 130,000 people have been diagnosed with type 1 diabetes. Australia ranks 6th-highest in the world with children under 14 years of age. Between 2000 and 2013, 31,895 new cases were established, with 2,323 in 2013, a rate of 10–13 cases per 100,00 people each year. Aboriginals and Torres Strait Islander people are less affected.[106][107]
Since the 1950s, the incidence of type 1 diabetes has been gradually increasing across the world by an average 3–4% per year.[26] The increase is more pronounced in countries that began with a lower incidence of type 1 diabetes.[26]  A single 2023 study suggested a relationship between COVID-19 infection and the incidence of type 1 diabetes in children;[108] confirmatory studies have not appeared to date.
The connection between diabetes and pancreatic damage was first described by the German pathologist Martin Schmidt, who in a 1902 paper noted inflammation around the pancreatic islet of a child who had died of diabetes.[109] The connection between this inflammation and diabetes onset was further developed through the 1920s by Shields Warren, and the term "insulitis" was coined by Hanns von Meyenburg in 1940 to describe the phenomenon.[109]
Type 1 diabetes was described as an autoimmune disease in the 1970s, based on observations that autoantibodies against islets were discovered in diabetics with other autoimmune deficiencies.[110] It was also shown in the 1980s that immunosuppressive therapies could slow disease progression, further supporting the idea that type 1 diabetes is an autoimmune disorder.[111] The name juvenile diabetes was used earlier as it often first is diagnosed in childhood.
Type 1 and 2 diabetes was estimated to cause $10.5 billion in annual medical costs ($875 per month per diabetic) and an additional $4.4 billion in indirect costs ($366 per month per person with diabetes) in the U.S.[112] In the United States $245 billion every year is attributed to diabetes. Individuals diagnosed with diabetes have 2.3 times the health care costs as individuals who do not have diabetes. One in ten health care dollars are spent on individuals with type 1 and 2 diabetes.[105]
Funding for research into type 1 diabetes originates from government, industry (e.g., pharmaceutical companies), and charitable organizations. Government funding in the United States is distributed via the National Institutes of Health, and in the UK via the National Institute for Health and Care Research or the Medical Research Council. The Juvenile Diabetes Research Foundation (JDRF), founded by parents of children with type 1 diabetes, is the world's largest provider of charity-based funding for type 1 diabetes research.[citation needed] Other charities include the American Diabetes Association, Diabetes UK, Diabetes Research and Wellness Foundation,[113] Diabetes Australia, the Canadian Diabetes Association.
Pluripotent stem cells can be used to generate beta cells but previously these cells did not function as well as normal beta cells.[114] In 2014 more mature beta cells were produced which released insulin in response to blood sugar when transplanted into mice.[115][116] Before these techniques can be used in humans more evidence of safety and effectiveness is needed.[114]
There has also been substantial effort to develop a fully automated insulin delivery system or "artificial pancreas" that could sense glucose levels and inject appropriate insulin without conscious input from the user.[117] Current "hybrid closed-loop systems" use a continuous glucose monitor to sense blood sugar levels, and a subcutaneous insulin pump to deliver insulin; however, due to the delay between insulin injection and its action, current systems require the user to initiate insulin before taking meals.[118] Several improvements to these systems are currently undergoing clinical trials in humans, including a dual-hormone system that injects glucagon in addition to insulin, and an implantable device that injects insulin intraperitoneally where it can be absorbed more quickly.[119]
Various animal models of disease are used to understand the pathogenesis and etiology of type 1 diabetes. Currently available models of T1D can be divided into spontaneously autoimmune, chemically induced, virus induced and genetically induced.[120]
The nonobese diabetic (NOD) mouse is the most widely studied model of type 1 diabetes.[120] It is an inbred strain that spontaneously develops type 1 diabetes in 30–100% of female mice depending on housing conditions.[121] Diabetes in NOD mice is caused by several genes, primarily MHC genes involved in antigen presentation.[121] Like diabetic humans, NOD mice develop islet autoantibodies and inflammation in the islet, followed by reduced insulin production and hyperglycemia.[121][122] Some features of human diabetes are exaggerated in NOD mice, namely the mice have more severe islet inflammation than humans, and have a much more pronounced sex bias, with females developing diabetes far more frequently than males.[121] In NOD mice the onset of insulitis occurs at 3–4 weeks of age. The islets of Langerhans are infiltrated by CD4+, CD8+ T lymphocytes, NK cells, B lymphocytes, dendritic cells, macrophages and neutrophils, similar to the disease process in humans.[123] In addition to sex, breeding conditions, gut microbiome composition or diet also influence the onset of T1D.[124]
The BioBreeding Diabetes-Prone (BB) rat is another widely used spontaneous experimental model for T1D. The onset of diabetes occurs, in up to 90% of individuals (regardless of sex) at 8–16 weeks of age.[123] During insulitis, the pancreatic islets are infiltrated by T lymphocytes, B lymphocytes, macrophages, and NK cells, with the difference from the human course of insulitis being that CD4 + T lymphocytes are markedly reduced and CD8 + T lymphocytes are almost absent. The aforementioned lymphopenia is the major drawback of this model. The disease is characterized by hyperglycemia, hypoinsulinemia, weight loss, ketonuria, and the need for insulin therapy for survival.[123] BB Rats are used to study the genetic aspects of T1D and are also used for interventional studies and diabetic nephropathy studies.[125]
LEW-1AR1 / -iddm rats are derived from congenital Lewis rats and represent a rarer spontaneous model for T1D. These rats develop diabetes at about 8–9 weeks of age with no sex differences unlike NOD mice.[126] In LEW mice, diabetes presents with hyperglycemia, glycosuria, ketonuria, and polyuria.[127][123] The advantage of the model is the progression of the prediabetic phase, which is very similar to human disease, with infiltration of islet by immune cells about a week before hyperglycemia is observed. This model is suitable for intervention studies or for the search for predictive biomarkers. It is also possible to observe individual phases of pancreatic infiltration by immune cells. The advantage of congenic LEW mice is also the good viability after the manifestation of T1D (compared to NOD mice and BB rats).[128]
The chemical compounds aloxan and streptozotocin (STZ) are commonly used to induce diabetes and destroy β-cells in mouse/rat animal models.[123] In both cases, it is a cytotoxic analog of glucose that passes GLUT2 transport and accumulates in β-cells, causing their destruction. The chemically induced destruction of β-cells leads to decreased insulin production, hyperglycemia and weight loss in the experimental animal.[129] The animal models prepared in this way are suitable for research into blood sugar-lowering drugs and therapies (e.g. for testing new insulin preparations). They are also the most commonly used genetically induced T1D model is the so-called AKITA mouse (originally C57BL/6NSIc mouse). The development of diabetes in AKITA mice is caused by a spontaneous point mutation in the Ins2 gene, which is responsible for the correct composition of insulin in the endoplasmic reticulum. Decreased insulin production is then associated with hyperglycemia, polydipsia and polyuria. If severe diabetes develops within 3–4 weeks, AKITA mice survive no longer than 12 weeks without treatment intervention. The description of the etiology of the disease shows that, unlike spontaneous models, the early stages of the disease are not accompanied by insulitis.[130] AKITA mice are used to test drugs targeting endoplasmic reticulum stress reduction, to test islet transplants, and to study diabetes-related complications such as nephropathy, sympathetic autonomic neuropathy, and vascular disease.[123][131] for testing transplantation therapies. Their advantage is mainly the low cost, the disadvantage is the cytotoxicity of the chemical compounds.[132]
Viral infections play a role in the development of a number of autoimmune diseases, including human type 1 diabetes. However, the mechanisms by which viruses are involved in the induction of type 1 DM are not fully understood. Virus-induced models are used to study the etiology and pathogenesis of the disease, in particular the mechanisms by which environmental factors contribute to or protect against the occurrence of type 1 DM.[133] Among the most commonly used are Coxsackie virus, lymphocytic choriomeningitis virus, encephalomyocarditis virus, and Kilham rat virus. Examples of virus-induced animals include NOD mice infected with coxsackie B4 that developed type 1 DM within two weeks.[134]

Hashimoto's thyroiditis, also known as chronic lymphocytic thyroiditis and Hashimoto's disease, is an autoimmune disease in which the thyroid gland is gradually destroyed.[1][6] A slightly broader term is autoimmune thyroiditis, identical other than that it is also used to describe a similar condition without a goitre.[7][8]
Early on, symptoms may not be noticed.[1] Over time, the thyroid may enlarge, forming a painless goiter.[1] Some people eventually develop hypothyroidism with accompanying weight gain, fatigue, constipation, depression, hair loss, and general pains.[1] After many years the thyroid typically shrinks in size.[1] Potential complications include thyroid lymphoma.[2] Furthermore, because it is common for untreated patients of Hashimoto's to develop hypothyroidism, further complications can include, but are not limited to, high cholesterol, heart disease, heart failure, high blood pressure, myxedema, and potential pregnancy problems.[9]
Hashimoto's thyroiditis is thought to be due to a combination of genetic and environmental factors.[4] Risk factors include a family history of the condition and having another autoimmune disease.[1] Diagnosis is confirmed with blood tests for TSH, T4, and antithyroid autoantibodies.[1] Other conditions that can produce similar symptoms include Graves' disease and nontoxic nodular goiter.[5]
Hashimoto's thyroiditis is typically treated with levothyroxine.[1][10] If hypothyroidism is not present, some may recommend no treatment, while others may treat to try to reduce the size of the goiter.[1][11] Those affected should avoid eating large amounts of iodine; however, sufficient iodine is required especially during pregnancy.[1] Surgery is rarely required to treat the goiter.[5]
Hashimoto's thyroiditis affects about 5% of Caucasians at some point in their lives.[4] It is the most common cause of hypothyroidism in iodine-sufficient areas of the world.[12] It typically begins between the ages of 30 and 50 and is much more common in women than men.[1][3] Rates of the disease appear to be increasing.[5] It was first described by the Japanese physician Hakaru Hashimoto in 1912.[13] In 1957, it was recognized as an autoimmune disorder.[14]
Many symptoms are attributed to the development of Hashimoto's thyroiditis.  The most common symptoms include: fatigue, weight gain, pale or puffy face, feeling cold, joint and muscle pain, constipation, dry and thinning hair, heavy menstrual flow or irregular periods, depression, panic disorder, a slowed heart rate, and problems getting pregnant and miscarriages.[15]
Some patients in the early stage of the disease may experience symptoms of hyperthyroidism due to the release of thyroid hormones from intermittent thyroid destruction.[16]
Hashimoto's disease is about seven times more common in women than in men. It can occur in teens and young women, but more commonly appears in middle age, particularly for men. People who develop Hashimoto's disease often have family members who have thyroid or other autoimmune diseases, and sometimes have other autoimmune diseases themselves.[17]
Early stages of autoimmune thyroiditis may have a normal physical exam with or without a goiter.[18] A goiter is a diffuse, often symmetric, swelling of the thyroid gland visible in the anterior neck that may develop.[18] The thyroid gland may become firm, large, and lobulated in Hashimoto's thyroiditis, but changes in the thyroid can also be nonpalpable.[19] Enlargement of the thyroid is due to lymphocytic infiltration and fibrosis, rather than tissue hypertrophy. While their role in the initial destruction of the follicles is unclear, antibodies against thyroid peroxidase   or thyroglobulin are relevant, as they serve as markers for detecting the disease and its severity.[20] They are thought to be the secondary products of the T cell-mediated destruction of the gland.[21]
As lymphocytic infiltration progresses, patients may exhibit signs of hypothyroidism in multiple bodily systems, including, but not limited to, a larger goiter, weight gain, cold intolerance, fatigue, myxedema, constipation, menstrual disturbances, pale or dry skin, and dry, brittle hair, depression, ataxia, and muscle weakness.[18][12]
Patients with goiters who have had autoimmune thyroiditis for many years might see their goiter shrink in the later stages of the disease due to destruction of the thyroid.[16]
While rare, more serious complications of the hypothyroidism resulting from autoimmune thyroiditis are pericardial effusion, pleural effusion, both of which require further medical attention, and myxedema coma, which is an endocrine emergency.[12]
Autoimmune thyroiditis is the most common cause of hypothyroidism in settings of sufficient iodine.[12] It is estimated to affect 2% of the world's population.[22] It may affect up to 5% of the United States' population.[23] Anyone may develop this disease, but autoimmune thyroiditis affects women more often than men by about 10 times.[22] The difference in prevalence amongst genders is due to the effects of sex hormones.[24] Incidence peaks in the fifth decade of life, but patients are usually diagnosed between age 30–50.[16][23]
Thyroid autoimmunity can be familial.[7]  Many patients report a family history of autoimmune thyroiditis or Graves disease.[18] Twin studies have revealed a concordance of Hashimoto's disease in monozygotic twins.[12]
Autoimmune thyroiditis has a higher prevalence in societies that have a higher intake of iodine in their diet, such as the United States and Japan. It is the most common cause of hypothyroidism in areas of sufficient iodine.[12] Also, the rate of lymphocytic infiltration increased in areas where the iodine intake was once low, but increased due to iodine supplementation.[7]
It has been shown that "the prevalence of positive tests for thyroid antibodies increases with age, with a frequency as high as 33 percent in women 70 years old or older."[7] Incidence peaks in the fifth decade of life and the prevalence increases with age.[12][23]
Graves disease may occur before or after the development of autoimmune thyroiditis.[24] Patients may also have coexisting autoimmune conditions of other organs. These may include Addison disease, type 1 diabetes, Sjogren's syndrome, Celiac disease, and rheumatoid arthritis.[18][16] Autoimmune thyroiditis has also been seen in patients with autoimmune polyendocrine syndromes type 1 and 2.[24]
The strong genetic component is borne out in studies on monozygotic twins, with a concordance of 38–55%, with an even higher concordance of circulating thyroid antibodies not in relation to clinical presentation (up to 80% in monozygotic twins). Neither result was seen to a similar degree in dizygotic twins, offering strong favour for high genetic aetiology.[25]
Certain medications or drugs have been associated with altering and interfering with thyroid function. Of these drugs, there are two main mechanisms of interference that they can have.[citation needed]
One of the mechanisms of interference is when a drug alters thyroid hormone serum transfer proteins.[26] Estrogen, tamoxifen, heroin, methadone, clofibrate, 5-flurouracile, mitotane, and perphenazine all increase thyroid binding globulin (TBG) concentration.[26] Androgens, anabolic steroids such as danazol, glucocorticoids, and slow release nicotinic acid all decrease TBG concentrations. Furosemide, fenoflenac, mefenamic acid, salicylates, phenytoin, diazepam, sulphonylureas, free fatty acids, and heparin all interfere with thyroid hormone binding to TBG and/or transthyretin.[citation needed]
The other mechanism that medications can utilize to interfere with thyroid function would be to alter extra-thryoidal metabolism of thyroid hormone. Propylthiouracil, glucocorticoids, propranolol, iondinated contrast agents, amiodarone, and clomipramine all inhibit conversion of T4 and T3.[26] Phenobarbital, rifampin, phenytoin and carbamazepine all increase hepatic metabolism.[26] Finally, cholestryamine, colestipol, aluminium hydroxide, ferrous sulphate, and sucralfate are all drugs that decrease T4 absorption or enhance excretion.[26]
The first gene locus associated with autoimmune thyroid disease was major histocompatibility complex (MHC) region on chromosome 6p21. It encodes HLAs. Specific HLA alleles have a higher affinity to autoantigenic thyroidal peptides and can contribute to autoimmune thyroid disease development. Specifically, in Hashimoto's disease, aberrant expression of HLA II on thyrocytes has been demonstrated. They can present thyroid autoantigens and initiate autoimmune thyroid disease.[27] Susceptibility alleles are not consistent in Hashimoto's disease. In Caucasians, various alleles are reported to be associated with the disease, including DR3, DR5 and DQ7.[28][29]
This gene is the second major immune-regulatory gene related to autoimmune thyroid disease. CTLA-4 gene polymorphisms may contribute to the reduced inhibition of T-cell proliferation and increase susceptibility to autoimmune response.[30] CTLA-4 is a major thyroid autoantibody susceptibility gene. A linkage of the CTLA-4 region to the presence of thyroid autoantibodies was demonstrated by a whole-genome linkage analysis.[31] CTLA-4 was confirmed as the main locus for thyroid autoantibodies.[32]
PTPN22 is the most recently identified immune-regulatory gene associated with autoimmune thyroid disease. It is located on chromosome 1p13 and expressed in lymphocytes. It acts as a negative regulator of T-cell activation. Mutation in this gene is a risk factor for many autoimmune diseases. Weaker T-cell signaling may lead to impaired thymic deletion of autoreactive T cells, and increased PTPN22 function may result in inhibition of regulatory T cells, which protect against autoimmunity.[33]
IFN-γ promotes cell-mediated cytotoxicity against thyroid mutations causing increased production of IFN-γ were associated with the severity of hypothyroidism.[34] Severe hypothyroidism is associated with mutations leading to lower production of IL-4 (Th2 cytokine suppressing cell-mediated autoimmunity),[35] lower secretion of TGF-β (inhibitor of cytokine production),[36] and mutations of FoxP3, an essential regulatory factor for the Tregs development.[37] Development of Hashimoto's disease was associated with mutation of the gene for TNF-α (stimulator of the IFN-γ production), causing its higher concentration.[38]
Preventable environmental factors, including high iodine intake, selenium deficiency, and infectious diseases and certain drugs, have been implicated in the development of autoimmune thyroid disease in genetically predisposed individuals.[39]
Excessive iodine intake is a well-established environmental factor for triggering thyroid autoimmunity. A higher prevalence of thyroid autoantibodies is in the areas with higher iodine supply. Several mechanisms by which iodine may promote thyroid autoimmunity have been proposed. Iodine exposure leads to higher iodination of thyroglobulin, increasing its immunogenicity by creating new iodine-containing epitopes or exposing cryptic epitopes. It may facilitate presentation by APC, enhance the binding affinity of the T-cell receptor, and activating specific T-cells.[40]
Iodine exposure has been shown to increase the level of reactive oxygen species. They enhance the expression of the intracellular adhesion molecule-1 on the thyroid follicular cells, which could attract the immunocompetent cells into the thyroid gland.[41]
Iodine is toxic to thyrocytes since highly reactive oxygen species may bind to membrane lipids and proteins. It causes thyrocyte damage and the release of autoantigens. Iodine also promotes follicular cell apoptosis and has an influence on immune cells (augmented maturation of dendritic cells, increased number of T cells, stimulated B-cell immunoglobulin production).[42][43]
Data from the Danish Investigation of Iodine Intake and Thyroid Disease shows that within two cohorts (males, females) with moderate and mild iodine deficiency, the levels of both thyroid peroxidase and thyroglobulin antibodies are higher in females, and prevalence rates of both antibodies increase with age.[44]
Study of healthy Danish twins divided to three groups (monozygotic and dizygotic same sex, and opposite sex twin pairs) estimated that genetic contribution to thyroid peroxidase antibodies susceptibility was 61% in males and 72% in females, and contribution to thyroglobulin antibodies susceptibility was 39% in males and 75% in females.[45]
The high female predominance in thyroid autoimmunity may be associated with the X chromosome. It contains sex and immune-related genes responsible for immune tolerance.[46]
A higher incidence of thyroid autoimmunity was reported in patients with a higher rate of X-chromosome monosomy in peripheral white blood cells.[47]
Another potential mechanism might be skewed X-chromosome inactivation, leading to the escape of X-linked self-antigens from presentation in the thymus and loss of T-cell tolerance.[citation needed]
Having other autoimmune diseases is a risk factor for developing Hashimoto's thyroiditis, and the opposite is also true.[1] Autoimmune diseases most commonly associated to Hashimoto's thyroiditis include celiac disease, type 1 diabetes, vitiligo, and alopecia.[48]
The genes implicated vary in different ethnic groups and the incidence is increased in people with chromosomal disorders, including Turner, Down, and Klinefelter syndromes usually associated with autoantibodies against thyroglobulin and thyroperoxidase. Progressive depletion of these cells as the cytotoxic immune response leads to higher degrees of primary hypothyroidism, presenting with low T3/T4 levels, and compensatory elevations of TSH.[citation needed]
The mechanism of autoimmune thyroiditis is not well understood, but is thought to develop as a result of a complex interaction of genetics and environmental factors.[22] Thyroid autoantibodies appear mostly with the presence of lymphocytes in the targeted organ.[7][49] Lymphocytes produce antibodies targeting three different thyroid proteins: Thyroid peroxidase Antibodies (TPOAb), Thyroglobulin Antibodies (TgAb), and Thyroid stimulating hormone receptor Antibodies (TRAb).[7][50]
The antibody attacks ultimately lead to hypothyroidism, which is caused by replacement of follicular cells with parenchymatous tissue.[51]
The two antibodies most commonly implicated in autoimmune thyroiditis are antibodies against thyroid peroxidase (TPOAb) and thyroglobulin (TgAb).[22] They are hypothesized to develop as a result of thyroid damage, where T-lymphocytes are sensitized to residual thyroid peroxidase and thyroglobulin, rather than as the cause of thyroid damage.[22] However, they may exacerbate further thyroid destruction by binding the complement system and triggering apoptosis of thyroid cells.[22] Environmental factors that may predispose patients to this type of immune dysregulation include toxins, medications, dietary factors, and infectious agents.[24]
Some patients who are healthy or asymptomatic may be positive for more than one of these antibodies.  Doctors who attend to such patients will most likely monitor these patients as there is a chance that they will develop some type of dysfunction with time.[50]
Gross morphological changes within the thyroid are seen in the general enlargement, which is far more locally nodular and irregular than more diffuse patterns (such as that of hyperthyroidism). While the capsule is intact and the gland itself is still distinct from surrounding tissue, microscopic examination can provide a more revealing indication of the level of damage.[52]
Gross pathology of a thyroid with autoimmune thyroiditis may show an symmetrically enlarged thyroid.[22] It is often paler in color, in comparison to normal thyroid tissue which is reddish-brown.[22] Microscopic examination will show infiltration of lymphocytes and plasma cells. The lymphocytes are predominately T-lymphocytes with a representation of both CD4 positive and CD8 positive cells.[22] The plasma cells are polyclonal, with present germinal centers resembling the structure of a lymph node.[22] Fibrous tissue may be found throughout the affected thyroid as well.[22] Generally, pathological findings of the thyroid are related to the amount of existing thyroid function - the more infiltration and fibrosis, the less likely a patient will have normal thyroid function.[22] In late stages of the disease, the thyroid may be atrophic.[12]
Histologically, the hypersensitivity is seen as diffuse parenchymal infiltration by lymphocytes, particularly plasma B-cells, which can often be seen as secondary lymphoid follicles (germinal centers, not to be confused with the normally present colloid-filled follicles that constitute the thyroid). Atrophy of the colloid bodies is lined by Hürthle cells, cells with intensely eosinophilic, granular cytoplasm, a metaplasia from the normal cuboidal cells that constitute the lining of the thyroid follicles. Severe thyroid atrophy presents often with denser fibrotic bands of collagen that remains within the confines of the thyroid capsule.[52]
It is also characterized by invasion of the thyroid tissue by leukocytes, mainly T-lymphocytes. A rare but serious complication is thyroid lymphoma, generally the B-cell type, non-Hodgkin lymphoma.[53]
Diagnosis is usually made by detecting elevated levels of antithyroid peroxidase antibodies in the serum, but seronegative (without circulating autoantibodies) thyroiditis is also possible.[54] An ultrasound may be useful in detecting Hashimoto thyroiditis, especially in those with seronegative thyroiditis, due to key features detected in the ultrasound of a person with Hashimoto's thyroiditis, such as "echogenicity, heterogeneity, hypervascularity, and presence of small cysts."[55]
Various tests can be chosen depending on the presenting symptoms. For patients with autoimmune thyroiditis, while it is known that many patients may have circulating antibodies before they present with any symptoms, patients may present to their doctors for evaluation with symptoms of hypothyroidism.[12] Physicians will often start by assessing reported symptoms and performing a thorough physical exam, including a neck exam.[12]
Given the relatively nonspecific symptoms of initial hypothyroidism, Hashimoto's thyroiditis is often misdiagnosed as depression, cyclothymia, premenstrual syndrome, chronic fatigue syndrome, fibromyalgia, and less frequently, as erectile dysfunction or an anxiety disorder. On gross examination, a hard goiter that is not painful to the touch often presents;[52] other symptoms seen with hypothyroidism, such as periorbital myxedema, depend on the current state of progression of the response, especially given the usually gradual development of clinically relevant hypothyroidism. Testing for thyroid-stimulating hormone (TSH), free T3, free T4, and the antithyroglobulin antibodies (anti-Tg), antithyroid peroxidase antibodies (anti-TPO, or TPOAb) and antimicrosomal antibodies can help obtain an accurate diagnosis.[56] Earlier assessment of the person may present with elevated levels of thyroglobulin owing to transient thyrotoxicosis, as inflammation within the thyroid causes damage to the integrity of thyroid follicle storage of thyroglobulin; TSH secretion from the anterior pituitary increases in response to a decrease in negative feedback inhibition secondary to decreased serum thyroid hormones. Typically, T4 is the preferred thyroid hormone test for hypothyroidism.[57] This exposure of the body to substantial amounts of previously isolated thyroid enzymes is thought to contribute to the exacerbation of tolerance breakdown, giving rise to the more pronounced symptoms seen later in the disease. Lymphocytic infiltration of the thyrocyte-associated tissues often leads to the histologically significant finding of germinal center development within the thyroid gland.[citation needed]
Hashimoto's when presenting as mania is known as Prasad's syndrome after Ashok Prasad, the psychiatrist who first described it.[58]
The initial diagnostic evaluation will start with plasma thyroid-stimulating hormone (TSH) concentration.[16] If elevated, it signifies hypothyroidism.[16] The elevation is usually a marked increase over the normal range and is generally greater than 20 mg/dl.[18] Free T4 levels will usually be lowered, but sometimes might be normal.[59]
Doctors may check thyroglobulin antibodies (TgAb) whenever a thyroglobulin test is performed to see if the antibody is interfering. TgAb may also be ordered in regular intervals after a person has been diagnosed with thyroid cancer, and just like TPOAb, it can be associated with Hashimoto's thyroiditis.[50] The most common complement of lab values in patients with autoimmune thyroiditis are high TSH, low T4, and positive TPO antibodies.[59]
When patients have normal laboratory values but symptoms of autoimmune thyroiditis, ultrasound plays a role in diagnosis.[16] Images obtained with ultrasound can evaluate the size of the thyroid and further support the diagnosis of autoimmune thyroiditis, reveal the presence of nodules, or provide clues to the diagnosis of other thyroid conditions.[16]
Hypothyroidism caused by Hashimoto's thyroiditis is treated with thyroid hormone replacement agents such as levothyroxine, triiodothyronine, or desiccated thyroid extract. A tablet taken once a day generally keeps the thyroid hormone levels normal. In most cases, the treatment needs to be taken for the rest of the person's life. If hypothyroidism is caused by Hashimoto's thyroiditis,  the TSH levels may be recommended to be kept under 3.0 mIU/l.[60]
The standard of care is levothyroxine therapy, which is an oral medication structured like endogenous T4.[22] Levothyroxine can be dosed based upon weight, most commonly, or TSH elevation.[22] Usually the dose prescribed ranges from 1.6 mcg/kg to 1.8 mcg/kg, but can be adjusted based upon each patient.[12] For example, the dose may be lowered for elderly patients or patients with certain cardiac conditions, but should be increased in pregnant patients.[12] It should be administered on a consistent schedule.[22] Some patients elect combination therapy with both levothyroxine and liothyronine, which is a synthetic T3, however studies of combination therapy are limited.[22]
Side effects of thyroid replacement therapy are associated with iatrogenic hyperthyroidism.[22] Symptoms to watch out for include, but are not limited to, anxiety, tremor, weight loss, heat sensitivity, diarrhea, and shortness of breath. More worrisome symptoms include atrial fibrillation and bone density loss.[22]
TSH is the laboratory value of choice for monitoring response to treatment with levothyroixine.[59] When treatment is first initiated, TSH levels may be monitored as often as a frequency of every 6–8 weeks.[59] Each time the dose is adjusted, TSH levels may be measured at that frequency until the correct dose is determined.[59] Once titrated to a proper dose, TSH levels will be monitored yearly.[59]
Surgery is not the initial treatment of choice for autoimmune, and it is not an indication for thyroidectomy.[22] Patients generally may begin discussing surgery with their doctor if they are experiencing significant pressure symptoms, cosmetic concerns, or have nodules present on ultrasound.[22]
Overt, symptomatic thyroid dysfunction is the most common complication, with about 5% of people with subclinical hypothyroidism and chronic autoimmune thyroiditis progressing to thyroid failure every year. Transient periods of thyrotoxicosis (over-activity of the thyroid) sometimes occur, and rarely the illness may progress to full hyperthyroid Graves' disease with active orbitopathy (bulging, inflamed eyes). Rare cases of fibrous autoimmune thyroiditis present with severe shortness of breath and difficulty swallowing, resembling aggressive thyroid tumors, but such symptoms always improve with surgery or corticosteroid therapy. Although primary thyroid B-cell lymphoma affects fewer than one in 1000 persons, it is more likely to affect those with long-standing autoimmune thyroiditis,[61] as there is a 67- to 80-fold increased risk of developing primary thyroid lymphoma in patients with Hashimoto's thyroiditis.[62]
Hashimoto's thyroiditis disorder is thought to be the most common cause of primary hypothyroidism in North America.[52] Within person, place, and time descriptive trends of epidemiology, it becomes more clear on how Hashimoto's thyroiditis develops in and impacts differing populations.
Overall, Hashimoto's thyroiditis affects up to 2% of the general population.[25] About 5% of Caucasians will develop Hashimoto's at some point in their lives.[4] In the US, the African-American population experiences it less commonly but has greater associated mortality.[63]  It is also less frequent in  Asian populations.[64] About 1.0 to 1.5 in 1000 people have this disease at any time.[52] It occurs between 8 and 15 times more often in women than in men. Some research suggests a connection to the role of the placenta as an explanation for the sex difference.[65] Though it may occur at any age, including in children, it is most often observed in women between 30 and 60 years of age.[61] The highest prevalence from one study was found in the elderly members of the community.[66]
Those that already have an autoimmune disease are at greater risk of developing Hashimoto's as the diseases generally coexist with each other.[25] Common diseases seen coexisting with Hashimoto's include celiac disease, multiple sclerosis, type 1 diabetes, vitiligo, and rheumatoid arthritis.[citation needed]
Congenital hypothyroidism affects 1 in 3500-4000 newborns at birth and is a version of intellectual disability that can be treated if caught early, but can be hard to diagnose given that symptoms are minimal at a young age.[66] Congenital hypothyroidism is generally caused by defects of the thyroid gland, but for most cases in Europe, Asia, and Africa, the iodine intake can cause hypothyroidism in newborns.
Diets consisting of low or high iodine intake determine a population's risk of developing thyroid-related disorders.[67] It is more common in regions of high iodine dietary intake, and among people who are genetically susceptible.[61] Geography plays a large role in which regions have access to diets with low or high iodine. Iodine levels in both water and salt should be heavily monitored in order to protect at-risk populations from developing hypothyroidism.[68]
Geographic trends of hypothyroidism vary across the world as different places have different ways of defining disease and reporting cases. Populations that are spread out or defined poorly may skew data in unexpected ways.[25]
Iodine deficiency disorder (IDD) is combated using an increase in iodine in a person's diet. When a dramatic change occurs in a person's diet, they become more at-risk of developing hypothyroidism and other thyroid disorders. Combatting IDD with high salt intakes should be done carefully and cautiously as risk for Hashimoto's may increase.[67] If making modifications to one's diet, it is important to use a clinician's discretion to ensure that the dietary changes are the best option as recommendations can vary person to person.[citation needed]
The secular trends of hypothyroidism reveal how the disease has changed over the course of time given changes in technology and treatment options. Even though ultrasound technology and treatment options have improved, the incidence of hypothyroidism has increased according to data focused on the US and Europe. Between 1993 and 2001, per 1000 women, the disease was found varying between 3.9 and 4.89. Between 1994 and 2001, per 1000 men, the disease increased from 0.65 to 1.01.[66]
Changes in the definition of hypothyroidism and treatment options modify the incidence and prevalence of the disease overall. Treatment using levothyroxine is individualized, and therefore allows the disease to be more manageable with time but does not work as a cure for the disease.[25]
Also known as Hashimoto's disease, Hashimoto's thyroiditis is named after Japanese physician Hakaru Hashimoto (1881−1934) of the medical school at Kyushu University,[69] who first described the symptoms of persons with struma lymphomatosa, an intense infiltration of lymphocytes within the thyroid, in 1912 in the German journal called Archiv für Klinische Chirurgie.[3][70] This paper was made up of 30 pages and 5 illustrations all describing the histological changes in the thyroid tissue. Furthermore, all results in his first study were collected from four women. These results explained the pathological characteristics observed in these women especially the infiltration of lymphoid and plasma cells as well as the formation of lymphoid follicles with germinal centers, fibrosis, degenerated thyroid epithelial cells and leukocytes in the lumen.[3] He described these traits to be histologically similar to those of Mikulic's disease. As mentioned above, once he discovered these traits in this new disease, he named the disease struma lymphomatosa. This disease emphasized the lymphoid cell infiltration and formation of the lymphoid follicles with germinal centers, neither of which had ever been previously reported.[3]
Despite Dr. Hashimoto's discovery and publication, the disease was not recognized as distinct from Reidel's thyroiditis, which was a common disease at that time in Europe. Although many other articles were reported and published by other researchers, Hashimoto's struma lymphomatosa was only recognized as an early phase of Reidel's thyroiditis in the early 1900s. It was not until 1931 that the disease was recognized as a disease in its own right, when researchers Allen Graham et al. from Cleveland reported its symptoms and presentation in the same detailed manner as Hakaru.[3]
In 1956, Drs. Rose and Witebsky were able to demonstrate how immunization of certain rodents with extracts of other rodents' thyroid resembled the disease Hakaru and other researchers were trying to describe.[3] These doctors were also able to describe anti-thyroglobulin antibodies in blood serum samples from these same animals.[citation needed]
Later on in the same year, researchers from the Middlesex Hospital in London were able to perform human experiments on patients who presented with similar symptoms. They purified anti-thyroglobulin antibody from their serum and were able to conclude that these sick patients had an immunological reaction to human thyroglobulin.[3] From this data, it was proposed that Hashimoto's struma could be an autoimmune disease of the thyroid gland.
In 1957, it was recognized as an autoimmune disorder and was the first organ-specific autoimmune disorder identified.[14]
Following this recognition, the same researchers from Middlesex Hospital published an article in 1962 in The Lancet that included a portrait of Hakaru Hashimoto.[3] The disease became more well known from that moment, and Hashimoto's disease started to appear more frequently in textbooks.[citation needed]
Since those discoveries, a number of autoimmune diseases have been discovered, with several of them having to do with thyroid-specific antibodies.[citation needed]
Pregnant women who are positive for Hashimoto's thyroiditis may have decreased thyroid function or the gland may fail entirely.[71] If a woman is TPOAb-positive, clinicians can inform her of the risks for herself and her infant if the disease goes untreated. "Thyroid peroxidase antibodies (TPOAb) are detected in 10% of pregnant women", which presents risks to those pregnancies.[71] Women who have low thyroid function that has not been stabilized are at greater risk of having an infant with: low birth weight, neonatal respiratory distress, hydrocephalus, hypospadias, miscarriage, and preterm delivery.[71][72] The embryo transplantion rate and successful pregnancy outcomes are improved when Hashimoto's is treated.[72] Recommendations are to treat pregnant women only if they are TPOAb-positive throughout the entirety of their pregnancies and to screen all pregnant women for thyroid levels.[71] Close cooperation between the endocrinologist and obstetrician benefits the woman and the infant.[71][73][74] The Endocrine Society recommends screening in pregnant women who are considered high-risk for thyroid autoimmune disease.[75]
Thyroid peroxides antibodies testing is recommended for women who have ever been pregnant regardless of pregnancy outcome. "[P]revious pregnancy plays a major role in development of autoimmune overt hypothyroidism in premenopausal women, and the number of previous pregnancies should be taken into account when evaluating the risk of hypothyroidism in a young women [sic]."[76]
Hormonal changes and trophoblast expression of key immunomodulatory molecules lead to immunosuppression and fetal tolerance. Main players in regulation of the immune response are Tregs. Both cell-mediated and humoral immune responses are attenuated, resulting in immune tolerance and suppression of autoimmunity. It has been reported that during pregnancy, levels of thyroid peroxidase and thyroglobulin antibodies decrease. After giving birth, Tregs rapidly decrease and immune responses are re-established. It may lead to the occurrence or aggravation of the autoimmune thyroid disease.[77] In up to 50% of females with thyroid peroxidase antibodies in the early pregnancy, thyroid autoimmunity in the postpartum period exacerbates in the form of postpartum thyroiditis.[78] Higher secretion of IFN-γ and IL-4, and lower plasma cortisol concentration during pregnancy has been reported in females with postpartum thyroiditis than in healthy females. It indicates that weaker immunosuppression during pregnancy could contribute to the postpartum thyroid dysfunction.[79]
Several years after the delivery, the chimeric male cells can be detected in the maternal peripheral blood, thyroid, lung, skin, or lymph nodes. The fetal immune cells in the maternal thyroid gland may become activated and act as a trigger that may initiate or exaggerate the autoimmune thyroid disease. In Hashimoto's disease patients, fetal microchimeric cells were detected in thyroid in significantly higher numbers than in healthy females.[80]
Hashimoto's disease is also known in chickens (Gallus domesticus),[81][82] rats (Rattus rattus),[82] mice (Mus musculus),[82] dogs (Canis familiaris),[82] and marmosets (Callitrichidae).[82]


Heart failure (HF), also known as congestive heart failure (CHF), is a syndrome, a group of signs and symptoms, caused by an impairment of the heart's blood pumping function. Symptoms typically include shortness of breath, excessive fatigue, and leg swelling. The shortness of breath may occur with exertion or while lying down, and may wake people up during the night.[3] Chest pain, including angina, is not usually caused by heart failure, but may occur if the heart failure was caused by a heart attack.[11][12] The severity of the heart failure is mainly decided based on ejection fraction and also measured by the severity of symptoms .[7] Other conditions that may have symptoms similar to heart failure include obesity, kidney failure, liver disease, anemia, and thyroid disease.[7]
Common causes of heart failure include coronary artery disease, heart attack, high blood pressure, atrial fibrillation, valvular heart disease, excessive alcohol consumption, infection, and cardiomyopathy.[4][6] These cause heart failure by altering the structure or the function of the heart or in some cases both.[6] There are different types of heart failure: right-sided heart failure, which affects the right heart, left-sided heart failure, which affects the left heart, and biventricular heart failure, which affects both sides of the heart.[13] Left-sided heart failure may be present with a reduced ejection fraction or with a preserved ejection fraction.[10] Heart failure is not the same as cardiac arrest, in which blood flow stops completely due to the failure of the heart to pump.[14][15]
Diagnosis is based on symptoms, physical findings, and echocardiography.[6] Blood tests, and a chest x-ray may be useful to determine the underlying cause.[16] Treatment depends on severity and case.[17] For people with chronic, stable, mild heart failure, treatment usually consists of lifestyle changes, such as not smoking, physical exercise, and dietary changes, as well as medications.[18][19][20] In heart failure due to left ventricular dysfunction, angiotensin-converting-enzyme inhibitors, angiotensin receptor blockers, or angiotensin receptor-neprilysin inhibitors, along with beta blockers, mineralocorticoid receptor antagonists and SGLT2 inhibitors are recommended.[6] Diuretics may also be prescribed to prevent fluid retention and the resulting shortness of breath.[21] Depending on the case, an implanted device such as a pacemaker or implantable cardiac defibrillator  may sometimes be recommended.[17] In some moderate or more severe cases, cardiac resynchronization therapy (CRT)[22] or cardiac contractility modulation may be beneficial.[23] In severe disease that persists despite all other measures, a cardiac assist device ventricular assist device, or, occasionally, heart transplantation may be recommended.[21]
Heart failure is a common, costly, and potentially fatal condition,[24] and is the leading cause of hospitalization and readmission in older adults.[25][26] Heart failure often leads to more drastic health impairments than failure of other, similarly complex organs such as the kidneys or liver.[27] In 2015, it affected about 40 million people worldwide.[8] Overall, heart failure affects about 2% of adults,[24] and more than 10% of those over the age of 70.[6] Rates are predicted to increase.[24] The risk of death in the first year after diagnosis is about 35%, while the risk of death in the second year is less than 10% in those still alive.[10] The risk of death is comparable to that of some cancers.[10] In the United Kingdom, the disease is the reason for 5% of emergency hospital admissions.[10] Heart failure has been known since ancient times; it is mentioned in the Ebers Papyrus around 1550 BCE.[28]
Heart failure is not a disease but a syndrome – a combination of signs and symptoms – caused by the failure of the heart to pump blood to support the circulatory system at rest or during activity.[6]: 3612 [3] It develops when the heart fails to properly fill with blood during diastole, resulting in a decrease in intracardiac pressures or in ejection during systole, reducing cardiac output to the rest of the body.[6]: 3612 [4]: e272  The filling failure and high intracardiac pressure can lead to fluid accumulation in the veins and tissue. This manifests as water retention and swelling due to fluid accumulation (edema) called congestion. Impaired ejection can lead to inadequate blood flow to the body tissues, resulting in ischemia.[29][30]
Congestive heart failure is a pathophysiological condition in which the heart's output is insufficient to meet the needs of the body and lungs.[10] The term "congestive heart failure" is often used because one of the most common symptoms is congestion or fluid accumulation in the tissues and veins of the lungs or other parts of a person's body.[10] Congestion manifests itself particularly in the form of fluid accumulation and swelling (edema), in the form of peripheral edema (causing swollen limbs and feet) and pulmonary edema (causing difficulty breathing) and ascites (swollen abdomen).[30] Pulse pressure, which is the difference between the systolic ("top number") and diastolic ("bottom number") blood pressures, is often low/narrow (ie. 25% or less of the level of the systolic) in people with heart failure, and this can be an early warning sign.[31]
Symptoms of heart failure are traditionally divided into left-sided and right-sided because the left and right ventricles supply different parts of the circulation. In biventricular heart failure, both sides of the heart are affected. Left-sided heart failure is the more common.[32]
The left side of the heart takes oxygen-rich blood from the lungs and pumps it to the rest of the circulatory system in the body (except for the pulmonary circulation). Failure of the left side of the heart causes blood to back up into the lungs, causing breathing difficulties and fatigue due to an insufficient supply of oxygenated blood. Common respiratory signs include increased respiratory rate and labored breathing (nonspecific signs of shortness of breath). Rales or crackles heard initially in the lung bases and when severe in all lung fields indicate the development of pulmonary edema (fluid in the alveoli). Cyanosis, indicates deficiency of oxygen in the blood, is a late sign of extremely severe pulmonary edema.[33]
Other signs of left ventricular failure include a laterally displaced apex beat (which occurs when the heart is enlarged) and a gallop rhythm (additional heart sounds), which may be heard as a sign of increased blood flow or increased intracardiac pressure. Heart murmurs may indicate the presence of valvular heart disease, either as a cause (e.g., aortic stenosis) or as a consequence (e.g., mitral regurgitation) of heart failure.[34]
Reverse insufficiency of the left ventricle causes congestion in the blood vessels of the lungs, so that symptoms are predominantly respiratory. Reverse insufficiency can be divided into the failure of the left atrium, the left ventricle, or both within the left circuit. Patients will experience shortness of breath (dyspnea) on exertion and, in severe cases, dyspnea at rest. Increasing breathlessness while lying down, called orthopnea, also occurs. It can be measured by the number of pillows required to lie comfortably, with extreme cases of orthopnea forcing the patient to sleep sitting up. Another symptom of heart failure is paroxysmal nocturnal dyspnea: a sudden nocturnal attack of severe shortness of breath, usually occurring several hours after falling asleep.[35] There may be "cardiac asthma" or wheezing. Impaired left ventricular forward function can lead to symptoms of poor systemic perfusion such as dizziness, confusion, and cool extremities at rest. Loss of consciousness may also occur due to loss of blood supply to the brain.[36]
Right-sided heart failure is often caused by pulmonary heart disease (cor pulmonale), which is typically caused by issues with pulmonary circulation such as pulmonary hypertension or pulmonic stenosis. Physical examination may reveal pitting peripheral edema, ascites, liver enlargement, and spleen enlargement. Jugular venous pressure is frequently assessed as a marker of fluid status, which can be accentuated by testing hepatojugular reflux. If the right ventricular pressure is increased, a parasternal heave which causes the compensatory increase in contraction strength may be present.[37]
Backward failure of the right ventricle leads to congestion of systemic capillaries. This generates excess fluid accumulation in the body. This causes swelling under the skin (peripheral edema or anasarca) and usually affects the dependent parts of the body first, causing foot and ankle swelling in people who are standing up and sacral edema in people who are predominantly lying down. Nocturia (frequent night-time urination) may occur when fluid from the legs is returned to the bloodstream while lying down at night. In progressively severe cases, ascites (fluid accumulation in the abdominal cavity causing swelling) and liver enlargement may develop. Significant liver congestion may result in impaired liver function (congestive hepatopathy), jaundice, and coagulopathy (problems of decreased or increased blood clotting).[38]
Dullness of the lung fields when percussed and reduced breath sounds at the base of the lungs may suggest the development of a pleural effusion (fluid collection between the lung and the chest wall). Though it can occur in isolated left- or right-sided heart failure, it is more common in biventricular failure because pleural veins drain into both the systemic and pulmonary venous systems. When unilateral, effusions are often right-sided.[39]
If a person with a failure of one ventricle lives long enough, it will tend to progress to failure of both ventricles. For example, left ventricular failure allows pulmonary edema and pulmonary hypertension to occur, which increase stress on the right ventricle. Though still harmful, right ventricular failure is not as deleterious to the left side.[40]
Since heart failure is a syndrome and not a disease, establishing the underlying cause is vital to diagnosis and treatment.[41][32] In heart failure, the structure or the function of the heart or in some cases both are altered.[6]: 3612  Heart failure is the potential end stage of all heart diseases.[42]
Common causes of heart failure include coronary artery disease, including a previous myocardial infarction (heart attack), high blood pressure, atrial fibrillation, valvular heart disease, excess alcohol use, infection, and cardiomyopathy of an unknown cause.[9][4]: e279 [6]: Table 5  In addition, viral infections of the heart can lead to inflammation of the muscular layer of the heart and subsequently contribute to the development of heart failure. Genetic predisposition plays an important role. If more than one cause is present, progression is more likely and prognosis is worse.[43]
Heart damage can predispose a person to develop heart failure later in life and has many causes including systemic viral infections (e.g., HIV), chemotherapeutic agents such as daunorubicin, cyclophosphamide, trastuzumab and substance use disorders of substances such as alcohol, cocaine, and methamphetamine. An uncommon cause is exposure to certain toxins such as lead and cobalt. Additionally, infiltrative disorders such as amyloidosis and connective tissue diseases such as systemic lupus erythematosus have similar consequences. Obstructive sleep apnea (a condition of sleep wherein disordered breathing overlaps with obesity, hypertension, and/or diabetes) is regarded as an independent cause of heart failure.[44] Recent reports from clinical trials have also linked variation in blood pressure to heart failure[45][46] and cardiac changes that may give rise to heart failure.[47]
High-output heart failure happens when the amount of blood pumped out is more than typical and the heart is unable to keep up.[48] This can occur in overload situations such as blood or serum infusions, kidney diseases, chronic severe anemia, beriberi (vitamin B1/thiamine deficiency), hyperthyroidism, cirrhosis, Paget's disease, multiple myeloma, arteriovenous fistulae, or arteriovenous malformations.[49][50]
Chronic stable heart failure may easily decompensate. This most commonly results from a concurrent illness (such as myocardial infarction (a heart attack) or pneumonia), abnormal heart rhythms, uncontrolled hypertension, or a person's failure to maintain a fluid restriction, diet, or medication.[51]
Other factors that may worsen CHF include: anemia, hyperthyroidism, excessive fluid or salt intake, and medication such as NSAIDs and thiazolidinediones.[52] NSAIDs increase the risk twofold.[53]
A number of medications may cause or worsen the disease. This includes NSAIDs, COX-2 inhibitors, a number of anesthetic agents such as ketamine, thiazolidinediones, some cancer medications, several antiarrhythmic medications, pregabalin, alpha-2 adrenergic receptor agonists, minoxidil, itraconazole, cilostazol, anagrelide, stimulants (e.g., methylphenidate), tricyclic antidepressants, lithium, antipsychotics, dopamine agonists, TNF inhibitors, calcium channel blockers (especially verapamil and diltiazem[54][55]), salbutamol, and tamsulosin.[56]
By inhibiting the formation of prostaglandins, NSAIDs may exacerbate heart failure through several mechanisms, including promotion of fluid retention, increasing blood pressure, and decreasing a person's response to diuretic medications.[56] Similarly, the ACC/AHA recommends against the use of COX-2 inhibitor medications in people with heart failure.[56] Thiazolidinediones have been strongly linked to new cases of heart failure and worsening of pre-existing congestive heart failure due to their association with weight gain and fluid retention.[56] Certain calcium channel blockers, such as diltiazem and verapamil, are known to decrease the force with which the heart ejects blood, thus are not recommended in people with heart failure with a reduced ejection fraction.[56]
Breast cancer patients are at high risk of heart failure due to several factors.[57] After analysing data from 26 studies (836,301 patients), the recent meta-analysis found that breast cancer survivors demonstrated a higher risk heart failure within first ten years after diagnosis (hazard ratio = 1.21; 95% CI: 1.1, 1.33).[58] The pooled incidence of heart failure in breast cancer survivors was 4.44 (95% CI 3.33-5.92) per 1000 person-years of follow-up.[58]
Certain alternative medicines carry a risk of exacerbating existing heart failure, and are not recommended.[56] This includes aconite, ginseng, gossypol, gynura, licorice, lily of the valley, tetrandrine, and yohimbine.[56] Aconite can cause abnormally slow heart rates and abnormal heart rhythms such as ventricular tachycardia.[56] Ginseng can cause abnormally low or high blood pressure, and may interfere with the effects of diuretic medications. Gossypol can increase the effects of diuretics, leading to toxicity. Gynura can cause low blood pressure. Licorice can worsen heart failure by increasing blood pressure and promoting fluid retention.[56] Lily of the valley can cause abnormally slow heart rates with mechanisms similar to those of digoxin. Tetrandrine can lead to low blood pressure through inhibition of L-type calcium channels. Yohimbine can exacerbate heart failure by increasing blood pressure through alpha-2 adrenergic receptor antagonism.[56]
 Heart failure is caused by any condition that reduces the efficiency of the heart muscle, through damage or overloading. Over time, these increases in workload, which are mediated by long-term activation of neurohormonal systems such as the renin–angiotensin system and the sympathoadrenal system, lead to fibrosis, dilation, and structural changes in the shape of the left ventricle from elliptical to spherical.[24]
The heart of a person with heart failure may have a reduced force of contraction due to overloading of the ventricle. In a normal heart, increased filling of the ventricle results in increased contraction force by the Frank–Starling law of the heart, and thus a rise in cardiac output. In heart failure, this mechanism fails, as the ventricle is loaded with blood to the point where heart muscle contraction becomes less efficient. This is due to reduced ability to cross-link actin and myosin myofilaments in over-stretched heart muscle.[59]
No diagnostic criteria have been agreed on as the gold standard for heart failure, especially heart failure with preserved ejection fraction (HFpEF).
In the UK, the National Institute for Health and Care Excellence recommends measuring N-terminal pro-BNP (NT-proBNP) followed by an ultrasound of the heart if positive.[16] In Europe, the European Society of Cardiology, and in the United States, the AHA/ACC/HFSA, recommend measuring NT-proBNP or BNP followed by an ultrasound of the heart if positive.[6][4] This is recommended in those with symptoms consistent with heart failure such as shortness of breath.[4]
The European Society of Cardiology defines the diagnosis of heart failure as symptoms and signs consistent with heart failure in combination with "objective evidence of cardiac structural or functional abnormalities".[6] This definition is consistent with an international 2021 report termed "Universal Definition of Heart Failure".[6]: 3613  Score-based algorithms have been developed to help in the diagnosis of HFpEF, which can be challenging for physicians to diagnose.[6]: 3630  The AHA/ACC/HFSA defines heart failure as symptoms and signs consistent with heart failure in combination with shown "structural and functional alterations of the heart as the underlying cause for the clinical presentation", for HFmrEF and HFpEF specifically requiring "evidence of spontaneous or provokable increased left ventricle filling pressures".[4]: e276–e277 
The European Society of Cardiology has developed a diagnostic algorithm for HFpEF, named HFA-PEFF.[6]: 3630 [60] HFA-PEFF considers symptoms and signs, typical clinical demographics (obesity, hypertension, diabetes, elderly, atrial fibrillation), and diagnostic laboratory tests, ECG, and echocardiography.[4]: e277 [60]
One historical method of categorizing heart failure is by the side of the heart involved (left heart failure versus right heart failure). Right heart failure was thought to compromise blood flow to the lungs compared to left heart failure compromising blood flow to the aorta and consequently to the brain and the remainder of the body's systemic circulation. However, mixed presentations are common and left heart failure is a common cause of right heart failure.[61]
More accurate classification of heart failure type is made by measuring ejection fraction, or the proportion of blood pumped out of the heart during a single contraction.[62] Ejection fraction is given as a percentage with the normal range being between 50 and 75%.[62] The types are:
Heart failure may also be classified as acute or chronic. Chronic heart failure is a long-term condition, usually kept stable by the treatment of symptoms. Acute decompensated heart failure is a worsening of chronic heart failure symptoms, which can result in acute respiratory distress.[66] High-output heart failure can occur when there is increased cardiac demand that results in increased left ventricular diastolic pressure which can develop into pulmonary congestion (pulmonary edema).[48]
Several terms are closely related to heart failure and may be the cause of heart failure, but should not be confused with it. Cardiac arrest and asystole refer to situations in which no cardiac output occurs at all. Without urgent treatment, these events result in sudden death. Myocardial infarction ("Heart attack") refers to heart muscle damage due to insufficient blood supply, usually as a result of a blocked coronary artery. Cardiomyopathy refers specifically to problems within the heart muscle, and these problems can result in heart failure.[67] Ischemic cardiomyopathy implies that the cause of muscle damage is coronary artery disease. Dilated cardiomyopathy implies that the muscle damage has resulted in enlargement of the heart.[68] Hypertrophic cardiomyopathy involves enlargement and thickening of the heart muscle.[69]
An echocardiogram (ultrasound of the heart) is commonly used to support a clinical diagnosis of heart failure. This can determine the stroke volume (SV, the amount of blood in the heart that exits the ventricles with each beat), the end-diastolic volume (EDV, the total amount of blood at the end of diastole), and the SV in proportion to the EDV, a value known as the ejection fraction (EF). In pediatrics, the shortening fraction is the preferred measure of systolic function. Normally, the EF should be between 50 and 70%; in systolic heart failure, it drops below 40%. Echocardiography can also identify valvular heart disease and assess the state of the pericardium (the connective tissue sac surrounding the heart). Echocardiography may also aid in deciding specific treatments, such as medication, insertion of an implantable cardioverter-defibrillator, or cardiac resynchronization therapy. Echocardiography can also help determine if acute myocardial ischemia is the precipitating cause, and may manifest as regional wall motion abnormalities on echo.[70]
Ultrasound showing severe systolic heart failure[71]
Ultrasound showing severe systolic heart failure[71]
Ultrasound of the lungs showing edema due to severe systolic heart failure[71]
Ultrasound showing severe systolic heart failure[71]
Ultrasound showing severe systolic heart failure[71]
Chest X-rays are frequently used to aid in the diagnosis of CHF. In a person who is compensated, this may show cardiomegaly (visible enlargement of the heart), quantified as the cardiothoracic ratio (proportion of the heart size to the chest). In left ventricular failure, evidence may exist of vascular redistribution (upper lobe blood diversion or cephalization), Kerley lines, cuffing of the areas around the bronchi, and interstitial edema. Ultrasound of the lung may also be able to detect Kerley lines.[72]
Congestive heart failure with small bilateral effusions
Kerley B lines
An electrocardiogram (ECG or EKG) may be used to identify arrhythmias, ischemic heart disease, right and left ventricular hypertrophy, and presence of conduction delay or abnormalities (e.g. left bundle branch block). Although these findings are not specific to the diagnosis of heart failure, a normal ECG virtually excludes left ventricular systolic dysfunction.[73]
N-terminal pro-BNP (NT-proBNP) is the favoured biomarker for the diagnosis of heart failure, according to guidelines published 2018 by NICE in the UK.[3] Brain natriuretic peptide 32 (BNP) is another biomarker commonly tested for heart failure.[74][6][75] An elevated NT-proBNP or BNP is a specific test indicative of heart failure. Additionally, NT-proBNP or BNP can be used to differentiate between causes of dyspnea due to heart failure from other causes of dyspnea. If myocardial infarction is suspected, various cardiac markers may be used.
Blood tests routinely performed include electrolytes (sodium, potassium), measures of kidney function, liver function tests, thyroid function tests, a complete blood count, and often C-reactive protein if infection is suspected.
Hyponatremia (low serum sodium concentration) is common in heart failure. Vasopressin levels are usually increased, along with renin, angiotensin II, and catecholamines to compensate for reduced circulating volume due to inadequate cardiac output. This leads to increased fluid and sodium retention in the body; the rate of fluid retention is higher than the rate of sodium retention in the body, this phenomenon causes hypervolemic hyponatremia (low sodium concentration due to high body fluid retention). This phenomenon is more common in older women with low body mass. Severe hyponatremia can result in accumulation of fluid in the brain, causing cerebral edema and intracranial hemorrhage.[76]
Angiography is the X-ray imaging of blood vessels, which is done by injecting contrast agents into the bloodstream through a thin plastic tube (catheter), which is placed directly in the blood vessel. X-ray images are called angiograms.[77] Heart failure may be the result of coronary artery disease, and its prognosis depends in part on the ability of the coronary arteries to supply blood to the myocardium (heart muscle). As a result, coronary catheterization may be used to identify possibilities for revascularisation through percutaneous coronary intervention or bypass surgery.
Heart failure is commonly stratified by the degree of functional impairment conferred by the severity of the heart failure, as reflected in the New York Heart Association (NYHA) functional classification.[78] The NYHA functional classes (I–IV) begin with class I, which is defined as a person who experiences no limitation in any activities and has no symptoms from ordinary activities. People with NYHA class II heart failure have slight, mild limitations with everyday activities; the person is comfortable at rest or with mild exertion. With NYHA class III heart failure, a marked limitation occurs with any activity; the person is comfortable only at rest. A person with NYHA class IV heart failure is symptomatic at rest and becomes quite uncomfortable with any physical activity. This score documents the severity of symptoms and can be used to assess response to treatment. While its use is widespread, the NYHA score is not very reproducible and does not reliably predict the walking distance or exercise tolerance on formal testing.[79]
In its 2001 guidelines, the American College of Cardiology/American Heart Association working group introduced four stages of heart failure:[80]
The ACC staging system is useful since stage A encompasses "pre-heart failure" – a stage where intervention with treatment can presumably prevent progression to overt symptoms. ACC stage A does not have a corresponding NYHA class. ACC stage B would correspond to NYHA class I. ACC stage C corresponds to NYHA class II and III, while ACC stage D overlaps with NYHA class IV.
Histopathology can diagnose heart failure in autopsies. The presence of siderophages indicates chronic left-sided heart failure, but is not specific for it.[81] It is also indicated by congestion of the pulmonary circulation.
A person's risk of developing heart failure is inversely related to level of physical activity. Those who achieved at least 500 MET-minutes/week (the recommended minimum by U.S. guidelines) had lower heart failure risk than individuals who did not report exercising during their free time; the reduction in heart failure risk was even greater in those who engaged in higher levels of physical activity than the recommended minimum.[82]
Heart failure can also be prevented by lowering high blood pressure and high blood cholesterol, and by controlling diabetes. Maintaining a healthy weight, and decreasing sodium, alcohol, and sugar intake, may help. Additionally, avoiding tobacco use has been shown to lower the risk of heart failure.[83] According to Johns Hopkins and the American Heart Association there are a few ways to help to prevent a cardiac event. Johns Hopkins states that stopping tobacco use, reducing high blood pressure, physical activity and your diet can drastically effect the chances of developing heart disease. High blood pressure accounts for most cardiovascular deaths. High blood pressure can be lowered into the normal range by making dietary decisions such as consuming less salt. Exercise also helps to bring blood pressure back down. One of the best ways to help avoid heart failure is to promote healthier eating habits like eating more vegetables, fruits, grains, and lean protein.[84]
Diabetes is a major risk factor for heart failure. For women with Coronary Heart disease (CHD), diabetes was the strongest risk factor for heart failure.[85] Diabetic women with depressed creatinine clearance or elevated BMI were at the highest risk of heart failure. While the annual incidence rate of heart failure for non-diabetic women with no risk factors is 0.4%, the annual incidence rate for diabetic women with elevated body mass index (BMI) and depressed creatinine clearance was 7% and 13%, respectively.[86]
Treatment focuses on improving the symptoms and preventing the progression of the disease. Reversible causes of heart failure also need to be addressed (e.g. infection, alcohol ingestion, anemia, thyrotoxicosis, arrhythmia, and hypertension). Treatments include lifestyle and pharmacological modalities, and occasionally various forms of device therapy. Rarely, cardiac transplantation is used as an effective treatment when heart failure has reached the end stage.[87]
In acute decompensated heart failure, the immediate goal is to re-establish adequate perfusion and oxygen delivery to end organs. This entails ensuring that airway, breathing, and circulation are adequate. Immediate treatments usually involve some combination of vasodilators such as nitroglycerin, diuretics such as furosemide, and possibly noninvasive positive pressure ventilation. Supplemental oxygen is indicated in those with oxygen saturation levels below 90%, but is not recommended in those with normal oxygen levels in normal atmosphere.[88]
The goals of the treatment for people with chronic heart failure are the prolongation of life, prevention of acute decompensation, and reduction of symptoms, allowing for greater activity.
Heart failure can result from a variety of conditions. In considering therapeutic options, excluding reversible causes is of primary importance, including thyroid disease, anemia, chronic tachycardia, alcohol use disorder, hypertension, and dysfunction of one or more heart valves. Treatment of the underlying cause is usually the first approach to treating heart failure. In the majority of cases, though, either no primary cause is found or treatment of the primary cause does not restore normal heart function. In these cases, behavioral, medical and device treatment strategies exist that can provide a significant improvement in outcomes, including the relief of symptoms, exercise tolerance, and a decrease in the likelihood of hospitalization or death. Breathlessness rehabilitation for chronic obstructive pulmonary disease and heart failure has been proposed with exercise training as a core component. Rehabilitation should also include other interventions to address shortness of breath including psychological and educational needs of people and needs of caregivers.[89] Iron supplementation appears to reduce hospitalization but not all-cause mortality in patients with iron deficiency and heart
failure.[90]
The latest evidence indicates that advance care planning (ACP) may help to increase documentation by medical staff regarding discussions with participants, and improve an individual's depression.[91] This involves discussing an individual's future care plan in consideration of the individual's preferences and values. The findings are however, based on low-quality evidence.[91]
The various measures often used to assess the progress of people being treated for heart failure include fluid balance (calculation of fluid intake and excretion) and monitoring body weight (which in the shorter term reflects fluid shifts).[92] Remote monitoring can be effective to reduce complications for people with heart failure.[93][94]
Behavior modification is a primary consideration in chronic heart failure management program, with dietary guidelines regarding fluid and salt intake.[95] Fluid restriction is important to reduce fluid retention in the body and to correct the hyponatremic status of the body.[76] The evidence of benefit of reducing salt, however, is poor as of 2018.[96] Thirst is a common and burdensome symptom for patients to cope with. Chewing gum has been shown to be an effective intervention to relieve thirst in patients experiencing heart failure, although patient acceptability remains an issue.
Exercise should be encouraged and tailored to suit individual's capabilities. A meta-analysis found that centre-based group interventions delivered by a physiotherapist are helpful in promoting physical activity in HF.[97] There is a need for additional training for physiotherapists in delivering behaviour change intervention alongside an exercise programme. An intervention is expected to be more efficacious in encouraging physical activity than the usual care if it includes Prompts and cues to walk or exercise, like a phone call or a text message. It is extremely helpful if a trusted clinician provides explicit advice to engage in physical activity (Credible source). Another highly effective strategy is to place objects that will serve as a cue to engage in physical activity in the everyday environment of the patient (Adding object to the environment; e.g., exercise step or treadmill). Encouragement to walk or exercise in various settings beyond CR (e.g., home, neighbourhood, parks) is also promising (Generalisation of target behaviour). Additional promising strategies are Graded tasks (e.g., gradual increase in intensity and duration of exercise training), Self-monitoring, Monitoring of physical activity by others without feedback, Action planning, and Goal-setting.[98] The inclusion of regular physical conditioning as part of a cardiac rehabilitation program can significantly improve quality of life and reduce the risk of hospital admission for worsening symptoms, but no evidence shows a reduction in mortality rates as a result of exercise.
Home visits and regular monitoring at heart-failure clinics reduce the need for hospitalization and improve life expectancy.[99]
Quadruple medical therapy using a combination of angiotensin receptor-neprilysin inhibitors (ARNI), beta blockers, mineralocorticoid receptor antagonists (MRA), and sodium/glucose cotransporter 2 inhibitors (SGLT2 inhibitors) is the standard of care as of 2021 for heart failure with reduced ejection fraction (HFrEF).[100][101]
There is no convincing evidence for pharmacological treatment of heart failure with preserved ejection fraction (HFpEF).[6] Medication for HFpEF is symptomatic treatment with diuretics to treat congestion.[6] Managing risk factors and comorbidities such as hypertension is recommended in HFpEF.[6]
Inhibitors of the renin–angiotensin system (RAS) are recommended in heart failure. The angiotensin receptor-neprilysin inhibitors (ARNI) sacubitril/valsartan is recommended as first choice of RAS inhibitors in American guidelines published by AHA/ACC in 2022.[4] Use of angiotensin-converting enzyme (ACE) inhibitors (ACE-I), or angiotensin receptor blockers (ARB) if the person develops a long-term cough as a side effect of the ACE-I,[102] is associated with improved survival, fewer hospitalizations for heart failure exacerbations, and improved quality of life in people with heart failure.[103] European guidelines published by ESC in 2021 recommends that ARNI should be used in those who still have symptoms while on an ACE-I or ARB, beta blocker, and a mineralocorticoid receptor antagonist. Use of the combination agent ARNI requires the cessation of ACE-I or ARB therapy at least 36 hours before its initiation.[4]
Beta-adrenergic blocking agents (beta blockers) add to the improvement in symptoms and mortality provided by ACE-I/ARB.[103][104] The mortality benefits of beta blockers in people with systolic dysfunction who also have atrial fibrillation is more limited than in those who do not have it.[105] If the ejection fraction is not diminished (HFpEF), the benefits of beta blockers are more modest; a decrease in mortality has been observed, but reduction in hospital admission for uncontrolled symptoms has not been observed.[106]
In people who are intolerant of ACE-I and ARB or who have significant kidney dysfunction, the use of combined hydralazine and a long-acting nitrate, such as isosorbide dinitrate, is an effective alternate strategy. This regimen has been shown to reduce mortality in people with moderate heart failure.[107] It is especially beneficial in the black population.[a][107]
Use of a mineralocorticoid antagonist, such as spironolactone or eplerenone, in addition to beta blockers and ACE-I, can improve symptoms and reduce mortality in people with symptomatic heart failure with reduced ejection fraction (HFrEF).[18]
SGLT2 inhibitors are used for heart failure.[4]
Second-line medications for CHF do not confer a mortality benefit. Digoxin is one such medication. Its narrow therapeutic window, a high degree of toxicity, and the failure of multiple trials to show a mortality benefit have reduced its role in clinical practice. It is now used in only a small number of people with refractory symptoms, who are in atrial fibrillation, and/or who have chronic hypotension.[108][109]
Diuretics have been a mainstay of treatment against symptoms of fluid accumulation, and include diuretics classes such as loop diuretics (such as furosemide), thiazide-like diuretics, and potassium-sparing diuretics. Although widely used, evidence on their efficacy and safety is limited, with the exception of mineralocorticoid antagonists such as spironolactone.[18][110]
Anemia is an independent factor in mortality in people with chronic heart failure. Treatment of anemia significantly improves quality of life for those with heart failure, often with a reduction in severity of the NYHA classification, and also improves mortality rates.[111][112] European Society of Cardiology recommends screening for iron deficiency and treating with intravenous iron if deficiency is found.[6]: 3668–3669 
The decision to anticoagulate people with HF, typically with left ventricular ejection fractions <35% is debated, but generally, people with coexisting atrial fibrillation, a prior embolic event, or conditions that increase the risk of an embolic event such as amyloidosis, left ventricular noncompaction, familial dilated cardiomyopathy, or a thromboembolic event in a first-degree relative.[80]
Vasopressin receptor antagonists can also be used to treat heart failure. Conivaptan is the first medication approved by US Food and Drug Administration for the treatment of euvolemic hyponatremia in those with heart failure.[76] In rare cases hypertonic 3% saline together with diuretics may be used to correct hyponatremia.[76]
Ivabradine is recommended for people with symptomatic heart failure with reduced left ventricular ejection fraction who are receiving optimized guideline-directed therapy (as above) including the maximum tolerated dose of beta-blocker, have a normal heart rhythm and continue to have a resting heart rate above 70 beats per minute.[113] Ivabradine has been found to reduce the risk of hospitalization for heart failure exacerbations in this subgroup of people with heart failure.[113]
In people with severe cardiomyopathy (left ventricular ejection fraction below 35%), or in those with recurrent VT or malignant arrhythmias, treatment with an automatic implantable cardioverter-defibrillator (AICD) is indicated to reduce the risk of severe life-threatening arrhythmias. The AICD does not improve symptoms or reduce the incidence of malignant arrhythmias but does reduce mortality from those arrhythmias, often in conjunction with antiarrhythmic medications. In people with left ventricular ejection (LVEF) below 35%, the incidence of ventricular tachycardia or sudden cardiac death is high enough to warrant AICD placement. Its use is therefore recommended in AHA/ACC guidelines.[22]
Cardiac contractility modulation (CCM) is a treatment for people with moderate to severe left ventricular systolic heart failure (NYHA class II–IV), which enhances both the strength of ventricular contraction and the heart's pumping capacity. The CCM mechanism is based on stimulation of the cardiac muscle by nonexcitatory electrical signals, which are delivered by a pacemaker-like device. CCM is particularly suitable for the treatment of heart failure with normal QRS complex duration (120 ms or less) and has been demonstrated to improve the symptoms, quality of life, and exercise tolerance.[23][114][115][116][117] CCM is approved for use in Europe, and was approved by the Food and Drug Administration for use in the United States in 2019.[118][119][120]
About one-third of people with LVEF below 35% have markedly altered conduction to the ventricles, resulting in dyssynchronous depolarization of the right and left ventricles. This is especially problematic in people with left bundle branch block (blockage of one of the two primary conducting fiber bundles that originate at the base of the heart and carry depolarizing impulses to the left ventricle). Using a special pacing algorithm, biventricular cardiac resynchronization therapy (CRT) can initiate a normal sequence of ventricular depolarization. In people with LVEF below 35% and prolonged QRS duration on ECG (LBBB or QRS of 150 ms or more), an improvement in symptoms and mortality occurs when CRT is added to standard medical therapy.[121] However, in the two-thirds of people without prolonged QRS duration, CRT may actually be harmful.[22][23][122]
People with the most severe heart failure may be candidates for ventricular assist devices, which have commonly been used as a bridge to heart transplantation, but have been used more recently as a destination treatment for advanced heart failure.[123]
In select cases, heart transplantation can be considered. While this may resolve the problems associated with heart failure, the person must generally remain on an immunosuppressive regimen to prevent rejection, which has its own significant downsides.[124] A major limitation of this treatment option is the scarcity of hearts available for transplantation.
People with heart failure often have significant symptoms, such as shortness of breath and chest pain. Palliative care should be initiated early in the HF trajectory, and should not be an option of last resort.[125] Palliative care can not only provide symptom management, but also assist with advanced care planning, goals of care in the case of a significant decline, and making sure the person has a medical power of attorney and discussed his or her wishes with this individual.[126] A 2016 and 2017 review found that palliative care is associated with improved outcomes, such as quality of life, symptom burden, and satisfaction with care.[125][127]
Without transplantation, heart failure may not be reversible and heart function typically deteriorates with time. The growing number of people with stage IV heart failure (intractable symptoms of fatigue, shortness of breath, or chest pain at rest despite optimal medical therapy) should be considered for palliative care or hospice, according to American College of Cardiology/American Heart Association guidelines.[126]
Prognosis in heart failure can be assessed in multiple ways, including clinical prediction rules and cardiopulmonary exercise testing. Clinical prediction rules use a composite of clinical factors such as laboratory tests and blood pressure to estimate prognosis. Among several clinical prediction rules for prognosticating acute heart failure, the 'EFFECT rule' slightly outperformed other rules in stratifying people and identifying those at low risk of death during hospitalization or within 30 days.[128] Easy methods for identifying people that are low-risk are:
A very important method for assessing prognosis in people with advanced heart failure is cardiopulmonary exercise testing (CPX testing). CPX testing is usually required prior to heart transplantation as an indicator of prognosis. CPX testing involves measurement of exhaled oxygen and carbon dioxide during exercise. The peak oxygen consumption (VO2 max) is used as an indicator of prognosis. As a general rule, a VO2 max less than 12–14 cc/kg/min indicates poor survival and suggests that the person may be a candidate for a heart transplant. People with a VO2 max <10 cc/kg/min have a clearly poorer prognosis. The most recent International Society for Heart and Lung Transplantation guidelines[129] also suggest two other parameters that can be used for evaluation of prognosis in advanced heart failure, the heart failure survival score and the use of a criterion of VE/VCO2 slope > 35 from the CPX test. The heart failure survival score is calculated using a combination of clinical predictors and the VO2 max from the CPX test.
Heart failure is associated with significantly reduced physical and mental health, resulting in a markedly decreased quality of life.[130][131] With the exception of heart failure caused by reversible conditions, the condition usually worsens with time. Although some people survive many years, progressive disease is associated with an overall annual mortality rate of 10%.[132]
Around 18 of every 1000 persons will experience an ischemic stroke during the first year after diagnosis of HF. As the duration of follow-up increases, the stroke rate rises to nearly 50 strokes per 1000 cases of HF by 5 years.[133]
In 2022, heart failure affected about 64 million people globally.[134] Overall, around 2% of adults have heart failure.[24] In those over the age of 75, rates are greater than 10%.[24]
Rates are predicted to increase.[24] Increasing rates are mostly because of increasing lifespan, but also because of increased risk factors (hypertension, diabetes, dyslipidemia, and obesity) and improved survival rates from other types of cardiovascular disease (myocardial infarction, valvular disease, and arrhythmias).[135][136][137] Heart failure is the leading cause of hospitalization in people older than 65.[138]
In the United States, heart failure affects 5.8 million people, and each year 550,000 new cases are diagnosed.[139] In 2011, heart failure was the most common reason for hospitalization for adults aged 85 years and older, and the second-most common for adults aged 65–84 years.[140] An estimated one in five adults at age 40 will develop heart failure during their remaining lifetimes and about half of people who develop heart failure die within 5 years of diagnosis.[141] Heart failure – much higher in African Americans, Hispanics, Native Americans, and recent immigrants from Eastern Europe countries – has been linked in these ethnic minority populations to high incidence of diabetes and hypertension.[142]
Nearly one of every four people (24.7%) hospitalized in the U.S. with congestive heart failure are readmitted within 30 days.[143] Additionally, more than 50% of people seek readmission within 6 months after treatment and the average duration of hospital stay is 6 days. Heart failure is a leading cause of hospital readmissions in the U.S. People aged 65 and older were readmitted at a rate of 24.5 per 100 admissions in 2011. In the same year, people under Medicaid were readmitted at a rate of 30.4 per 100 admissions, and uninsured people were readmitted at a rate of 16.8 per 100 admissions. These are the highest readmission rates for both categories. Notably, heart failure was not among the top-10 conditions with the most 30-day readmissions among the privately insured.[144]
In the UK, despite moderate improvements in prevention, heart failure rates have increased due to population growth and ageing.[145] Overall heart failure rates are similar to the four most common causes of cancer (breast, lung, prostate, and colon) combined.[145] People from deprived backgrounds are more likely to be diagnosed with heart failure and at a younger age.[145]
In tropical countries, the most common cause of heart failure is valvular heart disease or some type of cardiomyopathy. As underdeveloped countries have become more affluent, the incidences of diabetes, hypertension, and obesity have increased, which have in turn raised the incidence of heart failure.[citation needed]
Men have a higher incidence of heart failure, but the overall prevalence rate is similar in both sexes since women survive longer after the onset of heart failure.[146] Women tend to be older when diagnosed with heart failure (after menopause), they are more likely than men to have diastolic dysfunction, and seem to experience a lower overall quality of life than men after diagnosis.[146]
Some sources state that people of Asian descent are at a higher risk of heart failure than other ethnic groups.[147] Other sources however have found that rates of heart failure are similar to rates found in other ethnic groups.[148]
For centuries, the disease entity which would include many cases of what today would be called heart failure was dropsy; the term denotes generalized edema, a major manifestation of a failing heart, though also caused by other diseases. Writings of ancient civilizations include evidence of their acquaintance with dropsy and heart failure: Egyptians were the first to use bloodletting to relieve fluid accumulation and shortage of breath, and provided what may have been the first documented observations on heart failure in the Ebers papurus (around 1500 BCE);[149] Greeks described cases of dyspnea, fluid retention and fatigue compatible with heart failure;[150] Romans used the flowering plant Drimia maritima (sea squill), which contains cardiac glycosides, for the treatment of dropsy;[151] descriptions pertaining to heart failure are also known in the civilizations of ancient India and China.[152] However, the manifestations of failing heart were understood in the context of these peoples' medical theories – including ancient Egyptian religion, Hippocratic theory of humours, or ancient Indian and Chinese medicine, and the current concept of heart failure had not developed yet.[150][152] Although shortage of breath had been connected to heart disease by Avicenna round 1000 CE,[153] decisive for modern understanding of the nature of the condition were the description of pulmonary circulation by Ibn al-Nafis in the 13th century, and of systemic circulation by William Harvey in 1628.[150] The role of the heart in fluid retention began to be better appreciated, as dropsy of the chest (fluid accumulation in and round the lungs causing shortage of breath) became more familiar and the current concept of heart failure, which brings together swelling and shortage of breath due to fluid retention, began to be accepted, in the 17th and especially in the 18th century: Richard Lower linked dyspnea and foot swelling in 1679, and Giovanni Maria Lancisi connected jugular vein distention with right ventricular failure in 1728.[153] Dropsy attributable to other causes, e.g. kidney failure, was differentiated in the 19th century.[154][155][156] The stethoscope, invented by René Laennec in 1819, x-rays, discovered by Wilhelm Röntgen in 1895, and electrocardiography, described by Willem Einthoven in 1903, facilitated the investigation of heart failure.[42][156] 19th century also saw experimental and conceptual advances in the physiology of heart contraction, which led to the formulation of the Frank-Starling law of the heart (named after physiologists Otto Frank and Ernest Starling), a remarkable advance in understanding mechanisms of heart failure.[157]
One of the earliest treatments of heart failure, relief of swelling by bloodletting with various methods, including leeches, continued through the centuries.[158] Along with bloodletting, Jean-Baptiste de Sénac in 1749 recommended opiates for acute shortage of breath due to heart failure.[156] In 1785, William Withering described the therapeutic uses of the foxglove genus of plants in the treatment of edema; their extract contains cardiac glycosides, including digoxin, still used today in the treatment of heart failure.[151] The diuretic effects of inorganic mercury salts, which were used to treat syphilis, had already been noted in the 16th century by Paracelsus;[159] in the 19th century they were used by noted physicians like John Blackall and William Stokes.[160] In the meantime, cannulae (tubes) invented by English physician Reginald Southey in 1877 was another method of removing excess fluid by directly inserting into swollen limbs.[158] Use of organic mercury compounds as diuretics, beyond their role in syphilis treatment, started in 1920, though it was limited by their parenteral route of administration and their side-effects.[160][161] Oral mercurial diuretics were introduced in the 1950s; so were thiazide diuretics, which caused less toxicity, and are still used.[42][160] Around the same time, the invention of echocardiography by Inge Edler and Hellmuth Hertz in 1954 marked a new era in the evaluation of heart failure.[42] In the 1960s, loop diuretics were added to available treatments of fluid retention, while a patient with heart failure received the first heart transplant by Christiaan Barnard.[42][160] Over the following decades, new drug classes found their place in heart failure therapeutics, including vasodilators like hydralazine; renin-angiotensin system inhibitors; and beta-blockers.[162][163]
In 2011, nonhypertensive heart failure was one of the 10 most expensive conditions seen during inpatient hospitalizations in the U.S., with aggregate inpatient hospital costs more than $10.5 billion.[164]
Heart failure is associated with a high health expenditure, mostly because of the cost of hospitalizations; costs have been estimated to amount to 2% of the total budget of the National Health Service in the United Kingdom, and more than $35 billion in the United States.[165][166]
Some research indicates that stem cell therapy may help.[167] Although this research indicated benefits of stem cell therapy, other research does not indicate benefit.[168] There is tentative evidence of longer life expectancy and improved left ventricular ejection fraction in persons treated with bone marrow-derived stem cells.[167]
The maintenance of heart function depends on appropriate gene expression that is regulated at multiple levels by epignetic mechanisms including DNA methylation and histone post-translational modification.[169][170]  Currently, an increasing body of research is directed at understanding the role of perturbations of epigenetic processes in cardiac hypertrophy and fibrotic scarring.[169][170]
Psychogenic non-epileptic seizures (PNES), which have been more recently classified as functional seizures, are events resembling an epileptic seizure, but without the characteristic electrical discharges associated with epilepsy.[2][3] PNES fall under the category of disorders known as functional neurological disorders (FND), also known as conversion disorders.[4]  These are typically treated by psychologists or psychiatrists. PNES  has previously been called pseudoseizures, psychogenic seizures, and hysterical seizures, but these terms have fallen out of favor.[5]
The number of people with PNES ranges from 2 to 33 per 100,000.[6] PNES are most common in young adults, particularly women.[6] The prevalence for PNES is estimated to make up 5–20% of outpatient epilepsy clinics; 75–80% of these diagnoses are given to female patients and 83% are to individuals between 15 and 35 years old.[7]
PNES are seen in children after the age of eight, and occur equally among boys and girls before puberty. Diagnostic and treatment principles are similar to those for adults, except that in children there is a broader differential diagnosis of seizures so that other possible diagnoses specific to children may be considered.[8]
Individuals with PNES present with episodes that resemble epileptic seizures, and most have received a diagnosis of epilepsy and treatment for it.[9][10][11][12] PNES episodes are nearly indistinguishable from epileptic seizures. The main differences between a PNES episode and an epileptic seizure is the duration of episodes. Epileptic seizures typically last between 30 and 120 seconds depending on the type, while PNES episodes typically last for two to five minutes.[13]
The cause of PNES has not yet been established. One hypothesis is that they are a learned physical reaction or habit the body develops, similar to a reflex. The individual does not have control of the learned reaction, but this can be retrained to allow the patient to control the physical movements again.[12] The production of seizure-like symptoms is not under voluntary control, meaning that the person is not faking;[9][14] symptoms which are feigned or faked voluntarily would fall under the categories of factitious disorder or malingering.[15]
Risk factors for PNES include having a history of head injury, and having a diagnosis of epilepsy.[16] Approximately 10–30% of people diagnosed with PNES also have an epilepsy diagnosis. People diagnosed with PNES commonly report physical, sexual, or emotional trauma, but the reported incidence of these events may not differ between PNES and epilepsy.[17]
According to the Diagnostic and Statistical Manual of Mental Disorders (version 5) the criteria for receiving a diagnosis of PNES are:[18]
Additionally, the specific symptom type must be reported "with attacks or seizures".[18]
Some individuals with PNES have carried an erroneous diagnosis of epilepsy. On average, it takes seven years to receive a proper diagnosis. The differential diagnosis of PNES firstly involves ruling out epilepsy as the cause of the seizure episodes, along with other organic causes of non-epileptic seizures, including syncope, migraine, vertigo, anoxia, hypoglycemia, and stroke. However, 5–20% of people with PNES also have epilepsy.[19] Frontal lobe seizures can be mistaken for PNES, though these tend to have shorter duration, stereotyped patterns of movements, and occurrence during sleep.[20] Next, an exclusion of factitious disorder (a subconscious somatic symptom disorder, where seizures are caused by psychological reasons) and malingering (simulating seizures intentionally for conscious personal gain – such as monetary compensation or avoidance of criminal punishment) is conducted. Finally other psychiatric conditions that may superficially resemble seizures are eliminated, including panic disorder, schizophrenia, and depersonalisation disorder.[20]
The most definitive test to distinguish epilepsy from PNES is long term video-EEG monitoring, with the aim of capturing one or two episodes on both video recording and EEG simultaneously (some clinicians may use suggestion to attempt to trigger an episode).[21] Additional clinical criteria are usually considered in addition to video-EEG monitoring when diagnosing PNES.[22] By recording the event in question on video and EEG simultaneously, a clear diagnosis can usually be obtained.[23]
Laboratory testing can detect rising blood levels of serum prolactin if samples are taken in the right time window after most tonic-clonic or complex partial epileptic seizures. However, due to false positives and variability in results, this test is relied upon less frequently.[20]
Some features are more or less likely to suggest PNES but they are not conclusive and should be considered in the broader clinical picture. Features that are common in PNES but rarer in epilepsy include: biting the tip of the tongue, seizures lasting more than two minutes (easiest factor to distinguish), seizures having a gradual onset, a fluctuating course of disease severity, the eyes being closed during a seizure, and side to side head movements. Features that are uncommon in PNES include automatisms (automatic complex movements during the seizure), severe tongue biting, biting the inside of the mouth, and incontinence.[20]
If a person with suspected PNES has an episode during a clinical examination, there are a number of signs that can be elicited to help support or refute the diagnosis of PNES. Compared to people with epilepsy, people with PNES will tend to resist having their eyes forced open (if they are closed during the seizure), will stop their hands from hitting their own face if the hand is dropped over the head, and will fixate their eyes in a way suggesting an absence of neurological interference.[20]
Patient understanding of the new diagnosis is crucial for their treatment, which requires their active participation.[24] There are a number of recommended steps to explain to people their diagnosis in a sensitive and open manner. A negative diagnosis experience may cause frustration and could cause a person to reject any further attempts at treatment. Eight points recommended to explain the diagnosis to the person and their caregivers are:
Psychotherapy is the most frequently used treatment, which might include cognitive behavioral therapy or therapy to retrain the physical symptoms and allow the individual to regain control of the attacks (ReACT). There is also some evidence supporting selective serotonin reuptake inhibitor antidepressants.[25]
Cognitive behavioral therapy[26]
Cognitive behavioral therapy (CBT) treatments for PNES typically target fear avoidance and work to reattribute patients' symptoms to psychosocial issues.
Retraining and Control Therapy (ReACT)
ReACT, while new and understudied, has shown extremely promising outcomes for reduction of PNES episodes in pediatric patients.[27] This therapy focuses on the idea that PNES are caused by a learned physical reaction or habit the body develops, similar to a reflex. ReACT aims to retrain the learned reaction (PNES episodes) by targeting symptom catastrophizing and restoring sense of control over symptoms.
Functional seizures have been found to be as disabling and costly as epilepsy.[28] Though there is limited evidence, outcomes appear to be relatively poor with a review of outcome studies finding that two-thirds of people with PNES continue to experience episodes and more than half are dependent on the Social Security program at three-year follow-up.[24] This outcome data was obtained in a referral-based academic epilepsy center and loss to follow-up was considerable; the authors point out ways in which this may have biased their outcome data. Outcome was shown to be better in people with higher IQ,[29] social status,[30] greater educational attainments,[31] younger age of onset and diagnosis,[31] attacks with less dramatic features,[31] and fewer additional somatoform complaints.[31]
For individuals who pursue treatment for PNES, CBT has shown varying rates of success but it has been established as one of the most promising treatments to date.[32] ReACT has shown reduction in symptoms by 100% seven days after treatment and 82% of individuals who completed the therapy remained symptom free for 60 days. A follow-up has not been done to see if the therapy retained its reduction of symptoms beyond the 60 days.[27] In the Cognitive behavioural therapy for adults with dissociative seizures (CODES) trial, the largest regarding CBT treatment for PNES though found no significant reduction in monthly seizures compared to the control arm at 12 months, however there were significant improvements on a number of secondary outcomes, such as psychosocial functioning, and self-rated and clinician-rated global change.[33] A secondary analysis of the CODES trial demonstrated improved frequency of functional seizures at 6 months with CBT.
Hystero-epilepsy is a historical term that refers to a condition described by 19th-century French neurologist Jean-Martin Charcot[34] where people with neuroses "acquired" symptoms resembling seizures as a result of being treated on the same ward as people who genuinely had epilepsy.
The etiology of FND was historically explained in the context of psychoanalytic theory as a physical manifestation of psychological distress and repressed trauma. There is very little supporting evidence for this theory, as there is little research.[35]
The DSM-IV lists conversion disorders instead of the current FND. Additionally, in revision, the DSM-5 was updated to add emphasis to the positive physical signs inconsistent with recognized diseases. The requirement of a history of psychological stressors and that the symptom is not fake was removed as well.[36]
PNES rates and presenting symptoms are somewhat dependent on the culture and society. In some cultures, they, like epilepsy, are thought of as a curse or a demonic possession.[37] In cultures with a solid establishment of evidence-based medicine, they are considered a subtype of a larger category of psychiatric disease.
The use of older terms including pseudoseizures and hysterical seizures are discouraged.[38] In the English language, the word "seizure" usually refers to epileptic events, so some prefer to use more general terms like "events", "attacks", or "episodes", as the term "seizures" may cause confusion with epilepsy.[39][40]
PNES may also be referred to as "non-epileptic attack disorder" "functional seizures", "dissociative convulsions" or "dissociative non-epileptic seizures". These terms are more neutral as to cause, and given that a psychological cause cannot be identified in many cases, they may be more appropriate. Within DSM 5, patients presenting with PNES may meet the criteria for functional neurological disorder and in some cases, somatic symptom disorder, whilst in ICD 10 it may meet the criteria for a conversion disorder.[20]

Being overweight or fat is having more body fat than is optimally healthy. Being overweight is especially common where food supplies are plentiful and lifestyles are sedentary.
As of 2003[update], excess weight reached epidemic proportions globally, with more than 1 billion adults being either overweight or obese.[1] In 2013, this increased to more than 2 billion.[2] Increases have been observed across all age groups.
A healthy body requires a minimum amount of fat for proper functioning of the hormonal, reproductive, and immune systems, as thermal insulation, as shock absorption for sensitive areas, and as energy for future use; however, the accumulation of too much storage fat can impair movement, flexibility, and alter the appearance of the body.
The degree to which a person is overweight is generally described by the body mass index (BMI). Overweight is defined as a BMI of 25 or more, thus it includes pre-obesity defined as a BMI between 25 and 29.9 and obesity as defined by a BMI of 30 or more.[4][5] Pre-obese and overweight however are often used interchangeably, thus giving overweight a common definition of a BMI of between 25 and 29.9. There are, however, several other common ways to measure the amount of adiposity or fat present in an individual's body.
The most common method for discussing this subject and the one used primarily by researchers and advisory institutions is BMI. Definitions of what is considered overweight vary by ethnicity. The current definition proposed by the US National Institutes of Health (NIH) and the World Health Organization (WHO) designates whites, Hispanics and blacks with a BMI of 25 or more as overweight. For Asians, overweight is a BMI between 23 and 29.9 and obesity for all groups is a BMI of 30 or more.
BMI, however, does not account extremes of muscle mass, some rare genetic factors, the very young, and a few other individual variations.  Thus it is possible for an individual with a BMI of less than 25 to have excess body fat, while others may have a BMI that is significantly higher without falling into this category.[8] Some of the above methods for determining body fat are more accurate than BMI but are less convenient to measure.
If an individual is overweight and has excess body fat it can create or lead to health risks. Reports are surfacing, however, that being mildly overweight to slightly obese – BMI being between 24 and 31.9 – may be actually beneficial and that people with a BMI between 24 and 31.9 could actually live longer than normal weight or underweight persons.[9][10]
While some negative health outcomes associated with obesity are accepted within the medical community, the health implications of the overweight category are more controversial.
A 2016 review estimated that the risk of death increases by seven percent among overweight people with a BMI of 25 to 27.5 and 20 percent among overweight people with a BMI of 27.5 to 30.[11] Katherine Flegal et al., however, found that the mortality rate for individuals who are classified as overweight (BMI 25 to 29.9) may actually be lower than for those with an "ideal" weight (BMI 18.5 to 24.9), noting that many studies show that the lowest mortality rate is at a BMI close to 25.[12][13] The specific conclusions appear to depend on what other factors are controlled for,[11] and Flegal has accordingly alleged that the findings from the 2016 review are driven by bias toward preconceived opinions.[14]
Being overweight has been identified as a risk factor for cancer, and Walter Willett predicts that being overweight will overtake smoking as the primary cause of cancer in developed countries as cases of smoking-related cancer dwindle.[15] Being overweight also increases the risk of oligospermia and azoospermia in men.[16]
Psychological well-being is also at risk in the overweight individual due to social discrimination.
Being overweight has been shown not to increase mortality[qualify evidence] in older people: in a study of 70 to 75-year old Australians, mortality was lowest for "overweight" individuals (BMI 25 to 29.9),[17] while a study of Koreans found that, among those initially aged 65 or more, an increase in BMI to above 25 was not associated with increased risk of death.[18]
Being overweight is generally caused by the intake of more calories (by eating) than are expended by the body (by exercise and everyday activity). Factors that may contribute to this imbalance include:
People who have insulin dependent diabetes and chronically overdose insulin may gain weight, while people who already are overweight may develop insulin tolerance, and in the long run develop type II diabetes.
The usual treatments for overweight individuals is diet and physical exercise.
Dietitians generally recommend eating several balanced meals dispersed through the day, with a combination of progressive, primarily aerobic, physical exercise. In fact, some research found benefits from physical activity, diet and behaviour changes on BMI in children from 12 to 17 years old.[19]
Considering that most of the treatment strategies are directed to change lifestyle-related behaviours of individuals (namely in dietary and physical activity), the transtheoretical model (TTM) has been used as a framework to design weight management interventions. A systematic review assessed the effectiveness of dietary and physical activity interventions based on the TTM in producing sustainable (one year or longer) weight loss in overweight and obese adults. The included studies did not allow to produce conclusive evidence about the impact of the use of this model combined with these interventions on sustainable weight loss. Nevertheless, very low quality scientific evidence suggests that this approach may lead to improvements in physical activity and dietary habits, namely increased in both exercise duration and frequency, and fruits and vegetables consumption, along with reduced dietary fat intake.[20]
The World Health Organization (WHO) estimated that nearly 2 billion adults worldwide, aged 18 years and older, were overweight in 2016.[21]
According to the National Health and Nutrition Examination Survey (NHANES), an estimated 71.6% of the United States' adult population aged 20 and over is considered either overweight or obese, and this percentage has increased over the last four decades.[22]

Endometriosis is a disease of the female reproductive system in which cells similar to those in the endometrium, the layer of tissue that normally covers the inside of the uterus, grow outside the uterus.[7][8] Lesions can be found on ovaries, fallopian tubes, tissue around the uterus and ovaries (peritoneum), intestines, bladder, and diaphragm; it may also occur in other parts of the body.[2] Some symptoms include pelvic pain, heavy and painful periods, pain with bowel movements, painful urination, pain during sexual intercourse and infertility.[1][9] Nearly half of those affected have chronic pelvic pain, while in 70% pain occurs during menstruation.[1] Infertility occurs in up to half of affected individuals.[1] About 25% of individuals have no symptoms and 85% of those seen with infertility in a tertiary center have no pain.[1][10] Endometriosis can have both social and psychological effects.[11]
The cause is not entirely clear.[12] Risk factors include having a family history of the condition.[2]
The areas of endometriosis bleed each month (menstrual period), resulting in inflammation and scarring.[1][2] The growths due to endometriosis are not cancer.[2] Diagnosis is usually based on symptoms in combination with medical imaging;[2] however, biopsy is the surest method of diagnosis.[2] Other causes of similar symptoms include pelvic inflammatory disease, irritable bowel syndrome, interstitial cystitis, and fibromyalgia.[1] Endometriosis is commonly misdiagnosed and women often report being incorrectly told their symptoms are trivial or normal.[11] Women with endometriosis see an average of seven physicians before receiving a correct diagnosis, with an average delay of 6.7 years between the onset of symptoms and surgically obtained biopsies, the gold standard for diagnosing the condition. This average delay places endometriosis at the extreme end of diagnostic inefficiency.[13]
Tentative evidence suggests that the use of combined oral contraceptives reduces the risk of endometriosis.[2][14] Exercise and avoiding large amounts of alcohol may also be preventive.[2] There is no cure for endometriosis, but a number of treatments may improve symptoms.[1] This may include pain medication, hormonal treatments or surgery.[2] The recommended pain medication is usually a non-steroidal anti-inflammatory drug (NSAID), such as naproxen.[2] Taking the active component of the birth control pill continuously or using an intrauterine device with progestogen may also be useful.[2] Gonadotropin-releasing hormone agonist (GnRH agonist) may improve the ability of those who are infertile to get pregnant.[2] Surgical removal of endometriosis may be used to treat those whose symptoms are not manageable with other treatments.[2]
Pain and infertility are common symptoms, although 20–25% of affected women are asymptomatic.[1] Presence of pain symptoms are associated with the type of endometrial lesions as 50% of women with typical lesions, 10% of women with cystic ovarian lesions, and 5% of women with deep endometriosis do not have pain.[15]
A major symptom of endometriosis is recurring pelvic pain. The pain can range from mild to severe cramping or stabbing pain that occurs on both sides of the pelvis, in the lower back and rectal area, and even down the legs. The amount of pain a person feels correlates weakly with the extent or stage (1 through 4) of endometriosis, with some individuals having little or no pain despite having extensive endometriosis or endometriosis with scarring, while others may have severe pain even though they have only a few small areas of endometriosis.[16] The most severe pain is typically associated with menstruation. Pain can also start a week before a menstrual period, during and even a week after a menstrual period, or it can be constant. The pain can be debilitating and result in emotional stress.[17] Symptoms of endometriosis-related pain may include:
Compared with patients with superficial endometriosis, those with deep disease appear to be more likely to report shooting rectal pain and a sense of their insides being pulled down.[21] Individual pain areas and pain intensity appear to be unrelated to the surgical diagnosis, and the area of pain unrelated to the area of endometriosis.[21]
There are multiple causes of pain. Endometriosis lesions react to hormonal stimulation and may "bleed" at the time of menstruation. The blood accumulates locally if it is not cleared shortly by the immune, circulatory, and lymphatic system. This may further lead to swelling, which triggers inflammation with the activation of cytokines, which results in pain. Another source of pain is the organ dislocation that arises from adhesion binding internal organs to each other. The ovaries, the uterus, the oviducts, the peritoneum, and the bladder can be bound together. Pain triggered in this way can last throughout the menstrual cycle, not just during menstrual periods.[22]
Also, endometriotic lesions can develop their own nerve supply, thereby creating a direct and two-way interaction between lesions and the central nervous system, potentially producing a variety of individual differences in pain that can, in some cases, become independent of the disease itself.[16] Nerve fibres and blood vessels are thought to grow into endometriosis lesions by a process known as neuroangiogenesis.[23]
About a third of women with infertility have endometriosis.[1] Among those with endometriosis, about 40% are infertile.[1] The pathogenesis of infertility is dependent on the stage of disease: in early stage disease, it is hypothesised that this is secondary to an inflammatory response that impairs various aspects of conception, whereas in later stage disease distorted pelvic anatomy and adhesions contribute to impaired fertilisation.[24]
Other symptoms include diarrhea or constipation, chronic fatigue, nausea and vomiting, migraines, low-grade fevers, heavy (44%) and/or irregular periods (60%), and hypoglycemia.[18][25][26][19] There is an association between endometriosis and certain types of cancers, notably some types of ovarian cancer,[27] non-Hodgkin's lymphoma and brain cancer.[28] Endometriosis is unrelated to endometrial cancer.[29]
Rarely, endometriosis can cause endometrium-like tissue to be found in other parts of the body. Thoracic endometriosis occurs when endometrium-like tissue implants in the lungs or pleura. Manifestations of this include coughing up blood, a collapsed lung, or bleeding into the pleural space.[12][30] Endometriosis may also involve the nearby colon which in rare situations may progress to partial obstruction requiring emergency surgery.[31]
Stress may be a cause or a consequence of endometriosis.[32]
Complications of endometriosis include internal scarring, adhesions, pelvic cysts, chocolate cysts of ovaries, ruptured cysts, and bowel and ureter obstruction resulting from pelvic adhesions.[33] Endometriosis-associated infertility can be related to scar formation and anatomical distortions due to the endometriosis.[2]
Ovarian endometriosis may complicate pregnancy by decidualization, abscess and/or rupture.[34]
Thoracic endometriosis can be associated with recurrent thoracic endometriosis syndrome at times of a menstrual period that includes catamenial pneumothorax in 73% of women, catamenial hemothorax in 14%, catamenial hemoptysis in 7%, and pulmonary nodules in 6%.[35][36]
A 20-year study of 12,000 women with endometriosis found that individuals under 40 who are diagnosed with endometriosis are three times more likely to have heart problems than their healthy peers.[37]
Endometriosis may increase about 1% or less chance of getting ovarian, breast and thyroid cancers in women compared with those without.[38]
It results in few deaths with unadjusted and age-standardized death rates of 0.1 and 0.0 per 100,000.[5]
Sciatic endometriosis also called catamenial or cyclical sciatica is a sciatica whose cause is endometriosis and whose incidence is unknown. Diagnosis is usually made by an MRI or CT-myelography.[39]

"Endometriosis is associated with an elevated risk of developing depression and anxiety disorders".[40] Studies suggest this is partially due to the pelvic pain experienced by endometriosis patients.  "It has been demonstrated that pelvic pain has significant negative effects on women's mental health and quality of life; in particular, women who suffer from pelvic pain report high levels of anxiety and depression, loss of working ability, limitations in social activities and a poor quality of life" [41]
Endometriosis is a heritable condition that is influenced by both genetic and environmental factors.[42] Children or siblings of women with endometriosis are at higher risk of developing endometriosis themselves; low progesterone levels may be genetic, and may contribute to a hormone imbalance.[43] There is an approximate six-fold increased incidence in individuals with an affected first-degree relative.[44]
It has been proposed that endometriosis results from a series of multiple hits within target genes, in a mechanism similar to the development of cancer.[42] In this case, the initial mutation may be either somatic or heritable.[42]
Individual genomic changes (found by genotyping including genome-wide association studies) associated with endometriosis include 9 chromosome loci robustly replicated in various MNAs reaching genome-wide significance:[45][46][47][48]
There are many findings of altered gene expression and epigenetics, but both of these can also be a secondary result of, for example, environmental factors and altered metabolism. Examples of altered gene expression include that of miRNAs.[42]
Some factors associated with endometriosis include:
Several studies have investigated the potential link between exposure to dioxins and endometriosis, but the evidence is equivocal and potential mechanisms are poorly understood.[52] A 2004 review of studies of dioxin and endometriosis concluded that "the human data supporting the dioxin-endometriosis association are scanty and conflicting",[53] and a 2009 follow-up review also found that there was "insufficient evidence" in support of a link between dioxin exposure and developing endometriosis.[54] A 2008 review concluded that more work was needed, stating that "although preliminary work suggests a potential involvement of exposure to dioxins in the pathogenesis of endometriosis, much work remains to clearly define cause and effect and to understand the potential mechanism of toxicity".[55]
A growing body of evidence has shown a correlation between an imbalance in the vaginal microbiome and the appearance of endometriosis.[56] This correlation is mediated by an immune system overload in the context of retrograde menstruation, in which it fails to detect and kill cells that come outside of the vaginal environment. By disrupting normal immune function, dysbiosis leads to elevated levels of proinflammatory cytokines, a compromised immunosurveillance system and altered immune cell profiles. Indeed, the activation of Toll-like receptors in macrophages leads to a greater activity of this immune cell type. They, in turn, secrete factors (such as the pro-inflammatory cytokine interleukin 8) that help creating an inflammatory environment, ultimately favoring the proliferation and adhesion of endometrial cells. [56][57]
While the exact cause of endometriosis remains unknown, many theories have been presented to better understand and explain its development. These concepts do not necessarily exclude each other. The pathophysiology of endometriosis is likely to be multifactorial and to involve an interplay between several factors.[42]
The main theories for the formation of the ectopic endometrium-like tissue include retrograde menstruation, Müllerianosis, coelomic metaplasia, vascular dissemination of stem cells, and surgical transplantation were postulated as early as 1870. Each is further described below.[12][58][59]
The theory of retrograde menstruation (also called the implantation theory or transplantation theory) is the most commonly accepted theory for the dissemination and transformation of ectopic endometrium into endometriosis. It suggests that during a woman's menstrual flow, some of the endometrial debris flow backward through the Fallopian tubes and into the peritoneal cavity, attaching itself to the peritoneal surface (the lining of the abdominal cavity) where it can proceed to invade the tissue as or transform into endometriosis. It is not clear at what stage the transformation of endometrium, or any cell of origin such as stem cells or coelomic cells (see those theories below), to endometriosis begins.[42][58][60]
Proofs in support of the theory are based on retrospective epidemiological studies that an association with endometrial implants attached to the peritoneal cavity, which would develop into endometrial lesions and retrograde menstruation; and the fact that animals like rodents and non-human primates whose endometrium is not shed during the estrous cycle don't develop naturally endometriosis contrary to animals that have a natural menstrual cycle like rhesus monkeys and baboons.[61]
Retrograde menstruation alone is not able to explain all instances of endometriosis, and additional factors such as genetics, immunology, stem cell migration, and coelomic metaplasia (see "Other theories" on this page) are needed to account for disseminated disease and why many individuals with retrograde menstruation are not diagnosed with endometriosis. In addition, endometriosis has shown up in people who have never experienced menstruation including cisgender men,[62] fetuses,[63] and prepubescent girls.[64][65] Further theoretical additions are needed to complement the retrograde menstruation theory to explain why cases of endometriosis show up in the brain[66] and lungs.[67]
Researchers are investigating the possibility that the immune system may not be able to cope with the cyclic onslaught of retrograde menstrual fluid. In this context there is interest in studying the relationship of endometriosis to autoimmune disease, allergic reactions, and the impact of toxic materials.[68][69] It is still unclear what, if any, causal relationship exists between toxic materials or autoimmune disease and endometriosis. There are immune system changes in people with endometriosis, such as an increase of macrophage-derived secretion products, but it is unknown if these are contributing to the disorder or are reactions from it.[70]
Endometriotic lesions differ in their biochemistry, hormonal response, immunology, inflammatory response when compared to endometrium.[71][12] This is likely because the cells that give rise to endometriosis are a side population of cells.[42] Similarly, there are changes in, for example, the mesothelium of the peritoneum in people with endometriosis, such as loss of tight junctions, but it is unknown if these are causes or effects of the disorder.[70]
In rare cases where imperforate hymen does not resolve itself prior to the first menstrual cycle and goes undetected, blood and endometrium are trapped within the uterus until such time as the problem is resolved by surgical incision. Many health care practitioners never encounter this defect, and due to the flu-like symptoms it is often misdiagnosed or overlooked until multiple menstrual cycles have passed. By the time a correct diagnosis has been made, endometrium and other fluids have filled the uterus and Fallopian tubes with results similar to retrograde menstruation resulting in endometriosis. The initial stage of endometriosis may vary based on the time elapsed between onset and surgical procedure.[citation needed]
The theory of retrograde menstruation as a cause of endometriosis was first proposed by John A. Sampson.[58][72]
Most often, endometriosis is found on the:
Less common pelvic sites are:
Endometriosis may spread to the cervix and vagina or to sites of a surgical abdominal incision, known as "scar endometriosis."[83] Rectovaginal or bowel endometriosis affects approximately 5-12% of those with endometriosis, and can cause severe pain with bowel movements.[84][citation needed]
Deep infiltrating endometriosis (DIE) has been defined as the presence of endometrial glands and stroma infiltrating more than 5 mm in the subperitoneal tissue. The prevalence of DIE is estimated to be 1 to 2% in women of reproductive age. Deep endometriosis typically presents as a single nodule in the vesicouterine fold or in the lower 20 cm of the bowel. Deep endometriosis can be associated with severe pain. However, it can be present without severe levels of pain.[85]
Rarely, endometriosis appears in extrapelvic parts of the body, such as the lungs, brain, and skin.[2][36][83] "Scar endometriosis" can occur in surgical abdominal incisions.[83] Risk factors for scar endometriosis include previous abdominal surgeries, such as a hysterotomy or cesarean section, or ectopic pregnancies, salpingostomy puerperal sterilization, laparoscopy, amniocentesis, appendectomy, episiotomy, vaginal hysterectomies, and hernia repair.[86][87][88]
Endometriosis may also present with skin lesions in cutaneous endometriosis.[83]
Less commonly lesions can be found on the diaphragm or lungs. Diaphragmatic endometriosis is rare, almost always on the right hemidiaphragm, and may inflict the cyclic pain of the right scapula (shoulder) or cervical area (neck) during a menstrual period.[89] Pulmonary endometriosis can be associated with a thoracic endometriosis syndrome that can include catamenial (occurs during menstruation) pneumothorax seen in 73% of women with the syndrome, catamenial hemothorax in 14%, catamenial hemoptysis in 7%, and pulmonary nodules in 6%.[36]
A health history and a physical examination can lead the health care practitioner to suspect endometriosis. There is a clear benefit for performing a transvaginal ultrasound (TVUS) as a first step of testing for endometriosis.[85]
Definitive diagnosis is based on the morphology (form and structure) of the pelvic region, determined by observation (surgical or non-invasive imaging), classified into four different stages of endometriosis. The American Society of Reproductive Medicine's scale, revised in 1996, gives higher scores to deep, thick lesions or intrusions on the ovaries and dense, enveloping adhesions on the ovaries or fallopian tubes.[90] Additionally, histological studies, when performed, should show specific findings.
For many patients, there are significant delays in diagnosis. Studies show an average delay of 11.7 years in the United States. Patients in the UK have an average delay of 8 years and in Norway of 6.7 years.[91] A third of women had consulted their GP six or more times before being diagnosed.[91]
The most common sites of endometriosis are the ovaries, followed by the Douglas pouch, the posterior leaves of the broad ligaments, and the sacrouterine ligaments.[18]
As for deep infiltrating endometriosis, TVUS, TRUS and MRI are the techniques of choice for non-invasive diagnosis with a high sensitivity and specificity.[92]
Laparoscopy, a surgical procedure where a camera is used to look inside the abdominal cavity, is the only way to accurately diagnose the extent and severity of pelvic/abdominal endometriosis.[93] Laparoscopy is not an applicable test for extrapelvic sites such as umbilicus, hernia sacs, abdominal wall, lung, or kidneys.[93]
Reviews in 2019 and 2020 concluded that 1) with advances in imaging, endometriosis diagnosis should no longer be considered synonymous with immediate laparoscopy for diagnosis, and 2) endometriosis should be classified a syndrome that requires confirmation of visible lesions seen at laparoscopy in addition to characteristic symptoms.[94][95]
Laparoscopy permits lesion visualization unless the lesion is visible externally (e.g., an endometriotic nodule in the vagina) or is extra-abdominal.[93] If the growths (lesions) are not visible, a biopsy must be taken to determine the diagnosis.[96] Surgery for diagnoses also allows for surgical treatment of endometriosis at the same time.
During a laparoscopic procedure, lesions can appear dark blue, powder-burn black, red, white, yellow, brown or non-pigmented. Lesions vary in size.[97] Some within the pelvis walls may not be visible, as normal-appearing peritoneum of infertile women reveals endometriosis on biopsy in 6–13% of cases.[98] Early endometriosis typically occurs on the surfaces of organs in the pelvic and intra-abdominal areas.[97] Health care providers may call areas of endometriosis by different names, such as implants, lesions, or nodules. Larger lesions may be seen within the ovaries as endometriomas or "chocolate cysts", "chocolate" because they contain a thick brownish fluid, mostly old blood.[97]
Frequently during diagnostic laparoscopy, no lesions are found in individuals with chronic pelvic pain, a symptom common to other disorders including adenomyosis, pelvic adhesions, pelvic inflammatory disease, congenital anomalies of the reproductive tract, and ovarian or tubal masses.[99]
Vaginal ultrasound can be used to diagnosis endometriosis, or for localizing endometrioma before surgery.[100] This can be used to identify the spread of disease in individuals with well-established clinical suspicion of endometriosis.[100] Vaginal ultrasound is inexpensive, easily accessible, has no contraindications and requires no preparation.[100] By extending the ultrasound assessment into the posterior and anterior pelvic compartments a sonographer is able to evaluate structural mobility and look for deep infiltrating endometriotic nodules.[101] Better sonographic detection of deep infiltrating endometriosis could reduce the number of diagnostic laparoscopies, as well as guide disease management and enhance patient quality of life.[101]

MRI is another means of detecting lesions in a non-invasive manner.[93] MRI is not widely used due to its cost and limited availability, although it can be used to detect the most common form of endometriosis (endometrioma) with a sufficient accuracy.[93] A 2020 article recommended administering an anti-spasmodic agent (i.e. hyoscine butylbromide) and a big glass of water (if the bladder is empty), and scanning in the supine position with an abdominal strap, for better image quality.[102] It also recommended using pelvic-phased array coils and T1 (spin-lattice) weighted scanning, with and without suppression of fat for endometriomas, and sagittal, axial and oblique 2D T2 (spin-spin) weighting for deep infiltrating endometriosis.[102] By surgical observation, endometriosis can be classified as stage I–IV by the 1996 scale of the American Society of Reproductive Medicine.[90] The scale uses a point system that assesses lesions and adhesions in the pelvic organs. It is important to note that staging assesses physical disease only, not the level of pain or infertility.[103] A person with Stage I endometriosis may have a little disease and severe pain, while a person with Stage IV endometriosis may have severe disease and no pain or vice versa. The various stages are summarized by:
Stage I (Minimal)
Stage II (Mild)
Stage III (Moderate)
Stage IV (Severe)
An area of research is the search for endometriosis markers.[104]
In 2010, essentially all proposed biomarkers for endometriosis were of unclear medical use, although some appear to be promising.[104] The one biomarker that has been in use over the last 20 years is CA-125.[104] A 2016 review found that this biomarker was present in those with symptoms of endometriosis; and, once ovarian cancer has been ruled out, a positive CA-125 may confirm the diagnosis.[105] Its performance in ruling out endometriosis is low.[105] CA-125 levels appear to fall during endometriosis treatment, but it has not shown a correlation with disease response.[104]
Another review in 2011 identified several putative biomarkers upon biopsy, including findings of small sensory nerve fibers or defectively expressed β3 integrin subunit.[106] It has been postulated a future diagnostic tool for endometriosis will consist of a panel of several specific and sensitive biomarkers, including both substance concentrations and genetic predisposition.[104]
A 2016 review of endometrial biomarkers for diagnosing endometriosis was unable to draw conclusions due to the low quality of the evidence.[107]
MicroRNAs have the potential to be used in diagnostic and therapeutic decisions.[108]
For a histopathological diagnosis, at least two of the following three criteria should be present:[109]
Immunohistochemistry has been found to be useful in diagnosing endometriosis as stromal cells have a peculiar surface antigen, CD10, thus allowing the pathologist go straight to a staining area and confirm the presence of stromal cells and sometimes glandular tissue is identified that was missed on routine H&E staining.[110]
Endometriosis, abdominal wall
Micrograph showing endometriosis (right) and ovarian stroma (left)
Micrograph of the wall of an endometrioma. All features of endometriosis are present (endometrial glands, endometrial stroma and hemosiderin-laden macrophages).
The most common pain scale for quantification of endometriosis-related pain is the visual analogue scale (VAS); VAS and numerical rating scale (NRS) were the best adapted pain scales for pain measurement in endometriosis. For research purposes, and for more detailed pain measurement in clinical practice, VAS or NRS for each type of typical pain related to endometriosis (dysmenorrhea, deep dyspareunia and non-menstrual chronic pelvic pain), combined with the clinical global impression (CGI) and a quality of life scale, are used.[111]
Limited evidence indicates that the use of combined oral contraceptives is associated with a reduced risk of endometriosis, as is regular exercise and the avoidance of alcohol and caffeine.[2]There is little known information on preventing endometriosis.[112]
While there is no cure for endometriosis, there are two types of interventions; treatment of pain and treatment of endometriosis-associated infertility.[113] In many cases, menopause (natural or surgical) will abate the process.[114] In the reproductive years, endometriosis is merely managed: the goal is to provide pain relief, to restrict progression of the process, and to restore or preserve fertility where needed. In younger individuals, some surgical treatment attempts to remove endometriotic tissue and preserve the ovaries without damaging normal tissue.[12][115]
Pharmacotherapy for pain management can be initiated based on the presence of symptoms and examination and ultrasound findings that rule out other potential causes.[116]
In general, the diagnosis of endometriosis is confirmed during surgery, at which time removal can be performed. Further steps depend on circumstances: someone without infertility can manage symptoms with pain medication and hormonal medication that suppresses the natural cycle, while an infertile individual may be treated expectantly after surgery, with fertility medication, or with in vitro fertilisation (IVF).
A 2020 Cochrane systematic review found that for all types of endometriosis, "it is uncertain whether laparoscopic surgery improves overall pain compared to diagnostic laparoscopy".[117]
Based on strong evidence, experts recommend that surgery be performed laparoscopically (through keyhole surgery) rather than open.[96] Treatment consists of the ablation or excision of the endometriosis, electrocoagulation,[118] lysis of adhesions, resection of endometriomas, and restoration of normal pelvic anatomy as much as is possible.[96][119] When laparoscopic surgery is used, small instruments are inserted through the incisions to remove the endometriosis tissue and adhesions. Because the incisions are very small, there will only be small scars on the skin after the procedure, and most individuals recover from surgery quickly and have a reduced risk of adhesions.[120] Many endometriosis specialists believe that excision is the ideal surgical method to treat endometriosis.[121] A 2017 literature review found excision improved some outcomes over ablation.[122] In the United States, some specialists trained in excision for endometriosis do not accept health insurance, because insurance companies do not reimburse the higher costs of this procedure over ablation.[123]
As for deep endometriosis, a segmental resection or shaving of nodules is effective but is associated with an increased rate of complications, of which about 4.6% are major.[124]
Historically, a hysterectomy (removal of the uterus) was thought to be a cure for endometriosis in individuals who do not wish to conceive. Removal of the uterus may be beneficial as part of the treatment, if the uterus itself is affected by adenomyosis. However, this should only be done in combination with removal of the endometriosis by excision. If endometriosis is not also removed at the time of hysterectomy, pain may persist.[96] A study of hysterectomy patients found those with endometriosis were not using less pain medication 3 years after the procedure.[125]
Presacral neurectomy may be performed where the nerves to the uterus are cut. However, this technique is not usually used due to the high incidence of associated complications including presacral hematoma and irreversible problems with urination and constipation.[96]
The underlying process that causes endometriosis may not cease after a surgical or medical intervention. A study has shown that dysmenorrhea recurs at a rate of 30 percent within a year following laparoscopic surgery. Resurgence of lesions tend to appear in the same location if the lesions were not completely removed during surgery. It has been shown that laser ablation resulted in higher and earlier recurrence rates when compared with endometrioma cystectomy; and recurrence after repetitive laparoscopy was similar to that after the first surgery. Endometriosis has a 10% recurrence rate after hysterectomy and bilateral salpingo-oophorectomy.[126]
Endometriosis recurrence following conservative surgery is estimated as 21.5% at 2 years and 40-50% at 5 years.[127]
Recurrence rate for DIE after surgery is less than 1%.[128]
Risk of developing complications following surgery depend on the type of the lesion that has undergone surgery.[118] 55% to 100% of individuals develop adhesions following pelvic surgery,[129] which can result in infertility, chronic abdominal and pelvic pain, and difficult reoperative surgery. Trehan's temporary ovarian suspension, a technique in which the ovaries are suspended for a week after surgery, may be used to reduce the incidence of adhesions after endometriosis surgery.[130][131] Removal of cysts on the ovary without removing the ovary is a safe procedure.[118]
The overall effectiveness of manual physical therapy to treat endometriosis has not yet been identified.[153]
A 2021 meta-analysis found that GnRH analogues and combined hormonal contraceptives were the best treatment for reducing dyspareunia, menstrual and non menstrual pelvic pain.[154] A 2018 Swedish systematic review found a large number of studies but a general lack of scientific evidence for most treatments.[100] There was only one study of sufficient quality and relevance comparing the effect of surgery and non-surgery.[155] Cohort studies indicate that surgery is effective in decreasing pain.[155] Most complications occurred in cases of low intestinal anastomosis, while risk of fistula occurred in cases of combined abdominal or vaginal surgery, and urinary tract problems were common in intestinal surgery.[155] The evidence was found to be insufficient regarding surgical intervention.[155]
The advantages of physical therapy techniques are decreased cost, absence of major side-effects, it does not interfere with fertility, and near-universal increase of sexual function.[156] Disadvantages are that there are no large or long-term studies of its use for treating pain or infertility related to endometriosis.[156]
Surgery is more effective than medicinal intervention for addressing infertility associated with endometriosis.[115] Surgery attempts to remove endometrium-like tissue[12] and preserve the ovaries without damaging normal tissue.[115] Receiving hormonal suppression therapy after surgery might be positive regarding endometriosis recurrence and pregnancy.[157] In-vitro fertilization (IVF) procedures are effective in improving fertility in many individuals with endometriosis.[1]
During fertility treatment, the ultralong pretreatment with GnRH-agonist has a higher chance of resulting in pregnancy for individuals with endometriosis, compared to the short pretreatment.[100]
Preliminary research on mouse models showed that monoclonal antibodies, as well as inhibitors of MyD88 downstream signaling pathway, can reduce lesion volume. Thanks to that, clinical trials are being done on using a monoclonal antibody directed against IL-33 and using anakinra, an IL-1 receptor antagonist.[152]
Promising preclinical outcomes is pushing clinical trials into testing cannabinoid extracts, dichloroacetic acid and curcuma capsules.[152]
Determining how many people have endometriosis is challenging because definitive diagnosis requires surgical visualization through laparoscopic surgery.[158] Criteria that are commonly used to establish a diagnosis include pelvic pain, infertility, surgical assessment, and in some cases, magnetic resonance imaging. An ultrasound can identify large clumps of tissue as potential endometriosis lesions and ovarian cysts but it is not effective for all patients, especially in cases with smaller, superficial lesions.[159]
One estimate is that 10.8 million people are affected globally as of 2015[update].[5] Other sources estimate 6 to 10% of the general female population[1] and 2 to 11% of asymptomatic women[12] are affected. In addition, 11% of women in a general population have undiagnosed endometriosis that can be seen on magnetic resonance imaging (MRI).[160][158] Endometriosis is most common in those in their thirties and forties; however, it can begin in girls as early as eight years old.[2][3] It results in few deaths with unadjusted and age-standardized death rates of 0.1 and 0.0 per 100,000.[5] Endometriosis was first determined to be a separate condition in the 1920s.[161] Before that time, endometriosis and adenomyosis were considered together.[161] It is unclear who first described the disease.
It chiefly affects adults from premenarche to postmenopause, regardless of race or ethnicity or whether or not they have had children and is estimated to affect over 190 million women in their reproductive years.[162] Incidences of endometriosis have occurred in postmenopausal individuals,[163] and in less common cases, individuals may have had endometriosis symptoms before they even reach menarche.[164][65]
The rate of recurrence of endometriosis is estimated to be 40-50% for adults over a 5-year period.[165] The rate of recurrence has been shown to increase with time from surgery and is not associated with the stage of the disease, initial site, surgical method used, or post-surgical treatment.[165]
Endometriosis was first discovered microscopically by Karl von Rokitansky in 1860,[166] although the earliest antecedents may have stemmed from concepts published almost 4,000 years ago.[167] The Hippocratic Corpus outlines symptoms similar to endometriosis, including uterine ulcers, adhesions, and infertility.[167] Historically, women with these symptoms were treated with leeches, straitjackets, bloodletting, chemical douches, genital mutilation, pregnancy (as a form of treatment), hanging upside down, surgical intervention, and even killing due to suspicion of demonic possession.[167] Hippocratic doctors recognized and treated chronic pelvic pain as a true organic disorder 2,500 years ago, but during the Middle Ages, there was a shift into believing that women with pelvic pain were mad, immoral, imagining the pain, or simply misbehaving.[167] The symptoms of inexplicable chronic pelvic pain were often attributed to imagined madness, female weakness, promiscuity, or hysteria.[167] The historical diagnosis of hysteria, which was thought to be a psychological disease, may have indeed been endometriosis.[167] The idea that chronic pelvic pain was related to mental illness influenced modern attitudes regarding individuals with endometriosis, leading to delays in correct diagnosis and indifference to the patients' true pain throughout the 20th and into the 21st century.[167]
Hippocratic doctors believed that delaying childbearing could trigger diseases of the uterus, which caused endometriosis-like symptoms. Women with dysmenorrhea were encouraged to marry and have children at a young age.[167] The fact that Hippocratics were recommending changes in marriage practices due to an endometriosis-like illness implies that this disease was likely common, with rates higher than the 5-15% prevalence that is often cited today.[167] If indeed this disorder was so common historically, this may point away from modern theories that suggest links between endometriosis and dioxins, PCBs, and chemicals.[167]
The early treatment of endometriosis was surgical and included oophorectomy (removal of the ovaries) and hysterectomy (removal of the uterus).[168] In the 1940s, the only available hormonal therapies for endometriosis were high-dose testosterone and high-dose estrogen therapy.[169] High-dose estrogen therapy with diethylstilbestrol for endometriosis was first reported by Karnaky in 1948 and was the main pharmacological treatment for the condition in the early 1950s.[170][171][172] Pseudopregnancy (high-dose estrogen–progestogen therapy) for endometriosis was first described by Kistner in the late 1950s.[170][171] Pseudopregnancy as well as progestogen monotherapy dominated the treatment of endometriosis in the 1960s and 1970s.[172] These agents, although efficacious, were associated with intolerable side effects. Danazol was first described for endometriosis in 1971 and became the main therapy in the 1970s and 1980s.[170][171][172] In the 1980s GnRH agonists gained prominence for the treatment of endometriosis and by the 1990s had become the most widely used therapy.[171][172] Oral GnRH antagonists such as elagolix were introduced for the treatment of endometriosis in 2018.[173]
A number of public figures have spoken about their experience with endometriosis, including:
The economic burden of endometriosis is widespread and multifaceted.[210] Endometriosis is a chronic disease that has direct and indirect costs which include loss of work days, direct costs of treatment, symptom management, and treatment of other associated conditions such as depression or chronic pain.[210] One factor which seems to be associated with especially high costs is the delay between onset of symptoms and diagnosis.
Costs vary greatly between countries.[211] Two factors that contribute to the economic burden include healthcare costs and losses in productivity. A Swedish study of 400 endometriosis patients found "Absence from work was reported by 32% of the women, while 36% reported reduced time at work because of endometriosis".[212] An additional cross sectional study with Puerto Rican women, "found that endometriosis-related and coexisting symptoms disrupted all aspects of women's daily lives, including physical limitations that affected doing household chores and paid employment. The majority of women (85%) experienced a decrease in the quality of their work; 20% reported being unable to work because of pain, and over two-thirds of the sample continued to work despite their pain."[213]
There are a number of barriers that those affected face to receiving diagnosis and treatment for endometriosis. Some of these include outdated standards for laparoscopic evaluation, stigma about discussing menstruation and sex, lack of understanding of the disease, primary-care physicians' lack of knowledge, and assumptions about typical menstrual pain.[214] On average, those later diagnosed with endometriosis waited 2.3 years after the onset of symptoms before seeking treatment and nearly three quarters of women receive a misdiagnosis prior to endometriosis.[215] Self-help groups say practitioners delay making the diagnosis, often because they do not consider it a possibility. There is a typical delay of 7–12 years from symptom onset in affected individuals to professional diagnosis.[216] There is a general lack of knowledge about endometriosis among primary care physicians. Half of general health care providers surveyed in a 2013 study were unable to name three symptoms of endometriosis.[217] Health care providers are also likely to dismiss described symptoms as normal menstruation.[218] Younger patients may also feel uncomfortable discussing symptoms with a physician.[218]
Race and ethnicity may play a role in how endometriosis affects one's life. Endometriosis is less thoroughly studied among Black people, and the research that has been done is outdated.[219] Black people with endometriosis may face barriers in receiving care due to racist misconceptions about how Black people feel pain.[220] Since pain is the primary symptom of endometriosis, this makes it increasingly possible for doctors to dismiss pain symptoms when their patient is Black.[220]
Cultural differences among ethnic groups also contribute to attitudes toward and treatment of endometriosis, especially in Hispanic or Latino communities. A study done in Puerto Rico in 2020 found that health care and interactions with friends and family related to discussing endometriosis were affected by stigma.[221] The most common finding was a referral to those expressing pain related to endometriosis as "changuería" or "changas", terms used in Puerto Rico to describe pointless whining and complaining, often directed at children.[221]
The existing stigma surrounding women's health, speciffically endometriosis, can lead to patients not seeking diagnosses, lower quality of healthcare, increased barriers to care and treatment, and negative reception from members of society.[222] Additionally, it is crucial to note that menstrual stigma significantly contributes to the broader issue of endometriosis stigma, creating an interconnected challenge that extends beyond reproductive health.[223][224]
This article incorporates text in the public domain as a Swedish government "utterance" by URL§9

Cardiovascular disease (CVD) is any disease involving the heart or blood vessels.[3] CVDs constitute a class of diseases that includes: coronary artery diseases (e.g. angina, heart attack), heart failure, hypertensive heart disease, rheumatic heart disease, cardiomyopathy, arrhythmia, congenital heart disease, valvular heart disease, carditis, aortic aneurysms, peripheral artery disease, thromboembolic disease, and venous thrombosis.[3][4]
The underlying mechanisms vary depending on the disease.[3] It is estimated that dietary risk factors are associated with 53% of CVD deaths.[6] Coronary artery disease, stroke, and peripheral artery disease involve atherosclerosis.[3] This may be caused by high blood pressure, smoking, diabetes mellitus, lack of exercise, obesity, high blood cholesterol, poor diet, excessive alcohol consumption,[3]  and poor sleep,[7][8] among other things. High blood pressure is estimated to account for approximately 13% of CVD deaths, while tobacco accounts for 9%, diabetes 6%, lack of exercise 6%, and obesity 5%.[3] Rheumatic heart disease may follow untreated strep throat.[3]
It is estimated that up to 90% of CVD may be preventable.[9][10] Prevention of CVD involves improving risk factors through: healthy eating, exercise, avoidance of tobacco smoke and limiting alcohol intake.[3] Treating risk factors, such as high blood pressure, blood lipids and diabetes is also beneficial.[3] Treating people who have strep throat with antibiotics can decrease the risk of rheumatic heart disease.[11] The use of aspirin in people who are otherwise healthy is of unclear benefit.[12][13]
Cardiovascular diseases are the leading cause of death worldwide except Africa.[3] Together CVD resulted in 17.9 million deaths (32.1%) in 2015, up from 12.3 million (25.8%) in 1990.[5][4] Deaths, at a given age, from CVD are more common and have been increasing in much of the developing world, while rates have declined in most of the developed world since the 1970s.[14][15] Coronary artery disease and stroke account for 80% of CVD deaths in males and 75% of CVD deaths in females.[3] Most cardiovascular disease affects older adults. In the United States 11% of people between 20 and 40 have CVD, while 37% between 40 and 60, 71% of people between 60 and 80, and 85% of people over 80 have CVD.[2] The average age of death from coronary artery disease in the developed world is around 80, while it is around 68 in the developing world.[14] CVD is typically diagnosed seven to ten years earlier in men than in women.[3]: 48 
There are many cardiovascular diseases involving the blood vessels. They are known as vascular diseases.[citation needed]
There are also many cardiovascular diseases that involve the heart.
There are many risk factors for heart diseases: age, sex, tobacco use, physical inactivity, non-alcoholic fatty liver disease, excessive alcohol consumption, unhealthy diet, obesity, genetic predisposition and family history of cardiovascular disease, raised blood pressure (hypertension), raised blood sugar (diabetes mellitus), raised blood cholesterol (hyperlipidemia), undiagnosed celiac disease, psychosocial factors, poverty and low educational status, air pollution, and poor sleep.[3][17][18][19][20][21] While the individual contribution of each risk factor varies between different communities or ethnic groups the overall contribution of these risk factors is very consistent.[22] Some of these risk factors, such as age, sex or family history/genetic predisposition, are immutable; however, many important cardiovascular risk factors are modifiable by lifestyle change, social change, drug treatment (for example prevention of hypertension, hyperlipidemia, and diabetes).[23] People with obesity are at increased risk of atherosclerosis of the coronary arteries.[24]
Cardiovascular disease in a person's parents increases their risk by ~3 fold,[25] and genetics is an important risk factor for cardiovascular diseases. Genetic cardiovascular disease can occur either as
a consequence of single variant (Mendelian) or polygenic influences.[26] There are more than 40 inherited cardiovascular disease that can be traced to a single disease-causing DNA variant, although these conditions are rare.[26] Most common cardiovascular diseases are non-Mendelian and are thought to be due to hundreds or thousands of genetic variants (known as single nucleotide polymorphisms), each associated with a small effect.[27][28]
Age is the most important risk factor in developing cardiovascular or heart diseases, with approximately a tripling of risk with each decade of life.[29]  Coronary fatty streaks can begin to form in adolescence.[30] It is estimated that 82 percent of people who die of coronary heart disease are 65 and older.[31] Simultaneously, the risk of stroke doubles every decade after age 55.[32]
Multiple explanations are proposed to explain why age increases the risk of cardiovascular/heart diseases. One of them relates to serum cholesterol level.[33] In most populations, the serum total cholesterol level increases as age increases. In men, this increase levels off around age 45 to 50 years. In women, the increase continues sharply until age 60 to 65 years.[33]
Aging is also associated with changes in the mechanical and structural properties of the vascular wall, which leads to the loss of arterial elasticity and reduced arterial compliance and may subsequently lead to coronary artery disease.[34]
Men are at greater risk of heart disease than pre-menopausal women.[29][35] Once past menopause, it has been argued that a woman's risk is similar to a man's[35] although more recent data from the WHO and UN disputes this.[29] If a female has diabetes, she is more likely to develop heart disease than a male with diabetes.[36] Women who have high blood pressure and had complications in their pregnancy have three times the risk of developing cardiovascular disease compared to women with normal blood pressure who had no complications in pregnancy.[37][38]
Coronary heart diseases are 2 to 5 times more common among middle-aged men than women.[33]  In a study done by the World Health Organization, sex contributes to approximately 40% of the variation in sex ratios of coronary heart disease mortality.[39] Another study reports similar results finding that sex differences explains nearly half the risk associated with cardiovascular diseases[33] One of the proposed explanations for sex differences in cardiovascular diseases is hormonal difference.[33]  Among women, estrogen is the predominant sex hormone. Estrogen may have protective effects on glucose metabolism and hemostatic system, and may have direct effect in improving endothelial cell function.[33]  The production of estrogen decreases after menopause, and this may change the female lipid metabolism toward a more atherogenic form by decreasing the HDL cholesterol level while increasing LDL and total cholesterol levels.[33]
Among men and women, there are differences in body weight, height, body fat distribution, heart rate, stroke volume, and arterial compliance.[34] In the very elderly, age-related large artery pulsatility and stiffness are more pronounced among women than men.[34] This may be caused by the women's smaller body size and arterial dimensions which are independent of menopause.[34]
Cigarettes are the major form of smoked tobacco.[3] Risks to health from tobacco use result not only from direct consumption of tobacco, but also from exposure to second-hand smoke.[3] Approximately 10% of cardiovascular disease is attributed to smoking;[3] however, people who quit smoking by age 30 have almost as low a risk of death as never smokers.[40]
Insufficient physical activity (defined as less than 5 x 30 minutes of moderate activity per week, or less than 3 x 20 minutes of vigorous activity per week) is currently the fourth leading risk factor for mortality worldwide.[3]  In 2008, 31.3% of adults aged 15 or older (28.2% men and 34.4% women) were insufficiently physically active.[3]
The risk of ischemic heart disease and diabetes mellitus is reduced by almost a third in adults who participate in 150 minutes of moderate physical activity each week (or equivalent).[41] In addition, physical activity assists weight loss and improves blood glucose control, blood pressure, lipid profile and insulin sensitivity. These effects may, at least in part, explain its cardiovascular benefits.[3]
High dietary intakes of saturated fat, trans-fats and salt, and low intake of fruits, vegetables and fish are linked to cardiovascular risk, although whether all these associations indicate causes is disputed. The World Health Organization attributes approximately 1.7 million deaths worldwide to low fruit and vegetable consumption.[3]  Frequent consumption of high-energy foods, such as processed foods that are high in fats and sugars, promotes obesity and may increase cardiovascular risk.[3]  The amount of dietary salt consumed may also be an important determinant of blood pressure levels and overall cardiovascular risk.[3]  There is moderate quality evidence that reducing saturated fat intake for at least two years reduces the risk of cardiovascular disease.[42] High trans-fat intake has adverse effects on blood lipids and circulating inflammatory markers,[43] and elimination of trans-fat from diets has been widely advocated.[44][45] In 2018 the World Health Organization estimated that trans fats were the cause of more than half a million deaths per year.[45] There is evidence that higher consumption of sugar is associated with higher blood pressure and unfavorable blood lipids,[46] and sugar intake also increases the risk of diabetes mellitus.[47] High consumption of processed meats is associated with an increased risk of cardiovascular disease, possibly in part due to increased dietary salt intake.[19]
The relationship between alcohol consumption and cardiovascular disease is complex, and may depend on the amount of alcohol consumed.[48] There is a direct relationship between high levels of drinking alcohol and cardiovascular disease.[3]  Drinking at low levels without episodes of heavy drinking may be associated with a reduced risk of cardiovascular disease,[49] but there is evidence that associations between moderate alcohol consumption and protection from stroke are non-causal.[50] At the population level, the health risks of drinking alcohol exceed any potential benefits.[3][51]
Untreated celiac disease can cause the development of many types of cardiovascular diseases, most of which improve or resolve with a gluten-free diet and intestinal healing. However, delays in recognition and diagnosis of celiac disease can cause irreversible heart damage.[20]
A lack of good sleep, in amount or quality, is documented as increasing cardiovascular risk in both adults and teens. Recommendations suggest that Infants typically need 12 or more hours of sleep per day, adolescent at least eight or nine hours, and adults seven or eight. About one-third of adult Americans get less than the recommended seven hours of sleep per night, and in a study of teenagers, just 2.2 percent of those studied got enough sleep, many of whom did not get good quality sleep.  Studies have shown that short sleepers getting less than seven hours sleep per night have a 10 percent to 30 percent higher risk of cardiovascular disease.[7][52]
Sleep disorders such as sleep-disordered breathing and insomnia, are also associated with a higher cardiometabolic risk.[53]
An estimated 50 to 70 million Americans have insomnia, sleep apnea or other  chronic sleep disorders.[citation needed]
In addition, sleep research displays differences in race and class. Short sleep and poor sleep tend to be more frequently reported in ethnic minorities than in whites. African-Americans report experiencing short durations of sleep five times more often than whites, possibly as a result of social and environmental factors.  Black children and children living in disadvantaged neighborhoods have much higher rates of sleep apnea.[8]
Cardiovascular disease affects low- and middle-income countries even more than high-income countries.[54] There is relatively little information regarding social patterns of cardiovascular disease within low- and middle-income countries,[54] but within high-income countries low income and low educational status are consistently associated with greater risk of cardiovascular disease.[55] Policies that have resulted in increased socio-economic inequalities have been associated with greater subsequent socio-economic differences in cardiovascular disease[54] implying a cause and effect relationship. Psychosocial factors, environmental exposures, health behaviours, and health-care access and quality contribute to socio-economic differentials in cardiovascular disease.[56] The Commission on Social Determinants of Health recommended that more equal distributions of power, wealth, education, housing, environmental factors, nutrition, and health care were needed to address inequalities in cardiovascular disease and non-communicable diseases.[57]
Particulate matter has been studied for its short- and long-term exposure effects on cardiovascular disease. Currently, airborne particles under 2.5 micrometers in diameter (PM2.5) are the major focus, in which gradients are used to determine CVD risk. Overall, long-term PM exposure increased rate of atherosclerosis and inflammation. In regards to short-term exposure (2 hours), every 25 μg/m3 of PM2.5 resulted in a 48% increase of CVD mortality risk.[58] In addition, after only 5 days of exposure, a rise in systolic (2.8 mmHg) and diastolic (2.7 mmHg) blood pressure occurred for every 10.5 μg/m3 of PM2.5.[58]  Other research has implicated PM2.5 in irregular heart rhythm, reduced heart rate variability (decreased vagal tone), and most notably heart failure.[58][59]  PM2.5 is also linked to carotid artery thickening and increased risk of acute myocardial infarction.[58][59]
Existing cardiovascular disease or a previous cardiovascular event, such as a heart attack or stroke, is the strongest predictor of a future cardiovascular event.[60] Age, sex, smoking, blood pressure, blood lipids and diabetes are important predictors of future cardiovascular disease in people who are not known to have cardiovascular disease.[61] These measures, and sometimes others, may be combined into composite risk scores to estimate an individual's future risk of cardiovascular disease.[60] Numerous risk scores exist although their respective merits are debated.[62] Other diagnostic tests and biomarkers remain under evaluation but currently these lack clear-cut evidence to support their routine use. They include family history, coronary artery calcification score, high sensitivity C-reactive protein (hs-CRP), ankle–brachial pressure index,  lipoprotein subclasses and particle concentration, lipoprotein(a), apolipoproteins A-I and B, fibrinogen, white blood cell count, homocysteine, N-terminal pro B-type natriuretic peptide  (NT-proBNP), and markers of kidney function.[63][64] High blood phosphorus is also linked to an increased risk.[65]
There is evidence that mental health problems, in particular depression and traumatic stress, is linked to cardiovascular diseases. Whereas mental health problems are known to be associated with risk factors for cardiovascular diseases such as smoking, poor diet, and a sedentary lifestyle, these factors alone do not explain the increased risk of cardiovascular diseases seen in depression, stress, and anxiety.[66] Moreover, posttraumatic stress disorder is independently associated with increased risk for incident coronary heart disease, even after adjusting for depression and other covariates.[67]
Little is known about the relationship between work and cardiovascular disease, but links have been established between certain toxins, extreme heat and cold, exposure to tobacco smoke, and mental health concerns such as stress and depression.[68]
A 2015 SBU-report looking at non-chemical factors found an association for those:[69]
Specifically the risk of stroke was also increased by exposure to ionizing radiation.[69] Hypertension develops more often in those who experience job strain and who have shift-work.[69] Differences between women and men in risk are small, however men risk having and dying of heart attacks or stroke twice as often as women during working life.[69]
A 2017 SBU report found evidence that workplace exposure to silica dust, engine exhaust or welding fumes is associated with heart disease.[70] Associations also exist for exposure to arsenic, benzopyrenes, lead, dynamite, carbon disulphide, carbon monoxide, metalworking fluids and occupational exposure to tobacco smoke.[70] Working  with  the  electrolytic production of  aluminium  or  the  production  of  paper  when  the  sulphate pulping  process  is  used  is  associated with heart disease.[70] An association was also found  between  heart  disease  and  exposure  to  compounds  which  are  no  longer  permitted  in certain  work  environments,  such  as  phenoxy acids containing TCDD(dioxin) or asbestos.[70]
Workplace exposure to silica dust or asbestos is also associated with pulmonary heart disease. There is evidence that workplace  exposure to lead, carbon disulphide, phenoxyacids containing  TCDD, as well as working in an environment where aluminum is being electrolytically produced, is associated with stroke.[70]
As of 2017, evidence suggests that certain leukemia-associated mutations in blood cells may also lead to increased risk of cardiovascular disease. Several large-scale research projects looking at human genetic data have found a robust link between the presence of these mutations, a condition known as clonal hematopoiesis, and cardiovascular disease-related incidents and mortality.[71]
Radiation treatments (RT) for cancer can increase the risk of heart disease and death, as observed in breast cancer therapy.[72]  Therapeutic radiation increases the risk of a subsequent heart attack or stroke by 1.5 to 4 times;[73] the increase depends on the dose strength, volume, and location. Use of concomitant chemotherapy, e.g. anthracyclines, is an aggravating risk factor.[74] The occurrence rate of RT induced cardiovascular disease is estimated between 10% and 30%.[74]
Side-effects from radiation therapy for cardiovascular diseases have been termed radiation-induced heart disease or radiation-induced cardiovascular disease.[75][76] Symptoms are dose-dependent and include cardiomyopathy, myocardial fibrosis, valvular heart disease, coronary artery disease, heart arrhythmia and peripheral artery disease. Radiation-induced fibrosis, vascular cell damage and oxidative stress can lead to these and other late side-effect symptoms.[75]
Population-based studies show that atherosclerosis, the major precursor of cardiovascular disease, begins in childhood. The Pathobiological Determinants of Atherosclerosis in Youth (PDAY) study demonstrated that intimal lesions appear in all the aortas and more than half of the right coronary arteries of youths aged 7–9 years.[78]
Obesity and diabetes mellitus are linked to cardiovascular disease,[79] as are a history of chronic kidney disease and hypercholesterolaemia.[80]  In fact, cardiovascular disease is the most life-threatening of the diabetic complications and diabetics are two- to four-fold more likely to die of cardiovascular-related causes than nondiabetics.[81][82][83]
Screening ECGs (either at rest or with exercise) are not recommended in those without symptoms who are at low risk.[84] This includes those who are young without risk factors.[85] In those at higher risk the evidence for screening with ECGs is inconclusive.[86] Additionally echocardiography, myocardial perfusion imaging, and cardiac stress testing is not recommended in those at low risk who do not have symptoms.[87] Some biomarkers may add to conventional cardiovascular risk factors in predicting the risk of future cardiovascular disease; however, the value of some biomarkers is questionable.[88][89] Ankle-brachial index (ABI), high-sensitivity C-reactive protein (hsCRP), and coronary artery calcium, are also of unclear benefit in those without symptoms as of 2018.[90]
The NIH recommends lipid testing in children beginning at the age of 2 if there is a family history of heart disease or lipid problems.[91] It is hoped that early testing will improve lifestyle factors in those at risk such as diet and exercise.[92]
Screening and selection for primary prevention interventions has traditionally been done through absolute risk using a variety of scores (ex. Framingham or Reynolds risk scores).[93] This stratification has separated people who receive the lifestyle interventions (generally lower and intermediate risk) from the medication (higher risk). The number and variety of risk scores available for use has multiplied, but their efficacy according to a 2016 review was unclear due to lack of external validation or impact analysis.[94] Risk stratification models often lack sensitivity for population groups and do not account for the large number of negative events among the intermediate and low risk groups.[93] As a result, future preventative screening appears to shift toward applying prevention according to randomized trial results of each intervention rather than large-scale risk assessment.
Up to 90% of cardiovascular disease may be preventable if established risk factors are avoided.[9][95] Currently practised measures to prevent cardiovascular disease include:
Most guidelines recommend combining preventive strategies. There is some evidence that interventions aiming to reduce more than one cardiovascular risk factor may have beneficial effects on blood pressure, body mass index and waist circumference; however, evidence was limited and the authors were unable to draw firm conclusions on the effects on cardiovascular events and mortality.[127]
There is additional evidence to suggest that providing people with a cardiovascular disease risk score may reduce risk factors by a small amount compared to usual care.[128] However, there was some uncertainty as to whether providing these scores had any effect on cardiovascular disease events. It is unclear whether or not dental care in those with periodontitis affects their risk of cardiovascular disease.[129] According to a 2021 WHO study, working 55+ hours a week raises the risk of stroke by 35% and the risk of dying from heart conditions by 17%, when compared to a 35-40 hours week.[130]
A diet high in fruits and vegetables decreases the risk of cardiovascular disease and death.[131]
A 2021 review found that plant-based diets can provide a risk reduction for CVD if a healthy plant-based diet is consumed. Unhealthy plant-based diets do not provide benefits over diets including meat.[97] A similar meta-analysis and systematic review also looked into dietary patterns and found "that diets lower in animal foods and unhealthy plant foods, and higher in healthy plant foods are beneficial for CVD prevention".[98] A 2018 meta-analysis of observational studies concluded that "In most countries, a vegan diet is associated with a more favourable cardio-metabolic profile compared to an omnivorous diet."[99]
Evidence suggests that the Mediterranean diet may improve cardiovascular outcomes.[132] There is also evidence that a Mediterranean diet may be more effective than a low-fat diet in bringing about long-term changes to cardiovascular risk factors (e.g., lower cholesterol level and blood pressure).[133]
The DASH diet (high in nuts, fish, fruits and vegetables, and low in sweets, red meat and fat) has been shown to reduce blood pressure,[134] lower total and low density lipoprotein cholesterol[135]  and improve metabolic syndrome;[136] but the long-term benefits have been questioned.[137] A high-fiber diet is associated with lower risks of cardiovascular disease.[138]
Worldwide, dietary guidelines recommend a reduction in saturated fat,[139] and although the role of dietary fat in cardiovascular disease is complex and controversial there is a long-standing consensus that replacing saturated fat with unsaturated fat in the diet is sound medical advice.[140] Total fat intake has not been found to be associated with cardiovascular risk.[141][142] A 2020 systematic review found moderate quality evidence that reducing saturated fat intake for at least 2 years caused a reduction in cardiovascular events.[143]  A 2015 meta-analysis of observational studies however did not find a convincing association between saturated fat intake and cardiovascular disease.[144]  Variation in what is used as a substitute for saturated fat may explain some differences in findings.[140] The benefit from replacement with polyunsaturated fats appears greatest,[145]  while replacement of saturated fats with carbohydrates does not appear to have a beneficial effect.[145] A diet high in trans fatty acids is associated with higher rates of cardiovascular disease,[146] and in 2015 the Food and Drug Administration (FDA) determined that there was 'no longer a consensus among qualified experts that partially hydrogenated oils (PHOs), which are the primary dietary source of industrially produced trans fatty acids (IP-TFA), are generally recognized as safe (GRAS) for any use in human food'.[147] There is conflicting evidence concerning whether dietary supplements of omega-3 fatty acids (a type of polyunsaturated fat in oily fish) added to diet improve cardiovascular risk.[148][149]
The benefits of recommending a low-salt diet in people with high or normal blood pressure are not clear.[150] In those with heart failure, after one study was left out, the rest of the trials show a trend to benefit.[151][152] Another review of dietary salt concluded that there is strong evidence that high dietary salt intake increases blood pressure and worsens hypertension, and that it increases the number of cardiovascular disease events; both as a result of the increased blood pressure and probably through other mechanisms.[153][154] Moderate evidence was found that high salt intake increases cardiovascular mortality; and some evidence was found for an increase in overall mortality, strokes, and left ventricular hypertrophy.[153]
Overall, the current body of scientific evidence is uncertain on whether intermittent fasting could prevent cardiovascular disease.[155] Intermittent fasting may help people lose more weight than regular eating patterns, but was not different from energy restriction diets.[155]
Blood pressure medication reduces cardiovascular disease in people at risk,[114] irrespective of age,[156] the baseline level of cardiovascular risk,[157] or baseline blood pressure.[158] The commonly-used drug regimens have similar efficacy in reducing the risk of all major cardiovascular events, although there may be differences between drugs in their ability to prevent specific outcomes.[159] Larger reductions in blood pressure produce larger reductions in risk,[159] and most people with high blood pressure require more than one drug to achieve adequate reduction in blood pressure.[160] Adherence to medications is often poor, and while mobile phone text messaging has been tried to improve adherence, there is insufficient evidence that it alters secondary prevention of cardiovascular disease.[161]
Statins are effective in preventing further cardiovascular disease in people with a history of cardiovascular disease.[162] As the event rate is higher in men than in women, the decrease in events is more easily seen in men than women.[162] In those at risk, but without a history of cardiovascular disease (primary prevention), statins decrease the risk of death and combined fatal and non-fatal cardiovascular disease.[163] The benefit, however, is small.[164] A United States guideline recommends statins in those who have a 12% or greater risk of cardiovascular disease over the next ten years.[165] Niacin, fibrates and CETP Inhibitors, while they may increase HDL cholesterol do not affect the risk of cardiovascular disease in those who are already on statins.[166] Fibrates lower the risk of cardiovascular and coronary events, but there is no evidence to suggest that they reduce all-cause mortality.[167]
Anti-diabetic medication may reduce cardiovascular risk in people with Type 2 diabetes, although evidence is not conclusive.[168] A meta-analysis in 2009 including 27,049 participants and 2,370 major vascular events showed a 15% relative risk reduction in cardiovascular disease with more-intensive glucose lowering over an average follow-up period of 4.4 years, but an increased risk of major hypoglycemia.[169]
Aspirin has been found to be of only modest benefit in those at low risk of heart disease, as the risk of serious bleeding is almost equal to the protection against cardiovascular problems.[170] In those at very low risk, including those over the age of 70, it is not recommended.[171][172] The United States Preventive Services Task Force recommends against use of aspirin for prevention in women less than 55 and men less than 45 years old; however, it is recommended for some older people.[173]
The use of vasoactive agents for people with pulmonary hypertension with left heart disease or hypoxemic lung diseases may cause harm and unnecessary expense.[174]
Antibiotics for secondary prevention of coronary heart disease
Antibiotics may help patients with coronary disease to reduce the risk of heart attacks and strokes.[175] However, evidence in 2021 suggests that antibiotics for secondary prevention of coronary heart disease are harmful, with increased mortality and occurrence of stroke;[175] the use of antibiotics is not supported for preventing secondary coronary heart disease.
Exercise-based cardiac rehabilitation following a heart attack reduces the risk of death from cardiovascular disease and leads to less hospitalizations.[176] There have been few high-quality studies of the benefits of exercise training in people with increased cardiovascular risk but no history of cardiovascular disease.[177]
A systematic review estimated that inactivity is responsible for 6% of the burden of disease from coronary heart disease worldwide.[178] The authors estimated that 121,000 deaths from coronary heart disease could have been averted in Europe in 2008 if people had not been physically inactive. Low-quality evidence from a limited number of studies suggest that yoga has beneficial effects on blood pressure and cholesterol.[179] Tentative evidence suggests that home-based exercise programs may be more efficient at improving exercise adherence.[180]
While a healthy diet is beneficial, the effect of antioxidant supplementation (vitamin E, vitamin C, etc.) or vitamins has not been shown to protect against cardiovascular disease and in some cases may possibly result in harm.[181][182][183][184] Mineral supplements have also not been found to be useful.[185] Niacin, a type of vitamin B3, may be an exception with a modest decrease in the risk of cardiovascular events in those at high risk.[186][187] Magnesium supplementation lowers high blood pressure in a dose-dependent manner.[188] Magnesium therapy is recommended for people with ventricular arrhythmia associated with torsades de pointes who present with long QT syndrome, and for the treatment of people with digoxin intoxication-induced arrhythmias.[189] There is no evidence that omega-3 fatty acid supplementation is beneficial.[190] A 2022 review found that some dietary supplements, including micronutrients, may reduce risk factors for cardiovascular disease.[191]
Cardiovascular disease is treatable with initial treatment primarily focused on diet and lifestyle interventions.[3] Influenza may make heart attacks and strokes more likely and therefore influenza vaccination may decrease the chance of cardiovascular events and death in people with heart disease.[192]
Proper CVD management necessitates a focus on MI and stroke cases due to their combined high mortality rate, keeping in mind the cost-effectiveness of any intervention, especially in developing countries with low or middle-income levels.[93] Regarding MI, strategies using aspirin, atenolol, streptokinase or tissue plasminogen activator have been compared for quality-adjusted life-year (QALY) in regions of low and middle income. The costs for a single QALY for aspirin and atenolol were less than US$25, streptokinase was about $680, and t-PA was $16,000.[193] Aspirin, ACE inhibitors, beta-blockers, and statins used together for secondary CVD prevention in the same regions showed single QALY costs of $350.[193]
There are also surgical or procedural interventions that can save someone's life or prolong it. For heart valve problems, a person could have surgery to replace the valve. For arrhythmias, a pacemaker can be put in place to help reduce abnormal heart rhythms and for a heart attack, there are multiple options two of these are a coronary angioplasty and a coronary artery bypass surgery.[194]
There is probably no additional benefit in terms of mortality and serious adverse events when blood pressure targets were lowered to ≤ 135/85 mmHg from ≤ 140 to 160/90 to 100 mmHg.[195]
Cardiovascular diseases are the leading cause of death worldwide and in all regions except Africa.[3] In 2008, 30% of all global death was attributed to cardiovascular diseases. Death caused by cardiovascular diseases are also higher in low- and middle-income countries as over 80% of all global deaths caused by cardiovascular diseases occurred in those countries. It is also estimated that by 2030, over 23 million people will die from cardiovascular diseases each year.
It is estimated that 60% of the world's cardiovascular disease burden will occur in the South Asian subcontinent despite only accounting for 20% of the world's population. This may be secondary to a combination of genetic predisposition and environmental factors. Organizations such as the Indian Heart Association are working with the World Heart Federation to raise awareness about this issue.[196]
There is evidence that cardiovascular disease existed in pre-history,[197] and research into cardiovascular disease dates from at least the 18th century.[198]  The causes, prevention, and/or treatment of all forms of cardiovascular disease remain active fields of biomedical research, with hundreds of scientific studies being published on a weekly basis.
Recent areas of research include the link between inflammation and atherosclerosis[199]  the potential for novel therapeutic interventions,[200] and the genetics of coronary heart disease.[201]

Dermatitis is inflammation of the skin, typically characterized by itchiness, redness and a rash.[1] In cases of short duration, there may be small blisters, while in long-term cases the skin may become thickened.[1] The area of skin involved can vary from small to covering the entire body.[1][2] Dermatitis is often called eczema, and the difference between those terms is not standardized.
The exact cause of the condition is often unclear.[2] Cases may involve a combination of allergy and poor venous return.[1] The type of dermatitis is generally determined by the person's history and the location of the rash.[1] For example, irritant dermatitis often occurs on the hands of those who frequently get them wet.[1] Allergic contact dermatitis occurs upon exposure to an allergen, causing a hypersensitivity reaction in the skin.[1]
Prevention of atopic dermatitis is typically with essential fatty acids,[4] and may be treated with moisturizers and steroid creams.[5] The steroid creams should generally be of mid-to high strength and used for less than two weeks at a time, as side effects can occur.[7] Antibiotics may be required if there are signs of skin infection.[2] Contact dermatitis is typically treated by avoiding the allergen or irritant.[8][9] Antihistamines may help with sleep and decrease nighttime scratching.[2]
Dermatitis was estimated to affect 245 million people globally in 2015,[6] or 3.34% of the world population. Atopic dermatitis is the most common type and generally starts in childhood.[1][2] In the United States, it affects about 10–30% of people.[2] Contact dermatitis is twice as common in females as in males.[10] Allergic contact dermatitis affects about 7% of people at some point in their lives.[11] Irritant contact dermatitis is common, especially among people with certain occupations; exact rates are unclear.[12]
Many authors use the terms dermatitis and eczema synonymously,[1] and various dictionaries that treat the terms as differentiable nonetheless do not provide explicit criteria for differentiating them, as the aspects of inflammation, pruritus, and either exogenous or endogenous provoking agent all can apply to either term, and thus autoimmune components are not excluded from either.
Others use the term eczema to specifically mean atopic dermatitis.[13][14][15] Atopic dermatitis is also known as atopic eczema.[5] In some languages, dermatitis and eczema mean the same thing, while in other languages dermatitis implies an acute condition and eczema a chronic one.[16]
There are several types of dermatitis including atopic dermatitis, contact dermatitis, stasis dermatitis and seborrhoeic dermatitis.[2] Dermatitis symptoms vary with all different forms of the condition. Although every type of dermatitis has different symptoms, there are certain signs that are common for all of them, including redness of the skin, swelling, itching and skin lesions with sometimes oozing and scarring. Also, the area of the skin on which the symptoms appear tends to be different with every type of dermatitis, whether on the neck, wrist, forearm, thigh or ankle. Although the location may vary, the primary symptom of this condition is itchy skin. More rarely, it may appear on the genital area, such as the vulva or scrotum.[17][18] Symptoms of this type of dermatitis may be very intense and may come and go. Irritant contact dermatitis is usually more painful than itchy.
Although the symptoms of atopic dermatitis vary from person to person, the most common symptoms are dry, itchy, red skin, on light skin. However, this redness does not appear on darker skin and dermatitis can appear darker brown or purple in color.[19] Typical affected skin areas include the folds of the arms, the back of the knees, wrists, face and hands. Perioral dermatitis refers to a red bumpy rash around the mouth.[20]
Dermatitis herpetiformis symptoms include itching, stinging and a burning sensation. Papules and vesicles are commonly present.[21] The small red bumps experienced in this type of dermatitis are usually about 1 cm in size, red in color and may be found symmetrically grouped or distributed on the upper or lower back, buttocks, elbows, knees, neck, shoulders and scalp.
The symptoms of seborrhoeic dermatitis, on the other hand, tend to appear gradually, from dry or greasy scaling of the scalp (dandruff) to scaling of facial areas, sometimes with itching, but without hair loss.[22]  In newborns, the condition causes a thick and yellowish scalp rash, often accompanied by a diaper rash. In severe cases, symptoms may appear along the hairline, behind the ears, on the eyebrows, on the bridge of the nose, around the nose, on the chest, and on the upper back.[23]
Dermatitis
More severe dermatitis
A patch of dermatitis that has been scratched
Complex dermatitis
People with eczema should not receive the smallpox vaccination due to risk of developing eczema vaccinatum, a potentially severe and sometimes fatal complication.[24]
Other major health risks for people with dermatitis are viral and bacterial infections because atopic dermatitis patients have deficiencies in their proteins and lipids that have barrier functions along with defects in dendritic cells and as a result are unable to keep foreign invaders out leading to recurring infections.[25] If left untreated, these infections may be life-threatening, so skin barrier improvement (such as daily moisturizing to minimize transepidermal water loss) and anti-inflammatory therapy are recommended as preventative measures.[25]
The cause of dermatitis is unknown but is presumed to be a combination of genetic and environmental factors.[2]
The hygiene hypothesis postulates that the cause of asthma, eczema, and other allergic diseases is an unusually clean environment in childhood which leads to an insufficient human microbiota. It is supported by epidemiologic studies for asthma.[26] The hypothesis states that exposure to bacteria and other immune system modulators is important during development, and missing out on this exposure increases the risk for asthma and allergy.[27] One systematic review of literature on eczema found that urban areas have an increased prevalence of eczema compared to rural areas.[28]
While it has been suggested that eczema may sometimes be an allergic reaction to the excrement from house dust mites,[29] with up to 5% of people showing antibodies to the mites,[30] the overall role this plays awaits further corroboration.[31]
Essential fatty acid deficiency results in a dermatitis similar to that seen in zinc or biotin deficiency.[4]
A number of genes have been associated with eczema, one of which is filaggrin.[5] Genome-wide studies found three new genetic variants associated with eczema: OVOL1, ACTL9 and IL4-KIF3A.[32]
Eczema occurs about three times more frequently in individuals with celiac disease and about two times more frequently in relatives of those with celiac disease, potentially indicating a genetic link between the conditions.[33][34]
There have been various studies on the prevention of dermatitis through diet, none of which have proven any positive effect. 
Exclusive breastfeeding of infants during at least the first few months may decrease the risk.[35] There is no good evidence that a mother's diet during pregnancy or breastfeeding affects the risk,[35] nor is there evidence that delayed introduction of certain foods is useful.[35] There is tentative evidence that probiotics in infancy may reduce rates but it is insufficient to recommend its use.[36] There is moderate certainty evidence that the use of skin care interventions such as emollients within the first year of life of an infant's life is not effective in preventing eczema.[37] In fact, it may increase the risk of skin infection and of unwanted effects such as allergic reaction to certain moisturizers and a stinging sensation.[37]
There has not been adequate evaluation of changing the diet to reduce eczema.[38][39] There is some evidence that infants with an established egg allergy may have a reduction in symptoms if eggs are eliminated from their diets.[38] Benefits have not been shown for other elimination diets, though the studies are small and poorly executed.[38][39] Establishing that there is a food allergy before dietary change could avoid unnecessary lifestyle changes.[38]
Oils with fatty acids that have been studied to prevent dermatitis include:[40][41]
In the 1950s Arild Hansen showed that in humans: infants fed skimmed milk developed the essential fatty acid deficiency. It was characterized by an increased food intake, poor growth, and a scaly dermatitis, and was cured by the administration of corn oil.
There is no known cure for some types of dermatitis, with treatment aiming to control symptoms by reducing inflammation and relieving itching. Contact dermatitis is treated by avoiding what is causing it.
Seborrheic dermatitis is treated with antifungals such as anti-dandruff shampoo.[42]
Bathing once or more a day is recommended, usually for five to ten minutes in warm water.[5][43] Soaps should be avoided, as they tend to strip the skin of natural oils and lead to excessive dryness.[44] The American Academy of Dermatology suggests using a controlled amount of bleach diluted in a bath to help with atopic dermatitis.[45]
People can wear clothing designed to manage the itching, scratching and peeling.[46]
House dust mite reduction and avoidance measures have been studied in low quality trials and have not shown evidence of improving eczema.[47]
Low-quality evidence indicates that moisturizing agents (emollients) may reduce eczema severity and lead to fewer flares.[48] In children, oil–based formulations appear to be better, and water–based formulations are not recommended.[5] It is unclear if moisturizers that contain ceramides are more or less effective than others.[49] Products that contain dyes, perfumes, or peanuts should not be used.[5] Occlusive dressings at night may be useful.[5]
Some moisturizers or barrier creams may reduce irritation in occupational irritant hand dermatitis,[50] a skin disease that can affect people in jobs that regularly come into contact with water, detergents, chemicals or other irritants.[50] Some emollients may reduce the number of flares in people with dermatitis.[48]
If symptoms are well controlled with moisturizers, steroids may only be required when flares occur.[5] Corticosteroids are effective in controlling and suppressing symptoms in most cases.[51] Once daily use is generally enough.[5] For mild-moderate eczema a weak steroid may be used (e.g., hydrocortisone), while in more severe cases a higher-potency steroid (e.g., clobetasol propionate) may be used. In severe cases, oral or injectable corticosteroids may be used. While these usually bring about rapid improvements, they have greater side effects.
Long term use of topical steroids may result in skin atrophy, stria, telangiectasia.[5] Their use on delicate skin (face or groin) is therefore typically with caution.[5] They are, however, generally well tolerated.[52] Red burning skin, where the skin turns red upon stopping steroid use, has been reported among adults who use topical steroids at least daily for more than a year.[53]
There is little evidence supporting the use of antihistamine medications for the relief of dermatitis.[5][54] Sedative antihistamines, such as diphenhydramine, may be useful in those who are unable to sleep due to eczema.[5] Second generation antihistamines have minimal evidence of benefit.[55] Of the second generation antihistamines studied, fexofenadine is the only one to show evidence of improvement in itching with minimal side effects.[55]
Topical immunosuppressants like pimecrolimus and tacrolimus may be better in the short term and appear equal to steroids after a year of use.[56] Their use is reasonable in those who do not respond to or are not tolerant of steroids.[57][58] Treatments are typically recommended for short or fixed periods of time rather than indefinitely.[5][59] Tacrolimus 0.1% has generally proved more effective than pimecrolimus, and equal in effect to mid-potency topical steroids.[60] There is no association to increased risk of cancer from topical use of pimecrolimus nor tacrolimus.[59][61]
When eczema is severe and does not respond to other forms of treatment, systemic immunosuppressants are sometimes used. Immunosuppressants can cause significant side effects and some require regular blood tests. The most commonly used are ciclosporin, azathioprine, and methotrexate.
Dupilumab is a new medication that improves eczema lesions, especially moderate to severe eczema.[62] Dupilumab, a monoclonal antibody, suppresses inflammation by targeting the interleukin-4 receptor.
Antifungals are used in the treatment of seborrheic dermatitis.[42]
In September 2021, ruxolitinib cream (Opzelura) was approved by the U.S. Food and Drug Administration (FDA) for the topical treatment of mild to moderate atopic dermatitis.[63] It is a topical Janus kinase inhibitor.[63]
Atopic dermatitis (AD) may be treated with narrowband UVB,[64] which increases 25-hydroxyvitamin D3 in persons in individuals with AD.[65]
Light therapy using heliotherapy, balneophototherapy, psoralen plus UVA (PUVA), light has tentative support but the quality of the evidence is not very good compared with narrowband UVB, and UVA1.[66] However, UVB is more effective than UVA1 for treatment of atopical dermatitis.[67]
Overexposure to ultraviolet light carries its own risks, particularly that of skin cancer.[68]
Limited evidence suggests that acupuncture may reduce itching in those affected by atopic dermatitis.[69]
Chiropractic spinal manipulation lacks evidence to support its use for dermatitis.[70] There is little evidence supporting the use of psychological treatments.[71] While dilute bleach baths have been used for infected dermatitis there is little evidence for this practice.[72]
Eczema can be characterized by spongiosis which allows inflammatory mediators to accumulate. Different dendritic cells sub types, such as Langerhans cells, inflammatory dendritic epidermal cells and plasmacytoid dendritic cells have a role to play.[78][79]
Diagnosis of eczema is based mostly on the history and physical examination.[5] In uncertain cases, skin biopsy may be taken for a histopathologic diagnosis of dermatitis.[80] Those with eczema may be especially prone to misdiagnosis of food allergies.[81]
Patch tests are used in the diagnosis of allergic contact dermatitis.[82][83]
The term eczema refers to a set of clinical characteristics. Classification of the underlying diseases has been haphazard with numerous different classification systems, and many synonyms being used to describe the same condition.[84]
A type of dermatitis may be described by location (e.g., hand eczema), by specific appearance (eczema craquele or discoid) or by possible cause (varicose eczema). Further adding to the confusion, many sources use the term eczema interchangeably for the most common type: atopic dermatitis.[27]
The European Academy of Allergology and Clinical Immunology (EAACI) published a position paper in 2001, which simplifies the nomenclature of allergy-related diseases, including atopic and allergic contact eczemas.[85] Non-allergic eczemas are not affected by this proposal.
By histopathology, superficial dermatitis (in the epidermis, papillary dermis, and superficial vascular plexus) can basically be classified into either of the following groups:[86]
Diagnosis of types may be indicated by codes defined according to International Statistical Classification of Diseases and Related Health Problems (ICD).
Atopic dermatitis is an allergic disease believed to have a hereditary component and often runs in families whose members have asthma. Itchy rash is particularly noticeable on the head and scalp, neck, inside of elbows, behind knees, and buttocks. It is very common in developed countries and rising. Irritant contact dermatitis is sometimes misdiagnosed as atopic dermatitis. Stress can cause atopic dermatitis to worsen.[87]
Contact dermatitis is of two types: allergic (resulting from a delayed reaction to an allergen, such as poison ivy, nickel, or Balsam of Peru),[88] and irritant (resulting from direct reaction to a detergent, such as sodium lauryl sulfate, for example).
Some substances act both as allergen and irritants (wet cement, for example). Other substances cause a problem after sunlight exposure, bringing on phototoxic dermatitis. About three-quarters of cases of contact eczema are of the irritant type, which is the most common occupational skin disease. Contact eczema is curable, provided the offending substance can be avoided and its traces removed from one's environment. (ICD-10 L23; L24; L56.1; L56.0)
Seborrhoeic dermatitis or seborrheic dermatitis is a condition sometimes classified as a form of eczema that is closely related to dandruff. It causes dry or greasy peeling of the scalp, eyebrows, and face, and sometimes trunk. In newborns, it causes a thick, yellow, crusty scalp rash called cradle cap, which seems related to lack of biotin and is often curable. (ICD-10 L21; L21.0)
There is a connection between seborrheic dermatitis and Malassezia fungus, and antifungals such as anti-dandruff shampoo can be helpful in treating it.[42]
Dyshidrosis (dyshidrotic eczema, pompholyx, vesicular palmoplantar dermatitis) only occurs on palms, soles, and sides of fingers and toes. Tiny opaque bumps called vesicles, thickening, and cracks are accompanied by itching, which gets worse at night. A common type of hand eczema, it worsens in warm weather. (ICD-10 L30.1)
Discoid eczema (nummular eczema, exudative eczema, microbial eczema) is characterized by round spots of oozing or dry rash, with clear boundaries, often on lower legs. It is usually worse in winter. The cause is unknown, and the condition tends to come and go. (ICD-10 L30.0)
Venous eczema (gravitational eczema, stasis dermatitis, varicose eczema) occurs in people with impaired circulation, varicose veins, and edema, and is particularly common in the ankle area of people over 50. There is redness, scaling, darkening of the skin, and itching. The disorder predisposes to leg ulcers. (ICD-10 I83.1)
Dermatitis herpetiformis (Duhring's disease) causes an intensely itchy and typically symmetrical rash on arms, thighs, knees, and back. It is directly related to celiac disease, can often be put into remission with an appropriate diet, and tends to get worse at night. (ICD-10 L13.0)
Neurodermatitis (lichen simplex chronicus, localized scratch dermatitis) is an itchy area of thickened, pigmented eczema patch that results from habitual rubbing and scratching. Usually, there is only one spot. Often curable through behaviour modification and anti-inflammatory medication. Prurigo nodularis is a related disorder showing multiple lumps. (ICD-10 L28.0; L28.1)
Autoeczematization (id reaction, auto sensitization) is an eczematous reaction to an infection with parasites, fungi, bacteria, or viruses. It is completely curable with the clearance of the original infection that caused it. The appearance varies depending on the cause. It always occurs some distance away from the original infection. (ICD-10 L30.2)
There are eczemas overlaid by viral infections (eczema herpeticum or vaccinatum), and eczemas resulting from underlying disease (e.g., lymphoma).
Eczemas originating from ingestion of medications, foods, and chemicals, have not yet been clearly systematized. Other rare eczematous disorders exist in addition to those listed here.
Most cases are well managed with topical treatments and ultraviolet light.[5] About 2% of cases are not.[5] In more than 60% of young children, the condition subsides by adolescence.[5]
Globally dermatitis affected approximately 230 million people as of 2010 (3.5% of the population).[89] Dermatitis is most commonly seen in infancy, with female predominance of eczema presentations occurring during the reproductive period of 15–49 years.[90] In the UK about 20% of children have the condition, while in the United States about 10% are affected.[5]
Although little data on the rates of eczema over time exists prior to the 1940s, the rate of eczema has been found to have increased substantially in the latter half of the 20th century, with eczema in school-aged children being found to increase between the late 1940s and 2000.[91] In the developed world there has been rise in the rate of eczema over time.  The incidence and lifetime prevalence of eczema in England has been seen to increase in recent times.[5][92]
Dermatitis affected about 10% of U.S. workers in 2010, representing over 15 million workers with dermatitis. Prevalence rates were higher among females than among males and among those with some college education or a college degree compared to those with a high school diploma or less. Workers employed in healthcare and social assistance industries and life, physical, and social science occupations had the highest rates of reported dermatitis. About 6% of dermatitis cases among U.S. workers were attributed to work by a healthcare professional, indicating that the prevalence rate of work-related dermatitis among workers was at least 0.6%.[93]
from Ancient Greek ἔκζεμα ékzema,[94]  from ἐκζέ-ειν ekzé-ein,  from ἐκ ek 'out' + ζέ-ειν zé-ein 'to boil'
(OED)
The term atopic dermatitis was coined in 1933 by Wise and Sulzberger.[95] Sulfur as a topical treatment for eczema was fashionable in the Victorian and Edwardian eras.[73]
The word dermatitis is from the Greek δέρμα derma 'skin' and -ῖτις -itis 'inflammation' and eczema is from Greek: ἔκζεμα ekzema 'eruption'.[96]
Some cosmetics are marketed as hypoallergenic to imply that their use is less likely to lead to an allergic reaction than other products.[97] However, the term hypoallergenic is not regulated,[98] and no research has been done showing that products labeled hypoallergenic are less problematic than any others. In 1977, courts overruled the U.S. Food and Drug Administration's regulation of the use of the term hypoallergenic.[97] In 2019, the European Union released a document about claims made concerning cosmetics,[99] but this was issued as guidance, not a regulation.[100]
Monoclonal antibodies are under preliminary research to determine their potential as treatments for atopic dermatitis, with only dupilumab showing evidence of efficacy, as of 2018.[101][102]
Histamine intolerance, sometimes called histaminosis,[1] is a highly debated and unproven diagnosis, believed to be caused by over-accumulation of dietary histamine in the human body.[2] Histamine intolerance is sometimes informally called an allergy;[2] however, the belief is that this intolerance is technically caused by the gradual accumulation of extracellular histamine due to an imbalance.
According to some literature, roughly 1% of the population has histamine intolerance, however, high quality studies are lacking as no double-blinded oral provocation challenge data exists;[2] of those, 80% are middle-aged.[2]
The imbalance in histamine intolerance is between the synthesis and selective release of histamine from certain granulocytes (i.e., mast cells and basophils), versus the breakdown of histamine by the enzymes which metabolize it, such as diamine oxidase (DAO) and histamine N-methyltransferase (HNMT).[2]
In contrast, allergic reactions involving an immediate allergic response to an allergen are caused by anaphylactic degranulation, which is the abrupt and explosive release of "pre-formed mediators", including histamine, from mast cells and basophils throughout the body.[3]
Possible symptoms after ingestion of histamine-rich food include:[medical citation needed]
In the human body, histamine is metabolized extracellularly by the enzyme diamine oxidase (DAO), and intracellularly by histamine N-methyltransferase (HNMT)[4] and aldehyde oxidases (AOX1).[5][6]  In histamine intolerance, the activity of DAO is limited, and histamine taken up by the diet and formed in the body is only partially metabolized. The consumption of histamine-containing food (e.g., red wine or hard cheese) leads to a pseudoallergic reaction. It is unclear how histamine passes through the intestinal wall during absorption and enters the blood without coming into contact with the aldehyde oxidases expressed in intestinal cells and histamine N-methyltransferases.
The following food categories have been quoted in literature as histamine rich:
Active or passive exposure to tobacco smoke is suspected of favouring histamine intolerance, but has not been adequately studied.[7]
(This list is drawn from the German Wikipedia article on histamine intolerance.[10][circular reference])
For a diagnosis, the case history is essential. However, since many complaints such as headaches, migraines, bronchial asthma, hypotension, arrhythmia and dysmenorrhea (painful periods) may be caused by something other than histamine intolerance, it is not surprising that half of suspected diagnoses are not confirmed.[citation needed]
The diagnosis is usually made by intentionally provoking a reaction. However, since histamine can potentially cause life-threatening conditions, the following procedure is preferred: take blood samples before and after a 14-day diet, and measure changes in histamine and diamine oxidase (DAO) levels.[medical citation needed]
While serum DAO activity determination offers additional diagnostic utility for HIT, supplementing clinical evaluation and assessment, it is cautioned that sole reliance on DAO activity measurements may not sufficiently establish the diagnosis due to the limited correlation between the result serum DAO activity measurement and the condition.[12]
Rather than increase histamine during the test diet, eliminate it. This procedure does not endanger the patient. Quite the contrary: in the presence of histamine intolerance, the symptoms have improved or disappeared completely. At the same time, the histamine blood level halves and the DAO increases significantly. If there is no histamine intolerance, the blood levels do not change and neither do the symptoms. Simultaneously, food allergy, cross-reactions with pollen, fructose malabsorption, lactose intolerance, and celiac disease should be excluded.[citation needed]
The basis of treatment is a reduction of the dietary histamine through a histamine-poor diet. Certain foods (e.g., citrus fruits) and certain medicines (e.g., morphine) which do not contain histamine per se are also to be avoided, because they are known to release histamine stored in the body (histamine liberation).[13]
If eating histamine-containing foods is unavoidable, antihistamines and cromolyn sodium may be effective. The intake of diaminoxidase (DAO) in capsule form with meals may reduce the symptoms of histamine intolerance.[14]
In cases of high blood glutamate, such as can occur in some cases of eczema and histamine intolerance, Reinhart Jarisch recommends vitamin B6 treatment. This promotes the body's own synthesis of DAO and thus fights the effects of histamine intolerance. The reference ranges (normal values) for blood glutamic acid are 20–107 in infants, 18–65 in children and 28-92 μmol / ml in adults.[15]

Irritable bowel syndrome (IBS) is a "disorder of gut-brain interaction" characterized by a group of symptoms that commonly include abdominal pain, abdominal bloating and changes in the consistency of bowel movements.[1] These symptoms may occur over a long time, sometimes for years.[2] IBS can negatively affect quality of life and may result in missed school or work or reduced productivity at work.[9] Disorders such as anxiety, major depression, and chronic fatigue syndrome are common among people with IBS.[1][10][note 1] [11]
The causes of IBS may well be multi-factorial.[2] Theories include combinations of "gut–brain axis" problems, alterations in gut motility, visceral hypersensitivity, infections including small intestinal bacterial overgrowth, neurotransmitters, genetic factors, and food sensitivity.[2] Onset may be triggered by an intestinal infection[12] ("post-infectious irritable bowel syndrome") or a stressful life event.[13]
Diagnosis is based on symptoms in the absence of worrisome features and once other potential conditions have been ruled out.[3] Worrisome or "alarm" features include onset at greater than 50 years of age, weight loss, blood in the stool, or a family history of inflammatory bowel disease.[3] Other conditions that may present similarly include celiac disease, microscopic colitis, inflammatory bowel disease, bile acid malabsorption, and colon cancer.[3]
Treatment of IBS is carried out to improve symptoms and can be very effective. This may include dietary changes, medication, probiotics, and counseling.[5][14] Dietary measures include increasing soluble fiber intake, or a diet low in fermentable oligosaccharides, disaccharides, monosaccharides, and polyols (FODMAPs). The "low FODMAP" diet is meant for short to medium term use and is not intended as a life-long therapy.[3][15][16] The medication loperamide may be used to help with diarrhea while laxatives may be used to help with constipation.[3] There is strong clinical-trial evidence for the use of antidepressants, often in lower doses than that used for depression or anxiety, even in patients without comorbid mood disorder. Tricyclic antidepressants such as amitriptyline or nortriptyline and medications from the selective serotonin reuptake inhibitor (SSRI) group may improve overall symptoms and reduce pain.[3] Patient education and a good doctor–patient relationship are an important part of care.[3][17]
About 10–15% of people in the developed world are believed to be affected by IBS.[1][7] The prevalence varies according to country (from 1.1% to 45.0%) and criteria used to define IBS; however pooling the results of multiple studies gives an estimate of 11.2%.[8] It is more common in South America and less common in Southeast Asia.[3] In the Western world, it is twice as common in women as men and typically occurs before age 45.[1] However, women in East Asia are not more likely than their male counterparts to have IBS, indicating much lower rates among East Asian women.[18] There is likewise evidence that men from South America, South Asia and Africa are just as likely to have IBS as women in those regions, if not more so.[19] The condition appears to become less common with age.[3] IBS does not affect life expectancy or lead to other serious diseases.[6] The first description of the condition was in 1820, while the current term irritable bowel syndrome came into use in 1944.[20]
IBS can be classified as diarrhea-predominant (IBS-D), constipation-predominant (IBS-C), with mixed/alternating stool pattern (IBS-M/IBS-A) or pain-predominant.[21] In some individuals, IBS may have an acute onset and develop after an infectious illness characterized by two or more of: fever, vomiting, diarrhea, or positive stool culture. This post-infective syndrome has consequently been termed "post-infectious IBS" (IBS-PI).[22][23][24][25]
The primary symptoms of IBS are abdominal pain or discomfort in association with frequent diarrhea or constipation and a change in bowel habits.[26] Symptoms usually are experienced as acute attacks that subside within one day, but recurrent attacks are likely.[27] There may also be urgency for bowel movements, a feeling of incomplete evacuation (tenesmus) or bloating.[28] In some cases, the symptoms are relieved by bowel movements.[17] People with IBS, more commonly than others, have gastroesophageal reflux, symptoms relating to the genitourinary system, fibromyalgia, headache, backache, and psychiatric symptoms such as depression and anxiety.[10][28] About a third of adults who have IBS also report sexual dysfunction, typically in the form of a reduction in libido.[29]
While the causes of IBS are still unknown, it is believed that the entire gut–brain axis is affected.[30][31] Recent findings suggest that an allergy triggered peripheral immune mechanism may underlie the symptoms associated with abdominal pain in patients with irritable bowel syndrome.[32] IBS is more prevalent in obese patients.[33]
The risk of developing IBS increases six-fold after acute gastrointestinal infection.[34] Post-infection,[34] further risk factors are young age, prolonged fever, anxiety, and depression.[35] Psychological factors, such as depression or anxiety, have not been shown to cause or influence the onset of IBS, but may play a role in the persistence and perceived severity of symptoms.[36] Nevertheless, they may worsen IBS symptoms and quality of life.[36] Antibiotic use also appears to increase the risk of developing IBS.[37] Research has found that genetic defects in innate immunity and epithelial homeostasis increase the risk of developing both post-infectious as well as other forms of IBS.[38]
Publications suggesting the role of the brain–gut axis appeared in the 1990s[39] and childhood physical and psychological abuse is often associated with the development of IBS.[40] It is believed that psychological stress may trigger IBS in predisposed individuals.[41]
Given the high levels of anxiety experienced by people with IBS and the overlap with conditions such as fibromyalgia and chronic fatigue syndrome, a potential explanation for IBS involves a disruption of the stress system. The stress response in the body involves the hypothalamic–pituitary–adrenal axis (HPA) and the sympathetic nervous system, both of which have been shown to operate abnormally in people with IBS. Psychiatric illness or anxiety precedes IBS symptoms in two-thirds of people with IBS, and psychological traits predispose previously healthy people to developing IBS after gastroenteritis.[42][43]
Approximately 10 percent of IBS cases are triggered by an acute gastroenteritis infection.[44] The CdtB toxin is produced by bacteria causing gastroenteritis and the host may develop an autoimmunity when host antibodies to CdtB cross-react with vinculin.[45] Genetic defects relating to the innate immune system and epithelial barrier as well as high stress and anxiety levels appear to increase the risk of developing post-infectious IBS. Post-infectious IBS usually manifests itself as the diarrhea-predominant subtype. Evidence has demonstrated that the release of high levels of proinflammatory cytokines during acute enteric infection causes increased gut permeability leading to translocation of the commensal bacteria across the epithelial barrier; this in turn can result in significant damage to local tissues, which can develop into chronic gut abnormalities in sensitive individuals. However, increased gut permeability is strongly associated with IBS regardless of whether IBS was initiated by an infection or not.[38] A link between small intestinal bacterial overgrowth and tropical sprue has been proposed to be involved as a cause of post-infectious IBS.[46]
Small intestinal bacterial overgrowth (SIBO) occurs with greater frequency in people who have been diagnosed with IBS compared to healthy controls.[47] SIBO is most common in diarrhea-predominate IBS but also occurs in constipation-predominant IBS more frequently than healthy controls. Symptoms of SIBO include bloating, abdominal pain, diarrhea or constipation among others. IBS may be the result of the immune system interacting abnormally with gut microbiota resulting in an abnormal cytokine signalling profile.[48]
Certain bacteria are found in lower or higher abundance when compared with healthy individuals. Generally Bacteroidota, Bacillota, and Pseudomonadota are increased and Actinomycetota, Bifidobacteria, and Lactobacillus are decreased. Within the human gut, there are common phyla found. The most common is Bacillota. This includes Lactobacillus, which is found to have a decrease in people with IBS, and Streptococcus, which is shown to have an increase in abundance. Within this phylum, species in the class Clostridia are shown to have an increase, specifically Ruminococcus and Dorea. The family Lachnospiraceae presents an increase in IBS-D patients. The second most common phylum is Bacteroidota. In people with IBS, the Bacteroidota phylum has been shown to have an overall decrease, but an increase in the genus Bacteroides. IBS-D shows a decrease for the phylum Actinomycetota and an increase in Pseudomonadota, specifically in the family Enterobacteriaceae.[49]
There is growing evidence that alterations of gut microbiota (dysbiosis) are associated with the intestinal manifestations of IBS, but also with the psychiatric morbidity that coexists in up to 80% of people with IBS.[50] The role of the gut mycobiota, and especially of the abnormal proliferation of the yeast Candida albicans in some people with IBS, was under investigation as of 2005.[51]
Protozoal infections can cause symptoms that mirror specific IBS subtypes,[54] e.g., infection by certain substypes of Blastocystis hominis (blastocystosis).[55][56] Many people regard these organisms as incidental findings, and unrelated to symptoms of IBS.
As of 2017, evidence indicates that blastocystis colonisation occurs more commonly in IBS affected individuals and is a possible risk factor for developing IBS.[57] Dientamoeba fragilis has also been considered a possible organism to study, though it is also found in people without IBS.[58]
Vitamin D deficiency is more common in individuals affected by irritable bowel syndrome.[59][60] Vitamin D is involved in regulating triggers for IBS including the gut microbiome, inflammatory processes and immune responses, as well as psychosocial factors.[61]
SCN5A mutations are found in a small number of people who have IBS, particularly the constipation-predominant variant (IBS-C).[62][63] The resulting defect leads to disruption in bowel function, by affecting the Nav1.5 channel, in smooth muscle of the colon and pacemaker cells.[citation needed]
Genetic, environmental, and psychological factors seem to be important in the development of IBS. Studies have shown that IBS has a genetic component even though there is a predominant influence of environmental factors.[64]
Dysregulated brain-gut axis, abnormal serotonin/5-hydroxytryptamine (5-HT) metabolism, and high density of mucosal nerve fibers in the intestines have been implicated in the mechanisms of IBS. A number of 5-HT receptor subtypes were involved in the IBS symptoms, including 5-HT3, 5-HT4, and 5-HT7 receptors. High levels of 5-HT7 receptor-expressing mucosal nerve fibers were observed in the colon of IBS patients. A role of 5-HT7 receptor in intestinal hyperalgesia was demonstrated in mouse models with visceral hypersensitivity, of which a novel 5-HT7 receptor antagonist administered by mouth reduced intestinal pain levels.[65]
There is evidence that abnormalities occur in the gut flora of individuals who have IBS, such as reduced diversity, a decrease in bacteria belonging to the phylum Bacteroidota, and an increase in those belonging to the phylum Bacillota.[50] The changes in gut flora are most profound in individuals who have diarrhoea-predominant IBS. Antibodies against common components (namely flagellin) of the commensal gut flora are a common occurrence in IBS affected individuals.[66]
Chronic low-grade inflammation commonly occurs in IBS affected individuals with abnormalities found including increased enterochromaffin cells, intraepithelial lymphocytes, and mast cells resulting in chronic immune-mediated inflammation of the gut mucosa.[30][67] IBS has been reported in greater quantities in multigenerational families with IBS than in the regular population.[68] It is believed that psychological stress can induce increased inflammation and thereby cause IBS to develop in predisposed individuals.[41]
No specific laboratory or imaging tests can diagnose irritable bowel syndrome. Diagnosis should be based on symptoms, the exclusion of worrisome features, and the performance of specific investigations to rule out organic diseases that may present similar symptoms.[3][69]
The recommendations for physicians are to minimize the use of medical investigations.[70] The Rome criteria (see below) are usually used. They allow the diagnosis to be based only on symptoms, but no criteria based solely on symptoms is sufficiently accurate to diagnose IBS.[71][72] Worrisome features include onset at greater than 50 years of age, weight loss, blood in the stool, iron-deficiency anemia, or a family history of colon cancer, celiac disease, or inflammatory bowel disease.[3] The criteria for selecting tests and investigations also depends on the level of available medical resources.[36]
The Rome criteria are consensus guidelines, initially released in 1994 and updated periodically since then. These may pertain more closely to clinical trials as in practice, patient symptoms may vary considerably. The Rome IV criteria (2016) for IBS include recurrent abdominal pain, on average, at least one day/week in the last three months, associated with additional stool- or defecation-related criteria.[73]
The algorithm may include additional tests to guard against misdiagnosis of other diseases as IBS.  Such "red flag" symptoms may include weight loss, gastrointestinal bleeding, anemia, or nocturnal symptoms[vague]. However, red flag conditions may not always contribute to accuracy in diagnosis; for instance, as many as 31% of people with IBS have blood in their stool, many possibly from hemorrhoidal bleeding.[74]
The diagnostic algorithm identifies a name that can be applied to the person's condition based on the combination of symptoms of diarrhea, abdominal pain, and constipation. For example, the statement "50% of returning travellers had developed functional diarrhea while 25% had developed IBS" would mean half the travellers had diarrhea while a quarter had diarrhea with abdominal pain. While some researchers believe this categorization system will help physicians understand IBS, others have questioned the value of the system and suggested all people with IBS have the same underlying disease but with different symptoms.[75]
Colon cancer, inflammatory bowel disease, thyroid disorders (hyperthyroidism or hypothyroidism), and giardiasis can all feature abnormal defecation and abdominal pain. Less common causes of this symptom profile are carcinoid syndrome, microscopic colitis, bacterial overgrowth, and eosinophilic gastroenteritis; IBS is, however, a common presentation, and testing for these conditions would yield low numbers of positive results, so it is considered difficult to justify the expense.[76] Conditions that may present similarly include celiac disease, bile acid malabsorption, colon cancer, and dyssynergic defecation.[3]
Ruling out parasitic infections, lactose intolerance, small intestinal bacterial overgrowth, and celiac disease is recommended before a diagnosis of irritable bowel syndrome is made.[69] An upper endoscopy with small bowel biopsies is necessary to identify the presence of celiac disease.[77] An ileocolonoscopy with biopsies is useful to exclude Crohn's disease and ulcerative colitis (Inflammatory bowel disease).[77]
Some people, managed for years for IBS, may have non-celiac gluten sensitivity (NCGS).[4] Gastrointestinal symptoms of IBS are clinically indistinguishable from those of NCGS, but the presence of any of the following non-intestinal manifestations suggest a possible NCGS: headache or migraine, "foggy mind", chronic fatigue,[78] fibromyalgia,[79][80][81] joint and muscle pain,[78][79][82] leg or arm numbness,[78][79][82] tingling of the extremities,[78][82] dermatitis (eczema or skin rash),[78][82] atopic disorders,[78] allergy to one or more inhalants, foods or metals[78][79] (such as mites, graminaceae, parietaria, cat or dog hair/dander, shellfish, or nickel[79]), depression,[78][79][82] anxiety,[79] anemia,[78][82] iron-deficiency anemia, folate deficiency, asthma, rhinitis, eating disorders,[79] neuropsychiatric disorders (such as schizophrenia,[82][83] autism,[79][82][83] peripheral neuropathy,[82][83] ataxia,[83] attention deficit hyperactivity disorder[78]) or autoimmune diseases.[78] An improvement with a gluten-free diet of immune-mediated symptoms, including autoimmune diseases, once having reasonably ruled out celiac disease and wheat allergy, is another way to realize a differential diagnosis.[78]
Investigations are performed to exclude other conditions:[citation needed]
People with IBS are at increased risk of being given inappropriate surgeries such as appendectomy, cholecystectomy, and hysterectomy due to being misdiagnosed as other medical conditions.[84] Some common examples of misdiagnosis include infectious diseases, coeliac disease,[85] Helicobacter pylori,[86][87] parasites (non-protozoal).[54][88][89] The American College of Gastroenterology recommends all people with symptoms of IBS be tested for coeliac disease.[90]
Bile acid malabsorption is also sometimes missed in people with diarrhea-predominant IBS. SeHCAT tests suggest around 30% of people with D-IBS have this condition, and most respond to bile acid sequestrants.[91]
Several medical conditions, or comorbidities, appear with greater frequency in people with IBS.
A number of treatments have been found to be effective, including fiber, talk therapy, antispasmodic and antidepressant medication, and peppermint oil.[105][106][107]
FODMAPs are short-chain carbohydrates that are poorly absorbed in the small intestine. A 2018 systematic review found that although there is evidence of improved IBS symptoms with a low FODMAP diet, the evidence is of very low quality.[108] Symptoms most likely to improve on this type of diet include urgency, flatulence, bloating,[109] abdominal pain, and altered stool output. One national guideline advises a low FODMAP diet for managing IBS when other dietary and lifestyle measures have been unsuccessful.[110] The diet restricts various carbohydrates which are poorly absorbed in the small intestine, as well as fructose and lactose, which are similarly poorly absorbed in those with intolerances to them. Reduction of fructose and fructan has been shown to reduce IBS symptoms in a dose-dependent manner in people with fructose malabsorption and IBS.[111]
FODMAPs are fermentable oligo-, di-, monosaccharides and polyols, which are poorly absorbed in the small intestine and subsequently fermented by the bacteria in the distal small and proximal large intestine. This is a normal phenomenon, common to everyone. The resultant production of gas potentially results in bloating and flatulence.[112] Although FODMAPs can produce certain digestive discomfort in some people, not only do they not cause intestinal inflammation, but they help avoid it, because they produce beneficial alterations in the intestinal flora that contribute to maintaining the good health of the colon.[113][114][115] FODMAPs are not the cause of irritable bowel syndrome nor other functional gastrointestinal disorders, but rather a person develops symptoms when the underlying bowel response is exaggerated or abnormal.[112]
A low-FODMAP diet consists of restricting them from the diet. They are globally trimmed, rather than individually, which is more successful than for example restricting only fructose and fructans, which are also FODMAPs, as is recommended for those with fructose malabsorption.[112]
A low-FODMAP diet might help to improve short-term digestive symptoms in adults with irritable bowel syndrome,[116][110][117][16] but its long-term follow-up can have negative effects because it causes a detrimental impact on the gut microbiota and metabolome.[118][110][16][119] It should only be used for short periods of time and under the advice of a specialist.[120]  A low-FODMAP diet is highly restrictive in various groups of nutrients and can be impractical to follow in the long-term.[121] More studies are needed to assess the true impact of this diet on health.[110][16]
In addition, the use of a low-FODMAP diet without verifying the diagnosis of IBS may result in misdiagnosis of other conditions such as celiac disease.[122] Since the consumption of gluten is suppressed or reduced with a low-FODMAP diet, the improvement of the digestive symptoms with this diet may not be related to the withdrawal of the FODMAPs, but of gluten, indicating the presence of unrecognized celiac disease, avoiding its diagnosis and correct treatment, with the consequent risk of several serious health complications, including various types of cancer.[122][123]
Some evidence suggests soluble fiber supplementation (e.g., psyllium/ispagula husk) is effective.[15] It acts as a bulking agent, and for many people with IBS-D, allows for a more consistent stool. For people with IBS-C, it seems to allow for a softer, moister, more easily passable stool.[citation needed]
However, insoluble fiber (e.g., bran) has not been found to be effective for IBS.[124][125] In some people, insoluble fiber supplementation may aggravate symptoms.[126][127]
Fiber might be beneficial in those who have a predominance of constipation. In people who have IBS-C, soluble fiber can reduce overall symptoms but will not reduce pain. The research supporting dietary fiber contains conflicting small studies complicated by the heterogeneity of types of fiber and doses used.[128]
One meta-analysis found only soluble fiber improved global symptoms of irritable bowel, but neither type of fiber reduced pain.[128]
An updated meta-analysis by the same authors also found soluble fiber reduced symptoms, while insoluble fiber worsened symptoms in some cases.[129]  Positive studies have used 10–30 grams per day of ispaghula (psyllium).[130][131]  One study specifically examined the effect of dose, and found 20 g of ispaghula (psyllium) were better than 10 g and equivalent to 30 g per day.[132]
Recent studies have demonstrated the potential beneficial effects of physical activity on irritable bowel syndrome. Some randomised controlled trials (RCTs) have demonstrated a beneficial effect of physical activity on IBS symptoms. Three RCTs showed a significant improvement in Irritable Bowel Syndrome – Severity Scoring System, while 1 RCT showed a significant improvement only in symptoms of constipation.[133] In light of this, the latest British Society of Gastroenterology guidelines on the management of IBS have stated that all patients with IBS should be advised to take regular exercise (strong recommendation, weak certainty evidence),[134] whereas the American College of Gastroenterology guidelines have suggested with a lower certainty of evidence.[135] Exercise is Medicine recently provided simple practical indications based on world health organization guidelines,[136] which should be followed when physicians prescribing exercise training. As shown by the previous studies, a good Physical activity prescription during the visit could significantly improve patients’ adherence and, consequently, lead to a significant clinical benefit for symptoms of irritable bowel syndrome.[133]
Medications that may be useful include antispasmodics such as dicyclomine and antidepressants.[137] Both H1-antihistamines and mast cell stabilizers have shown efficacy in reducing pain associated with visceral hypersensitivity in IBS.[30]
A number of 5-HT3 antagonists or 5-HT4 agonists were proposed clinically to treat diarrhea-predominant IBS and constipation-predominant IBS, respectively. However, severe side effects have resulted in its withdrawal by food and drug administration and are now prescribed under emergency investigational drug protocol.[138] Other 5-HT receptor subtypes, such as 5-HT7 receptor, have yet to be developed.
For people who do not adequately respond to dietary fiber, osmotic laxatives such as polyethylene glycol, sorbitol, and lactulose can help avoid "cathartic colon" which has been associated with stimulant laxatives.[139] Lubiprostone is a gastrointestinal agent used for the treatment of constipation-predominant IBS.[140]
The use of antispasmodic drugs (e.g., anticholinergics such as hyoscyamine or dicyclomine) may help people who have cramps or diarrhea. A meta-analysis by the Cochrane Collaboration concludes if seven people are treated with antispasmodics, one of them will benefit.[137] Antispasmodics can be divided into two groups: neurotropics and musculotropics.  Musculotropics, such as mebeverine, act directly at the smooth muscle of the gastrointestinal tract, relieving spasm without affecting normal gut motility.[citation needed] Since this action is not mediated by the autonomic nervous system, the usual anticholinergic side effects are absent.[141] The antispasmodic otilonium may also be useful.[142]
Proton-pump inhibitors (PPIs) used to suppress stomach acid production may cause small intestinal bacterial overgrowth (SIBO) leading to IBS symptoms.[143] Discontinuation of PPIs in selected individuals has been recommended as it may lead to an improvement or resolution of IBS symptoms.[144]
Evidence is conflicting about the benefit of antidepressants in IBS. Some meta-analyses have found a benefit, while others have not.[145] There is good evidence that low doses of tricyclic antidepressants (TCAs) can be effective for IBS.[137][146] With TCAs, about one in three people improve.[147]
However, the evidence is less robust for the effectiveness of other antidepressant classes such as selective serotonin reuptake inhibitor antidepressants (SSRIs). Because of their serotonergic effect, SSRIs have been studied in IBS, especially for people who are constipation predominant. As of 2015, the evidence indicates that SSRIs do not help.[148] Antidepressants are not effective for IBS in people with depression, possibly because lower doses of antidepressants than the doses used to treat depression are required for relief of IBS.[149]
Magnesium aluminum silicates and alverine citrate drugs can be effective for IBS.[150][127]
Rifaximin may be useful as a treatment for IBS symptoms, including abdominal bloating and flatulence, although relief of abdominal distension is delayed.[41][151] It is especially useful where small intestinal bacterial overgrowth is involved.[41]
In individuals with IBS and low levels of vitamin D supplementation is recommended. Some evidence suggests that vitamin D supplementation may improve symptoms of IBS, but further research is needed before it can be recommended as a specific treatment for IBS.[59][60]
There is low quality evidence from studies with poor methodological quality that psychological therapies can be effective in the treatment of IBS.[149] Reducing stress may reduce the frequency and severity of IBS symptoms. Techniques that may be helpful include regular exercise, such as swimming, walking, or running.[152]
Vagus nerve stimulation has anti-inflammatory effects and its potential for the treatment of IBS is actively researched.[153][154][155]
A meta-analysis found no benefits of acupuncture relative to placebo for IBS symptom severity or IBS-related quality of life.[156]
Probiotics can be beneficial in the treatment of IBS; taking 10 billion to 100 billion beneficial bacteria per day is recommended for beneficial results. However, further research is needed on individual strains of beneficial bacteria for more refined recommendations.[151][157] Probiotics have positive effects such as enhancing the intestinal mucosal barrier, providing a physical barrier, bacteriocin production (resulting in reduced numbers of pathogenic and gas-producing bacteria), reducing intestinal permeability and bacterial translocation, and regulating the immune system both locally and systemically among other beneficial effects.[84] Probiotics may also have positive effects on the gut–brain axis by their positive effects countering the effects of stress on gut immunity and gut function.[158]
A number of probiotics have been found to be effective, including Lactobacillus plantarum,[84] and Bifidobacteria infantis;[159] but one review found only Bifidobacteria infantis showed efficacy.[160] B. infantis may have effects beyond the gut via it causing a reduction of proinflammatory cytokine activity and elevation of blood tryptophan levels, which may cause an improvement in symptoms of depression.[161] Some yogurt is made using probiotics that may help ease symptoms of IBS.[162] A probiotic yeast called Saccharomyces boulardii has some evidence of effectiveness in the treatment of irritable bowel syndrome.[163]
Certain probiotics have different effects on certain symptoms of IBS. For example, Bifidobacterium breve, B. longum, and Lactobacillus acidophilus have been found to alleviate abdominal pain. B. breve, B. infantis, L. casei, or L. plantarum species alleviated distension symptoms. B. breve, B. infantis, L. casei, L. plantarum, B. longum, L. acidophilus, L. bulgaricus, and Streptococcus salivarius ssp. thermophilus have all been found to affect flatulence levels. Most clinical studies show probiotics do not improve straining, sense of incomplete evacuation, stool consistency, fecal urgency, or stool frequency, although a few clinical studies did find some benefit of probiotic therapy. The evidence is conflicting for whether probiotics improve overall quality of life scores.[164]
Probiotics may exert their beneficial effects on IBS symptoms via preserving the gut microbiota, normalisation of cytokine blood levels, improving the intestinal transit time, decreasing small intestine permeability, and by treating small intestinal bacterial overgrowth of fermenting bacteria.[164] A fecal transplant does not appear useful as of 2019.[165]
Peppermint oil appears useful.[166] In a meta-analysis it was found to be superior to placebo for improvement of IBS symptoms, at least in the short term.[107] An earlier meta-analysis suggested the results of peppermint oil were tentative as the number of people studied was small and blinding of those receiving treatment was unclear.[105] Safety during pregnancy has not been established, however, and caution is required not to chew or break the enteric coating; otherwise, gastroesophageal reflux may occur as a result of lower esophageal sphincter relaxation. Occasionally, nausea and perianal burning occur as side effects.[125] Iberogast, a multi-herbal extract, was found to be superior in efficacy to placebo.[167]  A comprehensive meta-analysis using twelve random trials resulted that the use of peppermint oil is an effective therapy for adults with irritable bowel syndrome.[168]
Research into cannabinoids as treatment for IBS is limited. GI propulsion, secretion, and inflammation in the gut are all modulated by the ECS (Endocannabinoid system), providing a rationale for cannabinoids as treatment candidates for IBS.[169]
Only limited evidence exists for the effectiveness of other herbal remedies for IBS. As with all herbs, it is wise to be aware of possible drug interactions and adverse effects.[125]
The prevalence of IBS varies by country and by age range examined. The bar graph at right shows the percentage of the population reporting symptoms of IBS in studies from various geographic regions (see table below for references). The following table contains a list of studies performed in different countries that measured the prevalence of IBS and IBS-like symptoms:
10.5%[173]
Wilson, 2004
In western countries, women are around two to three times more likely to be diagnosed with IBS and four to five times more likely to seek specialty care for it than men.[178] However, women in East Asian countries are not more likely than men to have irritable bowel syndrome, and there are conflicting reports about the female predominance of the disease in Africa and other parts of Asia.[179] People diagnosed with IBS are usually younger than 45 years old.[1] Studies of females with IBS show symptom severity often fluctuates with the menstrual cycle, suggesting hormonal differences may play a role.[180] Endorsement of gender-related traits has been associated with quality of life and psychological adjustment in IBS.[181] The increase in gastrointestinal symptoms during menses or early menopause may be related to declining or low estrogen and progesterone, suggesting that estrogen withdrawal may play a role in IBS.[182] Gender differences in healthcare-seeking may also play a role.[183] Gender differences in trait anxiety may contribute to lower pain thresholds in women, putting them at greater risk for a number of chronic pain disorders.[184] Finally, sexual trauma is a major risk factor for IBS, as are other forms of abuse.[185] Because women are at higher risk of sexual abuse than men, sex-related risk of abuse may contribute to the higher rate of IBS in women.[186]
The concept of an "irritable bowel" was introduced by P.W. Brown, first in The Journal Of The Kansas Medical Society in 1947[187] and later in the Rocky Mountain Medical Journal in 1950.[188]  The term was used to categorize people who developed symptoms of diarrhea, abdominal pain, and constipation, but where no well-recognized infective cause could be found.  Early theories suggested the irritable bowel was caused by a psychosomatic or mental disorder.[189]
The aggregate cost of irritable bowel syndrome in the United States has been estimated at $1.7–10 billion in direct medical costs, with an additional $20 billion in indirect costs, for a total of $21.7–30 billion.[9] A study by a managed care company comparing medical costs for people with IBS to non-IBS controls identified a 49% annual increase in medical costs associated with a diagnosis of IBS.[190] People with IBS incurred average annual direct costs of $5,049 and $406 in out-of-pocket expenses in 2007.[191] A study of workers with IBS found that they reported a 34.6% loss in productivity, corresponding to 13.8 hours lost per 40 hour week.[192] A study of employer-related health costs from a Fortune 100 company conducted with data from the 1990s found people with IBS incurred US$4527 in claims costs vs. $3276 for controls.[193] A study on Medicaid costs conducted in 2003 by the University of Georgia College of Pharmacy and Novartis found IBS was associated in an increase of $962 in Medicaid costs in California, and $2191 in North Carolina.  People with IBS had higher costs for physician visits, outpatients visits, and prescription drugs.  The study suggested the costs associated with IBS were comparable to those found for people with asthma.[194]
Individuals with IBS have been found to have decreased diversity and numbers of Bacteroidota microbiota. Preliminary research into the effectiveness of fecal microbiota transplant in the treatment of IBS has been very favourable with a 'cure' rate of between 36 percent and 60 percent with remission of core IBS symptoms persisting at 9 and 19 months follow up.[195][196] Treatment with probiotic strains of bacteria has shown to be effective, though not all strains of microorganisms confer the same benefit and adverse side effects have been documented in a minority of cases.[197]
There is increasing evidence for the effectiveness of mesalazine (5-aminosalicylic acid) in the treatment of IBS.[198] Mesalazine is a drug with anti-inflammatory properties that has been reported to significantly reduce immune mediated inflammation in the gut of IBS affected individuals with mesalazine therapy resulting in improved IBS symptoms as well as feelings of general wellness in IBS affected people. It has also been observed that mesalazine therapy helps to normalise the gut flora which is often abnormal in people who have IBS. The therapeutic benefits of mesalazine may be the result of improvements to the epithelial barrier function.[199] Treatment based on "abnormally" high IgG antibodies cannot be recommended.[200]
Differences in visceral sensitivity and intestinal physiology have been noted in IBS. Mucosal barrier reinforcement in response to oral 5-HTP was absent in IBS compared to controls.[201] IBS/IBD individuals are less often HLA DQ2/8 positive than in upper functional gastrointestinal disease and healthy populations.[202]
A similar syndrome is found in rats (Rattus spp.).[203] In rats a short-chain fatty acid receptor is involved.[203] Karaki et al., 2006 finds a free fatty acid receptor 2 subtype –  GPR43  – that is expressed in both enteroendocrine cells and mucosal mast cells.[203] These cells then respond in an exaggerated way to the IBS rat's own large quantity of maldigestion products.[203]

Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels.[11][12] Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects.[13] Classic symptoms include thirst, polyuria, weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves.[3] Untreated or poorly treated diabetes accounts for approximately 1.5 million deaths every year.[11]
The major types of diabetes are type 1 and type 2, though other forms also exist. The most common treatment for type 1  is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that arises during pregnancy in some women, normally resolves shortly after delivery.
As of 2021, an estimated 537 million people had diabetes worldwide accounting for 10.5% of the adult population, with type 2 making up about 90% of all cases. It is estimated that by 2045, approximately 783 million adults, or 1 in 8, will be living with diabetes, representing a 46% increase from the current figures.[14] The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations.[15] Rates are similar in women and men, with diabetes being the seventh leading cause of death globally.[16][17] The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.[18]
The classic symptoms of untreated diabetes are polyuria, thirst, and weight loss.[19] Several other non-specific signs and symptoms may also occur, including fatigue, blurred vision, and genital itchiness due to Candida infection.[19] About half of affected individuals may also be asymptomatic.[19] Type 1 presents abruptly following a pre-clinical phase, while type 2 has a more insidious onset; patients may remain asymptomatic for many years.[20]
Diabetic ketoacidosis is a medical emergency that occurs most commonly in type 1, but may also occur in type 2 if it has been longstanding or if the individual has significant β-cell dysfunction.[21] Excessive production of ketone bodies leads to signs and symptoms including nausea, vomiting, abdominal pain, the smell of acetone in the breath, deep breathing known as Kussmaul breathing, and in severe cases decreased level of consciousness.[21] Hyperosmolar hyperglycemic state is another emergency characterised by dehydration secondary to severe hyperglycaemia, with resultant hypernatremia leading to an altered mental state and possibly coma.[22]
Hypoglycaemia is a recognised complication of insulin treatment used in diabetes.[23] An acute presentation can include mild symptoms such as sweating, trembling, and palpitations, to more serious effects including impaired cognition, confusion, seizures, coma, and rarely death.[23] Recurrent hypoglycaemic episodes may lower the glycaemic threshold at which symptoms occur, meaning mild symptoms may not appear before cognitive deterioration begins to occur.[23]
The major long-term complications of diabetes relate to damage to blood vessels at both macrovascular and microvascular levels.[24][25] Diabetes doubles the risk of cardiovascular disease, and about 75% of deaths in people with diabetes are due to coronary artery disease.[26] Other macrovascular morbidities include stroke and peripheral artery disease.[27]
Microvascular disease affects the eyes, kidneys, and nerves.[24] Damage to the retina, known as diabetic retinopathy, is the most common cause of blindness in people of working age.[19] The eyes can also be affected in other ways, including development of cataract and glaucoma.[19] It is recommended that people with diabetes visit an optometrist or ophthalmologist once a year.[28]
Diabetic nephropathy is a major cause of chronic kidney disease, accounting for over 50% of patients on dialysis in the United States.[29] Diabetic neuropathy, damage to nerves, manifests in various ways, including sensory loss, neuropathic pain, and autonomic dysfunction (such as postural hypotension, diarrhoea, and erectile dysfunction).[19] Loss of pain sensation predisposes to trauma that can lead to diabetic foot problems (such as ulceration), the most common cause of non-traumatic lower-limb amputation.[19]
Based on extensive data and numerous cases of gallstone disease, it appears that a causal link might exist between type 2 diabetes and gallstones. People with diabetes are at a higher risk of developing gallstones compared to those without diabetes.[30]
There is a link between cognitive deficit and diabetes; studies have shown that diabetic individuals are at a greater risk of cognitive decline, and have a greater rate of decline compared to those without the disease.[31] The condition also predisposes to falls in the elderly, especially those treated with insulin.[32]
(age standardized)
Diabetes is classified by the World Health Organization into six categories: type 1 diabetes, type 2 diabetes, hybrid forms of diabetes (including include slowly evolving, immune-mediated diabetes of adults and ketosis-prone type 2 diabetes), hyperglycemia first detected during pregnancy, "other specific types", and "unclassified diabetes".[41] Diabetes is a more variable disease than once thought, and individuals may have a combination of forms.[42]

Type 1 accounts for 5 to 10% of diabetes cases and is the most common type diagnosed in patients under 20 years;[43] however, the older term "juvenile-onset diabetes" is no longer used as the disease not uncommonly has onset in adulthood.[29] The disease is characterized by loss of the insulin-producing beta cells of the pancreatic islets, leading to severe insulin deficiency, and can be further classified as immune-mediated or idiopathic (without known cause).[43] The majority of cases are immune-mediated, in which a T cell-mediated autoimmune attack causes loss of beta cells and thus insulin deficiency.[44] Patients often have irregular and unpredictable blood sugar levels due to very low insulin and an impaired counter-response to hypoglycaemia.[45] Type 1 diabetes is partly inherited, with multiple genes, including certain HLA genotypes, known to influence the risk of diabetes. In genetically susceptible people, the onset of diabetes can be triggered by one or more environmental factors,[46] such as a viral infection or diet. Several viruses have been implicated, but to date there is no stringent evidence to support this hypothesis in humans.[46][47]
Type 1 diabetes can occur at any age, and a significant proportion is diagnosed during adulthood. Latent autoimmune diabetes of adults (LADA) is the diagnostic term applied when type 1 diabetes develops in adults; it has a slower onset than the same condition in children. Given this difference, some use the unofficial term "type 1.5 diabetes" for this condition. Adults with LADA are frequently initially misdiagnosed as having type 2 diabetes, based on age rather than a cause.[48] LADA leaves adults with higher levels of insulin production than type 1 diabetes, but not enough insulin production for healthy blood sugar levels.[49][50]
Type 2 diabetes is characterized by insulin resistance, which may be combined with relatively reduced insulin secretion.[13] The defective responsiveness of body tissues to insulin is believed to involve the insulin receptor. However, the specific defects are not known. Diabetes mellitus cases due to a known defect are classified separately. Type 2 diabetes is the most common type of diabetes mellitus accounting for 95% of diabetes.[2] Many people with type 2 diabetes have evidence of prediabetes (impaired fasting glucose and/or impaired glucose tolerance) before meeting the criteria for type 2 diabetes.[51] The progression of prediabetes to overt type 2 diabetes can be slowed or reversed by lifestyle changes or medications that improve insulin sensitivity or reduce the liver's glucose production.[52]
Type 2 diabetes is primarily due to lifestyle factors and genetics.[53] A number of lifestyle factors are known to be important to the development of type 2 diabetes, including obesity (defined by a body mass index of greater than 30), lack of physical activity, poor diet, stress, and urbanization.[33][54] Excess body fat is associated with 30% of cases in people of Chinese and Japanese descent, 60–80% of cases in those of European and African descent, and 100% of Pima Indians and Pacific Islanders.[13] Even those who are not obese may have a high waist–hip ratio.[13]
Dietary factors such as sugar-sweetened drinks are associated with an increased risk.[55][56] The type of fats in the diet is also important, with saturated fat and trans fats increasing the risk and polyunsaturated and monounsaturated fat decreasing the risk.[53] Eating white rice excessively may increase the risk of diabetes, especially in Chinese and Japanese people.[57] Lack of physical activity may increase the risk of diabetes in some people.[58]
Adverse childhood experiences, including abuse, neglect, and household difficulties, increase the likelihood of type 2 diabetes later in life by 32%, with neglect having the strongest effect.[59]
Antipsychotic medication side effects (specifically metabolic abnormalities, dyslipidemia and weight gain) and unhealthy lifestyles (including poor diet and decreased physical activity), are potential risk factors.[60]
Gestational diabetes resembles type 2 diabetes in several respects, involving a combination of relatively inadequate insulin secretion and responsiveness. It occurs in about 2–10% of all pregnancies and may improve or disappear after delivery.[61] It is recommended that all pregnant women get tested starting around 24–28 weeks gestation.[62] It is most often diagnosed in the second or third trimester because of the increase in insulin-antagonist hormone levels that occurs at this time.[62] However, after pregnancy approximately 5–10% of women with gestational diabetes are found to have another form of diabetes, most commonly type 2.[61] Gestational diabetes is fully treatable, but requires careful medical supervision throughout the pregnancy. Management may include dietary changes, blood glucose monitoring, and in some cases, insulin may be required.[63]
Though it may be transient, untreated gestational diabetes can damage the health of the fetus or mother. Risks to the baby include macrosomia (high birth weight), congenital heart and central nervous system abnormalities, and skeletal muscle malformations. Increased levels of insulin in a fetus's blood may inhibit fetal surfactant production and cause infant respiratory distress syndrome. A high blood bilirubin level may result from red blood cell destruction. In severe cases, perinatal death may occur, most commonly as a result of poor placental perfusion due to vascular impairment. Labor induction may be indicated with decreased placental function. A caesarean section may be performed if there is marked fetal distress[64] or an increased risk of injury associated with macrosomia, such as shoulder dystocia.[65]
Maturity onset diabetes of the young (MODY) is a rare autosomal dominant inherited form of diabetes, due to one of several single-gene mutations causing defects in insulin production.[66] It is significantly less common than the three main types, constituting 1–2% of all cases. The name of this disease refers to early hypotheses as to its nature. Being due to a defective gene, this disease varies in age at presentation and in severity according to the specific gene defect; thus, there are at least 13 subtypes of MODY. People with MODY often can control it without using insulin.[67]
Some cases of diabetes are caused by the body's tissue receptors not responding to insulin (even when insulin levels are normal, which is what separates it from type 2 diabetes); this form is very uncommon. Genetic mutations (autosomal or mitochondrial) can lead to defects in beta cell function. Abnormal insulin action may also have been genetically determined in some cases. Any disease that causes extensive damage to the pancreas may lead to diabetes (for example, chronic pancreatitis and cystic fibrosis). Diseases associated with excessive secretion of insulin-antagonistic hormones can cause diabetes (which is typically resolved once the hormone excess is removed). Many drugs impair insulin secretion and some toxins damage pancreatic beta cells, whereas others increase insulin resistance (especially glucocorticoids which can provoke "steroid diabetes"). The ICD-10 (1992) diagnostic entity, malnutrition-related diabetes mellitus (ICD-10 code E12), was deprecated by the World Health Organization (WHO) when the current taxonomy was introduced in 1999.[68]
Yet another form of diabetes that people may develop is double diabetes. This is when a type 1 diabetic becomes insulin resistant, the hallmark for type 2 diabetes or has a family history for type 2 diabetes.[69] It was first discovered in 1990 or 1991.
The following is a list of disorders that may increase the risk of diabetes:[70]
Insulin is the principal hormone that regulates the uptake of glucose from the blood into most cells of the body, especially liver, adipose tissue and muscle, except smooth muscle, in which insulin acts via the IGF-1.[citation needed] Therefore, deficiency of insulin or the insensitivity of its receptors play a central role in all forms of diabetes mellitus.[72]
The body obtains glucose from three main sources: the intestinal absorption of food; the breakdown of glycogen (glycogenolysis), the storage form of glucose found in the liver; and gluconeogenesis, the generation of glucose from non-carbohydrate substrates in the body.[73] Insulin plays a critical role in regulating glucose levels in the body. Insulin can inhibit the breakdown of glycogen or the process of gluconeogenesis, it can stimulate the transport of glucose into fat and muscle cells, and it can stimulate the storage of glucose in the form of glycogen.[73]
Insulin is released into the blood by beta cells (β-cells), found in the islets of Langerhans in the pancreas, in response to rising levels of blood glucose, typically after eating. Insulin is used by about two-thirds of the body's cells to absorb glucose from the blood for use as fuel, for conversion to other needed molecules, or for storage. Lower glucose levels result in decreased insulin release from the beta cells and in the breakdown of glycogen to glucose. This process is mainly controlled by the hormone glucagon, which acts in the opposite manner to insulin.[74]
If the amount of insulin available is insufficient, or if cells respond poorly to the effects of insulin (insulin resistance), or if the insulin itself is defective, then glucose is not absorbed properly by the body cells that require it, and is not stored appropriately in the liver and muscles. The net effect is persistently high levels of blood glucose, poor protein synthesis, and other metabolic derangements, such as metabolic acidosis in cases of complete insulin deficiency.[73]
When there is too much glucose in the blood for a long time, the kidneys can't absorb it all (reach a threshold of reabsorption) and the extra glucose gets passed out of the body through urine (glycosuria).[75] This increases the osmotic pressure of the urine and inhibits reabsorption of water by the kidney, resulting in increased urine production (polyuria) and increased fluid loss. Lost blood volume is replaced osmotically from water in body cells and other body compartments, causing dehydration and increased thirst (polydipsia).[73] In addition, intracellular glucose deficiency stimulates appetite leading to excessive food intake (polyphagia).[76]
Diabetes mellitus is diagnosed with a test for the glucose content in the blood, and is diagnosed by demonstrating any one of the following:[68]
A positive result, in the absence of unequivocal high blood sugar, should be confirmed by a repeat of any of the above methods on a different day. It is preferable to measure a fasting glucose level because of the ease of measurement and the considerable time commitment of formal glucose tolerance testing, which takes two hours to complete and offers no prognostic advantage over the fasting test.[80] According to the current definition, two fasting glucose measurements at or above 7.0 mmol/L (126 mg/dL) is considered diagnostic for diabetes mellitus.
Per the WHO, people with fasting glucose levels from 6.1 to 6.9 mmol/L (110 to 125 mg/dL) are considered to have impaired fasting glucose.[81] People with plasma glucose at or above 7.8 mmol/L (140 mg/dL), but not over 11.1 mmol/L (200 mg/dL), two hours after a 75 gram oral glucose load are considered to have impaired glucose tolerance. Of these two prediabetic states, the latter in particular is a major risk factor for progression to full-blown diabetes mellitus, as well as cardiovascular disease.[82] The American Diabetes Association (ADA) since 2003 uses a slightly different range for impaired fasting glucose of 5.6 to 6.9 mmol/L (100 to 125 mg/dL).[83]
Glycated hemoglobin is better than fasting glucose for determining risks of cardiovascular disease and death from any cause.[84]
There is no known preventive measure for type 1 diabetes.[2] However, islet autoimmunity and multiple antibodies can be a strong predictor of the onset of type 1 diabetes.[85]  Type 2 diabetes—which accounts for 85–90% of all cases worldwide—can often be prevented or delayed[86] by maintaining a normal body weight, engaging in physical activity, and eating a healthy diet.[2] Higher levels of physical activity (more than 90 minutes per day) reduce the risk of diabetes by 28%.[87] Dietary changes known to be effective in helping to prevent diabetes include maintaining a diet rich in whole grains and fiber, and choosing good fats, such as the polyunsaturated fats found in nuts, vegetable oils, and fish.[88] Limiting sugary beverages and eating less red meat and other sources of saturated fat can also help prevent diabetes.[88] Tobacco smoking is also associated with an increased risk of diabetes and its complications, so smoking cessation can be an important preventive measure as well.[89]
The relationship between type 2 diabetes and the main modifiable risk factors (excess weight, unhealthy diet, physical inactivity and tobacco use) is similar in all regions of the world. There is growing evidence that the underlying determinants of diabetes are a reflection of the major forces driving social, economic and cultural change: globalization, urbanization, population aging, and the general health policy environment.[90]
Diabetes management concentrates on keeping blood sugar levels close to normal, without causing low blood sugar.[91] This can usually be accomplished with dietary changes,[92] exercise, weight loss, and use of appropriate medications (insulin, oral medications).[91]
Learning about the disease and actively participating in the treatment is important, since complications are far less common and less severe in people who have well-managed blood sugar levels.[91][93] The goal of treatment is an A1C level below 7%.[94][95] Attention is also paid to other health problems that may accelerate the negative effects of diabetes. These include smoking, high blood pressure, metabolic syndrome obesity, and lack of regular exercise.[91][96] Specialized footwear is widely used to reduce the risk of diabetic foot ulcers by relieving the pressure on the foot.[97][98][99] Foot examination for patients living with diabetes should be done annually which includes sensation testing, foot biomechanics, vascular integrity and foot structure.[100]
Concerning those with severe mental illness, the efficacy of type 2 diabetes self-management interventions is still poorly explored, with insufficient scientific evidence to show whether these interventions have similar results to those observed in the general population.[101]
People with diabetes can benefit from education about the disease and treatment, dietary changes, and exercise, with the goal of keeping both short-term and long-term blood glucose levels within acceptable bounds. In addition, given the associated higher risks of cardiovascular disease, lifestyle modifications are recommended to control blood pressure.[102][103]
Weight loss can prevent progression from prediabetes to diabetes type 2, decrease the risk of cardiovascular disease, or result in a partial remission in people with diabetes.[104][105] No single dietary pattern is best for all people with diabetes.[106] Healthy dietary patterns, such as the Mediterranean diet, low-carbohydrate diet, or DASH diet, are often recommended, although evidence does not support one over the others.[104][105] According to the ADA, "reducing overall carbohydrate intake for individuals with diabetes has demonstrated the most evidence for improving glycemia", and for individuals with type 2 diabetes who cannot meet the glycemic targets or where reducing anti-glycemic medications is a priority, low or very-low carbohydrate diets are a viable approach.[105] For overweight people with type 2 diabetes, any diet that achieves weight loss is effective.[106][107]
A 2020 Cochrane systematic review compared several non-nutritive sweeteners to sugar, placebo and a nutritive low-calorie sweetener (tagatose), but the results were unclear for effects on HbA1c, body weight and adverse events.[108] The studies included were mainly of very low-certainty and did not report on health-related quality of life, diabetes complications, all-cause mortality or socioeconomic effects.[108]
Most medications used to treat diabetes act by lowering blood sugar levels through different mechanisms. There is broad consensus that when people with diabetes maintain tight glucose control – keeping the glucose levels in their blood within normal ranges – they experience fewer complications, such as kidney problems or eye problems.[109][110] There is however debate as to whether this is appropriate and cost effective for people later in life in whom the risk of hypoglycemia may be more significant.[111]
There are a number of different classes of anti-diabetic medications. Type 1 diabetes requires treatment with insulin, ideally using a "basal bolus" regimen that most closely matches normal insulin release: long-acting insulin for the basal rate and short-acting insulin with meals.[112] Type 2 diabetes is generally treated with medication that is taken by mouth (e.g. metformin) although some eventually require injectable treatment with insulin or GLP-1 agonists.[113]
Metformin is generally recommended as a first-line treatment for type 2 diabetes, as there is good evidence that it decreases mortality.[8] It works by decreasing the liver's production of glucose, and increasing the amount of glucose stored in peripheral tissue.[114] Several other groups of drugs, mainly oral medication, may also decrease blood sugar in type 2 diabetes. These include agents that increase insulin release (sulfonylureas), agents that decrease absorption of sugar from the intestines (acarbose), agents that inhibit the enzyme dipeptidyl peptidase-4 (DPP-4) that inactivates incretins such as GLP-1 and GIP (sitagliptin), agents that make the body more sensitive to insulin (thiazolidinedione) and agents that increase the excretion of glucose in the urine (SGLT2 inhibitors).[114] When insulin is used in type 2 diabetes, a long-acting formulation is usually added initially, while continuing oral medications.[8]
Some severe cases of type 2 diabetes may also be treated with insulin, which is increased gradually until glucose targets are reached.[8][115]
Cardiovascular disease is a serious complication associated with diabetes, and many international guidelines recommend blood pressure treatment targets that are lower than 140/90 mmHg for people with diabetes.[116] However, there is only limited evidence regarding what the lower targets should be. A 2016 systematic review found potential harm to treating to targets lower than 140 mmHg,[117] and a subsequent systematic review in 2019 found no evidence of additional benefit from blood pressure lowering to between 130 – 140mmHg, although there was an increased risk of adverse events.[118]
2015 American Diabetes Association recommendations are that people with diabetes and albuminuria should receive an inhibitor of the renin-angiotensin system to reduce the risks of progression to end-stage renal disease, cardiovascular events, and death.[119] There is some evidence that angiotensin converting enzyme inhibitors (ACEIs) are superior to other inhibitors of the renin-angiotensin system such as angiotensin receptor blockers (ARBs),[120] or aliskiren in preventing cardiovascular disease.[121] Although a more recent review found similar effects of ACEIs and ARBs on major cardiovascular and renal outcomes.[122] There is no evidence that combining ACEIs and ARBs provides additional benefits.[122]
The use of aspirin to prevent cardiovascular disease in diabetes is controversial.[119] Aspirin is recommended by some in people at high risk of cardiovascular disease, however routine use of aspirin has not been found to improve outcomes in uncomplicated diabetes.[123] 2015 American Diabetes Association recommendations for aspirin use (based on expert consensus or clinical experience) are that low-dose aspirin use is reasonable in adults with diabetes who are at intermediate risk of cardiovascular disease (10-year cardiovascular disease risk, 5–10%).[119] National guidelines for England and Wales by the National Institute for Health and Care Excellence (NICE) recommend against the use of aspirin in people with type 1 or type 2 diabetes who do not have confirmed cardiovascular disease.[112][113]
Weight loss surgery in those with obesity and type 2 diabetes is often an effective measure.[124] Many are able to maintain normal blood sugar levels with little or no medications following surgery[125] and long-term mortality is decreased.[126] There is, however, a short-term mortality risk of less than 1% from the surgery.[127] The body mass index cutoffs for when surgery is appropriate are not yet clear.[126] It is recommended that this option be considered in those who are unable to get both their weight and blood sugar under control.[128]
A pancreas transplant is occasionally considered for people with type 1 diabetes who have severe complications of their disease, including end stage kidney disease requiring kidney transplantation.[129]
In countries using a general practitioner system, such as the United Kingdom, care may take place mainly outside hospitals, with hospital-based specialist care used only in case of complications, difficult blood sugar control, or research projects. In other circumstances, general practitioners and specialists share care in a team approach. Home telehealth support can be an effective management technique.[130]
The use of technology to deliver educational programs for adults with type 2 diabetes includes computer-based self-management interventions to collect for tailored responses to facilitate self-management.[131] There is no adequate evidence to support effects on cholesterol, blood pressure, behavioral change (such as physical activity levels and dietary), depression, weight and health-related quality of life, nor in other biological, cognitive or emotional outcomes.[131][132]
In 2017, 425 million people had diabetes worldwide,[133] up from an estimated 382 million people in 2013[134] and from 108 million in 1980.[135] Accounting for the shifting age structure of the global population, the prevalence of diabetes is 8.8% among adults, nearly double the rate of 4.7% in 1980.[133][135] Type 2 makes up about 90% of the cases.[16][33] Some data indicate rates are roughly equal in women and men,[16] but male excess in diabetes has been found in many populations with higher type 2 incidence, possibly due to sex-related differences in insulin sensitivity, consequences of obesity and regional body fat deposition, and other contributing factors such as high blood pressure, tobacco smoking, and alcohol intake.[136][137]
The WHO estimates that diabetes resulted in 1.5 million deaths in 2012, making it the 8th leading cause of death.[138][135] However another 2.2 million deaths worldwide were attributable to high blood glucose and the increased risks of cardiovascular disease and other associated complications (e.g. kidney failure), which often lead to premature death and are often listed as the underlying cause on death certificates rather than diabetes.[135][139] For example, in 2017, the International Diabetes Federation (IDF) estimated that diabetes resulted in 4.0 million deaths worldwide,[133] using modeling to estimate the total number of deaths that could be directly or indirectly attributed to diabetes.[133]
Diabetes occurs throughout the world but is more common (especially type 2) in more developed countries. The greatest increase in rates has however been seen in low- and middle-income countries,[135] where more than 80% of diabetic deaths occur.[140] The fastest prevalence increase is expected to occur in Asia and Africa, where most people with diabetes will probably live in 2030.[141] The increase in rates in developing countries follows the trend of urbanization and lifestyle changes, including increasingly sedentary lifestyles, less physically demanding work and the global nutrition transition, marked by increased intake of foods that are high energy-dense but nutrient-poor (often high in sugar and saturated fats, sometimes referred to as the "Western-style" diet).[135][141] The global number of diabetes cases might increase by 48% between 2017 and 2045.[133]
As of 2020, 38% of all US adults had prediabetes.[142] Prediabetes is an early stage of diabetes.
Diabetes was one of the first diseases described,[143] with an Egyptian manuscript from c. 1500 BCE mentioning "too great emptying of the urine."[144] The Ebers papyrus includes a recommendation for a drink to take in such cases.[145] The first described cases are believed to have been type 1 diabetes.[144] Indian physicians around the same time identified the disease and classified it as madhumeha or "honey urine", noting the urine would attract ants.[144][145]
The term "diabetes" or "to pass through" was first used in 230 BCE by the Greek Apollonius of Memphis.[144] The disease was considered rare during the time of the Roman empire, with Galen commenting he had only seen two cases during his career.[144] This is possibly due to the diet and lifestyle of the ancients, or because the clinical symptoms were observed during the advanced stage of the disease. Galen named the disease "diarrhea of the urine" (diarrhea urinosa).[146]
The earliest surviving work with a detailed reference to diabetes is that of Aretaeus of Cappadocia (2nd or early 3rd century CE). He described the symptoms and the course of the disease, which he attributed to the moisture and coldness, reflecting the beliefs of the "Pneumatic School". He hypothesized a correlation between diabetes and other diseases, and he discussed differential diagnosis from the snakebite, which also provokes excessive thirst. His work remained unknown in the West until 1552, when the first Latin edition was published in Venice.[146]
Two types of diabetes were identified as separate conditions for the first time by the Indian physicians Sushruta and Charaka in 400–500 CE with one type being associated with youth and another type with being overweight.[144] Effective treatment was not developed until the early part of the 20th century when Canadians Frederick Banting and Charles Best isolated and purified insulin in 1921 and 1922.[144] This was followed by the development of the long-acting insulin NPH in the 1940s.[144]
The word diabetes (/ˌdaɪ.əˈbiːtiːz/ or /ˌdaɪ.əˈbiːtɪs/) comes from Latin diabētēs, which in turn comes from Ancient Greek διαβήτης (diabētēs), which literally means "a passer through; a siphon".[147] Ancient Greek physician Aretaeus of Cappadocia (fl. 1st century CE) used that word, with the intended meaning "excessive discharge of urine", as the name for the disease.[148][149] Ultimately, the word comes from Greek διαβαίνειν (diabainein), meaning "to pass through",[147] which is composed of δια- (dia-), meaning "through" and βαίνειν (bainein), meaning "to go".[148] The word "diabetes" is first recorded in English, in the form diabete, in a medical text written around 1425.
The word mellitus (/məˈlaɪtəs/ or /ˈmɛlɪtəs/) comes from the classical Latin word mellītus, meaning "mellite"[150] (i.e. sweetened with honey;[150] honey-sweet[151]). The Latin word comes from mell-, which comes from mel, meaning "honey";[150][151] sweetness;[151] pleasant thing,[151] and the suffix -ītus,[150] whose meaning is the same as that of the English suffix "-ite".[152] It was Thomas Willis who in 1675 added "mellitus" to the word "diabetes" as a designation for the disease, when he noticed the urine of a person with diabetes had a sweet taste (glycosuria). This sweet taste had been noticed in urine by the ancient Greeks, Chinese, Egyptians, Indians, and Persians[citation needed].
The 1989 "St. Vincent Declaration"[153][154] was the result of international efforts to improve the care accorded to those with diabetes. Doing so is important not only in terms of quality of life and life expectancy but also economically – expenses due to diabetes have been shown to be a major drain on health – and productivity-related resources for healthcare systems and governments.
Several countries established more and less successful national diabetes programmes to improve treatment of the disease.[155]
Diabetes stigma describes the negative attitudes, judgment, discrimination, or prejudice against people with diabetes. Often, the stigma stems from the idea that diabetes (particularly Type 2 diabetes) resulted from poor lifestyle and unhealthy food choices rather than other causal factors like genetics and social determinants of health.[156] Manifestation of stigma can be seen throughout different cultures and contexts. Scenarios include diabetes statuses affecting marriage proposals, workplace-employment, and socials standings in communities.[157]
Stigma is also seen internally, as people with diabetes can also have negative beliefs about themselves. Often these cases of self-stigma are associated with higher diabetes-specific distress, lower self-efficacy, and poorer provider-patient interactions during diabetes care.[158]
Racial and ethnic minorities are disproportionately affected with higher prevalence of diabetes compared to non-minority individuals.[159] While US adults overall have a 40% chance of developing type 2 diabetes, Hispanic/Latino adults chance is more than 50%.[160] African Americans also are much more likely to be diagnosed with diabetes compared to White Americans. Asians have increased risk of diabetes as diabetes can develop at lower BMI due to differences in visceral fat compared to other races. For Asians, diabetes can develop at a younger age and lower body fat compared to other groups. Additionally, diabetes is highly underreported in Asian American people, as 1 in 3 cases are diagnosed compared to the average 1 in 5 for the nation.[161]
People with diabetes who have neuropathic symptoms such as numbness or tingling in feet or hands are twice as likely to be unemployed as those without the symptoms.[162]
In 2010, diabetes-related emergency room (ER) visit rates in the United States were higher among people from the lowest income communities (526 per 10,000 population) than from the highest income communities (236 per 10,000 population). Approximately 9.4% of diabetes-related ER visits were for the uninsured.[163]
The term "type 1 diabetes" has replaced several former terms, including childhood-onset diabetes, juvenile diabetes, and insulin-dependent diabetes mellitus. Likewise, the term "type 2 diabetes" has replaced several former terms, including adult-onset diabetes, obesity-related diabetes, and noninsulin-dependent diabetes mellitus. Beyond these two types, there is no agreed-upon standard nomenclature.[164]
Diabetes mellitus is also occasionally known as "sugar diabetes" to differentiate it from diabetes insipidus.[165]
Diabetes can occur in mammals or reptiles.[166][167] Birds do not develop diabetes because of their unusually high tolerance for elevated blood glucose levels.[168]
In animals, diabetes is most commonly encountered in dogs and cats. Middle-aged animals are most commonly affected. Female dogs are twice as likely to be affected as males, while according to some sources, male cats are more prone than females. In both species, all breeds may be affected, but some small dog breeds are particularly likely to develop diabetes, such as Miniature Poodles.[169]
Feline diabetes is strikingly similar to human type 2 diabetes. The Burmese, Russian Blue, Abyssinian, and Norwegian Forest cat breeds are at higher risk than other breeds. Overweight cats are also at higher risk.[170]
The symptoms may relate to fluid loss and polyuria, but the course may also be insidious. Diabetic animals are more prone to infections. The long-term complications recognized in humans are much rarer in animals. The principles of treatment (weight loss, oral antidiabetics, subcutaneous insulin) and management of emergencies (e.g. ketoacidosis) are similar to those in humans.[169]

Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass. These cancerous cells have the ability to invade other parts of the body.[9] A number of types of pancreatic cancer are known.[10]
The most common, pancreatic adenocarcinoma, accounts for about 90% of cases,[11] and the term "pancreatic cancer" is sometimes used to refer only to that type.[10] These adenocarcinomas start within the part of the pancreas that makes digestive enzymes.[10] Several other types of cancer, which collectively represent the majority of the non-adenocarcinomas, can also arise from these cells.[10]
About 1–2% of cases of pancreatic cancer are neuroendocrine tumors, which arise from the hormone-producing cells of the pancreas.[10] These are generally less aggressive than pancreatic adenocarcinoma.[10]
Signs and symptoms of the most-common form of pancreatic cancer may include yellow skin, abdominal or back pain, unexplained weight loss, light-colored stools, dark urine, and loss of appetite.[1] Usually, no symptoms are seen in the disease's early stages, and symptoms that are specific enough to suggest pancreatic cancer typically do not develop until the disease has reached an advanced stage.[1][2] By the time of diagnosis, pancreatic cancer has often spread to other parts of the body.[10][12]
Pancreatic cancer rarely occurs before the age of 40, and more than half of cases of pancreatic adenocarcinoma occur in those over 70.[2] Risk factors for pancreatic cancer include tobacco smoking, obesity, diabetes, and certain rare genetic conditions.[2] About 25% of cases are linked to smoking,[3] and 5–10% are linked to inherited genes.[2]
Pancreatic cancer is usually diagnosed by a combination of medical imaging techniques such as ultrasound or computed tomography, blood tests, and examination of tissue samples (biopsy).[3][4] The disease is divided into stages, from early (stage I) to late (stage IV).[12] Screening the general population has not been found to be effective.[13]
The risk of developing pancreatic cancer is lower among non-smokers, and people who maintain a healthy weight and limit their consumption of red or processed meat;[5] however, the risk is greater for men, especially at very high levels of red meat consumption.[14] However, this is in debate, as a study performed by the International Journal of Cancer in 2013 did not find any statistically significant relationship between red meat consumption and pancreatic cancer, finding instead no male connection and only finding positive association of red meat consumption with pancreatic cancer risk in women after restriction to microscopically confirmed cases.[15] Smokers' chances of developing the disease decrease if they stop smoking and almost return to that of the rest of the population after 20 years.[10] Pancreatic cancer can be treated with surgery, radiotherapy, chemotherapy, palliative care, or a combination of these.[1] Treatment options are partly based on the cancer stage.[1] Surgery is the only treatment that can cure pancreatic adenocarcinoma,[12] and may also be done to improve quality of life without the potential for cure.[1][12] Pain management and medications to improve digestion are sometimes needed.[12] Early palliative care is recommended even for those receiving treatment that aims for a cure.[16]

Pancreatic cancer is among the most deadly forms of cancer globally, with one of the lowest survival rates. In 2015, pancreatic cancers of all types resulted in 411,600 deaths globally.[8] Pancreatic cancer is the fifth-most-common cause of death from cancer in the United Kingdom,[17] and the third most-common in the United States.[18] The disease occurs most often in the developed world, where about 70% of the new cases in 2012 originated.[10] Pancreatic adenocarcinoma typically has a very poor prognosis; after diagnosis, 25% of people survive one year and 12% live for five years.[6][10] For cancers diagnosed early, the five-year survival rate rises to about 20%.[19] Neuroendocrine cancers have better outcomes; at five years from diagnosis, 65% of those diagnosed are living, though survival considerably varies depending on the type of tumor.[10]The many types of pancreatic cancer can be divided into two general groups. The vast majority of cases (about 95%) occur in the part of the pancreas that produces digestive enzymes, known as the exocrine component. Several subtypes of exocrine pancreatic cancers are described, but their diagnosis and treatment have much in common.
The small minority of cancers that arise in the hormone-producing (endocrine) tissue of the pancreas have different clinical characteristics and are called pancreatic neuroendocrine tumors, sometimes abbreviated as "PanNETs". Both groups occur mainly (but not exclusively) in people over 40, and are slightly more common in men, but some rare subtypes mainly occur in women or children.[21][22]
The exocrine group is dominated by pancreatic adenocarcinoma (variations of this name may add "invasive" and "ductal"), which is by far the most common type, representing about 85% of all pancreatic cancers.[2] Nearly all these start in the ducts of the pancreas, as pancreatic ductal adenocarcinoma (PDAC).[23] This is despite the fact that the tissue from which it arises – the pancreatic ductal epithelium – represents less than 10% of the pancreas by cell volume, because it constitutes only the ducts (an extensive but capillary-like duct-system fanning out) within the pancreas.[24]  This cancer originates in the ducts that carry secretions (such as enzymes and bicarbonate) away from the pancreas. About 60–70% of adenocarcinomas occur in the head of the pancreas.[2]
The next-most common type, acinar cell carcinoma of the pancreas, arises in the clusters of cells that produce these enzymes, and represents 5% of exocrine pancreas cancers.[25] Like the 'functioning' endocrine cancers described below, acinar cell carcinomas may cause over-production of certain molecules, in this case digestive enzymes, which may cause symptoms such as skin rashes and joint pain.
Cystadenocarcinomas account for 1% of pancreatic cancers, and they have a better prognosis than the other exocrine types.[25]
Pancreatoblastoma is a rare form, mostly occurring in childhood, and with a relatively good prognosis.  Other exocrine cancers include adenosquamous carcinomas, signet ring cell carcinomas, hepatoid carcinomas, colloid carcinomas, undifferentiated carcinomas, and undifferentiated carcinomas with osteoclast-like giant cells. Solid pseudopapillary tumor is a rare low-grade neoplasm that mainly affects younger women, and generally has a very good prognosis.[2][26]
Pancreatic mucinous cystic neoplasms are a broad group of pancreas tumors that have varying malignant potential. They are being detected at a greatly increased rate as CT scans become more powerful and common, and discussion continues as how best to assess and treat them, given that many are benign.[27]
The small minority of tumors that arise elsewhere in the pancreas are mainly pancreatic neuroendocrine tumors (PanNETs).[28] Neuroendocrine tumors (NETs) are a diverse group of benign or malignant tumors that arise from the body's neuroendocrine cells, which are responsible for integrating the nervous and endocrine systems. NETs can start in most organs of the body, including the pancreas, where the various malignant types are all considered to be rare.  PanNETs are grouped into 'functioning' and 'nonfunctioning' types, depending on the degree to which they produce hormones. The functioning types secrete hormones such as insulin, gastrin, and glucagon into the bloodstream, often in large quantities, giving rise to serious symptoms such as low blood sugar, but also favoring relatively early detection. The most common functioning PanNETs are insulinomas and gastrinomas, named after the hormones they secrete. The nonfunctioning types do not secrete hormones in a sufficient quantity to give rise to overt clinical symptoms, so nonfunctioning PanNETs are often diagnosed only after the cancer has spread to other parts of the body.[29]
As with other neuroendocrine tumors, the history of the terminology and classification of PanNETs is complex.[28] PanNETs are sometimes called "islet cell cancers",[30]  though they are now known to not actually arise from islet cells as previously thought.[29]
Since pancreatic cancer usually does not cause recognizable symptoms in its early stages, the disease is typically not diagnosed until it has spread beyond the pancreas itself.[4]  This is one of the main reasons for the generally poor survival rates. Exceptions to this are the functioning PanNETs, where over-production of various active hormones can give rise to symptoms (which depend on the type of hormone).[31]
Common presenting symptoms of pancreatic adenocarcinoma include:
Other common manifestations of the disease include weakness and tiring easily, dry mouth, sleep problems, and a palpable abdominal mass.[33]
The spread of pancreatic cancer to other organs (metastasis) may also cause symptoms. Typically, pancreatic adenocarcinoma first spreads to nearby lymph nodes, and later to the liver or to the peritoneal cavity, large intestine, or lungs.[3] Uncommonly, it spreads to the bones or brain.[35]
Cancers in the pancreas may also be secondary cancers that have spread from other parts of the body.  This is uncommon, found in only about 2% of cases of pancreatic cancer. Kidney cancer is by far the most common cancer to spread to the pancreas, followed by colorectal cancer, and then cancers of the skin, breast, and lung.  Surgery may be performed on the pancreas in such cases, whether in hope of a cure or to alleviate symptoms.[36]
Risk factors for pancreatic adenocarcinoma include:[2][10][12][37][38][excessive citations]
Drinking alcohol excessively is a major cause of chronic pancreatitis, which in turn predisposes to pancreatic cancer, but considerable research has failed to firmly establish alcohol consumption as a direct risk factor for pancreatic cancer. Overall, the association is consistently weak and the majority of studies have found no association, with smoking a strong confounding factor. The evidence is stronger for a link with heavy drinking, of at least six drinks per day.[3][47]
Exocrine cancers are thought to arise from several types of precancerous lesions within the pancreas, but these lesions do not always progress to cancer, and the increased numbers detected as a byproduct of the increasing use of CT scans for other reasons are not all treated.[3]  Apart from pancreatic serous cystadenomas, which are almost always benign, four types of precancerous lesion are recognized.
The first is pancreatic intraepithelial neoplasia (PanIN). These lesions are microscopic abnormalities in the pancreas and are often found in autopsies of people with no diagnosed cancer. These lesions may progress from low to high grade and then to a tumor. More than 90% of cases at all grades carry a faulty KRAS gene, while in grades 2 and 3, damage to three further genes – CDKN2A (p16), p53, and SMAD4 – are increasingly often found.[2]
A second type is the intraductal papillary mucinous neoplasm (IPMN). These are macroscopic lesions, which are found in about 2% of all adults. This rate rises to about 10% by age 70. These lesions have about a 25% risk of developing into invasive cancer. They may have KRAS gene mutations (40–65% of cases) and in the GNAS Gs alpha subunit and RNF43, affecting the Wnt signaling pathway.[2] Even if removed surgically,  a considerably increased risk remains of pancreatic cancer developing subsequently.[3]
The third type, pancreatic mucinous cystic neoplasm (MCN), mainly occurs in women, and may remain benign or progress to cancer.[49] If these lesions become large, cause symptoms, or have suspicious features, they can usually be successfully removed by surgery.[3]
A fourth type of cancer that arises in the pancreas is the intraductal tubulopapillary neoplasm. This type was recognised by the WHO in 2010 and constitutes about 1–3% of all pancreatic neoplasms.  Mean age at diagnosis is 61 years (range 35–78 years). About 50% of these lesions become invasive. Diagnosis depends on histology, as these lesions are very difficult to differentiate from other lesions on either clinical or radiological grounds.[50]
The genetic events found in ductal adenocarcinoma have been well characterized, and complete exome sequencing has been done for the common types of tumor. Four genes have each been found to be mutated in the majority of adenocarcinomas: KRAS (in 95% of cases), CDKN2A (also in 95%), TP53 (75%), and SMAD4 (55%).  The last of these is especially associated with a poor prognosis.[3] SWI/SNF mutations/deletions occur in about 10–15% of the adenocarcinomas.[2]  The genetic alterations in several other types of pancreatic cancer and precancerous lesions have also been researched.[3] Transcriptomics analyses and mRNA sequencing for the common forms of pancreatic cancer have found that 75% of human genes are expressed in the tumors, with some 200 genes more specifically expressed in pancreatic cancer as compared to other tumor types.[51][52]
The genes often found mutated in pancreatic neuroendocrine tumors (PanNETs) are different from those in exocrine pancreatic cancer.[53] For example, KRAS mutation is normally absent. Instead, hereditary MEN1 gene mutations give risk to MEN1 syndrome, in which primary tumors occur in two or more endocrine glands. About 40–70% of people born with a MEN1 mutation eventually develop a PanNet.[54] Other genes that are frequently mutated include DAXX, mTOR, and ATRX.[29]
The symptoms of pancreatic adenocarcinoma do not usually appear in the disease's early stages, and they are not individually distinctive to the disease.[3][12][32]  The symptoms at diagnosis vary according to the location of the cancer in the pancreas, which anatomists divide (from left to right on most diagrams) into the thick head, the neck, and the tapering body, ending in the tail.
Regardless of a tumor's location, the most common symptom is unexplained weight loss, which may be considerable. A large minority (between 35% and 47%) of people diagnosed with the disease will have had nausea, vomiting, or a feeling of weakness.  Tumors in the head of the pancreas typically also cause jaundice, pain, loss of appetite, dark urine, and light-colored stools. Tumors in the body and tail typically also cause pain.[32]
People sometimes have recent onset of atypical type 2 diabetes that is difficult to control, a history of recent but unexplained blood vessel inflammation caused by blood clots (thrombophlebitis) known as Trousseau sign, or a previous attack of pancreatitis.[32] A doctor may suspect pancreatic cancer when the onset of diabetes in someone over 50 years old is accompanied by typical symptoms such as unexplained weight loss, persistent abdominal or back pain, indigestion, vomiting, or fatty feces.[12] Jaundice accompanied by a painlessly swollen gallbladder (known as Courvoisier's sign) may also raise suspicion, and can help differentiate pancreatic cancer from gallstones.[55]
Medical imaging techniques, such as computed tomography (CT scan) and endoscopic ultrasound (EUS) are used both to confirm the diagnosis and to help decide whether the tumor can be surgically removed (its "resectability").[12] On contrast CT scan, pancreatic cancer typically shows a gradually increasing radiocontrast uptake, rather than a fast washout as seen in a normal pancreas or a delayed washout as seen in chronic pancreatitis.[56] Magnetic resonance imaging and positron emission tomography may also be used,[2] and magnetic resonance cholangiopancreatography may be useful in some cases.[32] Abdominal ultrasound is less sensitive and will miss small tumors, but can identify cancers that have spread to the liver and build-up of fluid in the peritoneal cavity (ascites).[12]  It may be used for a quick and cheap first examination before other techniques.[57]
A biopsy by fine needle aspiration, often guided by endoscopic ultrasound, may be used where there is uncertainty over the diagnosis, but a histologic diagnosis is not usually required for removal of the tumor by surgery to go ahead.[12]
Liver function tests can show a combination of results indicative of bile duct obstruction (raised conjugated bilirubin, γ-glutamyl transpeptidase and alkaline phosphatase levels). CA19-9 (carbohydrate antigen 19.9) is a tumor marker that is frequently elevated in pancreatic cancer. However, it lacks sensitivity and specificity, not least because 5% of people lack the Lewis (a) antigen and cannot produce CA19-9. It has a sensitivity of 80% and specificity of 73% in detecting pancreatic adenocarcinoma, and is used for following known cases rather than diagnosis.[2][12]
The most common form of pancreatic cancer (adenocarcinoma) is typically characterized by moderately to poorly differentiated glandular structures on microscopic examination. There is typically considerable desmoplasia or formation of a dense fibrous stroma or structural tissue consisting of a range of cell types (including myofibroblasts, macrophages, lymphocytes and mast cells) and deposited material (such as type I collagen and hyaluronic acid).  This creates a tumor microenvironment that is short of blood vessels (hypovascular) and so of oxygen (tumor hypoxia).[2]  It is thought that this prevents many chemotherapy drugs from reaching the tumor, as one factor making the cancer especially hard to treat.[2][3]
Negative for:
Pancreatic cancer is usually staged following a CT scan.[32]  The most widely used cancer staging system for pancreatic cancer is the one formulated by the American Joint Committee on Cancer (AJCC) together with the Union for International Cancer Control (UICC). The AJCC-UICC staging system designates four main overall stages, ranging from early to advanced disease, based on TNM classification of Tumor size, spread to lymph Nodes, and Metastasis.[61]
To help decide treatment, the tumors are also divided into three broader categories based on whether surgical removal seems possible: in this way, tumors are judged to be "resectable", "borderline resectable", or "unresectable".[62] When the disease is still in an early stage (AJCC-UICC stages I and II), without spread to large blood vessels or distant organs such as the liver or lungs, surgical resection of the tumor can normally be performed, if the patient is willing to undergo this major operation and is thought to be sufficiently fit.[12]
The AJCC-UICC staging system allows distinction between stage III tumors that are judged to be "borderline resectable" (where surgery is technically feasible because the celiac axis and superior mesenteric artery are still free) and those that are "unresectable" (due to more locally advanced disease); in terms of the more detailed TNM classification, these two groups correspond to T3 and T4 respectively.[3]
Stage T1 pancreatic cancer
Stage T2 pancreatic cancer
Stage T3 pancreatic cancer
Stage T4 pancreatic cancer
Pancreatic cancer in nearby lymph nodes – Stage N1
Locally advanced adenocarcinomas have spread into neighboring organs, which may be any of the following (in roughly decreasing order of frequency): the duodenum, stomach, transverse colon, spleen, adrenal gland, or kidney. Very often they also spread to the important blood or lymphatic vessels and nerves that run close to the pancreas, making surgery far more difficult. Typical sites for metastatic spread (stage IV disease) are the liver, peritoneal cavity and lungs, all  of which occur in 50% or more of fully advanced cases.[63]
The 2010 WHO classification of tumors of the digestive system grades all the pancreatic neuroendocrine tumors (PanNETs) into three categories, based on their degree of cellular differentiation (from "NET G1" through to the poorly differentiated "NET G3").[22] The U.S. National Comprehensive Cancer Network recommends use of the same AJCC-UICC staging system as pancreatic adenocarcinoma.[64]: 52  Using this scheme, the stage-by-stage outcomes for PanNETs are dissimilar to those of the exocrine cancers.[65] A different TNM system for PanNETs has been proposed by the European Neuroendocrine Tumor Society.[22]
Apart from not smoking, the American Cancer Society recommends keeping a healthy weight, and increasing consumption of fruits, vegetables, and whole grains, while decreasing consumption of red and processed meat, although there is no consistent evidence this will prevent or reduce pancreatic cancer specifically.[66] A 2014 review of research concluded that there was evidence that consumption of citrus fruits and curcumin reduced risk of pancreatic cancer, while there was possibly a beneficial effect from whole grains, folate, selenium, and non-fried fish.[47]
In the general population, screening of large groups is not considered effective and may be harmful as of 2019,[67] although newer techniques, and the screening of tightly targeted groups, are being evaluated.[68][69] Nevertheless, regular screening with endoscopic ultrasound and MRI/CT imaging is recommended for those at high risk from inherited genetics.[4][57][69][70]
Use of aspirin is said to lower the risk of pancreatic cancer.[71][72]
A key assessment that is made after diagnosis is whether surgical removal of the tumor is possible (see Staging), as this is the only cure for this cancer. Whether or not surgical resection can be offered depends on how much the cancer has spread. The exact location of the tumor is also a significant factor, and CT can show how it relates to the major blood vessels passing close to the pancreas. The general health of the person must also be assessed, though age in itself is not an obstacle to surgery.[3]
Chemotherapy and, to a lesser extent, radiotherapy are likely to be offered to most people, whether or not surgery is possible. Specialists advise that the management of pancreatic cancer should be in the hands of a multidisciplinary team including specialists in several aspects of oncology, and is, therefore, best conducted in larger centers.[2][3]
Surgery with the intention of a cure is only possible in around one-fifth (20%) of new cases.[12]  Although CT scans help, in practice it can be difficult to determine whether the tumor can be fully removed (its "resectability"), and it may only become apparent during surgery that it is not possible to successfully remove the tumor without damaging other vital tissues. Whether or not surgical resection can be offered depends on various factors, including the precise extent of local anatomical adjacency to, or involvement of, the venous or arterial blood vessels,[2] as well as surgical expertise and a careful consideration of projected post-operative recovery.[73][74] The age of the person is not in itself a reason not to operate, but their general performance status needs to be adequate for a major operation.[12]
One particular feature that is evaluated is the encouraging presence, or discouraging absence, of a clear layer or plane of fat creating a barrier between the tumor and the vessels.[3]  Traditionally, an assessment is made of the tumor's proximity to major venous or arterial vessels, in terms of "abutment" (defined as the tumor touching no more than half a blood vessel's circumference without any fat to separate it), "encasement" (when the tumor encloses most of the vessel's circumference), or full vessel involvement.[75]: 22  A resection that includes encased sections of blood vessels may be possible in some cases,[76][77] particularly if preliminary neoadjuvant therapy is feasible,[78][79][80] using chemotherapy[74][75]: 36 [81] and/or radiotherapy.[75]: 29–30 
Even when the operation appears to have been successful, cancerous cells are often found around the edges ("margins") of the removed tissue,  when a pathologist examines them microscopically (this will always be done), indicating the cancer has not been entirely removed.[2] Furthermore, cancer stem cells are usually not evident microscopically, and if they are present they may continue to develop and spread.[82][83] An exploratory laparoscopy (a small, camera-guided surgical procedure) may therefore be performed to gain a clearer idea of the outcome of a full operation.[84]
For cancers involving the head of the pancreas, the Whipple procedure is the most commonly attempted curative surgical treatment. This is a major operation which involves removing the pancreatic head and the curve of the duodenum together ("pancreato-duodenectomy"), making a bypass for food from the stomach to the jejunum ("gastro-jejunostomy") and attaching a loop of jejunum to the cystic duct to drain bile ("cholecysto-jejunostomy"). It can be performed only if the person is likely to survive major surgery and if the cancer is localized without invading local structures or metastasizing. It can, therefore, be performed only in a minority of cases.  Cancers of the tail of the pancreas can be resected using a procedure known as a distal pancreatectomy, which often also entails removal of the spleen.[2][3] Nowadays, this can often be done using minimally invasive surgery.[2][3]
Although curative surgery no longer entails the very high death rates that occurred until the 1980s, a high proportion of people (about 30–45%) still have to be treated for a post-operative sickness that is not caused by the cancer itself.  The most common complication of surgery is difficulty in emptying the stomach.[3]  Certain more limited surgical procedures may also be used to ease symptoms (see Palliative care): for instance, if the cancer is invading or compressing the duodenum or colon. In such cases, bypass surgery might overcome the obstruction and improve quality of life but is not intended as a cure.[12]
After surgery, adjuvant chemotherapy with gemcitabine or 5-FU can be offered if the person is sufficiently fit, after a recovery period of one to two months.[4][57]  In people not suitable for curative surgery, chemotherapy may be used to extend life or improve its quality.[3] Before surgery, neoadjuvant chemotherapy or chemoradiotherapy  may be used in cases that are considered to be "borderline resectable" (see Staging) in order to reduce the cancer to a level where surgery could be beneficial. In other cases neoadjuvant therapy remains controversial, because it delays surgery.[3][4][85]
Gemcitabine was approved by the United States Food and Drug Administration (FDA) in 1997, after a clinical trial reported improvements in quality of life and a five-week improvement in median survival duration in people with advanced pancreatic cancer.[86] This was the first chemotherapy drug approved by the FDA primarily for a nonsurvival clinical trial endpoint.[87]  Chemotherapy using gemcitabine alone was the standard for about a decade, as a number of trials testing it in combination with other drugs failed to demonstrate significantly better outcomes. However, the combination of gemcitabine with erlotinib was found to increase survival modestly, and erlotinib was licensed by the FDA for use in pancreatic cancer in 2005.[88]
The FOLFIRINOX chemotherapy regimen using four drugs was found more effective than gemcitabine, but with substantial side effects, and is thus only suitable for people with good performance status.  This is also true of protein-bound paclitaxel (nab-paclitaxel), which was licensed by the FDA in 2013 for use with gemcitabine in pancreas cancer.[89] By the end of 2013, both FOLFIRINOX and nab-paclitaxel with gemcitabine were regarded as good choices for those able to tolerate the side-effects, and gemcitabine remained an effective option for those who were not.  A head-to-head trial between the two new options is awaited, and trials investigating other variations continue.  However, the changes of the last few years have only increased survival times by a few months.[86]  Clinical trials are often conducted for novel adjuvant therapies.[4]
The role of radiotherapy as an auxiliary (adjuvant) treatment after potentially curative surgery has been controversial since the 1980s.[3] In the early 2000s the European Study Group for Pancreatic Cancer Research (ESPAC) showed prognostic superiority of adjuvant chemotherapy over chemoradiotherapy.[90][91][4] The European Society for Medical Oncology recommends that adjuvant radiotherapy should only be used for people enrolled in clinical trials.[57]  However, there is a continuing tendency for clinicians in the US to be more ready to use adjuvant radiotherapy than those in Europe.  Many clinical trials have tested a variety of treatment combinations since the 1980s, but have failed to settle the matter conclusively.[3][4]
Radiotherapy may form part of treatment to attempt to shrink a tumor to a resectable state, but its use on unresectable tumors remains controversial as there are conflicting results from clinical trials.  The preliminary results of one trial, presented in 2013, "markedly reduced enthusiasm" for its use on locally advanced tumors.[2]
Treatment of PanNETs, including the less common malignant types, may include a number of approaches.[64][92][93][94]  Some small tumors of less than 1 cm. that are identified incidentally, for example on a CT scan performed for other purposes, may be followed by watchful waiting.[64] This depends on the assessed risk of surgery which is influenced by the site of the tumor and the presence of other medical problems.[64] Tumors within the pancreas only (localized tumors), or with limited metastases, for example to the liver, may be removed by surgery.  The type of surgery depends on the tumor location, and the degree of spread to lymph nodes.[22]
For localized tumors, the surgical procedure may be much less extensive than the types of surgery used to treat pancreatic adenocarcinoma described above, but otherwise surgical procedures are similar to those for exocrine tumors.  The range of possible outcomes varies greatly; some types have a very high survival rate after surgery while others have a poor outlook.  As all this group are rare, guidelines emphasize that treatment should be undertaken in a specialized center.[22][29] Use of liver transplantation may be considered in certain cases of liver metastasis.[95]
For functioning tumors, the somatostatin analog class of medications, such as octreotide, can reduce the excessive production of hormones.[22] Lanreotide can slow tumor growth.[96] If the tumor is not amenable to surgical removal and is causing symptoms, targeted therapy with everolimus or sunitinib can reduce symptoms and slow progression of the disease.[29][97][98] Standard cytotoxic chemotherapy is generally not very effective for PanNETs, but may be used when other drug treatments fail to prevent the disease from progressing,[29] or in poorly differentiated PanNET cancers.[99]
Radiation therapy is occasionally used if there is pain due to anatomic extension, such as metastasis to bone.  Some PanNETs absorb specific peptides or hormones, and these PanNETs may respond to nuclear medicine therapy with radiolabeled peptides or hormones such as iobenguane (iodine-131-MIBG).[100][101][102][103] Radiofrequency ablation (RFA), cryoablation, and hepatic artery embolization may also be used.[104][105]
Palliative care is medical care which focuses on treatment of symptoms from serious illness, such as cancer, and improving quality of life.[106]  Because pancreatic adenocarcinoma is usually diagnosed after it has progressed to an advanced stage, palliative care as a treatment of symptoms is often the only treatment possible.[107]
Palliative care focuses not on treating the underlying cancer, but on treating symptoms such as pain or nausea, and can assist in decision-making, including when or if hospice care will be beneficial.[108]  Pain can be managed with medications such as opioids or through procedural intervention, by a nerve block on the celiac plexus (CPB).  This alters or, depending on the technique used, destroys the nerves that transmit pain from the abdomen.  CPB is a safe and effective way to reduce the pain, which generally reduces the need to use opioid painkillers, which have significant negative side effects.[3][109]
Other symptoms or complications that can be treated with palliative surgery are obstruction by the tumor of the intestines or bile ducts.  For the latter, which occurs in well over half of cases, a small metal tube called a stent may be inserted by endoscope to keep the ducts draining.[32]  Palliative care can also help treat depression that often comes with the diagnosis of pancreatic cancer.[3]
Both surgery and advanced inoperable tumors often lead to digestive system disorders from a lack of the exocrine products of the pancreas (exocrine insufficiency).  These can be treated by taking pancreatin which contains manufactured pancreatic enzymes, and is best taken with food.[12]  Difficulty in emptying the stomach (delayed gastric emptying) is common and can be a serious problem, involving hospitalization. Treatment may involve a variety of approaches, including draining the stomach by nasogastric aspiration and drugs called proton-pump inhibitors or H2 antagonists, which both reduce production of gastric acid.[12] Medications like metoclopramide can also be used to clear stomach contents.
Pancreatic adenocarcinoma and the other less common exocrine cancers have a very poor prognosis, as they are normally diagnosed at a late stage when the cancer is already locally advanced or has spread to other parts of the body.[2] Outcomes are much better for PanNETs: Many are benign and completely without clinical symptoms, and even those cases not treatable with surgery have an average five-year survival rate of 16%,[62] although the outlook varies considerably according to the type.[31]
For locally advanced and metastatic pancreatic adenocarcinomas, which together represent over 80% of cases, numerous trials comparing chemotherapy regimes have shown increased survival times, but not to more than one year.[2][86] Overall five-year survival for pancreatic cancer in the US has improved from 2% in cases diagnosed in 1975–1977, and 4% in 1987–1989 diagnoses, to 6% in 2003–2009.[110] In the less than 20% of cases of pancreatic adenocarcinoma with a diagnosis of a localized and small cancerous growth (less than 2 cm in Stage T1), about 20% of Americans survive to five years.[19]
About 1500 genes are linked to outcomes in pancreatic adenocarcinoma. These include both unfavorable genes, where high expression is related to poor outcome, for example C-Met and MUC-1, and favorable genes where high expression is associated with better survival, for example the transcription factor PELP1.[51][52]
In 2015, pancreatic cancers of all types resulted in 411,600 deaths globally.[8] In 2014, an estimated 46,000 people in the US are expected to be diagnosed with pancreatic cancer and 40,000 to die of it.[2]  Although it accounts for only 2.5% of new cases, pancreatic cancer is responsible for 6% of cancer deaths each year.[111] It is the seventh highest cause of death from cancer worldwide.[10] Pancreatic cancer is the fifth most common cause of death from cancer in the United Kingdom,[17] and the third most common in the United States.[18]
Globally pancreatic cancer is the 11th most common cancer in women and the 12th most common in men.[10] The majority of recorded cases occur in developed countries.[10] People from the United States have an average lifetime risk of about 1 in 67 (or 1.5%) of developing the disease,[112] slightly higher than the figure for the UK.[113] The disease is more common in men than women,[2][10] though the difference in rates has narrowed over recent decades, probably reflecting earlier increases in female smoking.  In the United States the risk for African Americans is over 50% greater than for whites, but the rates in Africa and East Asia are much lower than those in North America or Europe. The United States, Central, and eastern Europe, and Argentina and Uruguay all have high rates.[10]
The annual incidence of clinically recognized pancreatic neuroendocrine tumors (PanNETs) is low (about 5 per one million person-years) and is dominated by the non-functioning types.[26] Somewhere between 45% and 90% of PanNETs are thought to be of the non-functioning types.[22][29] Studies of autopsies have uncovered small PanNETs rather frequently, suggesting that the prevalence of tumors that remain inert and asymptomatic may be relatively high.[29] Overall PanNETs are thought to account for about 1 to 2% of all pancreatic tumors.[26] The definition and classification of PanNETs has changed over time, affecting what is known about their epidemiology and clinical relevance.[53]
The earliest recognition of pancreatic cancer has been attributed to the 18th-century Italian scientist Giovanni Battista Morgagni, the historical father of modern-day anatomic pathology, who claimed to have traced several cases of cancer in the pancreas. Many 18th and 19th-century physicians were skeptical about the existence of the disease, given the similar appearance of pancreatitis. Some case reports were published in the 1820s and 1830s, and a genuine histopathologic diagnosis was eventually recorded by the American clinician Jacob Mendes Da Costa, who also doubted the reliability of Morgagni's interpretations. By the start of the 20th century, cancer of the head of the pancreas had become a well-established diagnosis.[114]
Regarding the recognition of PanNETs, the possibility of cancer of the islet cells was initially suggested in 1888. The first case of hyperinsulinism due to a tumor of this type was reported in 1927. Recognition of a non-insulin-secreting type of PanNET is generally ascribed to the American surgeons, R. M. Zollinger and E. H. Ellison, who gave their names to Zollinger–Ellison syndrome, after postulating the existence of a gastrin-secreting pancreatic tumor in a report of two cases of unusually severe peptic ulcers published in 1955.[114] In 2010, the WHO recommended that PanNETs be referred to as "neuroendocrine" rather than "endocrine" tumors.[28]
Small precancerous neoplasms for many pancreatic cancers are being detected at greatly increased rates by modern medical imaging. One type, the intraductal papillary mucinous neoplasm (IPMN) was first described by Japanese researchers in 1982. It was noted in 2010 that: "For the next decade, little attention was paid to this report; however, over the subsequent 15 years, there has been a virtual explosion in the recognition of this tumor."[63]
The first reported partial pancreaticoduodenectomy was performed by the Italian surgeon Alessandro Codivilla in 1898, but the patient only survived 18 days before succumbing to complications. Early operations were compromised partly because of mistaken beliefs that people would die if their duodenum were removed, and also, at first, if the flow of pancreatic juices stopped. Later it was thought, also mistakenly, that the pancreatic duct could simply be tied up without serious adverse effects; in fact, it will very often leak later on. In 1907–1908, after some more unsuccessful operations by other surgeons, experimental procedures were tried on corpses by French surgeons.[115]
In 1912 the German surgeon Walther Kausch was the first to remove large parts of the duodenum and pancreas together (en bloc). This was in Breslau, now Wrocław, in Poland. In 1918 it was demonstrated, in operations on dogs, that it is possible to survive even after complete removal of the duodenum, but no such result was reported in human surgery until 1935, when the American surgeon Allen Oldfather Whipple published the results of a series of three operations at Columbia Presbyterian Hospital in New York. Only one of the patients had the duodenum entirely removed, but he survived for two years before dying of metastasis to the liver.
The first operation was unplanned, as cancer was only discovered in the operating theater. Whipple's success showed the way for the future, but the operation remained a difficult and dangerous one until recent decades. He published several refinements to his procedure, including the first total removal of the duodenum in 1940, but he only performed a total of 37 operations.[115]
The discovery in the late 1930s that vitamin K prevented bleeding with jaundice, and the development of blood transfusion as an everyday process, both improved post-operative survival,[115] but about 25% of people never left hospital alive as late as the 1970s.[116] In the 1970s a group of American surgeons wrote urging that the procedure was too dangerous and should be abandoned. Since then outcomes in larger centers have improved considerably, and mortality from the operation is often less than 4%.[24]
In 2006 a report was published of a series of 1,000 consecutive pancreatico-duodenectomies performed by a single surgeon from Johns Hopkins Hospital between 1969 and 2003. The rate of these operations had increased steadily over this period, with only three of them before 1980, and the median operating time reduced from 8.8 hours in the 1970s to 5.5 hours in the 2000s, and mortality within 30 days or in hospital was only 1%.[115][116] Another series of 2,050 operations at the Massachusetts General Hospital between 1941 and 2011 showed a similar picture of improvement.[117]
Early-stage research on pancreatic cancer includes studies of genetics and early detection, treatment at different cancer stages, surgical strategies, and targeted therapies, such as inhibition of growth factors, immune therapies, and vaccines.[42][118][119][120][121] Bile acids may have a role in the carcinogenesis of pancreatic cancer.[122]
A key question is the timing of events as the disease develops and progresses – particularly the role of diabetes,[118][34] and how and when the disease spreads.[123] The knowledge that new onset of diabetes can be an early sign of the disease could facilitate timely diagnosis and prevention if a workable screening strategy can be developed.[118][34][124] The European Registry of Hereditary Pancreatitis and Familial Pancreatic Cancer (EUROPAC) trial is aiming to determine whether regular screening is appropriate for people with a family history of the disease.[125]
Keyhole surgery (laparoscopy) rather than Whipple's procedure, particularly in terms of recovery time, is being evaluated.[126] Irreversible electroporation is a relatively novel ablation technique with potential for downstaging and prolonging survival in persons with locally advanced disease, especially for tumors in proximity to peri-pancreatic vessels without risk of vascular trauma.[127][128]
Efforts are underway to develop new drugs, including those targeting molecular mechanisms for cancer onset,[129][130] stem cells,[83] and cell proliferation.[130][131] A further approach involves the use of immunotherapy, such as oncolytic viruses.[132] Galectin-specific mechanisms of the tumor microenvironment are under study.[133]
Cronkhite–Canada
Lymphoma is a group of blood and lymph tumors that develop from lymphocytes (a type of white blood cell).[7] The name typically refers to just the cancerous versions rather than all such tumours.[7] Signs and symptoms may include enlarged lymph nodes, fever, drenching sweats, unintended weight loss, itching, and constantly feeling tired.[1][2] The enlarged lymph nodes are usually painless.[1] The sweats are most common at night.[1][2]
Many subtypes of lymphomas are known.[8] The two main categories of lymphomas are the non-Hodgkin lymphoma (NHL) (90% of cases)[9][10] and Hodgkin lymphoma (HL) (10%).[9]  Lymphomas, leukemias and myelomas are a part of the broader group of tumors of the hematopoietic and lymphoid tissues.[11]
Risk factors for Hodgkin lymphoma include infection with Epstein–Barr virus and a history of the disease in the family.[1] Risk factors for common types of non-Hodgkin lymphomas include autoimmune diseases, HIV/AIDS, infection with human T-lymphotropic virus, immunosuppressant medications, and some pesticides.[2][12] In 2014, the International Agency for Research on Cancer updated its classification of trichloroethylene to Group 1, indicating that sufficient evidence exists that it causes cancer of the kidney in humans as well as some evidence of cancer of the liver and non-Hodgkin's lymphoma.[13] Eating large amounts of red meat and tobacco smoking may also increase the risk.[3][14][15] Diagnosis, if enlarged lymph nodes are present, is usually by lymph node biopsy.[1][2] Blood, urine, and bone marrow testing may also be useful in the diagnosis.[2] Medical imaging may then be done to determine if and where the cancer has spread.[1][2] Lymphoma most often spreads to the lungs, liver, and brain.[1][2]
Treatment may involve one or more of the following: chemotherapy, radiation therapy, proton therapy, targeted therapy, and surgery.[1][2] In some non-Hodgkin lymphomas, an increased amount of protein produced by the lymphoma cells causes the blood to become so thick that plasmapheresis is performed to remove the protein.[2] Watchful waiting may be appropriate for certain types.[2] The outcome depends on the subtype with some being curable and treatment prolonging survival in most.[9] The five-year survival rate in the United States for all Hodgkin lymphoma subtypes is 85%,[4] while that for non-Hodgkin lymphomas is 69%.[16] Worldwide, lymphomas developed in 566,000 people in 2012 and caused 305,000 deaths.[17] They make up 3–4% of all cancers, making them as a group the seventh-most common form.[17][18] In children, they are the third-most common cancer.[19] They occur more often in the developed world than the developing world.[17]
Lymphoma may present with certain nonspecific symptoms; if the symptoms are persistent, an evaluation to determine their cause, including possible lymphoma, should be undertaken.
Asymptomatic soft swelling, which may or may not be ulcerated, is primarily seen on the tonsils, buccal mucosa, palate, gums, salivary glands, tongue, the floor of the mouth, and retromolar region.[citation needed]
Lymphoma is definitively diagnosed by a lymph-node biopsy, meaning a partial or total excision of a lymph node examined under the microscope.[23] This examination reveals histopathological features that may indicate lymphoma. After lymphoma is diagnosed, a variety of tests may be carried out to look for specific features characteristic of different types of lymphoma. These include:
According to the World Health Organization (WHO), lymphoma classification should reflect in which lymphocyte population the neoplasm arises.[24] Thus, neoplasms that arise from precursor lymphoid cells are distinguished from those that arise from mature lymphoid cells.[24] Most mature lymphoid neoplasms comprise the non-Hodgkin lymphomas.[24] Historically, mature histiocytic and dendritic cell (HDC) neoplasms have been considered mature lymphoid neoplasms, since these often involve lymphoid tissue.[24]
Lymphoma can also spread to the central nervous system, often around the brain in the meninges, known as lymphomatous meningitis (LM).[25]
Hodgkin lymphoma accounts for about 15% of lymphomas.[26] It differs from other forms of lymphomas in its prognosis and several pathological characteristics. A division into Hodgkin and non-Hodgkin lymphomas is used in several of the older classification systems. A Hodgkin lymphoma is marked by the presence of a type of cell called the Reed–Sternberg cell.[27][28]
Non-Hodgkin lymphomas, which are defined as being all lymphomas except Hodgkin lymphoma, are more common than Hodgkin lymphoma. A wide variety of lymphomas are in this class, and the causes, the types of cells involved, and the prognoses vary by type. The number of cases per year of non-Hodgkin lymphoma increases with age. It is further divided into several subtypes.[citation needed]
Epstein–Barr virus-associated lymphoproliferative diseases are a group of benign, premalignant, and malignant diseases of lymphoid cells, i.e. B cells, T cells, NK cells, and histiocytic-dendritic cells in which one or more of these cell types is infected with the Epstein–Barr virus (EBV). The virus may be responsible for the development and/or progression of these diseases. In addition to EBV-positive Hodgkin lymphomas, the World Health Organization (2016) includes the following lymphomas, when associated with EBV infection, in this group of diseases: Burkitt lymphoma; large B cell lymphoma, not otherwise specified; diffuse large B cell lymphoma associated with chronic inflammation; fibrin-associated diffuse large B cell lymphoma; primary effusion lymphoma; plasmablastic lymphoma; extranodal NK/T cell lymphoma, nasal type; peripheral T cell lymphoma, not otherwise specified; angioimmunoblastic T cell lymphoma; follicular T cell lymphoma; and systemic T cell lymphoma of childhood.[29]
The WHO classification, published in 2001 and updated in 2008,[30][31] is based upon the foundations laid within the "revised European–American lymphoma classification" (REAL). This system groups lymphomas by cell type (i.e. the normal cell type that most resembles the tumor) and defining phenotypic, molecular, or cytogenetic characteristics. The five groups are shown in the table. Hodgkin lymphoma is considered separately within the WHO and preceding classifications, although it is recognized as being a tumor, albeit markedly abnormal, of lymphocytes of mature B cell lineage.[citation needed]
Of the many forms of lymphoma, some are categorized as indolent (e.g. small lymphocytic lymphoma), compatible with a long life even without treatment, whereas other forms are aggressive (e.g. Burkitt's lymphoma), causing rapid deterioration and death. However, most of the aggressive lymphomas respond well to treatment and are curable. The prognosis, therefore, depends on the correct diagnosis and classification of the disease, which is established after examination of a biopsy by a pathologist (usually a hematopathologist).[32]
Several previous classifications have been used, including Rappaport 1956, Lennert/Kiel 1974, BNLI, Working formulation (1982), and REAL (1994).
The Working Formulation of 1982 was a classification of non-Hodgkin lymphoma. It excluded the Hodgkin lymphomas and divided the remaining lymphomas into four grades (low, intermediate, high, and miscellaneous) related to prognosis, with some further subdivisions based on the size and shape of affected cells. This purely histological classification included no information about cell surface markers, or genetics, and it made no distinction between T-cell lymphomas and B-cell lymphomas. It was widely accepted at the time of its publication but by 2004 was obsolete.[39]
In 1994, the Revised European-American Lymphoma (REAL) classification applied immunophenotypic and genetic features in identifying distinct clinicopathologic entities among all the lymphomas except Hodgkin lymphoma.[40] For coding purposes, the ICD-O (codes 9590–9999)[41] and ICD-10 (codes C81-C96)[42] are available.
After a diagnosis and before treatment, cancer is staged. This refers to determining if the cancer has spread, and if so, whether locally or to distant sites. Staging is reported as a grade between I (confined) and IV (spread). The stage of a lymphoma helps predict a patient's prognosis and is used to help select the appropriate therapy.[43]
The Ann Arbor staging system is routinely used for staging of both HL and NHL. In this staging system, stage I represents localized disease contained within a lymph node group, II represents the presence of lymphoma in two or more lymph nodes groups, III represents spread of the lymphoma to lymph nodes groups on both sides of the diaphragm, and IV indicates spread to tissue outside the lymphatic system. Different suffixes imply the involvement of different organs, for example, S for the spleen and H for the liver. Extra-lymphatic involvement is expressed with the letter E. In addition, the presence of B symptoms (one or more of the following: unintentional loss of 10% body weight in the last 6 months, night sweats, or persistent fever of 38 °C or more) or their absence is expressed with B or A, respectively.[44]
CT scan or PET scan imaging modalities are used to stage cancer. PET scanning is advised for fluorodeoxyglucose-avid lymphomas, such as Hodgkin lymphoma, as a staging tool that can even replace bone marrow biopsy. For other lymphomas, CT scanning is recommended for staging.[43]
Age and poor performance status are other established poor prognostic factors.[45]  This means that people who are elderly or too sick to take care of themselves are more likely to be killed by lymphoma than others.
Mantle cell lymphoma: Notice the irregular nuclear contours of the medium-sized lymphoma cells and the presence of a pink histiocyte. By immunohistochemistry, the lymphoma cells expressed CD20, CD5, and Cyclin D1 (high-power view, H&E)
Hodgkin lymphoma, nodular lymphocyte predominant (low-power view): Notice the nodular architecture and the areas of "mottling".(H&E)
Hodgkin lymphoma, nodular lymphocyte predominant (high-power view): Notice the presence of L&H cells, also known as "popcorn cells". (H&E)
Certain lymphomas (extranodal NK/T-cell lymphoma, nasal type and type II enteropathy-associated T-cell lymphoma) can be mimicked by two benign diseases which involve the excessive proliferation of non-malignant NK cells in the GI tract, natural killer cell enteropathy, a disease wherein NK cell infiltrative lesions occur in the intestine, colon, stomach, or esophagus, and lymphomatoid gastropathy, a disease wherein these cells' infiltrative lesions are limited to the stomach. These diseases do not progress to cancer, may regress spontaneously and do not respond to, and do not require, chemotherapy or other lymphoma treatments.[46]
Prognoses and treatments are different for HL and between all the different forms of NHL,[47] and also depend on the grade of tumour, referring to how quickly a cancer replicates. Paradoxically, high-grade lymphomas are more readily treated and have better prognoses:[citation needed] Burkitt lymphoma, for example, is a high-grade tumour known to double within days, and is highly responsive to treatment.
Many low-grade lymphomas remain indolent (growing slowly or not at all) for many years – sometimes, for the rest of the person's life. With an indolent lymphoma, such as follicular lymphoma, watchful waiting is often the initial course of action, because monitoring is less risky and less harmful than early treatment.[48]
If a low-grade lymphoma becomes symptomatic, radiotherapy or chemotherapy are the treatments of choice.  Although these treatments do not permanently cure the lymphoma, they can alleviate the symptoms, particularly painful lymphadenopathy. People with these types of lymphoma can live near-normal lifespans, even though the disease is technically incurable.
Some centers advocate the use of single agent rituximab in the treatment of follicular lymphoma rather than the wait-and-watch approach. Watchful waiting is not a desirable strategy for everyone, as it leads to significant distress and anxiety in some people. It has been called "watch and worry".[49]
Treatment of some other, more aggressive, forms of lymphoma[which?] can result in a cure in the majority of cases, but the prognosis for people with a poor response to therapy is worse.[50] Treatment for these types of lymphoma typically consists of aggressive chemotherapy, including the CHOP or R-CHOP regimen. A number of people are cured with first-line chemotherapy. Most relapses occur within the first two years, and the relapse risk drops significantly thereafter.[51] For people who relapse, high-dose chemotherapy followed by autologous stem cell transplantation is a proven approach.[52]
The treatment of side effects is also important as they can occur due to the chemotherapy or the stem cell transplantation. It was evaluated whether mesenchymal stromal cells can be used for the treatment and prophylaxis of graft-versus-host diseases. The evidence is very uncertain about the therapeutic effect of mesenchymal stromal cells to treat graft-versus-host diseases on the all-cause mortality and complete disappear of chronic acute graft-versus-host diseases. Mesenchymal stromal cells may result in little to no difference in the all-cause mortality, relapse of malignant disease and incidence of acute and chronic graft-versus-host diseases if they are used for prophylactic reason.[53] Moreover, it was seen that platelet transfusions for people undergoing a chemotherapy or a stem cell transplantation for the prevention of bleeding events had different effects on the number of participants with a bleeding event, the number of days on which a bleeding occurred, the mortality secondary to bleeding and the number of platelet transfusions depending on the way they were used (therapeutic, depending on a threshold, different dose schedules or prophylactic).[54][55]
Four chimeric antigen receptor CAR-T cell therapies are FDA-approved for non-Hodgkin lymphoma, including lisocabtagene maraleucel (for relapsed or refractory large B-cell lymphoma with two failed systemic treatments), axicabtagene ciloleucel, tisagenlecleucel (for large B-cell lymphoma), and brexucabtagene autoleucel(for mantle cell lymphoma). These therapies come with certification and other restrictions.[56]
Hodgkin lymphoma typically is treated with radiotherapy alone, as long as it is localized.[57]
Advanced Hodgkin disease requires systemic chemotherapy, sometimes combined with radiotherapy.[58] Chemotherapy used includes the ABVD regimen, which is commonly used in the United States. Other regimens used in the management of Hodgkin lymphoma include BEACOPP and Stanford V. Considerable controversy exists regarding the use of ABVD or BEACOPP. Briefly, both regimens are effective, but BEACOPP is associated with more toxicity. Encouragingly, a significant number of people who relapse after ABVD can still be salvaged by stem cell transplant.[59]
Scientists evaluated whether positron-emission-tomography scans between the chemotherapy cycles can be used to make assumptions about the survival. The evidence is very uncertain about the effect of negative (= good prognosis) or positive (= bad prognosis) interim PET scan results on the progression-free survival. Negative interim PET scan results may result in an increase in progression-free survival compared if the adjusted result was measured. Negative interim PET scan results probably result in a large increase in the overall survival compared to those with a positive interim PET scan result.[60]
Current research evaluated whether Nivolumab can be used for the treatment of a Hodgkin's lymphoma. The evidence is very uncertain about the effect of Nivolumab for patients with a Hodgkin's lymphoma on the overall survival, the quality of life, the survival without a progression, the response rate (=complete disappear) and grade 3 or 4 serious adverse events.[61]
Palliative care, a specialized medical care focused on the symptoms, pain, and stress of a serious illness, is recommended by multiple national cancer treatment guidelines as an accompaniment to curative treatments for people with lymphoma.[62][63] It is used to address both the direct symptoms of lymphoma and many unwanted side effects that arise from treatments.[64][65] Palliative care can be especially helpful for children who develop lymphoma, helping both children and their families deal with the physical and emotional symptoms of the disease.[64][66][67][68] For these reasons, palliative care is especially important for people requiring bone marrow transplants.[69][70]
Adding physical exercises to the standard treatment for adult patients with haematological malignancies like lymphomas may result in little to no difference in the mortality, the quality of life and the physical functioning. These exercises may result in a slight reduction in depression. Furthermore, aerobic physical exercises probably reduce fatigue. The evidence is very uncertain about the effect on anxiety and serious adverse events.[71]   
Lymphoma is the most common form of hematological malignancy, or "blood cancer", in the developed world.
Taken together, lymphomas represent 5.3% of all cancers (excluding simple basal cell and squamous cell skin cancers) in the United States and 55.6% of all blood cancers.[73]
According to the U.S. National Institutes of Health, lymphomas account for about 5%, and Hodgkin lymphoma in particular accounts for less than 1% of all cases of cancer in the United States.[citation needed]
Because the whole lymphatic system is part of the body's immune system, people with a weakened immune system such as from HIV infection or from certain drugs or medication also have a higher number of cases of lymphoma.[74]
Thomas Hodgkin published the first description of lymphoma in 1832, specifically of the form named after him.[75] Since then, many other forms of lymphoma have been described.
The term "lymphoma" is from Latin lympha ("water") and from Greek -oma ("morbid growth, tumor").[76]
The two types of lymphoma research are clinical or translational research and basic research. Clinical/translational research focuses on studying the disease in a defined and generally immediately applicable way, such as testing a new drug in people. Studies may focus on effective means of treatment, better ways of treating the disease, improving the quality of life for people, or appropriate care in remission or after cures. Hundreds of clinical trials are being planned or conducted at any given time.[77]
Basic science research studies the disease process at a distance, such as seeing whether a suspected carcinogen can cause healthy cells to turn into lymphoma cells in the laboratory or how the DNA changes inside lymphoma cells as the disease progresses. The results from basic research studies are generally less immediately useful to people with the disease,[78] but can improve scientists' understanding of lymphoma and form the foundation for future, more effective treatments.

Coronavirus disease 2019 (COVID-19) is a contagious disease caused by the virus SARS-CoV-2. The first known case was identified in Wuhan, China, in December 2019.[6] The disease quickly spread worldwide, resulting in the COVID-19 pandemic.
The symptoms of COVID‑19 are variable but often include fever,[7] cough, headache,[8] fatigue, breathing difficulties, loss of smell, and loss of taste.[9][10][11] Symptoms may begin one to fourteen days after exposure to the virus. At least a third of people who are infected do not develop noticeable symptoms.[12][13] Of those who develop symptoms noticeable enough to be classified as patients, most (81%) develop mild to moderate symptoms (up to mild pneumonia), while 14% develop severe symptoms (dyspnea, hypoxia, or more than 50% lung involvement on imaging), and 5% develop critical symptoms (respiratory failure, shock, or multiorgan dysfunction).[14] Older people are at a higher risk of developing severe symptoms. Some people continue to experience a range of effects (long COVID) for months or years after infection, and damage to organs has been observed.[15] Multi-year studies are underway to further investigate the long-term effects of the disease.[16]
COVID‑19 transmits when infectious particles are breathed in or come into contact with the eyes, nose, or mouth. The risk is highest when people are in close proximity, but small airborne particles containing the virus can remain suspended in the air and travel over longer distances, particularly indoors. Transmission can also occur when people touch their eyes, nose or mouth after touching surfaces or objects that have been contaminated by the virus. People remain contagious for up to 20 days and can spread the virus even if they do not develop symptoms.[17]
Testing methods for COVID-19 to detect the virus's nucleic acid include real-time reverse transcription polymerase chain reaction (RT‑PCR),[18][19] transcription-mediated amplification,[18][19][20] and reverse transcription loop-mediated isothermal amplification (RT‑LAMP)[18][19] from a nasopharyngeal swab.[21]
Several COVID-19 vaccines have been approved and distributed in various countries, which have initiated mass vaccination campaigns. Other preventive measures include physical or social distancing, quarantining, ventilation of indoor spaces, use of face masks or coverings in public, covering coughs and sneezes, hand washing, and keeping unwashed hands away from the face. While work is underway to develop drugs that inhibit the virus, the primary treatment is symptomatic. Management involves the treatment of symptoms through supportive care, isolation, and experimental measures.
During the initial outbreak in Wuhan, the virus and disease were commonly referred to as "coronavirus" and "Wuhan coronavirus",[22][23][24] with the disease sometimes called "Wuhan pneumonia".[25][26] In the past, many diseases have been named after geographical locations, such as the Spanish flu,[27] Middle East respiratory syndrome, and Zika virus.[28] In January 2020, the World Health Organization (WHO) recommended 2019-nCoV[29] and 2019-nCoV acute respiratory disease[30] as interim names for the virus and disease per 2015 guidance and international guidelines against using geographical locations or groups of people in disease and virus names to prevent social stigma.[31][32][33] The official names COVID‑19 and SARS-CoV-2 were issued by the WHO on 11 February 2020 with COVID-19 being shorthand for "coronavirus disease 2019".[34][35] The WHO additionally uses "the COVID‑19 virus" and "the virus responsible for COVID‑19" in public communications.[34][36]
The symptoms of COVID-19 are variable depending on the type of variant contracted, ranging from mild symptoms to a potentially fatal illness.[37][38] Common symptoms include coughing, fever, loss of smell (anosmia) and taste (ageusia), with less common ones including headaches, nasal congestion and runny nose, muscle pain, sore throat, diarrhea, eye irritation,[39] and toes swelling or turning purple,[40] and in moderate to severe cases, breathing difficulties.[41] People with the COVID-19 infection may have different symptoms, and their symptoms may change over time. Three common clusters of symptoms have been identified: one respiratory symptom cluster with cough, sputum, shortness of breath, and fever; a musculoskeletal symptom cluster with muscle and joint pain, headache, and fatigue; and a cluster of digestive symptoms with abdominal pain, vomiting, and diarrhea.[41] In people without prior ear, nose, or throat disorders, loss of taste combined with loss of smell is associated with COVID-19 and is reported in as many as 88% of symptomatic cases.[42][43][44]
Of people who show symptoms, 81% develop only mild to moderate symptoms (up to mild pneumonia), while 14% develop severe symptoms (dyspnea, hypoxia, or more than 50% lung involvement on imaging) that require hospitalization, and 5% of patients develop critical symptoms (respiratory failure, septic shock, or multiorgan dysfunction) requiring ICU admission.[45][needs update]
At least a third of the people who are infected with the virus do not develop noticeable symptoms at any point in time.[46][47][48] These asymptomatic carriers tend not to get tested and can still spread the disease.[48][49][50][51] Other infected people will develop symptoms later (called "pre-symptomatic") or have very mild symptoms and can also spread the virus.[51]
As is common with infections, there is a delay between the moment a person first becomes infected and the appearance of the first symptoms. The median delay for COVID-19 is four to five days[52] possibly being infectious on 1-4 of those days.[53] Most symptomatic people experience symptoms within two to seven days after exposure, and almost all will experience at least one symptom within 12 days.[52][54]
Most people recover from the acute phase of the disease. However, some people continue to experience a range of effects, such as fatigue, for months, even after recovery.[55] This is the result of a condition called long COVID, which can be described as a range of persistent symptoms that continue for weeks or months at a time.[56] Long-term damage to organs has also been observed after the onset of COVID-19. Multi-year studies are underway to further investigate the potential long-term effects of the disease.[57]
Complications may include pneumonia, acute respiratory distress syndrome (ARDS), multi-organ failure, septic shock, and death.[59][60][61][62] Cardiovascular complications may include heart failure, arrhythmias (including atrial fibrillation), heart inflammation, and thrombosis, particularly venous thromboembolism.[63][64][65][66][67][68] Approximately 20–30% of people who present with COVID‑19 have elevated liver enzymes, reflecting liver injury.[69][70]
Neurologic manifestations include seizure, stroke, encephalitis, and Guillain–Barré syndrome (which includes loss of motor functions).[71][72] Following the infection, children may develop paediatric multisystem inflammatory syndrome, which has symptoms similar to Kawasaki disease, which can be fatal.[73][74] In very rare cases, acute encephalopathy can occur, and it can be considered in those who have been diagnosed with COVID‑19 and have an altered mental status.[75]
According to the US Centers for Disease Control and Prevention, pregnant women are at increased risk of becoming seriously ill from COVID‑19.[76] This is because pregnant women with COVID‑19 appear to be more likely to develop respiratory and obstetric complications that can lead to miscarriage, premature delivery and intrauterine growth restriction.[76]
Fungal infections such as aspergillosis, candidiasis, cryptococcosis and mucormycosis have been recorded in patients recovering from COVID‑19.[77][78]
COVID‑19 is caused by infection with a strain of coronavirus known as 'Severe Acute Respiratory Syndrome coronavirus 2' (SARS-CoV-2).[79]
COVID-19 is mainly transmitted when people breathe in air contaminated by droplets/aerosols and small airborne particles containing the virus. Infected people exhale those particles as they breathe, talk, cough, sneeze, or sing.[80][81][82][83] Transmission is more likely the closer people are. However, infection can occur over longer distances, particularly indoors.[80][84]
The transmission of the virus is carried out through virus-laden fluid particles, or droplets, which are created in the respiratory tract, and they are expelled by the mouth and the nose. There are three types of transmission: “droplet” and “contact”, which are associated with large droplets, and “airborne”, which is associated with small droplets.[85] If the droplets are above a certain critical size, they settle faster than they evaporate, and therefore they contaminate surfaces surrounding them.[85] Droplets that are below a certain critical size, evaporate faster than they settle; due to that fact, they form nuclei that remain airborne for a long period of time over extensive distances.[85]
Infectivity can begin four to five days before the onset of symptoms.[86] Infected people can spread the disease even if they are pre-symptomatic or asymptomatic.[87]  Most commonly, the peak viral load in upper respiratory tract samples occurs close to the time of symptom onset and declines after the first week after symptoms begin.[87] Current evidence suggests a duration of viral shedding and the period of infectiousness of up to ten days following symptom onset for people with mild to moderate COVID-19, and up to 20 days for persons with severe COVID-19, including immunocompromised people.[88][87]
Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a novel severe acute respiratory syndrome coronavirus. It was first isolated from three people with pneumonia connected to the cluster of acute respiratory illness cases in Wuhan.[95] All structural features of the novel SARS-CoV-2 virus particle occur in related coronaviruses in nature,[96] particularly in Rhinolophus sinicus aka Chinese horseshoe bats.[97]
Outside the human body, the virus is destroyed by household soap which bursts its protective bubble.[98] Hospital disinfectants, alcohols, heat, povidone-iodine, and ultraviolet-C (UV-C) irradiation are also effective disinfection methods for surfaces.[99]
SARS-CoV-2 is closely related to the original SARS-CoV.[100] It is thought to have an animal (zoonotic) origin. Genetic analysis has revealed that the coronavirus genetically clusters with the genus Betacoronavirus, in subgenus Sarbecovirus (lineage B) together with two bat-derived strains. It is 96% identical at the whole genome level to other bat coronavirus samples (BatCov RaTG13).[101][102][103] The structural proteins of SARS-CoV-2 include membrane glycoprotein (M), envelope protein (E), nucleocapsid protein (N), and the spike protein (S). The M protein of SARS-CoV-2 is about 98% similar to the M protein of bat SARS-CoV, maintains around 98% homology with pangolin SARS-CoV, and has 90% homology with the M protein of SARS-CoV; whereas, the similarity is only around 38% with the M protein of MERS-CoV.[104]
The many thousands of SARS-CoV-2 variants are grouped into either clades or lineages.[105][106] The WHO, in collaboration with partners, expert networks, national authorities, institutions and researchers, have established nomenclature systems for naming and tracking SARS-CoV-2 genetic lineages by GISAID, Nextstrain and Pango. The expert group convened by the WHO recommended the labelling of variants using letters of the Greek alphabet, for example, Alpha, Beta, Delta, and Gamma, giving the justification that they "will be easier and more practical to discussed by non-scientific audiences."[107] Nextstrain divides the variants into five clades (19A, 19B, 20A, 20B, and 20C), while GISAID divides them into seven (L, O, V, S, G, GH, and GR).[108] The Pango tool groups variants into lineages, with many circulating lineages being classed under the B.1 lineage.[106][109]
Several notable variants of SARS-CoV-2 emerged throughout 2020.[110][111] Cluster 5 emerged among minks and mink farmers in Denmark.[112] After strict quarantines and a mink euthanasia campaign, the cluster was assessed to no longer be circulating among humans in Denmark as of 1 February 2021.[113]
As of December 2021[update], there are five dominant variants of SARS-CoV-2 spreading among global populations: the Alpha variant (B.1.1.7, formerly called the UK variant), first found in London and Kent, the Beta variant (B.1.351, formerly called the South Africa variant), the Gamma variant (P.1, formerly called the Brazil variant), the Delta variant (B.1.617.2, formerly called the India variant),[114] and the Omicron variant (B.1.1.529), which had spread to 57 countries as of 7 December.[115][116]
The SARS-CoV-2 virus can infect a wide range of cells and systems of the body. COVID‑19 is most known for affecting the upper respiratory tract (sinuses, nose, and throat) and the lower respiratory tract (windpipe and lungs).[117] The lungs are the organs most affected by COVID‑19 because the virus accesses host cells via the receptor for the enzyme angiotensin-converting enzyme 2 (ACE2), which is most abundant on the surface of type II alveolar cells of the lungs.[118] The virus uses a special surface glycoprotein called a "spike" to connect to the ACE2 receptor and enter the host cell.[119]
Following viral entry, COVID‑19 infects the ciliated epithelium of the nasopharynx and upper airways.[120]
Autopsies of people who died of COVID‑19 have found diffuse alveolar damage, and lymphocyte-containing inflammatory infiltrates within the lung.[121]
One common symptom, loss of smell, results from infection of the support cells of the olfactory epithelium, with subsequent damage to the olfactory neurons.[122] The involvement of both the central and peripheral nervous system in COVID‑19 has been reported in many medical publications.[123] It is clear that many people with COVID-19 exhibit neurological or mental health issues. The virus is not detected in the central nervous system (CNS) of the majority of COVID-19 patients with neurological issues. However, SARS-CoV-2 has been detected at low levels in the brains of those who have died from COVID‑19, but these results need to be confirmed.[124] While virus has been detected in cerebrospinal fluid of autopsies, the exact mechanism by which it invades the CNS remains unclear and may first involve invasion of peripheral nerves given the low levels of ACE2 in the brain.[125][126][127] The virus may also enter the bloodstream from the lungs and cross the blood–brain barrier to gain access to the CNS, possibly within an infected white blood cell.[124]
 Research conducted when Alpha was the dominant variant has suggested COVID-19 may cause brain damage.[128] Later research showed that all variants studied (including Omicron) killed brain cells, but the exact cells killed varied by variant.[129] It is unknown if such damage is temporary or permanent.[130][131] Observed individuals infected with COVID-19 (most with mild cases) experienced an additional 0.2% to 2% of brain tissue lost in regions of the brain connected to the sense of smell compared with uninfected individuals, and the overall effect on the brain was equivalent on average to at least one extra year of normal ageing; infected individuals also scored lower on several cognitive tests. All effects were more pronounced among older ages.[132]
The virus also affects gastrointestinal organs as ACE2 is abundantly expressed in the glandular cells of gastric, duodenal and rectal epithelium[133] as well as endothelial cells and enterocytes of the small intestine.[134]
The virus can cause acute myocardial injury and chronic damage to the cardiovascular system.[135][136] An acute cardiac injury was found in 12% of infected people admitted to the hospital in Wuhan, China,[137] and is more frequent in severe disease.[138] Rates of cardiovascular symptoms are high, owing to the systemic inflammatory response and immune system disorders during disease progression, but acute myocardial injuries may also be related to ACE2 receptors in the heart.[136] ACE2 receptors are highly expressed in the heart and are involved in heart function.[136][139]
A high incidence of thrombosis and venous thromboembolism occurs in people transferred to intensive care units with COVID‑19 infections, and may be related to poor prognosis.[140] Blood vessel dysfunction and clot formation (as suggested by high D-dimer levels caused by blood clots) may have a significant role in mortality, incidents of clots leading to pulmonary embolisms, and ischaemic events (strokes) within the brain found as complications leading to death in people infected with COVID‑19.[141] Infection may initiate a chain of vasoconstrictive responses within the body, including pulmonary vasoconstriction – a possible mechanism in which oxygenation decreases during pneumonia.[141] Furthermore, damage of arterioles and capillaries was found in brain tissue samples of people who died from COVID‑19.[142][143]
COVID‑19 may also cause substantial structural changes to blood cells, sometimes persisting for months after hospital discharge.[144] A low level of blood lymphocytes may result from the virus acting through ACE2-related entry into lymphocytes.[145]
Another common cause of death is complications related to the kidneys.[141] Early reports show that up to 30% of hospitalised patients both in China and in New York have experienced some injury to their kidneys, including some persons with no previous kidney problems.[146]
Although SARS-CoV-2 has a tropism for ACE2-expressing epithelial cells of the respiratory tract, people with severe COVID‑19 have symptoms of systemic hyperinflammation. Clinical laboratory findings of elevated IL‑2, IL‑7, IL‑6, granulocyte-macrophage colony-stimulating factor (GM‑CSF), interferon gamma-induced protein 10 (IP‑10), monocyte chemoattractant protein 1 (MCP1), macrophage inflammatory protein 1‑alpha (MIP‑1‑alpha), and tumour necrosis factor (TNF‑α) indicative of cytokine release syndrome (CRS) suggest an underlying immunopathology.[137]
Interferon alpha plays a complex, Janus-faced role in the pathogenesis of COVID-19. Although it promotes the elimination of virus-infected cells, it also upregulates the expression of ACE-2, thereby facilitating the SARS-Cov2 virus to enter cells and to replicate.[147][148] A competition of negative feedback loops (via protective effects of interferon alpha) and positive feedback loops (via upregulation of ACE-2) is assumed to determine the fate of patients suffering from COVID-19.[149]
Additionally, people with COVID‑19 and acute respiratory distress syndrome (ARDS) have classical serum biomarkers of CRS, including elevated C-reactive protein (CRP), lactate dehydrogenase (LDH), D-dimer, and ferritin.[150]
Systemic inflammation results in vasodilation, allowing inflammatory lymphocytic and monocytic infiltration of the lung and the heart. In particular, pathogenic GM-CSF-secreting T cells were shown to correlate with the recruitment of inflammatory IL-6-secreting monocytes and severe lung pathology in people with COVID‑19.[151] Lymphocytic infiltrates have also been reported at autopsy.[121]
Multiple viral and host factors affect the pathogenesis of the virus. The S-protein, otherwise known as the spike protein, is the viral component that attaches to the host receptor via the ACE2 receptors. It includes two subunits: S1 and S2. S1 determines the virus-host range and cellular tropism via the receptor-binding domain. S2 mediates the membrane fusion of the virus to its potential cell host via the H1 and HR2, which are heptad repeat regions. Studies have shown that S1 domain induced IgG and IgA antibody levels at a much higher capacity. It is the focus spike proteins expression that are involved in many effective COVID‑19 vaccines.[152]
The M protein is the viral protein responsible for the transmembrane transport of nutrients. It is the cause of the bud release and the formation of the viral envelope.[153] The N and E protein are accessory proteins that interfere with the host's immune response.[153]
Human angiotensin converting enzyme 2 (hACE2) is the host factor that SARS-CoV-2 virus targets causing COVID‑19. Theoretically, the usage of angiotensin receptor blockers (ARB) and ACE inhibitors upregulating ACE2 expression might increase morbidity with COVID‑19, though animal data suggest some potential protective effect of ARB; however no clinical studies have proven susceptibility or outcomes. Until further data is available, guidelines and recommendations for hypertensive patients remain.[154]
The effect of the virus on ACE2 cell surfaces leads to leukocytic infiltration, increased blood vessel permeability, alveolar wall permeability, as well as decreased secretion of lung surfactants. These effects cause the majority of the respiratory symptoms. However, the aggravation of local inflammation causes a cytokine storm eventually leading to a systemic inflammatory response syndrome.[155]
Among healthy adults not exposed to SARS-CoV-2, about 35% have CD4+ T cells that recognise the SARS-CoV-2 S protein (particularly the S2 subunit) and about 50% react to other proteins of the virus, suggesting cross-reactivity from previous common colds caused by other coronaviruses.[156]
It is unknown whether different persons use similar antibody genes in response to COVID‑19.[157]
The severity of the inflammation can be attributed to the severity of what is known as the cytokine storm.[158] Levels of interleukin 1B, interferon-gamma, interferon-inducible protein 10, and monocyte chemoattractant protein 1 were all associated with COVID‑19 disease severity. Treatment has been proposed to combat the cytokine storm as it remains to be one of the leading causes of morbidity and mortality in COVID‑19 disease.[159]
A cytokine storm is due to an acute hyperinflammatory response that is responsible for clinical illness in an array of diseases but in COVID‑19, it is related to worse prognosis and increased fatality. The storm causes acute respiratory distress syndrome, blood clotting events such as strokes, myocardial infarction, encephalitis, acute kidney injury, and vasculitis. The production of IL-1, IL-2, IL-6, TNF-alpha, and interferon-gamma, all crucial components of normal immune responses, inadvertently become the causes of a cytokine storm. The cells of the central nervous system, the microglia, neurons, and astrocytes, are also involved in the release of pro-inflammatory cytokines affecting the nervous system, and effects of cytokine storms toward the CNS are not uncommon.[160]
There are many unknowns for pregnant women during the COVID-19 pandemic. Given that they are prone to have complications and severe disease infection with other types of coronaviruses, they have been identified as a vulnerable group and advised to take supplementary preventive measures.[161]
Physiological responses to pregnancy can include:
However, from the evidence base, it is difficult to conclude whether pregnant women are at increased risk of grave consequences of this virus.[161]
In addition to the above, other clinical studies have proved that SARS-CoV-2 can affect the period of pregnancy in different ways. On the one hand, there is little evidence of its impact up to 12 weeks gestation. On the other hand, COVID-19 infection may cause increased rates of unfavourable outcomes in the course of the pregnancy. Some examples of these could be foetal growth restriction, preterm birth, and perinatal mortality, which refers to the foetal death past 22 or 28 completed weeks of pregnancy as well as the death among live-born children up to seven completed days of life.[161] For preterm birth, a 2023 review indicates that there appears to be a correlation with COVID-19.[162]
Unvaccinated women in later stages of pregnancy with COVID-19 are more likely than other patients to need very intensive care. Babies born to mothers with COVID-19 are more likely to have breathing problems. Pregnant women are strongly encouraged to get vaccinated.[163]
COVID‑19 can provisionally be diagnosed on the basis of symptoms and confirmed using reverse transcription polymerase chain reaction (RT-PCR) or other nucleic acid testing of infected secretions.[21][164] Along with laboratory testing, chest CT scans may be helpful to diagnose COVID‑19 in individuals with a high clinical suspicion of infection.[165] Detection of a past infection is possible with serological tests, which detect antibodies produced by the body in response to the infection.[21]
The standard methods of testing for presence of SARS-CoV-2 are nucleic acid tests,[21][166] which detects the presence of viral RNA fragments.[167] As these tests detect RNA but not infectious virus, its "ability to determine duration of infectivity of patients is limited."[168] The test is typically done on respiratory samples obtained by a nasopharyngeal swab; however, a nasal swab or sputum sample may also be used.[169][170] Results are generally available within hours.[21] The WHO has published several testing protocols for the disease.[171]
Several laboratories and companies have developed serological tests, which detect antibodies produced by the body in response to infection. Several have been evaluated by Public Health England and approved for use in the UK.[172]
The University of Oxford's CEBM has pointed to mounting evidence[173][174] that "a good proportion of 'new' mild cases and people re-testing positives after quarantine or discharge from hospital are not infectious, but are simply clearing harmless virus particles which their immune system has efficiently dealt with" and have called for "an international effort to standardize and periodically calibrate testing"[175] In September 2020, the UK government issued "guidance for procedures to be implemented in laboratories to provide assurance of positive SARS-CoV-2 RNA results during periods of low prevalence, when there is a reduction in the predictive value of positive test results".[176]
Chest CT scans may be helpful to diagnose COVID‑19 in individuals with a high clinical suspicion of infection but are not recommended for routine screening.[165][177] Bilateral multilobar ground-glass opacities with a peripheral, asymmetric, and posterior distribution are common in early infection.[165][178] Subpleural dominance, crazy paving (lobular septal thickening with variable alveolar filling), and consolidation may appear as the disease progresses.[165][179] Characteristic imaging features on chest radiographs and computed tomography (CT) of people who are symptomatic include asymmetric peripheral ground-glass opacities without pleural effusions.[180]
Many groups have created COVID‑19 datasets that include imagery such as the Italian Radiological Society which has compiled an international online database of imaging findings for confirmed cases.[181] Due to overlap with other infections such as adenovirus, imaging without confirmation by rRT-PCR is of limited specificity in identifying COVID‑19.[180] A large study in China compared chest CT results to PCR and demonstrated that though imaging is less specific for the infection, it is faster and more sensitive.[164]
In late 2019, the WHO assigned emergency ICD-10 disease codes U07.1 for deaths from lab-confirmed SARS-CoV-2 infection and U07.2 for deaths from clinically or epidemiologically diagnosed COVID‑19 without lab-confirmed SARS-CoV-2 infection.[182]
The main pathological findings at autopsy are:
Preventive measures to reduce the chances of infection include getting vaccinated, staying at home, wearing a mask in public, avoiding crowded places, keeping distance from others, ventilating indoor spaces, managing potential exposure durations,[188] washing hands with soap and water often and for at least twenty seconds, practising good respiratory hygiene, and avoiding touching the eyes, nose, or mouth with unwashed hands.[189][190]
Those diagnosed with COVID‑19 or who believe they may be infected are advised by the CDC to stay home except to get medical care, call ahead before visiting a healthcare provider, wear a face mask before entering the healthcare provider's office and when in any room or vehicle with another person, cover coughs and sneezes with a tissue, regularly wash hands with soap and water and avoid sharing personal household items.[191][192]
The first COVID‑19 vaccine was granted regulatory approval on 2 December 2020 by the UK medicines regulator MHRA.[193] It was evaluated for emergency use authorisation (EUA) status by the US FDA, and in several other countries.[194] Initially, the US National Institutes of Health guidelines do not recommend any medication for prevention of COVID‑19, before or after exposure to the SARS-CoV-2 virus, outside the setting of a clinical trial.[195][70] Without a vaccine, other prophylactic measures, or effective treatments, a key part of managing COVID‑19 is trying to decrease and delay the epidemic peak, known as "flattening the curve".[196] This is done by slowing the infection rate to decrease the risk of health services being overwhelmed, allowing for better treatment of active cases, and delaying additional cases until effective treatments or a vaccine become available.[196][197]

Prior to the COVID‑19 pandemic, an established body of knowledge existed about the structure and function of coronaviruses causing diseases like severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS). This knowledge accelerated the development of various vaccine platforms in early 2020.[199] The initial focus of SARS-CoV-2 vaccines was on preventing symptomatic, often severe, illness.[200] In 2020, the first COVID‑19 vaccines were developed and made available to the public through emergency authorizations[201] and conditional approvals.[202][203] Initially, most COVID‑19 vaccines were two-dose vaccines, with the sole exception being the single-dose Janssen COVID‑19 vaccine.[201] However, immunity from the vaccines has been found to wane over time, requiring people to get booster doses of the vaccine to maintain protection against COVID‑19.[201]
The COVID‑19 vaccines are widely credited for their role in reducing the spread of COVID‑19 and reducing the severity and death caused by COVID‑19.[201][204] According to a June 2022 study, COVID‑19 vaccines prevented an additional 14.4 to 19.8 million deaths in 185 countries and territories from 8 December 2020 to 8 December 2021.[205][206] Many countries implemented phased distribution plans that prioritized those at highest risk of complications, such as the elderly, and those at high risk of exposure and transmission, such as healthcare workers.[207][208]
Common side effects of COVID‑19 vaccines include soreness, redness, rash, inflammation at the injection site, fatigue, headache, myalgia (muscle pain), and arthralgia (joint pain), which resolve without medical treatment within a few days.[209][210] COVID‑19 vaccination is safe for people who are pregnant or are breastfeeding.[211]
As of 9 November 2023[update], 13.53 billion doses of COVID‑19 vaccines have been administered worldwide, based on official reports from national public health agencies.[212] By December 2020, more than 10 billion vaccine doses had been preordered by countries,[213] with about half of the doses purchased by high-income countries comprising 14% of the world's population.[214]
Despite the extremely rapid development of effective mRNA and viral vector vaccines, worldwide vaccine equity has not been achieved. The development and use of whole inactivated virus (WIV) and protein-based vaccines have also been recommended, especially for use in developing countries.[215][216]
In community and healthcare settings, the use of face masks is intended as source control to limit transmission of the virus and for personal protection to prevent infection.[220] Properly worn masks both limit the respiratory droplets and aerosols spread by infected individuals and help protect healthy individuals from infection.[221][222]
Reviews of various kinds of scientific studies have concluded that masking is effective in protecting the individual against COVID-19.[221][223][224] Various case-control and population-based studies have also shown that increased levels of masking in a community reduces the spread of SARS-CoV-2,[223][224] though there is a paucity of evidence from randomized controlled trials (RCTs).[225][226] Masks vary in how well they work, with N95 and surgical masks outperforming cloth masks,[227] but even cloth masks, with their variability in fabric type and mask fit, provide wearers with substantial protection from particles carrying COVID-19.[228]
The CDC states that avoiding crowded indoor spaces reduces the risk of COVID-19 infection.[229] When indoors, increasing the rate of air change, decreasing recirculation of air and increasing the use of outdoor air can reduce transmission.[229][230] The WHO recommends ventilation and air filtration in public spaces to help clear out infectious aerosols.[231][232][233]
Exhaled respiratory particles can build-up within enclosed spaces with inadequate ventilation. The risk of COVID‑19 infection increases especially in spaces where people engage in physical exertion or raise their voice (e.g., exercising, shouting, singing) as this increases exhalation of respiratory droplets. Prolonged exposure to these conditions, typically more than 15 minutes, leads to higher risk of infection.[229]
Displacement ventilation with large natural inlets can move stale air directly to the exhaust in laminar flow while significantly reducing the concentration of droplets and particles. Passive ventilation reduces energy consumption and maintenance costs but may lack controllability and heat recovery. Displacement ventilation can also be achieved mechanically with higher energy and maintenance costs. The use of large ducts and openings helps to prevent mixing in closed environments. Recirculation and mixing should be avoided because recirculation prevents dilution of harmful particles and redistributes possibly contaminated air, and mixing increases the concentration and range of infectious particles and keeps larger particles in the air.[234]
Thorough hand hygiene after any cough or sneeze is required.[235] The WHO also recommends that individuals wash hands often with soap and water for at least twenty seconds, especially after going to the toilet or when hands are visibly dirty, before eating and after blowing one's nose.[236] When soap and water are not available, the CDC recommends using an alcohol-based hand sanitiser with at least 60% alcohol.[237] For areas where commercial hand sanitisers are not readily available, the WHO provides two formulations for local production. In these formulations, the antimicrobial activity arises from ethanol or isopropanol. Hydrogen peroxide is used to help eliminate bacterial spores in the alcohol; it is "not an active substance for hand antisepsis." Glycerol is added as a humectant.[238]
Social distancing (also known as physical distancing) includes infection control actions intended to slow the spread of the disease by minimising close contact between individuals. Methods include quarantines; travel restrictions; and the closing of schools, workplaces, stadiums, theatres, or shopping centres. Individuals may apply social distancing methods by staying at home, limiting travel, avoiding crowded areas, using no-contact greetings, and physically distancing themselves from others.[239]
In 2020, outbreaks occurred in prisons due to crowding and an inability to enforce adequate social distancing.[240][241] In the United States, the prisoner population is ageing and many of them are at high risk for poor outcomes from COVID‑19 due to high rates of coexisting heart and lung disease, and poor access to high-quality healthcare.[240]
After being expelled from the body, coronaviruses can survive on surfaces for hours to days. If a person touches the dirty surface, they may deposit the virus at the eyes, nose, or mouth where it can enter the body and cause infection.[242] Evidence indicates that contact with infected surfaces is not the main driver of COVID‑19,[243][244][245] leading to recommendations for optimised disinfection procedures to avoid issues such as the increase of antimicrobial resistance through the use of inappropriate cleaning products and processes.[246][247] Deep cleaning and other surface sanitation has been criticised as hygiene theatre, giving a false sense of security against something primarily spread through the air.[248][249]
The amount of time that the virus can survive depends significantly on the type of surface, the temperature, and the humidity.[250] Coronaviruses die very quickly when exposed to the UV light in sunlight.[250] Like other enveloped viruses, SARS-CoV-2 survives longest when the temperature is at room temperature or lower, and when the relative humidity is low (<50%).[250]
On many surfaces, including glass, some types of plastic, stainless steel, and skin, the virus can remain infective for several days indoors at room temperature, or even about a week under ideal conditions.[250][251] On some surfaces, including cotton fabric and copper, the virus usually dies after a few hours.[250] The virus dies faster on porous surfaces than on non-porous surfaces due to capillary action within pores and faster aerosol droplet evaporation.[252][245][250] However, of the many surfaces tested, two with the longest survival times are N95 respirator masks and surgical masks, both of which are considered porous surfaces.[250]
The CDC says that in most situations, cleaning surfaces with soap or detergent, not disinfecting, is enough to reduce risk of transmission.[253][254] The CDC recommends that if a COVID‑19 case is suspected or confirmed at a facility such as an office or day care, all areas such as offices, bathrooms, common areas, shared electronic equipment like tablets, touch screens, keyboards, remote controls, and ATMs used by the ill persons should be disinfected.[255] Surfaces may be decontaminated with 62–71 per cent ethanol, 50–100 per cent isopropanol, 0.1 per cent sodium hypochlorite, 0.5 per cent hydrogen peroxide, 0.2–7.5 per cent povidone-iodine, or 50–200 ppm hypochlorous acid. Other solutions, such as benzalkonium chloride and chlorhexidine gluconate, are less effective. Ultraviolet germicidal irradiation may also be used,[231] although popular devices require 5–10 min exposure and may deteriorate some materials over time.[256] A datasheet comprising the authorised substances to disinfection in the food industry (including suspension or surface tested, kind of surface, use dilution, disinfectant and inoculum volumes) can be seen in the supplementary material of.[246]
Self-isolation at home has been recommended for those diagnosed with COVID‑19 and those who suspect they have been infected. Health agencies have issued detailed instructions for proper self-isolation.[257] Many governments have mandated or recommended self-quarantine for entire populations. The strongest self-quarantine instructions have been issued to those in high-risk groups.[258] Those who may have been exposed to someone with COVID‑19 and those who have recently travelled to a country or region with the widespread transmission have been advised to self-quarantine for 14 days from the time of last possible exposure.[259]
A 2021 Cochrane rapid review found that based upon low-certainty evidence, international travel-related control measures such as restricting cross-border travel may help to contain the spread of COVID‑19.[260] Additionally, symptom/exposure-based screening measures at borders may miss many positive cases.[260] While test-based border screening measures may be more effective, it could also miss many positive cases if only conducted upon arrival without follow-up. The review concluded that a minimum 10-day quarantine may be beneficial in preventing the spread of COVID‑19 and may be more effective if combined with an additional control measure like border screening.[260]
Although several medications have been approved in different countries as of April 2022, not all countries have these medications. Patients with mild to moderate symptoms who are in the risk groups can take nirmatrelvir/ritonavir (marketed as Paxlovid) or remdesivir, either of which reduces the risk of serious illness or hospitalization.[261] In the US, the Biden Administration COVID-19 action plan includes the Test to Treat initiative, where people can go to a pharmacy, take a COVID test, and immediately receive free Paxlovid if they test positive.[262]
Highly effective vaccines have reduced mortality related to SARS-CoV-2; however, for those awaiting vaccination, as well as for the estimated millions of immunocompromised persons who are unlikely to respond robustly to vaccination, treatment remains important.[263] The cornerstone of management of COVID-19 has been supportive care, which includes treatment to relieve symptoms, fluid therapy, oxygen support and prone positioning as needed, and medications or devices to support other affected vital organs.[264][265][266]
Most cases of COVID-19 are mild. In these, supportive care includes medication such as paracetamol or NSAIDs to relieve symptoms (fever, body aches, cough), proper intake of fluids, rest, and nasal breathing.[267][268][269][270] Good personal hygiene and a healthy diet are also recommended.[271] As of April 2020 the U.S. Centers for Disease Control and Prevention (CDC) recommended that those who suspect they are carrying the virus isolate themselves at home and wear a face mask.[272]
The severity of COVID‑19 varies. The disease may take a mild course with few or no symptoms, resembling other common upper respiratory diseases such as the common cold. In 3–4% of cases (7.4% for those over age 65) symptoms are severe enough to cause hospitalisation.[280] Mild cases typically recover within two weeks, while those with severe or critical diseases may take three to six weeks to recover. Among those who have died, the time from symptom onset to death has ranged from two to eight weeks.[101] The Italian Istituto Superiore di Sanità reported that the median time between the onset of symptoms and death was twelve days, with seven being hospitalised. However, people transferred to an ICU had a median time of ten days between hospitalisation and death.[281] Abnormal sodium levels during hospitalisation with COVID-19 are associated with poor prognoses: high sodium with a greater risk of death, and low sodium with an increased chance of needing ventilator support.[282][283] Prolonged prothrombin time and elevated C-reactive protein levels on admission to the hospital are associated with severe course of COVID‑19 and with a transfer to ICU.[284][285]
Some early studies suggest 10% to 20% of people with COVID‑19 will experience symptoms lasting longer than a month.[286][287] A majority of those who were admitted to hospital with severe disease report long-term problems including fatigue and shortness of breath.[288] On 30 October 2020, WHO chief Tedros Adhanom warned that "to a significant number of people, the COVID virus poses a range of serious long-term effects." He has described the vast spectrum of COVID‑19 symptoms that fluctuate over time as "really concerning". They range from fatigue, a cough and shortness of breath, to inflammation and injury of major organs – including the lungs and heart, and also neurological and psychologic effects. Symptoms often overlap and can affect any system in the body. Infected people have reported cyclical bouts of fatigue, headaches, months of complete exhaustion, mood swings, and other symptoms. Tedros therefore concluded that a strategy of achieving herd immunity by infection, rather than vaccination, is "morally unconscionable and unfeasible".[289]
In terms of hospital readmissions about 9% of 106,000 individuals had to return for hospital treatment within two months of discharge. The average to readmit was eight days since first hospital visit. There are several risk factors that have been identified as being a cause of multiple admissions to a hospital facility. Among these are advanced age (above 65 years of age) and presence of a chronic condition such as diabetes, COPD, heart failure or chronic kidney disease.[290][291]
According to scientific reviews smokers are more likely to require intensive care or die compared to non-smokers.[292][293] Acting on the same ACE2 pulmonary receptors affected by smoking, air pollution has been correlated with the disease.[293] Short term[294] and chronic[295] exposure to air pollution seems to enhance morbidity and mortality from COVID‑19.[296][297][298] Pre-existing heart and lung diseases[299] and also obesity, especially in conjunction with fatty liver disease, contributes to an increased health risk of COVID‑19.[293][300][301][302]
It is also assumed that those that are immunocompromised are at higher risk of getting severely sick from SARS-CoV-2.[303] One research study that looked into the COVID‑19 infections in hospitalised kidney transplant recipients found a mortality rate of 11%.[304]
Men with untreated hypogonadism were 2.4 times more likely than men with eugonadism to be hospitalised if they contracted COVID-19; Hypogonad men treated with testosterone were less likely to be hospitalised for COVID-19 than men who were not treated for hypogonadism.[305]
Genetics plays an important role in the ability to fight off Covid.[306] For instance, those that do not produce detectable type I interferons or produce auto-antibodies against these may get much sicker from COVID‑19.[307][308] Genetic screening is able to detect interferon effector genes.[309] Some genetic variants are risk factors in specific populations. For instance, an allele of the DOCK2 gene (dedicator of cytokinesis 2 gene) is a common risk factor in Asian populations but much less common in Europe. The mutation leads to lower expression of DOCK2 especially in younger patients with severe Covid.[310] In fact, many other genes and genetic variants have been found that determine the outcome of SARS-CoV-2 infections.[311]
While very young children have experienced lower rates of infection, older children have a rate of infection that is similar to the population as a whole.[312][313] Children are likely to have milder symptoms and are at lower risk of severe disease than adults.[314] The CDC reports that in the US roughly a third of hospitalised children were admitted to the ICU,[315] while a European multinational study of hospitalised children from June 2020, found that about 8% of children admitted to a hospital needed intensive care.[316] Four of the 582 children (0.7%) in the European study died, but the actual mortality rate may be "substantially lower" since milder cases that did not seek medical help were not included in the study.[317][318]
Around 10% to 30% of non-hospitalised people with COVID-19 go on to develop long COVID. For those that do need hospitalisation, the incidence of long-term effects is over 50%.[15] Long COVID is an often severe multisystem disease with a large set of symptoms. There are likely various, possibly coinciding, causes.[15] Organ damage from the acute infection can explain a part of the symptoms, but long COVID is also observed in people where organ damage seems to be absent.[319]
By a variety of mechanisms, the lungs are the organs most affected in COVID‑19.[320] In people requiring hospital admission, up to 98% of CT scans performed show lung abnormalities after 28 days of illness even if they had clinically improved.[321] People with advanced age, severe disease, prolonged ICU stays, or who smoke are more likely to have long-lasting effects, including pulmonary fibrosis.[322] Overall, approximately one-third of those investigated after four weeks will have findings of pulmonary fibrosis or reduced lung function as measured by DLCO, even in asymptomatic people, but with the suggestion of continuing improvement with the passing of more time.[320] After severe disease, lung function can take anywhere from three months to a year or more to return to previous levels.[323]
The risks of cognitive deficit, dementia, psychotic disorders, and epilepsy or seizures persists at an increased level two years after infection.[324]
The immune response by humans to SARS-CoV-2 virus occurs as a combination of the cell-mediated immunity and antibody production,[325] just as with most other infections.[326] B cells interact with T cells and begin dividing before selection into the plasma cell, partly on the basis of their affinity for antigen.[327] Since SARS-CoV-2 has been in the human population only since December 2019, it remains unknown if the immunity is long-lasting in people who recover from the disease.[328] The presence of neutralising antibodies in blood strongly correlates with protection from infection, but the level of neutralising antibody declines with time. Those with asymptomatic or mild disease had undetectable levels of neutralising antibody two months after infection. In another study, the level of neutralising antibodies fell four-fold one to four months after the onset of symptoms. However, the lack of antibodies in the blood does not mean antibodies will not be rapidly produced upon reexposure to SARS-CoV-2. Memory B cells specific for the spike and nucleocapsid proteins of SARS-CoV-2 last for at least six months after the appearance of symptoms.[328]
As of August 2021, reinfection with COVID‑19 was possible but uncommon. The first case of reinfection was documented in August 2020.[329] A systematic review found 17 cases of confirmed reinfection in medical literature as of May 2021.[329] With the Omicron variant, as of 2022, reinfections have become common, albeit it is unclear how common.[330] COVID-19 reinfections are thought to likely be less severe than primary infections, especially if one was previously infected by the same variant.[330][additional citation(s) needed]
Several measures are commonly used to quantify mortality.[331] These numbers vary by region and over time and are influenced by the volume of testing, healthcare system quality, treatment options, time since the initial outbreak, and population characteristics such as age, sex, and overall health.[332]
The mortality rate reflects the number of deaths within a specific demographic group divided by the population of that demographic group. Consequently, the mortality rate reflects the prevalence as well as the severity of the disease within a given population. Mortality rates are highly correlated to age, with relatively low rates for young people and relatively high rates among the elderly.[333][334][335] In fact, one relevant factor of mortality rates is the age structure of the countries' populations. For example, the case fatality rate for COVID‑19 is lower in India than in the US since India's younger population represents a larger percentage than in the US.[336]
The case fatality rate (CFR) reflects the number of deaths divided by the number of diagnosed cases within a given time interval. Based on Johns Hopkins University statistics, the global death-to-case ratio is 1.02% (6,881,955/676,609,955) as of 10 March 2023.[337] The number varies by region.[338][339]
Total confirmed cases over time
Total confirmed cases of COVID‑19 per million people[340]
Total confirmed deaths over time
Total confirmed deaths due to COVID‑19 per million people[341]
A key metric in gauging the severity of COVID‑19 is the infection fatality rate (IFR), also referred to as the infection fatality ratio or infection fatality risk.[342][343][344] This metric is calculated by dividing the total number of deaths from the disease by the total number of infected individuals; hence, in contrast to the CFR, the IFR incorporates asymptomatic and undiagnosed infections as well as reported cases.[345]
A December 2020 systematic review and meta-analysis estimated that population IFR during the first wave of the pandemic was about 0.5% to 1% in many locations (including France, Netherlands, New Zealand, and Portugal), 1% to 2% in other locations (Australia, England, Lithuania, and Spain), and exceeded 2% in Italy.[346] That study also found that most of these differences in IFR reflected corresponding differences in the age composition of the population and age-specific infection rates; in particular, the metaregression estimate of IFR is very low for children and younger adults (e.g., 0.002% at age 10 and 0.01% at age 25) but increases progressively to 0.4% at age 55, 1.4% at age 65, 4.6% at age 75, and 15% at age 85.[346] These results were also highlighted in a December 2020 report issued by the WHO.[347]
An analysis of those IFR rates indicates that COVID‑19 is hazardous not only for the elderly but also for middle-aged adults, for whom the infection fatality rate of COVID-19 is two orders of magnitude greater than the annualised risk of a fatal automobile accident and far more dangerous than seasonal influenza.[346]
At an early stage of the pandemic, the World Health Organization reported estimates of IFR between 0.3% and 1%.[348][349] On 2 July, The WHO's chief scientist reported that the average IFR estimate presented at a two-day WHO expert forum was about 0.6%.[350][351] In August, the WHO found that studies incorporating data from broad serology testing in Europe showed IFR estimates converging at approximately 0.5–1%.[352] Firm lower limits of IFRs have been established in a number of locations such as New York City and Bergamo in Italy since the IFR cannot be less than the population fatality rate. (After sufficient time however, people can get reinfected).[353] As of 10 July, in New York City, with a population of 8.4 million, 23,377 individuals (18,758 confirmed and 4,619 probable) have died with COVID‑19 (0.3% of the population).[354] Antibody testing in New York City suggested an IFR of ≈0.9%,[355] and ≈1.4%.[356] In Bergamo province, 0.6% of the population has died.[357] In September 2020, the U.S. Centers for Disease Control and Prevention (CDC) reported preliminary estimates of age-specific IFRs for public health planning purposes.[358]
COVID‑19 case fatality rates are higher among men than women in most countries. However, in a few countries like India, Nepal, Vietnam, and Slovenia the fatality cases are higher in women than men.[336] Globally, men are more likely to be admitted to the ICU and more likely to die.[360][361] One meta-analysis found that globally, men were more likely to get COVID‑19 than women; there were approximately 55 men and 45 women per 100 infections (CI: 51.43–56.58).[362]
The Chinese Center for Disease Control and Prevention reported the death rate was 2.8% for men and 1.7% for women.[363] Later reviews in June 2020 indicated that there is no significant difference in susceptibility or in CFR between genders.[364][365] One review acknowledges the different mortality rates in Chinese men, suggesting that it may be attributable to lifestyle choices such as smoking and drinking alcohol rather than genetic factors.[366] Smoking, which in some countries like China is mainly a male activity, is a habit that contributes to increasing significantly the case fatality rates among men.[336] Sex-based immunological differences, lesser prevalence of smoking in women and men developing co-morbid conditions such as hypertension at a younger age than women could have contributed to the higher mortality in men.[367] In Europe as of February 2020, 57% of the infected people were men and 72% of those died with COVID‑19 were men.[368] As of April 2020, the US government is not tracking sex-related data of COVID‑19 infections.[369] Research has shown that viral illnesses like Ebola, HIV, influenza and SARS affect men and women differently.[369]
In the US, a greater proportion of deaths due to COVID‑19 have occurred among African Americans and other minority groups.[370] Structural factors that prevent them from practising social distancing include their concentration in crowded substandard housing and in "essential" occupations such as retail grocery workers, public transit employees, health-care workers and custodial staff. Greater prevalence of lacking health insurance and care of underlying conditions such as diabetes,[371] hypertension, and heart disease also increase their risk of death.[372] Similar issues affect Native American and Latino communities.[370] On the one hand, in the Dominican Republic there is a clear example of both gender and ethnic inequality. In this Latin American territory, there is great inequality and precariousness that especially affects Dominican women, with greater emphasis on those of Haitian descent.[373] According to a US health policy non-profit, 34% of American Indian and Alaska Native People (AIAN) non-elderly adults are at risk of serious illness compared to 21% of white non-elderly adults.[374] The source attributes it to disproportionately high rates of many health conditions that may put them at higher risk as well as living conditions like lack of access to clean water.[375]
Leaders have called for efforts to research and address the disparities.[376] In the UK, a greater proportion of deaths due to COVID‑19 have occurred in those of a Black, Asian, and other ethnic minority background.[377][378][379] More severe impacts upon patients including the relative incidence of the necessity of hospitalisation requirements, and vulnerability to the disease has been associated via DNA analysis to be expressed in genetic variants at chromosomal region 3, features that are associated with European Neanderthal heritage. That structure imposes greater risks that those affected will develop a more severe form of the disease.[380] The findings are from Professor Svante Pääbo and researchers he leads at the Max Planck Institute for Evolutionary Anthropology and the Karolinska Institutet.[380] This admixture of modern human and Neanderthal genes is estimated to have occurred roughly between 50,000 and 60,000 years ago in Southern Europe.[380]
Biological factors (immune response) and the general behaviour (habits) can strongly determine the consequences of COVID‑19.[336] Most of those who die of COVID‑19 have pre-existing (underlying) conditions, including hypertension, diabetes mellitus,[371] and cardiovascular disease.[381] According to March data from the United States, 89% of those hospitalised had preexisting conditions.[382] The Italian Istituto Superiore di Sanità reported that out of 8.8% of deaths where medical charts were available, 96.1% of people had at least one comorbidity with the average person having 3.4 diseases.[281] According to this report the most common comorbidities are hypertension (66% of deaths), type 2 diabetes (29.8% of deaths), ischaemic heart disease (27.6% of deaths), atrial fibrillation (23.1% of deaths) and chronic renal failure (20.2% of deaths).
Most critical respiratory comorbidities according to the US Centers for Disease Control and Prevention (CDC), are: moderate or severe asthma, pre-existing COPD, pulmonary fibrosis, cystic fibrosis.[383] Evidence stemming from meta-analysis of several smaller research papers also suggests that smoking can be associated with worse outcomes.[384][385] When someone with existing respiratory problems is infected with COVID‑19, they might be at greater risk for severe symptoms.[386] COVID‑19 also poses a greater risk to people who misuse opioids and amphetamines, insofar as their drug use may have caused lung damage.[387]
In August 2020, the CDC issued a caution that tuberculosis (TB) infections could increase the risk of severe illness or death. The WHO recommended that people with respiratory symptoms be screened for both diseases, as testing positive for COVID‑19 could not rule out co-infections. Some projections have estimated that reduced TB detection due to the pandemic could result in 6.3 million additional TB cases and 1.4 million TB-related deaths by 2025.[388]
The virus is thought to be of natural animal origin, most likely through spillover infection.[96][389][390] A joint-study conducted in early 2021 by the People's Republic of China and the World Health Organization indicated that the virus descended from a coronavirus that infects wild bats, and likely spread to humans through an intermediary wildlife host.[391] There are several theories about where the index case originated and investigations into the origin of the pandemic are ongoing.[392] According to articles published in July 2022 in Science, virus transmission into humans occurred through two spillover events in November 2019 and was likely due to live wildlife trade on the Huanan wet market in the city of Wuhan (Hubei, China).[393][394][395] Doubts about the conclusions have mostly centered on the precise site of spillover.[396] Earlier phylogenetics estimated that SARS-CoV-2 arose in October or November 2019.[397][398][399] A phylogenetic algorithm analysis suggested that the virus may have been circulating in Guangdong before Wuhan.[400]
Most scientists believe the virus spilled into human populations through natural zoonosis, similar to the SARS-CoV-1 and MERS-CoV outbreaks, and consistent with other pandemics in human history.[401][402] According to the Intergovernmental Panel on Climate Change several social and environmental factors including climate change, natural ecosystem destruction and wildlife trade increased the likelihood of such zoonotic spillover.[403][404] One study made with the support of the European Union found climate change increased the likelihood of the pandemic by influencing distribution of bat species.[405][406]
Available evidence suggests that the SARS-CoV-2 virus was originally harboured by bats, and spread to humans multiple times from infected wild animals at the Huanan Seafood Market in Wuhan in December 2019.[401][402] A minority of scientists and some members of the U.S intelligence community believe the virus may have been unintentionally leaked from a laboratory such as the Wuhan Institute of Virology.[407][408] The US intelligence community has mixed views on the issue,[409][410] but overall agrees with the scientific consensus that the virus was not developed as a biological weapon and is unlikely to have been genetically engineered.[411][412][413][414] There is no evidence SARS-CoV-2 existed in any laboratory prior to the pandemic.[415][416][417]
The first confirmed human infections were in Wuhan. A study of the first 41 cases of confirmed COVID‑19, published in January 2020 in The Lancet, reported the earliest date of onset of symptoms as 1 December 2019.[418][419][420] Official publications from the WHO reported the earliest onset of symptoms as 8 December 2019.[421] Human-to-human transmission was confirmed by the WHO and Chinese authorities by 20 January 2020.[422][423] According to official Chinese sources, these were mostly linked to the Huanan Seafood Wholesale Market, which also sold live animals.[424] In May 2020, George Gao, the director of the CDC, said animal samples collected from the seafood market had tested negative for the virus, indicating that the market was the site of an early superspreading event, but that it was not the site of the initial outbreak.[425] Traces of the virus have been found in wastewater samples that were collected in Milan and Turin, Italy, on 18 December 2019.[426]
By December 2019, the spread of infection was almost entirely driven by human-to-human transmission.[363][427] The number of COVID-19 cases in Hubei gradually increased, reaching sixty by 20 December,[428] and at least 266 by 31 December.[429] On 24 December, Wuhan Central Hospital sent a bronchoalveolar lavage fluid (BAL) sample from an unresolved clinical case to sequencing company Vision Medicals. On 27 and 28 December, Vision Medicals informed the Wuhan Central Hospital and the Chinese CDC of the results of the test, showing a new coronavirus.[430] A pneumonia cluster of unknown cause was observed on 26 December and treated by the doctor Zhang Jixian in Hubei Provincial Hospital, who informed the Wuhan Jianghan CDC on 27 December.[431] On 30 December, a test report addressed to Wuhan Central Hospital, from company CapitalBio Medlab, stated an erroneous positive result for SARS, causing a group of doctors at Wuhan Central Hospital to alert their colleagues and relevant hospital authorities of the result. The Wuhan Municipal Health Commission issued a notice to various medical institutions on "the treatment of pneumonia of unknown cause" that same evening.[432] Eight of these doctors, including Li Wenliang (punished on 3 January),[433] were later admonished by the police for spreading false rumours and another, Ai Fen, was reprimanded by her superiors for raising the alarm.[434]
The Wuhan Municipal Health Commission made the first public announcement of a pneumonia outbreak of unknown cause on 31 December, confirming 27 cases[435][436][437] – enough to trigger an investigation.[438]
During the early stages of the outbreak, the number of cases doubled approximately every seven and a half days.[439] In early and mid-January 2020, the virus spread to other Chinese provinces, helped by the Chinese New Year migration and Wuhan being a transport hub and major rail interchange.[101] On 20 January, China reported nearly 140 new cases in one day, including two people in Beijing and one in Shenzhen.[440] Later official data shows 6,174 people had already developed symptoms by then,[363] and more may have been infected.[441] A report in The Lancet on 24 January indicated human transmission, strongly recommended personal protective equipment for health workers, and said testing for the virus was essential due to its "pandemic potential".[137][442] On 30 January, the WHO declared COVID-19 a Public Health Emergency of International Concern.[441] By this time, the outbreak spread by a factor of 100 to 200 times.[443]
Italy had its first confirmed cases on 31 January 2020, two tourists from China.[444] Italy overtook China as the country with the most deaths on 19 March 2020.[445] By 26 March the United States had overtaken China and Italy with the highest number of confirmed cases in the world.[446] Research on coronavirus genomes indicates the majority of COVID-19 cases in New York came from European travellers, rather than directly from China or any other Asian country.[447] Retesting of prior samples found a person in France who had the virus on 27 December 2019,[448][449] and a person in the United States who died from the disease on 6 February 2020.[450]
RT-PCR testing of untreated wastewater samples from Brazil and Italy have suggested detection of SARS-CoV-2 as early as November and December 2019, respectively, but the methods of such sewage studies have not been optimised, many have not been peer-reviewed, details are often missing, and there is a risk of false positives due to contamination or if only one gene target is detected.[451] A September 2020 review journal article said, "The possibility that the COVID‑19 infection had already spread to Europe at the end of last year is now indicated by abundant, even if partially circumstantial, evidence", including pneumonia case numbers and radiology in France and Italy in November and December.[452]
As of 1 October 2021[update], Reuters reported that it had estimated the worldwide total number of deaths due to COVID‑19 to have exceeded five million.[453]
The Public Health Emergency of International Concern for COVID-19 ended on May 5, 2023. By this time, everyday life in most countries had returned to how it was before the pandemic.[454][455]
After the initial outbreak of COVID‑19, misinformation and disinformation regarding the origin, scale, prevention, treatment, and other aspects of the disease rapidly spread online.[456][457][458]
In September 2020, the US Centers for Disease Control and Prevention (CDC) published preliminary estimates of the risk of death by age groups in the United States, but those estimates were widely misreported and misunderstood.[459][460]
Humans appear to be capable of spreading the virus to some other animals,[461][462] a type of disease transmission referred to as zooanthroponosis.[463][464]
Some pets, especially cats and ferrets, can catch this virus from infected humans.[465][466] Symptoms in cats include respiratory (such as a cough) and digestive symptoms.[465] Cats can spread the virus to other cats, and may be able to spread the virus to humans, but cat-to-human transmission of SARS-CoV-2 has not been proven.[465][467] Compared to cats, dogs are less susceptible to this infection.[467] Behaviours which increase the risk of transmission include kissing, licking, and petting the animal.[467]
The virus does not appear to be able to infect pigs, ducks, or chickens at all.[465] Mice, rats, and rabbits, if they can be infected at all, are unlikely to be involved in spreading the virus.[467]
Tigers and lions in zoos have become infected as a result of contact with infected humans.[467] As expected, monkeys and great ape species such as orangutans can also be infected with the COVID‑19 virus.[467]
Minks, which are in the same family as ferrets, have been infected.[467] Minks may be asymptomatic, and can also spread the virus to humans.[467] Multiple countries have identified infected animals in mink farms.[468] Denmark, a major producer of mink pelts, ordered the slaughter of all minks over fears of viral mutations,[468] following an outbreak referred to as Cluster 5. A vaccine for mink and other animals is being researched.[468]
International research on vaccines and medicines in COVID‑19 is underway by government organisations, academic groups, and industry researchers.[469][470] The CDC has classified it to require a BSL3 grade laboratory.[471] There has been a great deal of COVID‑19 research, involving accelerated research processes and publishing shortcuts to meet the global demand.[472]
As of December 2020[update], hundreds of clinical trials have been undertaken, with research happening on every continent except Antarctica.[473] As of November 2020[update], more than 200 possible treatments have been studied in humans.[474]
Modelling research has been conducted with several objectives, including predictions of the dynamics of transmission,[475] diagnosis and prognosis of infection,[476] estimation of the impact of interventions,[477][478] or allocation of resources.[479] Modelling studies are mostly based on compartmental models in epidemiology,[480] estimating the number of infected people over time under given conditions. Several other types of models have been developed and used during the COVID‑19 pandemic including computational fluid dynamics models to study the flow physics of COVID‑19,[481] retrofits of crowd movement models to study occupant exposure,[482] mobility-data based models to investigate transmission,[483] or the use of macroeconomic models to assess the economic impact of the pandemic.[484]
Repurposed antiviral drugs make up most of the research into COVID‑19 treatments.[485][486] Other candidates in trials include vasodilators, corticosteroids, immune therapies, lipoic acid, bevacizumab, and recombinant angiotensin-converting enzyme 2.[486]
In March 2020, the World Health Organization (WHO) initiated the Solidarity trial to assess the treatment effects of some promising drugs: an experimental drug called remdesivir; anti-malarial drugs chloroquine and hydroxychloroquine; two anti-HIV drugs, lopinavir/ritonavir; and interferon-beta.[487][488] More than 300 active clinical trials are underway as of April 2020.[70]
Research on the antimalarial drugs hydroxychloroquine and chloroquine showed that they were ineffective at best,[489][490] and that they may reduce the antiviral activity of remdesivir.[491] By May 2020[update], France, Italy, and Belgium had banned the use of hydroxychloroquine as a COVID‑19 treatment.[492]
In June, initial results from the randomised RECOVERY Trial in the United Kingdom showed that dexamethasone reduced mortality by one third for people who are critically ill on ventilators and one fifth for those receiving supplemental oxygen.[493] Because this is a well-tested and widely available treatment, it was welcomed by the WHO, which is in the process of updating treatment guidelines to include dexamethasone and other steroids.[494][495] Based on those preliminary results, dexamethasone treatment has been recommended by the NIH for patients with COVID‑19 who are mechanically ventilated or who require supplemental oxygen but not in patients with COVID‑19 who do not require supplemental oxygen.[496]
In September 2020, the WHO released updated guidance on using corticosteroids for COVID‑19.[497][498] The WHO recommends systemic corticosteroids rather than no systemic corticosteroids for the treatment of people with severe and critical COVID‑19 (strong recommendation, based on moderate certainty evidence).[497] The WHO suggests not to use corticosteroids in the treatment of people with non-severe COVID‑19 (conditional recommendation, based on low certainty evidence).[497] The updated guidance was based on a meta-analysis of clinical trials of critically ill COVID‑19 patients.[499][500]
In September 2020, the European Medicines Agency (EMA) endorsed the use of dexamethasone in adults and adolescents from twelve years of age and weighing at least 40 kilograms (88 lb) who require supplemental oxygen therapy.[501][502] Dexamethasone can be taken by mouth or given as an injection or infusion (drip) into a vein.[501]
In November 2020, the US Food and Drug Administration (FDA) issued an emergency use authorisation for the investigational monoclonal antibody therapy bamlanivimab for the treatment of mild-to-moderate COVID‑19.[503] Bamlanivimab is authorised for people with positive results of direct SARS-CoV-2 viral testing who are twelve years of age and older weighing at least 40 kilograms (88 lb), and who are at high risk for progressing to severe COVID‑19 or hospitalisation.[503] This includes those who are 65 years of age or older, or who have chronic medical conditions.[503]
In February 2021, the FDA issued an emergency use authorisation (EUA) for bamlanivimab and etesevimab administered together for the treatment of mild to moderate COVID‑19 in people twelve years of age or older weighing at least 40 kilograms (88 lb) who test positive for SARS‑CoV‑2 and who are at high risk for progressing to severe COVID‑19. The authorised use includes treatment for those who are 65 years of age or older or who have certain chronic medical conditions.[504]
In April 2021, the FDA revoked the emergency use authorisation (EUA) that allowed for the investigational monoclonal antibody therapy bamlanivimab, when administered alone, to be used for the treatment of mild-to-moderate COVID‑19 in adults and certain paediatric patients.[505]
A cytokine storm can be a complication in the later stages of severe COVID‑19. A cytokine storm is a potentially deadly immune reaction where a large amount of pro-inflammatory cytokines and chemokines are released too quickly. A cytokine storm can lead to ARDS and multiple organ failure.[506] Data collected from Jin Yin-tan Hospital in Wuhan, China indicates that patients who had more severe responses to COVID‑19 had greater amounts of pro-inflammatory cytokines and chemokines in their system than patients who had milder responses. These high levels of pro-inflammatory cytokines and chemokines indicate presence of a cytokine storm.[507]
Tocilizumab has been included in treatment guidelines by China's National Health Commission after a small study was completed.[508][509] It is undergoing a Phase II non-randomised trial at the national level in Italy after showing positive results in people with severe disease.[510][511] Combined with a serum ferritin blood test to identify a cytokine storm (also called cytokine storm syndrome, not to be confused with cytokine release syndrome), it is meant to counter such developments, which are thought to be the cause of death in some affected people.[512] The interleukin-6 receptor (IL-6R) antagonist was approved by the FDA to undergo a Phase III clinical trial assessing its effectiveness on COVID‑19 based on retrospective case studies for the treatment of steroid-refractory cytokine release syndrome induced by a different cause, CAR T cell therapy, in 2017.[513] There is no randomised, controlled evidence that tocilizumab is an efficacious treatment for CRS. Prophylactic tocilizumab has been shown to increase serum IL-6 levels by saturating the IL-6R, driving IL-6 across the blood–brain barrier, and exacerbating neurotoxicity while having no effect on the incidence of CRS.[514]
Lenzilumab, an anti-GM-CSF monoclonal antibody, is protective in murine models for CAR T cell-induced CRS and neurotoxicity and is a viable therapeutic option due to the observed increase of pathogenic GM-CSF secreting T cells in hospitalised patients with COVID‑19.[515]
Transferring purified and concentrated antibodies produced by the immune systems of those who have recovered from COVID‑19 to people who need them is being investigated as a non-vaccine method of passive immunisation.[516][517][needs update] Viral neutralisation is the anticipated mechanism of action by which passive antibody therapy can mediate defence against SARS-CoV-2. The spike protein of SARS-CoV-2 is the primary target for neutralising antibodies.[518] As of 8 August 2020, eight neutralising antibodies targeting the spike protein of SARS-CoV-2 have entered clinical studies.[519] It has been proposed that selection of broad-neutralising antibodies against SARS-CoV-2 and SARS-CoV might be useful for treating not only COVID‑19 but also future SARS-related CoV infections.[518] Other mechanisms, however, such as antibody-dependant cellular cytotoxicity or phagocytosis, may be possible.[516] Other forms of passive antibody therapy, for example, using manufactured monoclonal antibodies, are in development.[516]
The use of passive antibodies to treat people with active COVID‑19 is also being studied. This involves the production of convalescent serum, which consists of the liquid portion of the blood from people who recovered from the infection and contains antibodies specific to this virus, which is then administered to active patients.[516] This strategy was tried for SARS with inconclusive results.[516] An updated Cochrane review in Feb 2023 found high certainty evidence that, for the treatment of people with moderate to severe COVID‑19, convalescent plasma did not reduce mortality or bring about symptom improvement.[517] There continues to be uncertainty about the safety of convalescent plasma administration to people with COVID‑19 and differing outcomes measured in different studies limits their use in determining efficacy.[517]
Since the outbreak of the COVID‑19 pandemic, scholars have explored the bioethics, normative economics, and political theories of healthcare policies related to the public health crisis.[520] Academics have pointed to the moral distress of healthcare workers, ethics of distributing scarce healthcare resources such as ventilators,[521] and the global justice of vaccine diplomacies.[citation needed] The socio-economic inequalities between genders,[522] races,[523] groups with disabilities,[524] communities,[525] regions, countries,[526] and continents have also drawn attention in academia and the general public.

Stroke (brain attack) is a medical condition in which poor blood flow to the brain causes cell death.[5] There are two main types of stroke: ischemic, due to lack of blood flow, and hemorrhagic, due to bleeding.[5] Both cause parts of the brain to stop functioning properly.[5]
Signs and symptoms of stroke may include an inability to move or feel on one side of the body, problems understanding or speaking, dizziness, or loss of vision to one side.[2][3] Signs and symptoms often appear soon after the stroke has occurred.[3] If symptoms last less than one or two hours, the stroke is a transient ischemic attack (TIA), also called a mini-stroke.[3] Hemorrhagic stroke may also be associated with a severe headache.[3] The symptoms of stroke can be permanent.[5] Long-term complications may include pneumonia and loss of bladder control.[3]
The biggest risk factor for stroke is high blood pressure.[7] Other risk factors include high blood cholesterol, tobacco smoking, obesity, diabetes mellitus, a previous TIA, end-stage kidney disease, and atrial fibrillation.[2][7][8] Ischemic stroke is typically caused by blockage of a blood vessel, though there are also less common causes.[13][14][15] Hemorrhagic stroke is caused by either bleeding directly into the brain or into the space between the brain's membranes.[13][16] Bleeding may occur due to a ruptured brain aneurysm.[13] Diagnosis is typically based on a physical exam and supported by medical imaging such as a CT scan or MRI scan.[9] A CT scan can rule out bleeding, but may not necessarily rule out ischemia, which early on typically does not show up on a CT scan.[10] Other tests such as an electrocardiogram (ECG) and blood tests are done to determine risk factors and rule out other possible causes.[9] Low blood sugar may cause similar symptoms.[9]
Prevention includes decreasing risk factors, surgery to open up the arteries to the brain in those with problematic carotid narrowing, and warfarin in people with atrial fibrillation.[2] Aspirin or statins may be recommended by physicians for prevention.[2] Stroke or TIA often requires emergency care.[5] Ischemic stroke, if detected within three to four-and-a-half hours, may be treatable with a medication that can break down the clot.[2] Some cases of hemorrhagic stroke benefit from surgery.[2] Treatment to attempt recovery of lost function is called stroke rehabilitation, and ideally takes place in a stroke unit; however, these are not available in much of the world.[2]
In 2013, approximately 6.9 million people had ischemic stroke and 3.4 million people had hemorrhagic stroke.[17] In 2015, there were about 42.4 million people who had previously had stroke and were still alive.[11] Between 1990 and 2010 the annual incidence of stroke decreased by approximately 10% in the developed world, but increased by 10% in the developing world.[18] In 2015, stroke was the second most frequent cause of death after coronary artery disease, accounting for 6.3 million deaths (11% of the total).[12] About 3.0 million deaths resulted from ischemic stroke while 3.3 million deaths resulted from hemorrhagic stroke.[12] About half of people who have had stroke live less than one year.[2] Overall, two thirds of cases of stroke occurred in those over 65 years old.[18]
Stroke can be classified into two major categories: ischemic and hemorrhagic.[19] Ischemic stroke is caused by interruption of the blood supply to the brain, while hemorrhagic stroke results from the rupture of a blood vessel or an abnormal vascular structure. About 87% of stroke is ischemic, with the rest being hemorrhagic. Bleeding can develop inside areas of ischemia, a condition known as "hemorrhagic transformation." It is unknown how many cases of hemorrhagic stroke actually start as ischemic stroke.[2]
In the 1970s the World Health Organization defined "stroke" as a "neurological deficit of cerebrovascular cause that persists beyond 24 hours or is interrupted by death within 24 hours",[20] although the word "stroke" is centuries old. This definition was supposed to reflect the reversibility of tissue damage and was devised for the purpose, with the time frame of 24 hours being chosen arbitrarily. The 24-hour limit divides stroke from transient ischemic attack, which is a related syndrome of stroke symptoms that resolve completely within 24 hours.[2] With the availability of treatments that can reduce stroke severity when given early, many now prefer alternative terminology, such as "brain attack" and "acute ischemic cerebrovascular syndrome" (modeled after heart attack and acute coronary syndrome, respectively), to reflect the urgency of stroke symptoms and the need to act swiftly.[21]
During ischemic stroke, blood supply to part of the brain is decreased, leading to dysfunction of the brain tissue in that area. There are four reasons why this might happen:
Stroke without an obvious explanation is termed cryptogenic stroke (idiopathic); this constitutes 30–40% of all cases of ischemic stroke.[2][24]
There are various classification systems for acute ischemic stroke. The Oxford Community Stroke Project classification (OCSP, also known as the Bamford or Oxford classification) relies primarily on the initial symptoms; based on the extent of the symptoms, the stroke episode is classified as total anterior circulation infarct (TACI), partial anterior circulation infarct (PACI), lacunar infarct (LACI) or posterior circulation infarct (POCI). These four entities predict the extent of the stroke, the area of the brain that is affected, the underlying cause, and the prognosis.[25][26] The TOAST (Trial of Org 10172 in Acute Stroke Treatment) classification is based on clinical symptoms as well as results of further investigations; on this basis, stroke is classified as being due to (1) thrombosis or embolism due to atherosclerosis of a large artery, (2) an embolism originating in the heart, (3) complete blockage of a small blood vessel, (4) other determined cause, (5) undetermined cause (two possible causes, no cause identified, or incomplete investigation).[27] Users of stimulants such as cocaine and methamphetamine are at a high risk for ischemic stroke.[28]
There are two main types of hemorrhagic stroke:[29][30]
The above two main types of hemorrhagic stroke are also two different forms of intracranial hemorrhage, which is the accumulation of blood anywhere within the cranial vault; but the other forms of intracranial hemorrhage, such as epidural hematoma (bleeding between the skull and the dura mater, which is the thick outermost layer of the meninges that surround the brain) and subdural hematoma (bleeding in the subdural space), are not considered "hemorrhagic stroke".[31]
Hemorrhagic stroke may occur on the background of alterations to the blood vessels in the brain, such as cerebral amyloid angiopathy, cerebral arteriovenous malformation and an intracranial aneurysm, which can cause intraparenchymal or subarachnoid hemorrhage.[32]
In addition to neurological impairment, hemorrhagic stroke usually causes specific symptoms (for instance, subarachnoid hemorrhage classically causes a severe headache known as a thunderclap headache) or reveal evidence of a previous head injury.
Stroke symptoms typically start suddenly, over seconds to minutes, and in most cases do not progress further. The symptoms depend on the area of the brain affected. The more extensive the area of the brain affected, the more functions that are likely to be lost. Some forms of stroke can cause additional symptoms. For example, in intracranial hemorrhage, the affected area may compress other structures. Most forms of stroke are not associated with a headache, apart from subarachnoid hemorrhage and cerebral venous thrombosis and occasionally intracerebral hemorrhage.[32]
Various systems have been proposed to increase recognition of stroke. Different findings are able to predict the presence or absence of stroke to different degrees. Sudden-onset face weakness, arm drift (i.e., if a person, when asked to raise both arms, involuntarily lets one arm drift downward) and abnormal speech are the findings most likely to lead to the correct identification of a case of stroke, increasing the likelihood by 5.5 when at least one of these is present. Similarly, when all three of these are absent, the likelihood of stroke is decreased (– likelihood ratio of 0.39).[33] While these findings are not perfect for diagnosing stroke, the fact that they can be evaluated relatively rapidly and easily make them very valuable in the acute setting.
A mnemonic to remember the warning signs of stroke is FAST (facial droop, arm weakness, speech difficulty, and time to call emergency services),[34] as advocated by the Department of Health (United Kingdom) and the Stroke Association, the American Stroke Association, the National Stroke Association (US), the Los Angeles Prehospital Stroke Screen (LAPSS)[35] and the Cincinnati Prehospital Stroke Scale (CPSS).[36] Use of these scales is recommended by professional guidelines.[37] FAST is less reliable in the recognition of posterior circulation stroke.[38]
For people referred to the emergency room, early recognition of stroke is deemed important as this can expedite diagnostic tests and treatments. A scoring system called ROSIER (recognition of stroke in the emergency room) is recommended for this purpose; it is based on features from the medical history and physical examination.[37][39]
If the area of the brain affected includes one of the three prominent central nervous system pathways—the spinothalamic tract, corticospinal tract, and the dorsal column–medial lemniscus pathway, symptoms may include:
In most cases, the symptoms affect only one side of the body (unilateral). Depending on the part of the brain affected, the defect in the brain is usually on the opposite side of the body. However, since these pathways also travel in the spinal cord and any lesion there can also produce these symptoms, the presence of any one of these symptoms does not necessarily indicate stroke. In addition to the above CNS pathways, the brainstem gives rise to most of the twelve cranial nerves. A brainstem stroke affecting the brainstem and brain, therefore, can produce symptoms relating to deficits in these cranial nerves:[citation needed]
If the cerebral cortex is involved, the CNS pathways can again be affected, but can also produce the following symptoms:
If the cerebellum is involved, ataxia might be present and this includes:
Loss of consciousness, headache, and vomiting usually occur more often in hemorrhagic stroke than in thrombosis because of the increased intracranial pressure from the leaking blood compressing the brain.
If symptoms are maximal at onset, the cause is more likely to be a subarachnoid hemorrhage or an embolic stroke.
In thrombotic stroke, a thrombus[41] (blood clot) usually forms around atherosclerotic plaques. Since blockage of the artery is gradual, onset of symptomatic thrombotic stroke is slower than that of hemorrhagic stroke. A thrombus itself (even if it does not completely block the blood vessel) can lead to an embolic stroke (see below) if the thrombus breaks off and travels in the bloodstream, at which point it is called an embolus. Two types of thrombosis can cause stroke:
Anemia causes increase blood flow in the blood circulatory system. This causes the endothelial cells of the blood vessels to express adhesion factors which encourages the clotting of blood and formation of thrombus.[45] Sickle-cell anemia, which can cause blood cells to clump up and block blood vessels, can also lead to stroke. Stroke is the second leading cause of death in people under 20 with sickle-cell anemia.[46] Air pollution may also increase stroke risk.[47]
An embolic stroke refers to an arterial embolism (a blockage of an artery) by an embolus, a traveling particle or debris in the arterial bloodstream originating from elsewhere. An embolus is most frequently a thrombus, but it can also be a number of other substances including fat (e.g., from bone marrow in a broken bone), air, cancer cells or clumps of bacteria (usually from infectious endocarditis).[48]
Because an embolus arises from elsewhere, local therapy solves the problem only temporarily. Thus, the source of the embolus must be identified. Because the embolic blockage is sudden in onset, symptoms are usually maximal at the start. Also, symptoms may be transient as the embolus is partially resorbed and moves to a different location or dissipates altogether.
Emboli most commonly arise from the heart (especially in atrial fibrillation) but may originate from elsewhere in the arterial tree. In paradoxical embolism, a deep vein thrombosis embolizes through an atrial or ventricular septal defect in the heart into the brain.[48]
Causes of stroke related to the heart can be distinguished between high and low-risk:[49]
Among those who have a complete blockage of one of the carotid arteries, the risk of stroke on that side is about one percent per year.[50]
A special form of embolic stroke is the embolic stroke of undetermined source (ESUS). This subset of cryptogenic stroke is defined as a non-lacunar brain infarct without proximal arterial stenosis or cardioembolic sources. About one out of six cases of ischemic stroke could be classified as ESUS.[51]
Cerebral hypoperfusion is the reduction of blood flow to all parts of the brain. The reduction could be to a particular part of the brain depending on the cause. It is most commonly due to heart failure from cardiac arrest or arrhythmias, or from reduced cardiac output as a result of myocardial infarction, pulmonary embolism, pericardial effusion, or bleeding.[citation needed] Hypoxemia (low blood oxygen content) may precipitate the hypoperfusion. Because the reduction in blood flow is global, all parts of the brain may be affected, especially vulnerable "watershed" areas—border zone regions supplied by the major cerebral arteries. A watershed stroke refers to the condition when the blood supply to these areas is compromised. Blood flow to these areas does not necessarily stop, but instead it may lessen to the point where brain damage can occur.
Cerebral venous sinus thrombosis leads to stroke due to locally increased venous pressure, which exceeds the pressure generated by the arteries. Infarcts are more likely to undergo hemorrhagic transformation (leaking of blood into the damaged area) than other types of ischemic stroke.[23]
It generally occurs in small arteries or arterioles and is commonly due to hypertension,[52] intracranial vascular malformations (including cavernous angiomas or arteriovenous malformations), cerebral amyloid angiopathy, or infarcts into which secondary hemorrhage has occurred.[2] Other potential causes are trauma, bleeding disorders, amyloid angiopathy, illicit drug use (e.g., amphetamines or cocaine). The hematoma enlarges until pressure from surrounding tissue limits its growth, or until it decompresses by emptying into the ventricular system, CSF or the pial surface. A third of intracerebral bleed is into the brain's ventricles. ICH has a mortality rate of 44 percent after 30 days, higher than ischemic stroke or subarachnoid hemorrhage (which technically may also be classified as a type of stroke[2]).
Other causes may include spasm of an artery. This may occur due to cocaine.[53]
Silent stroke is stroke that does not have any outward symptoms, and people are typically unaware they had experienced stroke. Despite not causing identifiable symptoms, silent stroke still damages the brain and places the person at increased risk for both transient ischemic attack and major stroke in the future. Conversely, those who have had major stroke are also at risk of having silent stroke.[54] In a broad study in 1998, more than 11 million people were estimated to have experienced stroke in the United States. Approximately 770,000 of these were symptomatic and 11 million were first-ever silent MRI infarcts or hemorrhages. Silent stroke typically causes lesions which are detected via the use of neuroimaging such as MRI. Silent stroke is estimated to occur at five times the rate of symptomatic stroke.[55][56] The risk of silent stroke increases with age, but they may also affect younger adults and children, especially those with acute anemia.[55][57]
Ischemic stroke occurs because of a loss of blood supply to part of the brain, initiating the ischemic cascade.[58] Atherosclerosis may disrupt the blood supply by narrowing the lumen of blood vessels leading to a reduction of blood flow by causing the formation of blood clots within the vessel or by releasing showers of small emboli through the disintegration of atherosclerotic plaques.[59] Embolic infarction occurs when emboli formed elsewhere in the circulatory system, typically in the heart as a consequence of atrial fibrillation, or in the carotid arteries, break off, enter the cerebral circulation, then lodge in and block brain blood vessels. Since blood vessels in the brain are now blocked, the brain becomes low in energy, and thus it resorts to using anaerobic metabolism within the region of brain tissue affected by ischemia. Anaerobic metabolism produces less adenosine triphosphate (ATP) but releases a by-product called lactic acid. Lactic acid is an irritant which could potentially destroy cells since it is an acid and disrupts the normal acid-base balance in the brain. The ischemia area is referred to as the "ischemic penumbra".[60] After the initial ischemic event the penumbra transitions from a tissue remodeling characterized by damage to a remodeling characterized by repair.[61]
As oxygen or glucose becomes depleted in ischemic brain tissue, the production of high energy phosphate compounds such as adenosine triphosphate (ATP) fails, leading to failure of energy-dependent processes (such as ion pumping) necessary for tissue cell survival. This sets off a series of interrelated events that result in cellular injury and death. A major cause of neuronal injury is the release of the excitatory neurotransmitter glutamate. The concentration of glutamate outside the cells of the nervous system is normally kept low by so-called uptake carriers, which are powered by the concentration gradients of ions (mainly Na+) across the cell membrane. However, stroke cuts off the supply of oxygen and glucose which powers the ion pumps maintaining these gradients. As a result, the transmembrane ion gradients run down, and glutamate transporters reverse their direction, releasing glutamate into the extracellular space. Glutamate acts on receptors in nerve cells (especially NMDA receptors), producing an influx of calcium which activates enzymes that digest the cells' proteins, lipids, and nuclear material. Calcium influx can also lead to the failure of mitochondria, which can lead further toward energy depletion and may trigger cell death due to programmed cell death.[62]
Ischemia also induces production of oxygen free radicals and other reactive oxygen species. These react with and damage a number of cellular and extracellular elements. Damage to the blood vessel lining or endothelium may occur. These processes are the same for any type of ischemic tissue and are referred to collectively as the ischemic cascade. However, brain tissue is especially vulnerable to ischemia since it has little respiratory reserve and is completely dependent on aerobic metabolism, unlike most other organs.
The brain can compensate inadequate blood flow in a single artery by the collateral system. This system relies on the efficient connection between the carotid and vertebral arteries through the circle of Willis and, to a lesser extent, the major arteries supplying the cerebral hemispheres. However, variations in the circle of Willis, caliber of collateral vessels, and acquired arterial lesions such as atherosclerosis can disrupt this compensatory mechanism, increasing the risk of brain ischemia resulting from artery blockage.[63]
The extent of damage depends on the duration and severity of the ischemia. If ischemia persists for more than 5 minutes with perfusion below 5% of normal, some neurons will die. However, if ischemia is mild, the damage will occur slowly and may take up to 6 hours to completely destroy the brain tissue. In case of severe ischemia lasting more than 15 to 30 minutes, all of the affected tissue will die, leading to infarction. The rate of damage is affected by temperature, with hyperthermia accelerating damage and hypothermia slowing it down and various other factors. Prompt restoration of blood flow to ischemic tissues can reduce or reverse injury, especially if the tissues are not yet irreversibly damaged. This is particularly important for the moderately ischemic areas (penumbras) surrounding areas of severe ischemia, which may still be salvageable due to collateral flow.[63][64][65]
Hemorrhagic stroke is classified based on their underlying pathology. Some causes of hemorrhagic stroke are hypertensive hemorrhage, ruptured aneurysm, ruptured AV fistula, transformation of prior ischemic infarction, and drug-induced bleeding.[66] They result in tissue injury by causing compression of tissue from an expanding hematoma or hematomas. In addition, the pressure may lead to a loss of blood supply to affected tissue with resulting infarction, and the blood released by brain hemorrhage appears to have direct toxic effects on brain tissue and vasculature.[46][67] Inflammation contributes to the secondary brain injury after hemorrhage.[67]
Stroke is diagnosed through several techniques: a neurological examination (such as the NIHSS), CT scans (most often without contrast enhancements) or MRI scans, Doppler ultrasound, and arteriography. The diagnosis of stroke itself is clinical, with assistance from the imaging techniques. Imaging techniques also assist in determining the subtypes and cause of stroke. There is yet no commonly used blood test for the stroke diagnosis itself, though blood tests may be of help in finding out the likely cause of stroke.[68] In deceased people, an autopsy of stroke may help establishing the time between stroke onset and death.
A physical examination, including taking a medical history of the symptoms and a neurological status, helps giving an evaluation of the location and severity of stroke. It can give a standard score on e.g., the NIH stroke scale.
For diagnosing ischemic (blockage) stroke in the emergency setting:[69]
For diagnosing hemorrhagic stroke in the emergency setting:
For detecting chronic hemorrhages, an MRI scan is more sensitive.[70]
For the assessment of stable stroke, nuclear medicine scans such as single-photon emission computed tomography (SPECT) and positron emission tomography–computed tomography (PET/CT) may be helpful. SPECT documents cerebral blood flow, whereas PET with an FDG isotope shows cerebral glucose metabolism.
CT scans may not detect ischemic stroke, especially if it is small, of recent onset,[10] or in the brainstem or cerebellum areas (posterior circulation infarct). MRI is better at detecting a posterior circulation infarct with diffusion-weighted imaging.[71] A CT scan is used more to rule out certain stroke mimics and detect bleeding.[10] The presence of leptomeningeal collateral circulation in the brain is associated with better clinical outcomes after recanalization treatment.[72] Cerebrovascular reserve capacity is another factor that affects stroke outcome – it is the amount of increase in cerebral blood flow after a purposeful stimulation of blood flow by the physician, such as by giving inhaled carbon dioxide or intravenous acetazolamide. The increase in blood flow can be measured by PET scan or transcranial doppler sonography.[73] However, in people with obstruction of the internal carotid artery of one side, the presence of leptomeningeal collateral circulation is associated with reduced cerebral reserve capacity.[74]
When stroke has been diagnosed, various other studies may be performed to determine the underlying cause. With the current treatment and diagnosis options available, it is of particular importance to determine whether there is a peripheral source of emboli. Test selection may vary since the cause of stroke varies with age, comorbidity and the clinical presentation. The following are commonly used techniques:
For hemorrhagic stroke, a CT or MRI scan with intravascular contrast may be able to identify abnormalities in the brain arteries (such as aneurysms) or other sources of bleeding, and structural MRI if this shows no cause. If this too does not identify an underlying reason for the bleeding, invasive cerebral angiography could be performed but this requires access to the bloodstream with an intravascular catheter and can cause further stroke as well as complications at the insertion site and this investigation is therefore reserved for specific situations.[75] If there are symptoms suggesting that the hemorrhage might have occurred as a result of venous thrombosis, CT or MRI venography can be used to examine the cerebral veins.[75]
Among people with ischemic stroke, misdiagnosis occurs 2 to 26% of the time.[76] A "stroke chameleon" (SC) is stroke which is diagnosed as something else.[76][77]
People not having stroke may also be misdiagnosed with the condition. Giving thrombolytics (clot-busting) in such cases causes intracerebral bleeding 1 to 2% of the time, which is less than that of people with stroke. This unnecessary treatment adds to health care costs. Even so, the AHA/ASA guidelines state that starting intravenous tPA in possible mimics is preferred to delaying treatment for additional testing.[76]
Women, African-Americans, Hispanic-Americans, Asian and Pacific Islanders are more often misdiagnosed for a condition other than stroke when in fact having stroke. In addition, adults under 44 years of age are seven times more likely to have stroke missed than are adults over 75 years of age. This is especially the case for younger people with posterior circulation infarcts.[76] Some medical centers have used hyperacute MRI in experimental studies for people initially thought to have a low likelihood of stroke, and in some of these people, stroke has been found which were then treated with thrombolytic medication.[76]
Given the disease burden of stroke, prevention is an important public health concern.[78] Primary prevention is less effective than secondary prevention (as judged by the number needed to treat to prevent one stroke per year).[78] Recent guidelines detail the evidence for primary prevention in stroke.[79] In those who are otherwise healthy, aspirin does not appear beneficial and thus is not recommended.[80] In people who have had a myocardial infarction or those with a high cardiovascular risk, it provides some protection against a first stroke.[81][82] In those who have previously had stroke, treatment with medications such as aspirin, clopidogrel, and dipyridamole may be beneficial.[81] The U.S. Preventive Services Task Force (USPSTF) recommends against screening for carotid artery stenosis in those without symptoms.[83]
The most important modifiable risk factors for stroke are high blood pressure and atrial fibrillation although the size of the effect is small; 833 people have to be treated for 1 year to prevent one stroke.[84][85] Other modifiable risk factors include high blood cholesterol levels, diabetes mellitus, end-stage kidney disease,[8] cigarette smoking[86][87] (active and passive), heavy alcohol use,[88] drug use,[89] lack of physical activity, obesity, processed red meat consumption,[90] and unhealthy diet.[91] Smoking just one cigarette per day increases the risk more than 30%.[92] Alcohol use could predispose to ischemic stroke, as well as intracerebral and subarachnoid hemorrhage via multiple mechanisms (for example, via hypertension, atrial fibrillation, rebound thrombocytosis and platelet aggregation and clotting disturbances).[93] Drugs, most commonly amphetamines and cocaine, can induce stroke through damage to the blood vessels in the brain and acute hypertension.[66][94] Migraine with aura doubles a person's risk for ischemic stroke.[95][96] Untreated, celiac disease regardless of the presence of symptoms can be an underlying cause of stroke, both in children and adults.[97] According to a 2021 WHO study, working 55+ hours a week raises the risk of stroke by 35% and the risk of dying from heart conditions by 17%, when compared to a 35-40 hours week.[98]
High levels of physical activity reduce the risk of stroke by about 26%.[99] There is a lack of high quality studies looking at promotional efforts to improve lifestyle factors.[100] Nonetheless, given the large body of circumstantial evidence, best medical management for stroke includes advice on diet, exercise, smoking and alcohol use.[101] Medication is the most common method of stroke prevention; carotid endarterectomy can be a useful surgical method of preventing stroke.
High blood pressure accounts for 35–50% of stroke risk.[102] Blood pressure reduction of 10 mmHg systolic or 5 mmHg diastolic reduces the risk of stroke by ~40%.[103] Lowering blood pressure has been conclusively shown to prevent both ischemic and hemorrhagic stroke.[104][105] It is equally important in secondary prevention.[106] Even people older than 80 years and those with isolated systolic hypertension benefit from antihypertensive therapy.[107][108][109] The available evidence does not show large differences in stroke prevention between antihypertensive drugs—therefore, other factors such as protection against other forms of cardiovascular disease and cost should be considered.[110][111] The routine use of beta-blockers following stroke or TIA has not been shown to result in benefits.[112]
High cholesterol levels have been inconsistently associated with (ischemic) stroke.[105][113] Statins have been shown to reduce the risk of stroke by about 15%.[114] Since earlier meta-analyses of other lipid-lowering drugs did not show a decreased risk,[115] statins might exert their effect through mechanisms other than their lipid-lowering effects.[114]
Diabetes mellitus increases the risk of stroke by 2 to 3 times.[clarification needed][citation needed] While intensive blood sugar control has been shown to reduce small blood vessel complications such as kidney damage and damage to the retina of the eye it has not been shown to reduce large blood vessel complications such as stroke.[116][117]
Oral anticoagulants such as warfarin have been the mainstay of stroke prevention for over 50 years. However, several studies have shown that aspirin and other antiplatelets are highly effective in secondary prevention after stroke or transient ischemic attack.[81] Low doses of aspirin (for example 75–150 mg) are as effective as high doses but have fewer side effects; the lowest effective dose remains unknown.[118] Thienopyridines (clopidogrel, ticlopidine) might be slightly more effective than aspirin and have a decreased risk of gastrointestinal bleeding, but are more expensive.[119] Both aspirin and clopidogrel may be useful in the first few weeks after a minor stroke or high risk TIA.[120] Clopidogrel has less side effects than ticlopidine.[119] Dipyridamole can be added to aspirin therapy to provide a small additional benefit, even though headache is a common side effect.[121] Low-dose aspirin is also effective for stroke prevention after having a myocardial infarction.[82]
Those with atrial fibrillation have a 5% a year risk of stroke, and this risk is higher in those with valvular atrial fibrillation.[122] Depending on the stroke risk, anticoagulation with medications such as warfarin or aspirin is useful for prevention with various levels of comparative effectiveness depending on the type of treatment used.[123][124] Oral anticoagulants, especially Xa (apixaban) and thrombin (dabigatran) inhibitors have been shown to be superior to warfarin in stroke reduction and have a lower or similar bleeding risk in patients with atrial fibrillation.[124] Except in people with atrial fibrillation, oral anticoagulants are not advised for stroke prevention—any benefit is offset by bleeding risk.[125]
In primary prevention, however, antiplatelet drugs did not reduce the risk of ischemic stroke but increased the risk of major bleeding.[126][127] Further studies are needed to investigate a possible protective effect of aspirin against ischemic stroke in women.[128][129]
Carotid endarterectomy or carotid angioplasty can be used to remove atherosclerotic narrowing of the carotid artery. There is evidence supporting this procedure in selected cases.[101] Endarterectomy for a significant stenosis has been shown to be useful in preventing further stroke in those who have already had the condition.[130] Carotid artery stenting has not been shown to be equally useful.[131][132] People are selected for surgery based on age, gender, degree of stenosis, time since symptoms and the person's preferences.[101] Surgery is most efficient when not delayed too long—the risk of recurrent stroke in a person who has a 50% or greater stenosis is up to 20% after 5 years, but endarterectomy reduces this risk to around 5%. The number of procedures needed to cure one person was 5 for early surgery (within two weeks after the initial stroke), but 125 if delayed longer than 12 weeks.[133][134]
Screening for carotid artery narrowing has not been shown to be a useful test in the general population.[135] Studies of surgical intervention for carotid artery stenosis without symptoms have shown only a small decrease in the risk of stroke.[136][137] To be beneficial, the complication rate of the surgery should be kept below 4%. Even then, for 100 surgeries, 5 people will benefit by avoiding stroke, 3 will develop stroke despite surgery, 3 will develop stroke or die due to the surgery itself, and 89 will remain stroke-free but would also have done so without intervention.[101]
Nutrition, specifically the Mediterranean-style diet, has the potential for decreasing the risk of having stroke by more than half.[138] It does not appear that lowering levels of homocysteine with folic acid affects the risk of stroke.[139][140]
A number of specific recommendations have been made for women including taking aspirin after the 11th week of pregnancy if there is a history of previous chronic high blood pressure and taking blood pressure medications during pregnancy if the blood pressure is greater than 150 mmHg systolic or greater than 100 mmHg diastolic. In those who have previously had preeclampsia other risk factors should be treated more aggressively.[141]
Keeping blood pressure below 140/90 mmHg is recommended.[142] Anticoagulation can prevent recurrent ischemic stroke. Among people with nonvalvular atrial fibrillation, anticoagulation can reduce stroke by 60% while antiplatelet agents can reduce stroke by 20%.[143] However, a recent meta-analysis suggests harm from anticoagulation started early after an embolic stroke.[144][145] Stroke prevention treatment for atrial fibrillation is determined according to the CHA2DS2–VASc score. The most widely used anticoagulant to prevent thromboembolic stroke in people with nonvalvular atrial fibrillation is the oral agent warfarin while a number of newer agents including dabigatran are alternatives which do not require prothrombin time monitoring.[142]
Anticoagulants, when used following stroke, should not be stopped for dental procedures.[146]
If studies show carotid artery stenosis, and the person has a degree of residual function on the affected side, carotid endarterectomy (surgical removal of the stenosis) may decrease the risk of recurrence if performed rapidly after stroke.
Aspirin reduces the overall risk of recurrence by 13% with greater benefit early on.[147] Definitive therapy within the first few hours is aimed at removing the blockage by breaking the clot down (thrombolysis), or by removing it mechanically (thrombectomy). The philosophical premise underlying the importance of rapid stroke intervention was summed up as Time is Brain! in the early 1990s.[148] Years later, that same idea, that rapid cerebral blood flow restoration results in fewer brain cells dying, has been proved and quantified.[149]
Tight blood sugar control in the first few hours does not improve outcomes and may cause harm.[150] High blood pressure is also not typically lowered as this has not been found to be helpful.[151][152] Cerebrolysin, a mixture of pig brain-derived neurotrophic factors used widely to treat acute ischemic stroke in China, Eastern Europe, Russia, post-Soviet countries, and other Asian countries, does not improve outcomes or prevent death and may increase the risk of severe adverse events.[153] There is also no evidence that cerebrolysin‐like peptide mixtures which are extracted from cattle brain is helpful in treating acute ischemic stroke.[153]
Thrombolysis, such as with recombinant tissue plasminogen activator (rtPA), in acute ischemic stroke, when given within three hours of symptom onset, results in an overall benefit of 10% with respect to living without disability.[154][155] It does not, however, improve chances of survival.[154] Benefit is greater the earlier it is used.[154] Between three and four and a half hours the effects are less clear.[156][157][158] The AHA/ASA recommend it for certain people in this time frame.[159] A 2014 review found a 5% increase in the number of people living without disability at three to six months; however, there was a 2% increased risk of death in the short term.[155] After four and a half hours thrombolysis worsens outcomes.[156] These benefits or lack of benefits occurred regardless of the age of the person treated.[160] There is no reliable way to determine who will have an intracranial bleed post-treatment versus who will not.[161] In those with findings of savable tissue on medical imaging between 4.5 hours and 9 hours or who wake up with stroke, alteplase results in some benefit.[162]
Its use is endorsed by the American Heart Association, the American College of Emergency Physicians and the American Academy of Neurology as the recommended treatment for acute stroke within three hours of onset of symptoms as long as there are no other contraindications (such as abnormal lab values, high blood pressure, or recent surgery). This position for tPA is based upon the findings of two studies by one group of investigators[163] which showed that tPA improves the chances for a good neurological outcome. When administered within the first three hours thrombolysis improves functional outcome without affecting mortality.[164] 6.4% of people with large stroke developed substantial brain bleeding as a complication from being given tPA thus part of the reason for increased short term mortality.[165] The American Academy of Emergency Medicine had previously stated that objective evidence regarding the applicability of tPA for acute ischemic stroke was insufficient.[166] In 2013 the American College of Emergency Medicine refuted this position,[167] acknowledging the body of evidence for the use of tPA in ischemic stroke;[168] but debate continues.[169][170] Intra-arterial fibrinolysis, where a catheter is passed up an artery into the brain and the medication is injected at the site of thrombosis, has been found to improve outcomes in people with acute ischemic stroke.[171]
Mechanical removal of the blood clot causing the ischemic stroke, called mechanical thrombectomy, is a potential treatment for occlusion of a large artery, such as the middle cerebral artery. In 2015, one review demonstrated the safety and efficacy of this procedure if performed within 12 hours of the onset of symptoms.[172][173] It did not change the risk of death, but reduced disability compared to the use of intravenous thrombolysis which is generally used in people evaluated for mechanical thrombectomy.[174][175] Certain cases may benefit from thrombectomy up to 24 hours after the onset of symptoms.[176]
Stroke affecting large portions of the brain can cause significant brain swelling with secondary brain injury in surrounding tissue. This phenomenon is mainly encountered in stroke affecting brain tissue dependent upon the middle cerebral artery for blood supply and is also called "malignant cerebral infarction" because it carries a dismal prognosis. Relief of the pressure may be attempted with medication, but some require hemicraniectomy, the temporary surgical removal of the skull on one side of the head. This decreases the risk of death, although some people – who would otherwise have died – survive with disability.[177][178]
People with intracerebral hemorrhage require supportive care, including blood pressure control if required. People are monitored for changes in the level of consciousness, and their blood sugar and oxygenation are kept at optimum levels. Anticoagulants and antithrombotics can make bleeding worse and are generally discontinued (and reversed if possible).[citation needed] A proportion may benefit from neurosurgical intervention to remove the blood and treat the underlying cause, but this depends on the location and the size of the hemorrhage as well as patient-related factors, and ongoing research is being conducted into the question as to which people with intracerebral hemorrhage may benefit.[179]
In subarachnoid hemorrhage, early treatment for underlying cerebral aneurysms may reduce the risk of further hemorrhages. Depending on the site of the aneurysm this may be by surgery that involves opening the skull or endovascularly (through the blood vessels).[180]
Ideally, people who have had stroke are admitted to a "stroke unit", a ward or dedicated area in a hospital staffed by nurses and therapists with experience in stroke treatment. It has been shown that people admitted to stroke units have a higher chance of surviving than those admitted elsewhere in hospital, even if they are being cared for by doctors without experience in stroke.[2][181] Nursing care is fundamental in maintaining skin care, feeding, hydration, positioning, and monitoring vital signs such as temperature, pulse, and blood pressure.[182]
Stroke rehabilitation is the process by which those with disabling stroke undergo treatment to help them return to normal life as much as possible by regaining and relearning the skills of everyday living. It also aims to help the survivor understand and adapt to difficulties, prevent secondary complications, and educate family members to play a supporting role. Stroke rehabilitation should begin almost immediately with a multidisciplinary approach. The rehabilitation team may involve physicians trained in rehabilitation medicine, neurologists, clinical pharmacists, nursing staff, physiotherapists, occupational therapists, speech-language pathologists, and orthotists. Some teams may also include psychologists and social workers, since at least one-third of affected people manifests post stroke depression. Validated instruments such as the Barthel scale may be used to assess the likelihood of a person who has had stroke being able to manage at home with or without support subsequent to discharge from a hospital.[183]
Stroke rehabilitation should be started as quickly as possible and can last anywhere from a few days to over a year. Most return of function is seen in the first few months, and then improvement falls off with the "window" considered officially by U.S. state rehabilitation units and others to be closed after six months, with little chance of further improvement.[medical citation needed] However, some people have reported that they continue to improve for years, regaining and strengthening abilities like writing, walking, running, and talking.[medical citation needed] Daily rehabilitation exercises should continue to be part of the daily routine for people who have had stroke. Complete recovery is unusual but not impossible and most people will improve to some extent: proper diet and exercise are known to help the brain to recover.
The current body of evidence is uncertain on the efficacy of cognitive rehabilitation for reducing the disabling effects of neglect and increasing independence remains unproven.[184] However, there is limited evidence that cognitive rehabilitation may have an immediate beneficial effect on tests of neglect.[184] Overall, no rehabilitation approach can be supported by evidence for spatial neglect.
The current body of evidence is uncertain whether the use of rehabilitation can improve on-road driving skills following stroke.[185] There is limited evidence that training on a driving simulator will improve performance on recognizing road signs after training.[185] The findings are based on low-quality evidence as further research is needed involving large numbers of participants.
Based on low quality evidence, it is currently uncertain whether yoga has a significant benefit for stroke rehabilitation on measures of quality of life, balance, strength, endurance, pain, and disability scores.[186] Yoga may reduce anxiety and could be included as part of patient-centred stroke rehabilitation.[186] Further research is needed assessing the benefits and safety of yoga in stroke rehabilitation.
Low-quality evidence suggests that action observation (a type of physiotherapy that is meant to improve neural plasticity through the mirror-neuronal system) may be of some benefit and has no significant adverse effects, however this benefit may not be clinically significant and further research is suggested.[187]
The current body of scientific evidence is uncertain on the effectiveness of cognitive rehabilitation for attention deficits in patients following stroke.[188] While there may be an immediate effect after treatment on attention, the findings are based on low to moderate quality and small number of studies.[188] Further research is needed to assess whether the effect can be sustained in day-to-day tasks requiring attention.
The latest evidence supports the short-term benefits of motor imagery (MI) on walking speed in individuals who have had stroke, in comparison to other therapies.[189] MI does not improve motor function after stroke and does not seem to cause significant adverse events.[189] The findings are based on low-quality evidence as further research is needed to estimate the effect of MI on walking endurance and the dependence on personal assistance.
Physical and occupational therapy have overlapping areas of expertise; however, physical therapy focuses on joint range of motion and strength by performing exercises and relearning functional tasks such as bed mobility, transferring, walking and other gross motor functions. Physiotherapists can also work with people who have had stroke to improve awareness and use of the hemiplegic side. Rehabilitation involves working on the ability to produce strong movements or the ability to perform tasks using normal patterns. Emphasis is often concentrated on functional tasks and people's goals. One example physiotherapists employ to promote motor learning involves constraint-induced movement therapy. Through continuous practice the person relearns to use and adapt the hemiplegic limb during functional activities to create lasting permanent changes.[190] Physical therapy is effective for recovery of function and mobility after stroke.[191] Occupational therapy is involved in training to help relearn everyday activities known as the activities of daily living (ADLs) such as eating, drinking, dressing, bathing, cooking, reading and writing, and toileting. Approaches to helping people with urinary incontinence include physical therapy, cognitive therapy, and specialized interventions with experienced medical professionals, however, it is not clear how effective these approaches are at improving urinary incontinence following stroke.[192]
Treatment of spasticity related to stroke often involves early mobilizations, commonly performed by a physiotherapist, combined with elongation of spastic muscles and sustained stretching through various different positions.[40] Gaining initial improvement in range of motion is often achieved through rhythmic rotational patterns associated with the affected limb.[40] After full range has been achieved by the therapist, the limb should be positioned in the lengthened positions to prevent against further contractures, skin breakdown, and disuse of the limb with the use of splints or other tools to stabilize the joint.[40] Cold ice wraps or ice packs may briefly relieve spasticity by temporarily reducing neural firing rates.[40] Electrical stimulation to the antagonist muscles or vibrations has also been used with some success.[40] Physical therapy is sometimes suggested for people who experience sexual dysfunction following stroke.[193]
With the prevalence of vision problems increasing with age in stroke patients, the overall effect of interventions for age-related visual problems is currently uncertain. It is also not sure whether people with stroke respond differently from the general population when treating eye problems.[194] Further research in this area is needed as current body of evidence is very low quality.
Speech and language therapy is appropriate for people with the speech production disorders: dysarthria[195] and apraxia of speech,[196] aphasia,[197] cognitive-communication impairments, and problems with swallowing. 
Speech and language therapy for aphasia following stroke improves functional communication, reading, writing and expressive language. Speech and language therapy that is higher intensity, higher dose or provided over a long duration of time leads to significantly better functional communication but people might be more likely to drop out of high intensity treatment (up to 15 hours per week).[198] A total of 20-50 hours of speech and language therapy is necessary for the best recovery. The most improvement happens when 2-5 hours of therapy is provided each week over 4-5 days. Recovery is further improved when besides the therapy people practice tasks at home.[199][200] Speech and language therapy is also effective if it is delivered online through video or by a family member who has been trained by a professional therapist.[199][200]
Recovery with therapy for aphasia is also dependent on the recency of stroke and the age of the person. Receiving therapy within a month after the stroke leads to the greatest improvements. 3 or 6 months after the stroke more therapy will be needed but symptoms can still be improved. People with aphasia who are younger than 55 years are the most likely to improve but people older than 75 years can still get better with therapy.[201][202]
People who have had stroke may have particular problems, such as dysphagia, which can cause swallowed material to pass into the lungs and cause aspiration pneumonia. The condition may improve with time, but in the interim, a nasogastric tube may be inserted, enabling liquid food to be given directly into the stomach. If swallowing is still deemed unsafe, then a percutaneous endoscopic gastrostomy (PEG) tube is passed and this can remain indefinitely. Swallowing therapy has mixed results as of 2018.[203]
Often, assistive technology such as wheelchairs, walkers and canes may be beneficial. Many mobility problems can be improved by the use of ankle foot orthoses.[204]
Stroke can also reduce people's general fitness.[205] Reduced fitness can reduce capacity for rehabilitation as well as general health.[206] Physical exercises as part of a rehabilitation program following stroke appear safe.[205] Cardiorespiratory fitness training that involves walking in rehabilitation can improve speed, tolerance and independence during walking, and may improve balance.[205] There are inadequate long-term data about the effects of exercise and training on death, dependence and disability after stroke.[205] The future areas of research may concentrate on the optimal exercise prescription and long-term health benefits of exercise. The effect of physical training on cognition also may be studied further.
The ability to walk independently in their community, indoors or outdoors, is important following stroke. Although no negative effects have been reported, it is unclear if outcomes can improve with these walking programs when compared to usual treatment.[207]
Some current and future therapy methods include the use of virtual reality and video games for rehabilitation. These forms of rehabilitation offer potential for motivating people to perform specific therapy tasks that many other forms do not.[208] While virtual reality and interactive video gaming are not more effective than conventional therapy for improving upper limb function, when used in conjunction with usual care these approaches may improve upper limb function and ADL function.[209] There are inadequate data on the effect of virtual reality and interactive video gaming on gait speed, balance, participation and quality of life.[209] Many clinics and hospitals are adopting the use of these off-the-shelf devices for exercise, social interaction, and rehabilitation because they are affordable, accessible and can be used within the clinic and home.[208]
Mirror therapy is associated with improved motor function of the upper extremity in people who have had stroke.[210]
Other non-invasive rehabilitation methods used to augment physical therapy of motor function in people recovering from stroke include transcranial magnetic stimulation and transcranial direct-current stimulation.[211] and robotic therapies.[212] Constraint‐induced movement therapy (CIMT), mental practice, mirror therapy, interventions for sensory impairment, virtual reality and a relatively high dose of repetitive task practice may be effective in improving upper limb function. However, further primary research, specifically of CIMT, mental practice, mirror therapy and virtual reality is needed.[213]
Clinical studies confirm the importance of orthoses in stroke rehabilitation.[214][215][216] The orthosis supports the therapeutic applications and also helps to mobilize the patient at an early stage. With the help of an orthosis, physiological standing and walking can be learned again, and late health consequences caused by a wrong gait pattern can be prevented. A treatment with an orthosis can therefore be used to support the therapy.
Stroke can affect the ability to live independently and with quality. Self-management programs are a special training that educates stroke survivors about stroke and its consequences, helps them acquire skills to cope with their challenges, and helps them set and meet their own goals during their recovery process. These programs are tailored to the target audience, and led by someone trained and expert in stroke and its consequences (most commonly professionals, but also stroke survivors and peers). A 2016 review reported that these programs improve the quality of life after stroke, without negative effects. People with stroke felt more empowered, happy and satisfied with life after participating in this training.[217]
Disability affects 75% of stroke survivors enough to decrease their ability to work.[218]
Stroke can affect people physically, mentally, emotionally, or a combination of the three. The results of stroke vary widely depending on size and location of the lesion.[219]
Some of the physical disabilities that can result from stroke include muscle weakness, numbness, pressure sores, pneumonia, incontinence, apraxia (inability to perform learned movements), difficulties carrying out daily activities, appetite loss, speech loss, vision loss and pain. If the stroke is severe enough, or in a certain location such as parts of the brainstem, coma or death can result. Up to 10% of people following stroke develop seizures, most commonly in the week subsequent to the event; the severity of the stroke increases the likelihood of a seizure.[220][221] An estimated 15% of people experience urinary incontinence for more than a year following stroke.[192] 50% of people have a decline in sexual function (sexual dysfunction) following stroke.[193]
Emotional and mental dysfunctions correspond to areas in the brain that have been damaged. Emotional problems following stroke can be due to direct damage to emotional centers in the brain or from frustration and difficulty adapting to new limitations. Post-stroke emotional difficulties include anxiety, panic attacks, flat affect (failure to express emotions), mania, apathy and psychosis. Other difficulties may include a decreased ability to communicate emotions through facial expression, body language and voice.[222]
Disruption in self-identity, relationships with others, and emotional well-being can lead to social consequences after stroke due to the lack of ability to communicate. Many people who experience communication impairments after stroke find it more difficult to cope with the social issues rather than physical impairments. Broader aspects of care must address the emotional impact speech impairment has on those who experience difficulties with speech after stroke.[195] Those who experience stroke are at risk of paralysis which could result in a self disturbed body image which may also lead to other social issues.[223]
30 to 50% of stroke survivors develop post-stroke depression, which is characterized by lethargy, irritability, sleep disturbances, lowered self-esteem and withdrawal.[224] Depression can reduce motivation and worsen outcome, but can be treated with social and family support, psychotherapy and, in severe cases, antidepressants. Psychotherapy sessions may have a small effect on improving mood and preventing depression after stroke;[225] however, psychotherapy does not appear to be effective at treating depression after stroke.[226][needs update] Antidepressant medications may be useful for treating depression after stroke.[226]
Emotional lability, another consequence of stroke, causes the person to switch quickly between emotional highs and lows and to express emotions inappropriately, for instance with an excess of laughing or crying with little or no provocation. While these expressions of emotion usually correspond to the person's actual emotions, a more severe form of emotional lability causes the affected person to laugh and cry pathologically, without regard to context or emotion.[218] Some people show the opposite of what they feel, for example crying when they are happy.[227] Emotional lability occurs in about 20% of those who have had stroke. Those with a right hemisphere stroke are more likely to have empathy problems which can make communication harder.[228]
Cognitive deficits resulting from stroke include perceptual disorders, aphasia,[229] dementia,[230][231] and problems with attention[232] and memory.[233] Stroke survivors may be unaware of their own disabilities, a condition called anosognosia. In a condition called hemispatial neglect, the affected person is unable to attend to anything on the side of space opposite to the damaged hemisphere. Cognitive and psychological outcome after stroke can be affected by the age at which the stroke happened, pre-stroke baseline intellectual functioning, psychiatric history and whether there is pre-existing brain pathology.[234]
Stroke was the second most frequent cause of death worldwide in 2011, accounting for 6.2 million deaths (~11% of the total).[236] Approximately 17 million people had stroke in 2010 and 33 million people have previously had stroke and were still alive.[18] Between 1990 and 2010 the incidence of stroke decreased by approximately 10% in the developed world and increased by 10% in the developing world.[18] Overall, two-thirds of stroke occurred in those over 65 years old.[18] South Asians are at particularly high risk of stroke, accounting for 40% of global stroke deaths.[237] Incidence of ischemic stroke is ten times more frequent than haemorrhagic stroke.[238]
It is ranked after heart disease and before cancer.[2] In the United States stroke is a leading cause of disability, and recently declined from the third leading to the fourth leading cause of death.[239] Geographic disparities in stroke incidence have been observed, including the existence of a "stroke belt" in the southeastern United States, but causes of these disparities have not been explained.
The risk of stroke increases exponentially from 30 years of age, and the cause varies by age.[240] Advanced age is one of the most significant stroke risk factors. 95% of stroke occurs in people age 45 and older, and two-thirds of stroke occurs in those over the age of 65.[46][224]
A person's risk of dying if he or she does have stroke also increases with age. However, stroke can occur at any age, including in childhood.[citation needed]
Family members may have a genetic tendency for stroke or share a lifestyle that contributes to stroke. Higher levels of Von Willebrand factor are more common amongst people who have had ischemic stroke for the first time.[241] The results of this study found that the only significant genetic factor was the person's blood type. Having stroke in the past greatly increases one's risk of future stroke.
Men are 25% more likely to develop stroke than women,[46] yet 60% of deaths from stroke occur in women.[227] Since women live longer, they are older on average when they have stroke and thus more often killed.[46] Some risk factors for stroke apply only to women. Primary among these are pregnancy, childbirth, menopause, and the treatment thereof (HRT).
Episodes of stroke and familial stroke have been reported from the 2nd millennium BC onward in ancient Mesopotamia and Persia.[242] Hippocrates (460 to 370 BC) was first to describe the phenomenon of sudden paralysis that is often associated with ischemia. Apoplexy, from the Greek word meaning "struck down with violence", first appeared in Hippocratic writings to describe this phenomenon.[243][244]
The word stroke was used as a synonym for apoplectic seizure as early as 1599,[245] and is a fairly literal translation of the Greek term. The term apoplectic stroke is an archaic, nonspecific term, for a cerebrovascular accident accompanied by haemorrhage or haemorrhagic stroke.[246] Martin Luther was described as having an apoplectic stroke that deprived him of his speech shortly before his death in 1546.[247]
In 1658, in his Apoplexia, Johann Jacob Wepfer (1620–1695) identified the cause of hemorrhagic stroke when he suggested that people who had died of apoplexy had bleeding in their brains.[46][243]
Wepfer also identified the main arteries supplying the brain, the vertebral and carotid arteries, and identified the cause of a type of ischemic stroke known as a cerebral infarction when he suggested that apoplexy might be caused by a blockage to those vessels.[46] Rudolf Virchow first described the mechanism of thromboembolism as a major factor.[248]
The term cerebrovascular accident was introduced in 1927, reflecting a "growing awareness and acceptance of vascular theories and (...) recognition of the consequences of a sudden disruption in the vascular supply of the brain".[249] Its use is now discouraged by a number of neurology textbooks, reasoning that the connotation of fortuitousness carried by the word accident insufficiently highlights the modifiability of the underlying risk factors.[250][251][252] Cerebrovascular insult may be used interchangeably.[253]
The term brain attack was introduced for use to underline the acute nature of stroke according to the American Stroke Association,[253] which has used the term since 1990,[254] and is used colloquially to refer to both ischemic as well as hemorrhagic stroke.[255]
As of 2017, angioplasty and stents were under preliminary clinical research to determine the possible therapeutic advantages of these procedures in comparison to therapy with statins, antithrombotics, or antihypertensive drugs.[256]
Creutzfeldt–Jakob disease

An epileptic seizure, informally known as a seizure, is a period of symptoms due to abnormally excessive or synchronous neuronal activity in the brain.[6] Outward effects vary from uncontrolled shaking movements involving much of the body with loss of consciousness (tonic-clonic seizure), to shaking movements involving only part of the body with variable levels of consciousness (focal seizure), to a subtle momentary loss of awareness (absence seizure).[3] Most of the time these episodes last less than two minutes and it takes some time to return to normal.[5][8] Loss of bladder control may occur.[3]
Seizures may be provoked and unprovoked.[6] Provoked seizures are due to a temporary event such as low blood sugar, alcohol withdrawal, abusing alcohol together with prescription medication, low blood sodium, fever, brain infection, flashing images or concussion.[3][6] Unprovoked seizures occur without a known or fixable cause such that ongoing seizures are likely.[5][3][6][7] Unprovoked seizures may be exacerbated by stress or sleep deprivation.[3] Epilepsy describes brain disease in which there has been at least one unprovoked seizure and where there is a high risk of additional seizures in the future.[6] Conditions that look like epileptic seizures but are not include: fainting, nonepileptic psychogenic seizure and tremor.[3]
A seizure that lasts for more than a brief period is a medical emergency.[10] Any seizure lasting longer than five minutes should be treated as status epilepticus.[8] A first seizure generally does not require long-term treatment with anti-seizure medications unless a specific problem is found on electroencephalogram (EEG) or brain imaging.[7] Typically it is safe to complete the work-up following a single seizure as an outpatient.[3] In many, with what appears to be a first seizure, other minor seizures have previously occurred.[11]
Up to 10% of people have had at least one epileptic seizure in their lifetime.[5][9] Provoked seizures occur in about 3.5 per 10,000 people a year while unprovoked seizures occur in about 4.2 per 10,000 people a year.[5] After one seizure, the chance of experiencing a second one is about 40%.[12][13] Epilepsy affects about 1% of the population at any given time.[9]
The signs and symptoms of seizures vary depending on the type.[14] The most common and stereotypical type of seizure is convulsive (60%), typically called a tonic-clonic seizure.[15] Two-thirds of these begin as focal seizures prior to developing into tonic-clonic seizures.[15] The remaining 40% of seizures are non-convulsive, an example of which is absence seizure.[16] When EEG monitoring shows evidence of a seizure, but no symptoms are present, it is referred to as a subclinical seizure.[17]
Focal seizures often begin with certain experiences, known as an aura.[14] These may include sensory (including visual, auditory, etc.), cognitive, autonomic, olfactory or motor phenomena.[18]
In a complex partial seizure, a person may appear confused or dazed and cannot respond to questions or direction.[18]
Jerking activity may start in a specific muscle group and spread to surrounding muscle groups—known as a Jacksonian march.[19] Unusual activities that are not consciously created may occur.[19] These are known as automatisms and include simple activities like smacking of the lips or more complex activities such as attempts to pick something up.[19]
There are six main types of generalized seizures: tonic-clonic, tonic, clonic, myoclonic, absence, and atonic seizures.[20] They all involve a loss of consciousness and typically happen without warning.[21]
A seizure can last from a few seconds to more than five minutes, at which point it is known as status epilepticus.[22] Most tonic-clonic seizures last less than two or three minutes.[22] Absence seizures are usually around 10 seconds in duration.[16]
After the active portion of a seizure, there is typically a period of confusion called the postictal period, before a normal level of consciousness returns.[14] This period usually lasts 3 to 15 minutes,[23] but may last for hours.[24] Other symptoms during this period include: tiredness, headache, difficulty speaking, and abnormal behavior.[24] Psychosis after a seizure occurs in 6–10% of people.[25][24]
Seizures have a number of causes. Of those who have a seizure, about 25% have epilepsy.[26] A number of conditions are associated with seizures but are not epilepsy including: most febrile seizures and those that occur around an acute infection, stroke, or toxicity.[27] These seizures are known as "acute symptomatic" or "provoked" seizures and are part of the seizure-related disorders.[27] In many the cause is unknown.
Different causes of seizures are common in certain age groups.
Dehydration can trigger epileptic seizures if it is severe enough.[31] A number of disorders including: low blood sugar, low blood sodium, hyperosmolar nonketotic hyperglycemia, high blood sodium, low blood calcium and high blood urea levels may cause seizures.[21] As may hepatic encephalopathy and the genetic disorder porphyria.[21]
Both medication and drug overdoses can result in seizures,[21] as may certain medication and drug withdrawal.[21] Common drugs involved include: antidepressants, antipsychotics, cocaine, insulin, and the local anaesthetic lidocaine.[21] Difficulties with withdrawal seizures commonly occur after prolonged alcohol or sedative use, a condition known as delirium tremens.[21] In people who are at risk of developing epileptic seizures, common herbal medicines such as ephedra, ginkgo biloba and wormwood can provoke seizures.[33]
Stress can induce seizures in people with epilepsy, and is a risk factor for developing epilepsy. Severity, duration, and time at which stress occurs during development all contribute to frequency and susceptibility to developing epilepsy. It is one of the most frequently self-reported triggers in patients with epilepsy.[37][38]
Stress exposure results in hormone release that mediates its effects in the brain. These hormones act on both excitatory and inhibitory neural synapses, resulting in hyper-excitability of neurons in the brain. The hippocampus is known to be a region that is highly sensitive to stress and prone to seizures. This is where mediators of stress interact with their target receptors to produce effects.[39]
Seizures may occur as a result of high blood pressure, known as hypertensive encephalopathy, or in pregnancy as eclampsia when accompanied by either seizures or a decreased level of consciousness.[21] Very high body temperatures may also be a cause.[21] Typically this requires a temperature greater than 42 °C (107.6 °F).[21]
Normally, brain electrical activity is non-synchronous.[18] In epileptic seizures, due to problems within the brain,[44] a group of neurons begin firing in an abnormal, excessive,[15] and synchronized manner.[18] This results in a wave of depolarization known as a paroxysmal depolarizing shift.[45]
Normally after an excitatory neuron fires it becomes more resistant to firing for a period of time.[18] This is due in part from the effect of inhibitory neurons, electrical changes within the excitatory neuron, and the negative effects of adenosine.[18] In epilepsy the resistance of excitatory neurons to fire during this period is decreased.[18] This may occur due to changes in ion channels or inhibitory neurons not functioning properly.[18] Forty-one ion-channel genes and over 1,600 ion-channel mutations have been implicated in the development of epileptic seizure.[46] These ion channel mutations tend to confer a depolarized resting state to neurons resulting in pathological hyper-excitability.[47] This long-lasting depolarization in individual neurons is due to an influx of Ca2+ from outside of the cell and leads to extended opening of Na+ channels and repetitive action potentials.[48] The following hyperpolarization is facilitated by γ-aminobutyric acid (GABA) receptors or potassium (K+) channels, depending on the type of cell.[48] Equally important in epileptic neuronal hyper-excitability, is the reduction in the activity of inhibitory GABAergic neurons, an effect known as disinhibition. Disinhibition may result from inhibitory neuron loss, dysregulation of axonal sprouting from the inhibitory neurons in regions of neuronal damage, or abnormal GABAergic signaling within the inhibitory neuron.[49] Neuronal hyper-excitability results in a specific area from which seizures may develop, known as a "seizure focus".[18] Following an injury to the brain, another mechanism of epilepsy may be the up regulation of excitatory circuits or down regulation of inhibitory circuits.[18][50] These secondary epilepsies occur through processes known as epileptogenesis.[18][50] Failure of the blood–brain barrier may also be a causal mechanism.[51] While blood-brain barrier disruption alone does appear to cause epileptogenesis, it has been correlated to increased seizure activity.[52] Furthermore, it has been implicated in chronic epileptic conditions through experiments inducing barrier permeability with chemical compounds.[52] Disruption may lead to fluid leaking out of the blood vessels into the area between cells and driving epileptic seizures.[53] Preliminary findings of blood proteins in the brain after a seizure support this theory.[52]
Focal seizures begin in one hemisphere of the brain while generalized seizures begin in both hemispheres.[20] Some types of seizures may change brain structure, while others appear to have little effect.[54] Gliosis, neuronal loss, and atrophy of specific areas of the brain are linked to epilepsy but it is unclear if epilepsy causes these changes or if these changes result in epilepsy.[54]
Seizure activity may be propagated through the brain's endogenous electrical fields.[55] Proposed mechanisms that may cause the spread and recruitment of neurons include an increase in K+ from outside the cell,[56][unreliable medical source] and increase of Ca2+ in the presynaptic terminals.[48] These mechanisms blunt hyperpolarization and depolarizes nearby neurons, as well as increasing neurotransmitter release.[48]
Seizures may be divided into provoked and unprovoked.[6] Provoked seizures may also be known as "acute symptomatic seizures" or "reactive seizures".[6] Unprovoked seizures may also be known as "reflex seizures".[6] Depending on the presumed cause blood tests and lumbar puncture may be useful.[7] Hypoglycemia may cause seizures and should be ruled out. An electroencephalogram and brain imaging with CT scan or MRI scan is recommended in the work-up of seizures not associated with a fever.[7][57]
Seizure types are organized by whether the source of the seizure is localized (focal seizures) or distributed (generalized seizures) within the brain.[20] Generalized seizures are divided according to the effect on the body and include tonic-clonic (grand mal), absence (petit mal), myoclonic, clonic, tonic, and atonic seizures.[20][58] Some seizures such as epileptic spasms are of an unknown type.[20]
Focal seizures (previously called partial seizures[15]) are divided into simple partial or complex partial seizure.[20] Current practice no longer recommends this, and instead prefers to describe what occurs during a seizure.[20]
The classification of seizures can also be made according to dynamical criteria, observable in electrophysiological measurements. It is a classification according to their type of onset and offset.[59][60]
Most people are in a postictal state (drowsy or confused) following a seizure. They may show signs of other injuries. A bite mark on the side of the tongue helps confirm a seizure when present, but only a third of people who have had a seizure have such a bite.[61] When present in people thought to have had a seizure, this physical sign tentatively increases the likelihood that a seizure was the cause.[62]
An electroencephalography is only recommended in those who likely had an epileptic seizure and may help determine the type of seizure or syndrome present. In children it is typically only needed after a second seizure. It cannot be used to rule out the diagnosis and may be falsely positive in those without the disease. In certain situations it may be useful to prefer the EEG while sleeping or sleep deprived.[63]
Diagnostic imaging by CT scan and MRI is recommended after a first non-febrile seizure to detect structural problems inside the brain.[63] MRI is generally a better imaging test except when intracranial bleeding is suspected.[7] Imaging may be done at a later point in time in those who return to their normal selves while in the emergency room.[7] If a person has a previous diagnosis of epilepsy with previous imaging repeat imaging is not usually needed with subsequent seizures.[63]
In adults, testing electrolytes, blood glucose and calcium levels is important to rule these out as causes, as is an electrocardiogram.[63] A lumbar puncture may be useful to diagnose a central nervous system infection but is not routinely needed.[7] Routine antiseizure medical levels in the blood are not required in adults or children.[63] In children additional tests may be required.[63]
A high blood prolactin level within the first 20 minutes following a seizure may be useful to confirm an epileptic seizure as opposed to psychogenic non-epileptic seizure.[64][65] Serum prolactin level is less useful for detecting partial seizures.[66] If it is normal an epileptic seizure is still possible[65] and a serum prolactin does not separate epileptic seizures from syncope.[67] It is not recommended as a routine part of diagnosis epilepsy.[63]
Differentiating an epileptic seizure from other conditions such as syncope can be difficult.[14] Other possible conditions that can mimic a seizure include: decerebrate posturing, psychogenic seizures, tetanus, dystonia, migraine headaches, and strychnine poisoning.[14] In addition, 5% of people with a positive tilt table test may have seizure-like activity that seems due to cerebral hypoxia.[68] Convulsions may occur due to psychological reasons and this is known as a psychogenic non-epileptic seizure. Non-epileptic seizures may also occur due to a number of other reasons.
A number of measures have been attempted to prevent seizures in those at risk. Following traumatic brain injury anticonvulsants decrease the risk of early seizures but not late seizures.[69]
In those with a history of febrile seizures, some medications (both antipyretics and anticonvulsants) have been found effective for reducing reoccurrence, however due to the frequency of adverse effects and the benign nature of febrile seizures the decision to use medication should be weighted carefully against potential negative effects.[70]
There is no clear evidence that antiepileptic drugs are effective or not effective at preventing seizures following a craniotomy,[71] following subdural hematoma,[72] after a stroke,[73][74] or after subarachnoid haemorrhage,[75] for both people who have had a previous seizure, and those who have not.
Potentially sharp or dangerous objects should be moved from the area around a person experiencing a seizure so that the individual is not hurt. After the seizure, if the person is not fully conscious and alert, they should be placed in the recovery position. A seizure longer than five minutes, or two or more seizures occurring within the time of five minutes is a medical emergency known as status epilepticus.[22][76] Contrary to a common misconception, bystanders should not attempt to force objects into the mouth of the person having a seizure, as doing so may cause injury to the teeth and gums.[77]
Treatments of a person that is actively seizing follows a progression from initial response, through first line, second line, and third line treatments.[78] The initial response involves ensuring the person is protected from potential harms (such as nearby objects) and managing their airway, breathing, and circulation.[78] Airway management should include placing the person on their side, known as the recovery position, to prevent them from choking.[78] If they are unable to breathe because something is blocking their airway, they may require treatments to open their airway.[78]
The first line medication for an actively seizing person is a benzodiazepine, with most guidelines recommending lorazepam.[57][79] Diazepam and midazolam are alternatives. This may be repeated if there is no effect after 10 minutes.[57] If there is no effect after two doses, barbiturates or propofol may be used.[57]
Second-line therapy for adults is phenytoin or fosphenytoin and phenobarbital for children.[80][page needed] Third-line medications include phenytoin for children and phenobarbital for adults.[80][page needed]
Ongoing anti-epileptic medications are not typically recommended after a first seizure except in those with structural lesions in the brain.[57][81] They are generally recommended after a second one has occurred.[57] Approximately 70% of people can obtain full control with continuous use of medication.[44] Typically one type of anticonvulsant is preferred. Following a first seizure, while immediate treatment with an anti-seizure drug lowers the probability of seizure recurrence up to five years it does not change the risk of death and there are potential side effects.[82]
In seizures related to toxins, up to two doses of benzodiazepines should be used.[83] If this is not effective pyridoxine is recommended.[83] Phenytoin should generally not be used.[83]
There is a lack of evidence for preventive anti-epileptic medications in the management of seizures related to intracranial venous thrombosis.[74]
Helmets may be used to provide protection to the head during a seizure. Some claim that seizure response dogs, a form of service dog, can predict seizures.[84] Evidence for this, however, is poor.[84] At present there is not enough evidence to support the use of cannabis for the management of seizures, although this is an ongoing area of research.[85][86] There is low quality evidence that a ketogenic diet may help in those who have epilepsy and is reasonable in those who do not improve following typical treatments.[87]
Following a first seizure, the risk of more seizures in the next two years is around 40%.[12][13] The greatest predictors of more seizures are problems either on the electroencephalogram or on imaging of the brain.[7] In adults, after 6 months of being seizure-free after a first seizure, the risk of a subsequent seizure in the next year is less than 20% regardless of treatment.[88] Up to 7% of seizures that present to the emergency department (ER) are in status epilepticus.[57] In those with a status epilepticus, mortality is between 10% and 40%.[14] Those who have a seizure that is provoked (occurring close in time to an acute brain event or toxic exposure) have a low risk of re-occurrence, but have a higher risk of death compared to those with epilepsy.[89]
Approximately 8–10% of people will experience an epileptic seizure during their lifetime.[90] In adults, the risk of seizure recurrence within the five years following a new-onset seizure is 35%; the risk rises to 75% in persons who have had a second seizure.[90] In children, the risk of seizure recurrence within the five years following a single unprovoked seizure is about 50%; the risk rises to about 80% after two unprovoked seizures.[91] In the United States in 2011, seizures resulted in an estimated 1.6 million emergency department visits; approximately 400,000 of these visits were for new-onset seizures.[90] The exact incidence of epileptic seizures in low-income and middle-income countries is unknown, however it probably exceeds that in high-income countries.[92] This may be due to increased risks of traffic accidents, birth injuries, and malaria and other parasitic infections.[92]
Epileptic seizures were first described in an Akkadian text from 2000 B.C.[93] Early reports of epilepsy often saw seizures and convulsions as the work of "evil spirits".[94] The perception of epilepsy, however, began to change in the time of Ancient Greek medicine. The term "epilepsy" itself is a Greek word, which is derived from the verb "epilambanein", meaning "to seize, possess, or afflict".[93] Although the Ancient Greeks referred to epilepsy as the "sacred disease", this perception of epilepsy as a "spiritual" disease was challenged by Hippocrates in his work On the Sacred Disease, who proposed that the source of epilepsy was from natural causes rather than supernatural ones.[94]
Early surgical treatment of epilepsy was primitive in Ancient Greek, Roman and Egyptian medicine.[95] The 19th century saw the rise of targeted surgery for the treatment of epileptic seizures, beginning in 1886 with localized resections performed by Sir Victor Horsley, a neurosurgeon in London.[94] Another advancement was that of the development by the Montreal procedure by Canadian neurosurgeon Wilder Penfield, which involved use of electrical stimulation among conscious patients to more accurately identify and resect the epileptic areas in the brain.[94]
Seizures result in direct economic costs of about one billion dollars in the United States.[7] Epilepsy results in economic costs in Europe of around €15.5 billion in 2004.[15] In India, epilepsy is estimated to result in costs of US$1.7 billion or 0.5% of the GDP.[44] They make up about 1% of emergency department visits (2% for emergency departments for children) in the United States.[29]
Many areas of the world require a minimum of six months from the last seizure before people can drive a vehicle.[7]
Scientific work into the prediction of epileptic seizures began in the 1970s. Several techniques and methods have been proposed, but evidence regarding their usefulness is still lacking.[96]
Two promising areas include gene therapy,[97] and seizure detection and seizure prediction.[98]
Gene therapy for epilepsy consists of employing vectors to deliver pieces of genetic material to areas of the brain involved in seizure onset.[97]
Seizure prediction is a special case of seizure detection in which the developed systems is able to issue a warning before the clinical onset of the epileptic seizure.[96][98]
Computational neuroscience has been able to bring a new point of view on the seizures by considering the dynamical aspects.[60]
Creutzfeldt–Jakob disease
An underweight person is a person whose body weight is considered too low to be healthy. A person who is underweight is malnourished.
The body mass index, a ratio of a person's weight to their height, has traditionally been used to assess the health of a person as it pertains to weight: under the cut-off point at a BMI of 18.5, a person is considered underweight.[2] The calculation is either weight in kilograms divided by height in meters, squared, or weight in pounds times 703, divided by height in inches, squared. Another measure of underweight is through comparison to the average weight of a cohort of people of a similar age and height: people who are at least 15% to 20% below the average weight for the group are considered underweight.[3]
Body fat percentage has been suggested as another way to assess whether a person is underweight. Unlike the body mass index, which is a proxy measurement, the body fat percentage takes into account the difference in composition between adipose tissue (fat cells) and muscle tissue and their different roles in the body.[4] The American Council on Exercise defines the amount of essential fat, below which a person is underweight, as 10–13% for women and 2–5% for men.[5] The greater amount of essential body fat in women supports reproductive function.[citation needed]
Using the body mass index as a measure of weight-related health, with data from 2014, age-standardised global prevalence of underweight in women and men were 9.7% and 8.8%, respectively. These values were lower than what was reported for 1975 as 14.6% and 13.8%, respectively, indicating a worldwide reduction in the extent of undernutrition.[6]
A person may be underweight due to genetics,[7][8] poor absorption of nutrients, increased metabolic rate or energy expenditure, lack of food (frequently due to poverty), drugs that affect appetite, illness (physical or mental) or the eating disorder anorexia nervosa.[9][10]
Being underweight is associated with certain medical conditions, including type 1 diabetes,[11] hyperthyroidism,[12] cancer,[13] and tuberculosis.[14] People with gastrointestinal or liver problems may be unable to absorb nutrients adequately. People with certain eating disorders can also be underweight due to one or more nutrient deficiencies or excessive exercise, which exacerbates nutrient deficiencies.[15][16]
A common belief is that healthy underweight individuals can ‘eat what they want’ and then burn it off either by high levels of activity or elevated metabolism. It has been shown, however, that individuals with BMI < 18.5 eat about 12% less calories than individuals with normal BMI (21.5 to 25) and they are 23% less physically active (by accelerometry).[17]
Being underweight can be a symptom of an underlying condition, in which case it is secondary. Unexplained weight loss may require a professional medical diagnosis by a physician.[18]
Being underweight can also cause other conditions, in which case it is primary. Severely underweight individuals may have poor physical stamina and a weak immune system, leaving them open to infection. According to Robert E. Black of the Johns Hopkins School of Public Health (JHSPH), "Underweight status ... and micronutrient deficiencies also cause decreases in immune and non-immune host defenses, and should be classified as underlying causes of death if followed by infectious diseases that are the terminal associated causes."[19]
People who are malnourished raise special concerns, as not only gross caloric intake may be inadequate, but also intake and absorption of other vital nutrients, especially essential amino acids and micronutrients such as vitamins and minerals.[citation needed]
In women, being severely underweight, as a result of an eating disorder or due to excessive strenuous exercise, can result in amenorrhea (absence of menstruation),[20] infertility or complications during pregnancy if gestational weight gain is too low.[citation needed]
Malnourishment can also cause anemia and hair loss. 
Being underweight is an established[21] risk factor for osteoporosis, even for young people. This is seen in individuals suffering from relative energy deficiency in sport, formerly known as female athlete triad: when disordered eating or excessive exercise cause amenorrhea, hormone changes during ovulation leads to loss of bone mineral density.[22][23] After this low bone mineral density causes the first spontaneous fractures, the damage is often irreversible.
Although being underweight has been reported to increase mortality at rates comparable to that seen in morbidly obese people,[24] the effect is much less drastic when restricted to non-smokers with no history of disease,[25] suggesting that smoking and disease-related weight loss are the leading causes of the observed effect.
Underweight individuals may be advised to gain weight by increasing calorie intake. This can be done by eating a sufficient volume of sufficiently calorie-dense foods.[26][27][28] Body weight may also be increased through the consumption of liquid nutritional supplements.[29]
Another way for underweight people to gain weight is by exercising, since muscle hypertrophy increases body mass. Weight lifting exercises are effective in helping to improve muscle tone as well as helping with weight gain.[30] Weight lifting has also been shown to improve bone mineral density,[31] which underweight people are more likely to lack.[32]
Exercise is catabolic, which results in a brief reduction in mass. However, during recovery, anabolic overcompensation causes the muscles to grow, which results in an overall increase in mass. This can happen through an increase in muscle proteins, or through enhanced storage of glycogen in muscles.[citation needed] Exercise can also help stimulate the appetite of a person who is not inclined to eat.
Certain drugs may increase appetite either as their primary effect or as a side effect. Antidepressants, such as mirtazapine or amitriptyline, and antipsychotics, particularly chlorpromazine  and haloperidol, as well as tetrahydrocannabinol (found in cannabis), all present an increase in appetite as a side effect. In states where it is approved, medicinal cannabis may be prescribed for severe appetite loss, such as that caused by cancer, AIDS, or severe levels of persistent anxiety. Other drugs or supplements which may increase appetite include antihistamines (such as diphenhydramine, promethazine or cyproheptadine).[33]

A BRCA mutation is a mutation in either of the BRCA1 and BRCA2 genes, which are tumour suppressor genes. Hundreds of different types of mutations in these genes have been identified, some of which have been determined to be harmful, while others have no proven impact. Harmful mutations in these genes may produce a hereditary breast–ovarian cancer syndrome in affected persons. Only 5–10% of breast cancer cases in women are attributed to BRCA1 and BRCA2 mutations (with BRCA1 mutations being slightly more common than BRCA2 mutations), but the impact on women with the gene mutation is more profound.[2] Women with harmful mutations in either BRCA1 or BRCA2 have a risk of breast cancer that is about five times the normal risk, and a risk of ovarian cancer that is about ten to thirty times normal.[3] The risk of breast and ovarian cancer is higher for women with a high-risk BRCA1 mutation than with a BRCA2 mutation. Having a high-risk mutation does not guarantee that the woman will develop any type of cancer, or imply that any cancer that appears was actually caused by the mutation, rather than some other factor.
High-risk mutations, which disable an important error-free DNA repair process (homology directed repair), significantly increase the person's risk of developing breast cancer, ovarian cancer and certain other cancers. Why BRCA1 and BRCA2 mutations lead preferentially to cancers of the breast and ovary is not known, but lack of BRCA1 function seems to lead to non-functional X-chromosome inactivation. Not all mutations are high-risk; some appear to be harmless variations.  The cancer risk associated with any given mutation varies significantly and depends on the exact type and location of the mutation and possibly other individual factors.
Mutations can be inherited from either parent and may be passed on to both sons and daughters.  Each child of a genetic carrier, regardless of sex, has a 50% chance of inheriting the mutated gene from the parent who carries the mutation. As a result, half of the people with BRCA gene mutations are male, who would then pass the mutation on to 50% of their offspring, male or female.  The risk of BRCA-related breast cancers for men with the mutation is higher than for other men, but still low.[4] However, BRCA mutations can increase the risk of other cancers, such as colon cancer, pancreatic cancer, and prostate cancer.
Methods to diagnose the likelihood of a patient with mutations in BRCA1 and BRCA2 getting cancer were covered by patents owned or controlled by Myriad Genetics.[5][6] Myriad's business model of exclusively offering the diagnostic test led to Myriad growing from being a startup in 1994 to being a publicly traded company with 1200 employees and about $500M in annual revenue in 2012;[7] it also led to controversy over high prices and the inability to get second opinions from other diagnostic labs, which in turn led to the landmark Association for Molecular Pathology v. Myriad Genetics lawsuit.[8]
Biallelic and homozygous inheritance of a BRCA gene leads to a severe form of Fanconi anemia, and is embryonically lethal in the majority of the cases.
Women with deleterious mutations in either the BRCA1 or BRCA2 genes have a high risk of developing breast and/or ovarian cancer.  Because different studies look at different populations, and because different types of mutations have somewhat different risks, the risk is best expressed as a range, rather than a single number.[9]: 89–111 
Approximately 50% to 65% of women born with a deleterious mutation in BRCA1 will develop breast cancer by age 70, and 35% to 46% will develop ovarian cancer by age 70.  Approximately 40% to 57% of women with a deleterious mutation in BRCA2 will develop breast cancer by age 70, and 13% to 23% will develop ovarian cancer by age 70.[9]: 89–111 [10]
Women with a breast cancer associated with a BRCA mutation have up to a 40% probability of developing a new primary breast cancer within 10 years following initial diagnosis if they did not receive tamoxifen treatment or have an oophorectomy.[4] The woman's ten-year risk for ovarian cancer is also increased by 6-12% under these conditions.[4]
Statistics for BRCA-related ovarian cancer typically encompass not only cancer of the ovaries themselves, but also peritoneal cancer and the very rare, but somewhat easier to detect, cancer of the Fallopian tubes.  Women with a BRCA mutation have more than 100 times the normal rate of Fallopian tube cancer.[9]: 275–302   These three types of these cancers can be difficult to distinguish in their advanced stages.
BRCA-related breast cancer appears at an earlier age than sporadic breast cancer.[9]: 89–111   It has been asserted that BRCA-related breast cancer is more aggressive than normal breast cancer, however most studies in specific populations suggest little or no difference in survival rates despite seemingly worse prognostic factors.[11][12][13]
BRCA-related ovarian and Fallopian tube cancer is more treatable than average because it is unusually susceptible to platinum-based chemotherapy like cisplatin.[9]: 275–302  BRCA1-related ovarian cancer appears at younger ages, but the risk for women with BRCA2 climbs markedly at or shortly after menopause.[9]: 275–302 
A 25-year-old woman with no mutation in her BRCA genes has an 84% probability to reach at least the age of 70.[14] Of those not surviving, 11% die from either breast or ovarian cancer, and 89% from other causes.
Compared to that, a woman with a high-risk BRCA1 mutation, if she had breast cancer screening but no prophylactic medical or surgical intervention, would have only 59% chance to reach age 70, twenty-five percentage points lower than normal. Of those women not surviving, 26% would die of breast cancer, 46% ovarian cancer, and 28% other causes.[14]
Women with high-risk BRCA2 mutations, with screening but with no prophylactic medical or surgical intervention, would have only 71% chance to reach age 70, thirteen percentage points lower than normal. Of those not surviving, 21% would die of breast cancer, 25% ovarian cancer and 54% other causes.[14]
The likelihood of surviving to at least age 70 can be improved by several medical interventions, notably prophylactic mastectomy and oophorectomy.[14]
Men with a BRCA mutation have a dramatically elevated relative risk of developing breast cancer, but because the overall incidence of breast cancer in men is so low, the absolute risk is equal to or lower than the risk for women without a BRCA mutation.[9]: Ch8   Approximately 1% to 2% of men with a BRCA1 mutation will develop breast cancer by age 70.  Approximately 6% of men with a BRCA2 mutation will develop breast cancer by age 70, which is approximately equal to the risk for women without a BRCA mutation.  Very few men, with or without a predisposing mutation, develop breast cancer before age 50.[9]: Ch8 
Approximately half of men who develop breast cancer have a mutation in a BRCA gene or in one of the other genes associated with hereditary breast–ovarian cancer syndromes.
Breast cancer in men can be treated as successfully as breast cancer in women, but men often ignore the signs and symptoms of cancer, such as a painful area or an unusual swelling, which may be no bigger than a grain of rice, until it has reached a late stage.[9]: Ch8 
Unlike other men, men with a BRCA mutation, especially a BRCA2 mutation, may benefit from professional and self breast exams.  Medical imaging is not usually recommended, but because male BRCA2 carriers have a risk of breast cancer that is very similar to the general female population, the standard annual mammogram program can be adapted to these high-risk men.[9]: Ch8 
Mutations have been associated with increased risk of developing any kind of invasive cancer, including stomach cancer, pancreatic cancer, prostate cancer, and colon cancer.[15] Carriers have the normal risks of developing cancer (and other diseases) associated with increased age, smoking, alcohol consumption, poor diet, lack of exercise, and other known risk factors, plus the additional risk from the genetic mutations and an increased susceptibility to damage from ionizing radiation, including natural background radiation.[9]: 39–50 
Men with BRCA mutations cannot get ovarian cancer, but they may be twice as likely as non-carriers to develop prostate cancer at a younger age.[9]: Ch8    The risk is smaller and disputed for BRCA1 carriers; up to one-third of BRCA2 mutation carriers are expected to develop prostate cancer before age 65.  Prostate cancer in BRCA mutation carriers tends to appear a decade earlier than normal, and it tends to be more aggressive than normal.  As a result, annual prostate screening, including a digital rectal examination, is appropriate at age 40 among known carriers, rather than age 50.[9]: Ch8 
Cancer of the pancreas tends to run in families, even among BRCA families.[9]: Ch8    A BRCA1 mutation approximately doubles or triples the lifetime risk of developing pancreatic cancer; a BRCA2 mutation triples to quintuples it.  Between 4% and 7% of people with pancreatic cancer have a BRCA mutation.[15] However, since pancreatic cancer is relatively rare, people with a BRCA2 mutation probably face an absolute risk of about 5%.  Like ovarian cancer, it tends not to produce symptoms in the early, treatable stages.  Like prostate cancer, pancreatic cancer associated with a BRCA mutation tends to appear about a decade earlier than non-hereditary cases.[15] Asymptomatic screening is invasive and may be recommended only to BRCA2 carriers who also have a family history of pancreatic cancer.[9]: Ch8 
Melanoma is the most deadly skin cancer, although it is easily cured in the early stages.  The normal likelihood of developing melanoma depends on race, the number of moles the person has, family history, age, sex, and how much the person has been exposed to UV radiation. BRCA2 mutation carriers have approximately double or triple the risk that they would normally have, including a higher than average risk of melanoma of the eye.[9]: Ch8 [15]
Cancer of the colon is approximately as common in both men and women in the developed world as breast cancer is among average-risk women, with about 6% of people being diagnosed with it, usually over the age of 50.[9]: Ch8    Like sporadic prostate cancer, it is a multifactorial disease, and is affected by age, diet, and similar factors.  BRCA mutation carriers have a higher than average risk of this common cancer, but the risk is not as high as in some other hereditary cancers.  The risk might be as high as four times normal in some BRCA1 families, and double the normal risk among BRCA2 carriers.  Like pancreatic cancer, it may be that only some BRCA mutations or some BRCA families have the extra risk; unlike other BRCA-caused cancers, it does not appear at an earlier age than usual.[9]: Ch8    Normal colon cancer screening is usually recommended to BRCA mutation carriers.
Mutations in BRCA1 and BRCA2 are strongly implicated in some hematological malignancies.  BRCA1 mutations are associated acute myelogenous leukemia and chronic myelogenous leukemia.[16] Mutations of BRCA2 are also found in many T-cell lymphomas and chronic lymphocytic leukemias.[16]
The dilemma of whether or not to have children may be a source of stress for women who learn of their BRCA mutations during their childbearing years.[17]
There is likely little or no effect of a BRCA gene mutation on overall fertility,[18] although women with a BRCA mutation may be more likely to have primary ovarian insufficiency.[19]
[20] BRCA mutation carriers may be more likely to give birth to girls than boys,[21] however this observation has been attributed to ascertainment bias.[22][23]
If both parents are carriers of a BRCA mutation, then pre-implantation genetic diagnosis is sometimes used to prevent the birth of a child with BRCA mutations.[9]: 82–85   Inheriting two BRCA1 mutations (one from each parent) has never been reported and is believed to be a lethal birth defect.  Inheriting one BRCA1 mutation and one BRCA2 mutation has been reported occasionally; the child's risk for any given type of cancer is the higher risk of the two genes (e.g., the ovarian cancer risk from BRCA1 and the pancreatic cancer risk from BRCA2).  Inheriting two BRCA2 mutations produces Fanconi anemia.[9]: 82–85 
Each pregnancy in genetically typical women is associated with a significant reduction in the mother's risk of developing breast cancer after age 40.[17] The younger the woman is at the time of her first birth, the more protection against breast cancer she receives.[9]: 113–142   Breastfeeding for more than one year protects against breast cancer.[9]: 113–142   Pregnancy also protects against ovarian cancer in genetically typical women.[17]
Although some studies have produced different results, women with BRCA mutations are generally not expected to receive these significant protective benefits.[9]: 113–142 [17] Current research is too limited and imprecise to permit calculation of specific risks.[17] However, the following general trends have been identified:
Reports of patients biallelic or homozygous for a deleterious BRCA allele conferring a greatly increased risk of breast cancer are rare. This is because deleterious BRCA alleles are lethal alleles; this condition is embryonically lethal in the majority of the cases. [25] For live cases, inheriting both mutations lead to a grave prognosis, characterized by Wilms tumors, leukemias, and early-onset brain malignancies.[26]
Both BRCA genes are tumor suppressor genes that produce proteins that are used by the cell in an enzymatic pathway that makes very precise, perfectly matched repairs to DNA molecules that have double-stranded breaks.[9]: 39–50 [27] The pathway requires proteins produced by several other genes, including CHK2, FANCD2 and ATM.[15] Harmful mutations in any of these genes disable the gene or the protein that it produces.
The cancer risk caused by BRCA1 and BRCA2 mutations are inherited in a dominant fashion even though usually only one mutated allele is directly inherited.[28] This is because people with the mutation are likely to acquire a second mutation, leading to dominant expression of the cancer. A mutated BRCA gene can be inherited from either parent.  Because they are inherited from the parents, they are classified as  hereditary or germline mutations rather than acquired or somatic mutations.  Cancer caused by a mutated gene inherited from an individual's parents is a hereditary cancer rather than a sporadic cancer.
Because humans have a diploid genome, each cell has two copies of the gene (one from each biological parent).  Typically only one copy contains a disabling, inherited mutation, so the affected person is heterozygous for the mutation.  If the functional copy is harmed, however, then the cell is forced to use alternate DNA repair mechanisms, which are more error-prone.  The loss of the functional copy is called loss of heterozygosity (LOH).[29] Any resulting errors in DNA repair may result in cell death or a cancerous transformation of the cell.[9]: 39–50 
There are many variations in BRCA genes, and not all changes confer the same risks.
[9]: 39–50 
Some variants are harmless; others are known to be very harmful.  Some single nucleotide polymorphisms may confer only a small risk, or may only confer risk in the presence of other mutations or under certain circumstances.  In other cases, whether the variant is harmful is unknown.  Variants are classified as follows:[9]: 39–50 : 109 
Deleterious mutations have high, but not complete, genetic penetrance, which means that people with the mutation have a high risk of developing disease as a result, but that some people will not develop cancer despite carrying a harmful mutation.
Genetic counseling is recommended in women whose personal or family health history suggests a greater than average likelihood of a mutation.[30] The purpose of genetic counseling is to educate the person about the likelihood of a positive result, the risks and benefits of being tested, the limitations of the tests, the practical meaning of the results, and the risk-reducing actions that could be taken if the results are positive.  They are also trained to support people through any emotional reactions and to be a neutral person who helps the client make his or her own decision in an informed consent model, without pushing the client to do what the counselor might do.  Because the knowledge of a mutation can produce substantial anxiety, some people choose not to be tested or to postpone testing until a later date.[9]: 51–74 
Relative indications for testing for a mutation in BRCA1 or BRCA2 for newly diagnosed or family members include a family history among 1st (FDR), 2nd (SDR), or 3rd(TDR) degree relatives usually on the same side of the family but not limited:[31]
Testing young children is considered medically unethical because the test results would not change the way the child's health is cared for.[9]: 82–85 
Two types of tests are available.[9]: 51–74   Both commonly use a blood sample, although testing can be done on saliva. The quickest, simplest, and lowest cost test uses positive test results from a blood relative and checks only for the single mutation that is known to be present in the family.  If no relative has previously disclosed positive test results, then a full test that checks the entire sequence of both BRCA1 and BRCA2 can be performed.  In some cases, because of the founder effect, Jewish ethnicity can be used to narrow the testing to quickly check for the three most common mutations seen among Ashkenazi Jews.[9]: 51–74 
Testing is commonly covered by health insurance and public healthcare programs for people at high risk for having a mutation, and not covered for people at low risk.[9]: 51–74  The purpose of limiting the testing to high-risk people is to increase the likelihood that the person will receive a meaningful, actionable result from the test, rather than identifying a variant of unknown significance (VUS). In Canada, people who demonstrate their high-risk status by meeting specified guidelines are referred initially to a specialized program for hereditary cancers, and, if they choose to be tested, the cost of the test is fully covered.  In the US in 2010, single-site testing had a retail cost of US$400 to $500, and full-length analysis cost about $3,000 per gene, and the costs were commonly covered by private health insurance for people deemed to be at high risk.
The test is ordered by a physician, usually an oncologist, and the results are always returned to the physician, rather than directly to the patient.  How quickly results are returned depends on the test—single-site analysis requires less lab time—and on the infrastructure in place.  In the US, test results are commonly returned within one to several weeks; in Canada, patients commonly wait for eight to ten months for test results.[9]: 51–74 
A positive test result for a known deleterious mutation is proof of a predisposition, although it does not guarantee that the person will develop any type of cancer.  A negative test result, if a specific mutation is known to be present in the family, shows that the person does not have a BRCA-related predisposition for cancer, although it does not guarantee that the person will not develop a non-hereditary case of cancer.  By itself, a negative test result does not mean that the patient has no hereditary predisposition for breast or ovarian cancer.  The family may have some other genetic predisposition for cancer, involving some other gene.[9]: 89–111 
A variety of screening options and interventions are available to manage BRCA-related cancer risks.  Screenings are adjusted to individual and familial risk factors.[citation needed]
As these screening methods do not prevent cancer, but merely attempt to catch it early, numerous methods of prevention are sometimes practiced, with varying results.[9]: 175–207 
An intensive cancer screening regimen is usually advised for women with deleterious or suspected deleterious BRCA mutations in order to detect new cancers as early as possible.  A typical recommendation includes frequent breast cancer screening as well as tests to detect ovarian cancer.[9]: 175–207 
Breast imaging studies usually include a breast MRI (magnetic resonance imaging) once a year, beginning between ages 20 and 30, depending on the age at which any relatives were diagnosed with breast cancer. Mammograms are typically used only at advanced age as there is reason to believe that BRCA carriers are more susceptible to breast cancer induction by X-ray damage than general population.[34]
Alternatives include breast ultrasonography, CT scans, PET scans, scintimammography, elastography, thermography, ductal lavage, and experimental screening protocols, some of which hope to identify biomarkers for breast cancer (molecules that appear in the blood when breast cancer begins).[9]: 175–207 
Ovarian cancer screening usually involves ultrasonography of the pelvic region, typically twice a year.[9]: 175–207   Women may also use a blood test for CA-125 and clinical pelvic exams.  The blood test has relatively poor sensitivity and specificity for ovarian cancer.[9]: 175–207 [35]
In both breast and ovarian screening, areas of tissue that look suspicious are investigated with either more imaging, possibly using a different type of imaging or after a delay, or with biopsies of the suspicious areas.
Birth control pills are associated with substantially lower risk of ovarian cancer in women with BRCA mutations.[36][37] A 2013 meta-analysis found that oral contraceptive use was associated with a 42% reduction of the relative risk of ovarian cancer, the association was similar for BRCA1 and BRCA2 mutations. Use of oral contraceptives was not significantly associated with breast cancer risk although a small increase of risk that did not reach statistical significance was observed.[36][37] A 2011 meta-analysis found that OC use was associated with a 43% relative reduction in risk of ovarian cancer in women with BRCA mutations, while data on the risk of breast cancer in BRCA mutation carriers with oral contraceptive use were heterogeneous and results were inconsistent.[38]
Selective estrogen receptor modulators, specifically tamoxifen, have been found to reduce breast cancer risk in women with BRCA mutations who do not have their breast removed.[9]: 113–142    It is effective as for primary prevention (preventing the first case of breast cancer) in women with BRCA2 mutations, but not BRCA1 mutations, and for secondary prevention (preventing a second, independent breast cancer) in both groups of women.  Taking tamoxifen for five years has been found to halve the breast cancer risk in women who have a high risk of breast cancer for any reason, but potentially serious adverse effects like cataracts, blood clots, and endometrial cancer, along with quality of life issues like hot flashes, result in some women discontinuing its use and some physicians limiting its use to women with atypical growths in the breasts.  Tamoxifen is contraindicated for women who are most likely to be harmed by the common complications.  Raloxifene (Evista), which has a reduced risk of side effects, is used as an alternative, but it has not been studied in BRCA mutation carriers specifically.  Tamoxifen use can be combined with oophorectomy for even greater reduction of breast cancer risk, particularly in women with BRCA2 mutations.[9]: 113–142 
Aromatase inhibitors are medications that prevent estrogen production in the adrenal glands and adipose tissue.  They have fewer side effects than selective estrogen receptor modulators like tamoxifen, but do not work in premenopausal women, because they do not prevent the ovaries from producing estrogen.[9]: 113–142 
Several type of preventive surgeries are known to substantially reduce cancer risk for women with high-risk BRCA mutations.[39] The surgeries may be used alone, in combination with each other, or in combination with non-surgical interventions to reduce the risk of breast and ovarian cancer. Surgeries such as mastectomy and oophorectomy do not eliminate the chance of breast cancer; cases have reportedly emerged despite these procedures.[40]
Whether and when to perform which preventive surgeries is a complex personal decision. Current medical knowledge offers some guidance about the risks and benefits. Even carriers of the same mutation or from the same family may have substantially different risks for the kind and severity of cancer they are likely to get, as well as the age at which they may get them. Different people also have different values. They may choose to focus on total cancer prevention, psychological benefits, current quality of life, or overall survival. The possible impact of future medical developments in treatment or prognosis may also be of some importance for very young women and family planning.  The decision is individualized and is usually based on many factors, such as earliest occurrence of BRCA-related cancer in close relatives. 
An increasing number women who test positive for faulty BRCA1 or BRCA2 genes choose to have risk-reducing surgery. At the same time the average waiting time for undergoing the procedure is two-years which is much longer than recommended.[42][43]
The protective effect of prophylactic surgery is greater when done at young age; however, oophorectomy also has adverse effects that are greatest when done long before natural menopause.  For this reason, oophorectomy is mostly recommended after age 35 or 40, assuming childbearing is complete.  The risk of ovarian cancer is low before this age, and the negative effects of oophorectomy are less serious as the woman nears natural menopause.[14][44]
For comparison, women in the general population have an 84% chance of living to age 70.
Research has looked into the effects of risk-reducing surgery on the psychological and social wellbeing of women with a BRCA mutation.[45] Due to limited evidence, a 2019 meta analysis was unable to draw conclusions on whether interventions can help with the psychological effects of surgery in female BRCA carriers. More research is needed to conclude how best to support women who choose surgery.[45]
In a woman who has not developed breast cancer, removing the breasts may reduce her risk of ever being diagnosed with breast cancer by 90%, to a level that is approximately half the average woman's risk.[9]: 209–244 
Bilateral mastectomy is the removal of both breasts by a breast surgeon.[9]: 209–244  The modified radical mastectomy is only used in women diagnosed with invasive breast cancer.  Techniques for prophylactic mastectomies include:[9]: 209–244 
Which technique is used is determined by the existence of any cancer and overall health, as well as by the woman's desire, if any, for breast reconstruction surgery for aesthetic purposes.[9]: 209–244   Women who choose a flat-chested appearance or use external breast prostheses typically choose simple mastectomy, with its greater risk reduction.[9]: 209–244 
Breast reconstruction is usually done by a plastic surgeon, and may be started as part of the same multi-hour surgery that removes the breasts.  Multiple techniques for reconstruction have been used, with different locations and amounts of scarring.  Some techniques use tissue from another part of the body, such as fat tissue from the lower abdomen or occasionally muscles from other parts of the torso.  Others use breast implants, possibly preceded by tissue expanders, to provide volume.  Some reconstruction techniques require multiple surgeries.  Afterwards, some women have tattoos added to simulate breast areolas or have the skin reshaped to form a nipple.[9]: 209–244 
Oophorectomy (surgical removal of the ovaries) and salpingectomy (surgical removal of the Fallopian tubes) are strongly recommended to women with BRCA mutations.[9]: 275–302  Salpingo-oophorectomy is the single most effective method of preventing ovarian and Fallopian tube cancer in women with a BRCA mutation.  However, a small risk of primary peritoneal cancer remains, at least among women with BRCA1 mutations, since the peritoneal lining is the same type of cells as parts of the ovary.  This risk is estimated to produce about five cases of peritoneal cancer per 100 women with harmful BRCA1 mutations in the 20 years after the surgery.[9]: 275–302 
BRCA2 related ovarian cancer tends to present in perimenopausal or menopausal women, so salpingo-oophorectomy is recommended between ages 45 and 50.[9]: 275–302 
The surgery is often done in conjunction with a hysterectomy (surgical removal of the uterus) and sometimes a cervicectomy (surgical removal of the cervix), especially in women who want to take tamoxifen, which is known to cause uterine cancer, or who have uterine fibroids.[9]: 275–302   Multiple styles of surgery are available, including laparoscopic (keyhole) surgery.  Because about 5% of women with a BRCA mutation have undetected ovarian cancer at the time of their planned surgery, the surgery should be treated as if it were a removal of a known cancer.[9]: 275–302 
Salpingo-oophorectomy makes the woman sterile (unable to bear children).  Infertility services can be used to preserve her eggs, if wanted.  However, as the benefits to the surgery are greatest close to menopause, most women simply postpone the surgery until they have already borne as many children as they choose to.[9]: 275–302 
The surgery also artificially induces menopause, which causes hot flashes, sleep disturbances, mood swings, vaginal dryness, sexual difficulties, difficulty with word recall, and other medical signs and symptoms. The side effects range from mild to severe; most can be treated at least partially.  Many women with a BRCA take hormone replacement therapy to reduce these effects:  estrogen-progesterone combinations for women who have a uterus, and unopposed estrogen for women whose uterus was removed.  Estrogen can cause breast cancer, but as the amount of estrogen taken is less than the amount produced by the now-removed ovaries, the net risk is usually judged to be acceptable.[9]: 303–317 
Some sources assume that oophorectomy before age 50 doubles the risk of cardiovascular disease and increases risk of hip fractures caused by osteoporosis in the relevant population.[14]
Given the high risks and the low benefit of lifestyle choices in BRCA mutation carriers, no lifestyle choices provide sufficient protection.[9]: 113–142 
Having her first child at a younger age, having more children than average, and breastfeeding for more than one year decreases the risk of breast cancer for an average-risk woman.[9]: 113–142   Studies about this effect among BRCA mutation carriers have produced conflicting results, but generally speaking, having children is believed to provide little or no protection against breast cancer for women with BRCA1 mutations, and to paradoxically increase the risk of breast cancer for women with BRCA2 mutations.[9]: 113–142 [17]
Being physically active and maintaining a healthy body weight prevents breast and other cancers in the general population, as well as preventing heart disease and other medical conditions.  Among women with a BRCA mutation, being physically active and having had a healthy body weight as an adolescent has no effect on ovarian cancer and delays, but does not entirely prevent, breast cancer after menopause.[9]: 113–142 [46] In some studies, only significant, strenuous exercise produced any benefit.[9]: 113–142  Obesity and weight gain as an adult are associated with breast cancer diagnoses.[9]: 113–142 
Studies on specific foods, diets, or dietary supplements have generally produced conflicting information or, in the case of dietary fat, soy consumption, and drinking green tea, have only been conducted in average-risk women.[9]: 113–142   The only dietary intervention that is generally accepted as preventing breast cancer in BRCA mutation carriers is minimizing consumption of alcoholic beverages.  Consuming more than one alcoholic drink per day is strongly associated with a higher risk of developing breast cancer, and carriers are usually encouraged to consume no more than one alcoholic drink per day, and no more than four total in a week.[9]: 113–142 
In a study conducted with Ashkenazi Jewish women, it was observed that mutation carriers born before 1940 have a much lower risk of being diagnosed with breast cancer by age 50 than those born after 1940; this was also observed in the non-carrier population.[46] The reasons for the difference is unknown.  Unlike the general population, age at menarche and age at menopause has no effect on breast cancer risk for BRCA mutation carriers.[9]: 113–142 
Several hypotheses propose that BRCA mutations might have evolutionary advantages, such as higher intelligence. The Ashkenazi intelligence hypothesis was proposed by Gregory Cochran and asserts that a defect in the BRCA1 gene might unleash neural growth.[47]
Studies have shown that BRCA1 mutations are not random, but under adaptive selection, indicating that although BRCA1 mutations are linked to breast cancer, the mutations likely have a beneficial effect as well.[48]
A patent application for the isolated BRCA1 gene and cancer-cancer promoting mutations discussed above, as well as methods to diagnose the likelihood of getting breast cancer, was filed by the University of Utah, National Institute of Environmental Health Sciences (NIEHS) and Myriad Genetics in 1994;[5] over the next year, Myriad, in collaboration with investigators from Endo Recherche, Inc., HSC Research & Development Limited Partnership, and University of Pennsylvania, isolated and sequenced the BRCA2 gene and identified key mutations, and the first BRCA2 patent was filed in the US by Myriad and other institutions in 1995.[6] Myriad is the exclusive licensee of these patents and has enforced them in the US against clinical diagnostic labs.[8] This business model led to Myriad growing being a startup in 1994 to being a publicly traded company with 1200 employees and about $500M in annual revenue in 2012;[7] it also led to controversy over high prices and the inability to get second opinions from other diagnostic labs, which in turn led to the landmark Association for Molecular Pathology v. Myriad Genetics lawsuit.[8][49] The patents began to expire in 2014.
According to an article published in the journal, Genetic Medicine, in 2010, "The patent story outside the United States is more complicated.... For example, patents have been obtained but the patents are being ignored by provincial health systems in Canada. In Australia and the UK, Myriad's licensee permitted use by health systems, but announced a change of plans in August 2008. ... Only a single mutation has been patented in Myriad's lone European-wide patent, although some patents remain under review of an opposition proceeding. In effect, the United States is the only jurisdiction where Myriad's strong patent position has conferred sole-provider status."[50][51] Peter Meldrum, CEO of Myriad Genetics, has acknowledged that Myriad has "other competitive advantages that may make such [patent] enforcement unnecessary" in Europe.[52]
Legal decisions surrounding the BRCA1 and BRCA2 patents will affect the field of genetic testing in general.[53] In June 2013, in  Association for Molecular Pathology v. Myriad Genetics (No. 12-398), the US Supreme Court unanimously ruled that, "A naturally occurring DNA segment is a product of nature and not patent eligible merely because it has been isolated," invalidating Myriad's patents on the BRCA1 and BRCA2 genes. However, the Court also held that manipulation of a gene to create something not found in nature could still be eligible for patent protection.[54]

Ulcerative colitis (UC) is a type of inflammatory bowel disease (IBD).[1] It is a long-term condition that results in inflammation and ulcers of the colon and rectum.[1][7] The primary symptoms of active disease are abdominal pain and diarrhea mixed with blood (hematochezia).[1] Weight loss, fever, and anemia may also occur.[1] Often, symptoms come on slowly and can range from mild to severe.[1] Symptoms typically occur intermittently with periods of no symptoms between flares.[1] Complications may include abnormal dilation of the colon (megacolon), inflammation of the eye, joints, or liver, and colon cancer.[1][3]
The cause of UC is unknown.[1] Theories involve immune system dysfunction, genetics, changes in the normal gut bacteria, and environmental factors.[1][8] Rates tend to be higher in the developed world with some proposing this to be the result of less exposure to intestinal infections, or to a Western diet and lifestyle.[7][9] The removal of the appendix at an early age may be protective.[9] Diagnosis is typically by colonoscopy with tissue biopsies.[1]
Dietary changes, such as maintaining a high-calorie diet or lactose-free diet, may improve symptoms.[1] Several medications are used to treat symptoms and bring about and maintain remission, including aminosalicylates such as mesalazine or sulfasalazine, steroids, immunosuppressants such as azathioprine, and  biologic therapy.[1] Removal of the colon by surgery may be necessary if the disease is severe, does not respond to treatment, or if complications such as colon cancer develop.[1] Removal of the colon and rectum generally cures the condition.[1][9]
People with ulcerative colitis usually present with diarrhea mixed with blood,[12] of gradual onset that persists for an extended period of time (weeks). It is estimated that 90% of people experience rectal bleeding (of varying severity), 90% experience watery or loose stools with increased stool frequency (diarrhea), and 75-90% of people experience bowel urgency.[13] Additional symptoms may include fecal incontinence, mucous rectal discharge, and nocturnal defecations.[12] With proctitis (inflammation of the rectum), people with UC may experience urgency or rectal tenesmus, which is the urgent desire to evacuate the bowels but with the passage of little stool.[12] Tenesmus may be misinterpreted as constipation, due to the urge to defecate despite small volume of stool passage. Bloody diarrhea and abdominal pain may be more prominent features in severe disease.[12] The severity of abdominal pain with UC varies from mild discomfort to very painful bowel movements and abdominal cramping.[14] High frequency of bowel movements, weight loss, nausea, fatigue, and fever are also common during disease flares. Chronic bleeding from the GI tract, chronic inflammation, and iron deficiency often leads to anemia, which can affect quality of life.[15]
The clinical presentation of ulcerative colitis depends on the extent of the disease process.[16] Up to 15% of individuals may have severe disease upon initial onset of symptoms.[12] A substantial proportion (up to 45%) of people with a history of UC without any ongoing symptoms (clinical remission) have objective evidence of ongoing inflammation.[17] Ulcerative colitis is associated with a generalized inflammatory process that can affect many parts of the body. Sometimes, these associated extra-intestinal symptoms are the initial signs of the disease.[18]
In contrast to Crohn's disease, which can affect areas of the gastrointestinal tract outside of the colon, ulcerative colitis is usually confined to the colon. Inflammation in ulcerative colitis is usually continuous, typically involving the rectum, with involvement extending proximally (to sigmoid colon, ascending colon, etc.).[19] In contrast, inflammation with Crohn's disease is often patchy, with so-called "skip lesions" (intermittent regions of inflamed bowel).[20]
The disease is classified by the extent of involvement, depending on how far the disease extends:[14] proctitis (rectal inflammation), left sided colitis (inflammation extending to descending colon), and extensive colitis (inflammation proximal to the descending colon).[19] Proctosigmoiditis describes inflammation of the rectum and sigmoid colon. Pancolitis describes involvement of the entire colon, extending from the rectum to the cecum. While usually associated with Crohn's disease, ileitis (inflammation of the ileum) also occurs in UC. About 17% of individuals with UC have ileitis.[21] Ileitis more commonly occurs in the setting of pancolitis (occurring in 20% of cases of pancolitis),[12] and tends to correlate with the activity of colitis. This so-called "backwash ileitis" can occur in 10–20% of people with pancolitis and is believed to be of little clinical significance.[22]
In addition to the extent of involvement, UC is also characterized by severity of disease.[19] Severity of disease is defined by symptoms, objective markers of inflammation (endoscopic findings, blood tests), disease course, and the impact of the disease on day-to-day life.[19] Most patients are categorized through endoscopy and fecal calprotectin levels. Indicators of low risk for future complications in mild and moderate UC include the following parameters: exhibiting less than 6 stools daily and lack of fever/weight loss. Other indicators include lack of extraintestinal symptoms, low levels of the inflammatory markers C-reactive protein (CRP), and erythrocyte sedimentation rate (ESR), and fecal calprotectin, and later age of diagnosis (over 40 years).[23] Mild disease correlates with fewer than four stools daily; in addition, mild urgency and rectal bleeding may occur intermittently.[19]  Mild disease lacks systemic signs of toxicity (e.g. fever, chills, weight changes) and exhibits normal levels of the serum inflammatory markers ESR and CRP.[23]
Moderate to severe disease correlates with more than six stools daily, frequent bloody stools and urgency.[19] Moderate abdominal pain, low-grade fever, 38 to 39 °C (100 to 102 °F), and anemia may develop.[19] ESR and CRP are usually elevated.[19]
The Mayo Score, which incorporates a combination of clinical symptoms (stool frequency and amount of rectal bleeding) with endoscopic findings and a physicians assessment of severity, is often used clinically to classify UC as mild, moderate or severe.[13]
Acute-Severe Ulcerative Colitis (ASUC) is a severe form which presents acutely and with severe symptoms. This fulminant type is associated with severe symptoms (usually diarrhea, rectal bleeding and abdominal pain) and is usually associated with systemic symptoms including fever.[13] It is associated with a high mortality rate as compared to milder forms of UC, with a 3-month and 12 month mortality rate of 0.84% and 1% respectively.[13] People with fulminant UC may have inflammation extending beyond just the mucosal layer, causing impaired colonic motility and leading to toxic megacolon. Toxic megacolon represents a medical emergency, one often treated surgically. If the serous membrane is involved, a colonic perforation may ensue, which has a 50% mortality rate in people with UC.[24] Other complications include hemorrhage, venous thromboembolism, and secondary infections of the colon including C. difficile or cytomegalovirus colitis.[13]
Ulcerative colitis may improve and enter remission.[19]
UC is characterized by immune dysregulation and systemic inflammation, which may result in symptoms and complications outside the colon. Commonly affected organs include: eyes, joints, skin, and liver.[28] The frequency of such extraintestinal manifestations has been reported as between 6 and 47%.[29][30]
UC may affect the mouth. About 8% of individuals with UC develop oral manifestations.[31] The two most common oral manifestations are aphthous stomatitis and angular cheilitis.[31] Aphthous stomatitis is characterized by ulcers in the mouth, which are benign, noncontagious and often recurrent. Angular chelitis is characterized by redness at the corners of the mouth, which may include painful sores or breaks in the skin.[31] Very rarely, benign pustules may occur in the mouth (pyostomatitis vegetans).[31]
UC may affect the eyes manifesting in scleritis, iritis, and conjunctivitis. Patients may be asymptomatic or experience redness, burning, or itching in eyes. Inflammation may occur in the interior portion of the eye, leading to uveitis and iritis.[32] Uveitis can cause blurred vision and eye pain, especially when exposed to light (photophobia). Untreated, uveitis can lead to permanent vision loss.[32] Inflammation may also involve the white part of the eye (sclera) or the overlying connective tissue (episclera), causing conditions called scleritis and episcleritis.[33] Ulcerative colitis is most commonly associated with uveitis and episcleritis.[34]
UC may cause several joint manifestations, including a type of rheumatologic disease known as seronegative arthritis, which may affect few large joints (oligoarthritis), the vertebra (ankylosing spondylitis) or several small joints of the hands and feet (peripheral arthritis).[28] Often the insertion site where muscle attaches to bone (entheses) becomes inflamed (enthesitis). Inflammation may affect the sacroiliac joint (sacroiliitis).[18] It is estimated that around 50% of IBD patients suffer from migratory arthritis. Synovitis, or inflammation of the synovial fluid surrounding a joint, can occur for months and recur in later times but usually does not erode the joint. The symptoms of arthritis include joint pain, swelling, and effusion, and often leads to significant morbidity.[18] Ankylosing spondylitis and sacroilitis usually occur independent of bowel disease activity in UC.[13]
Ulcerative colitis may affect the skin. The most common type of skin manifestation, erythema nodosum, presents in up to 3% of UC patients. It develops as raised, tender red nodules usually appearing on the outer areas of the arms or legs, especially in the anterior tibial area (shins).[34] The nodules have diameters that measure approximately 1–5 cm. Erythema nodosum is due to inflammation of the underlying subcutaneous tissue (panniculitis), and biopsy will display focal panniculitis (although is often unnecessary in diagnosis). In contrast to joint-related manifestations, erythema nodosum often occurs alongside intestinal disease. Thus, treatment of UC can often lead to resolution of skin nodules.[35]
Another skin condition associated with UC is pyoderma gangrenosum, which presents as deep skin ulcerations. Pyoderma gangrenosum is seen in about 1% of patients with UC and its formation is usually independent of bowel inflammation.[13] Pyoderma gangrenosum is characterized by painful lesions or nodules that become ulcers which progressively grow. The ulcers are often filled with sterile pus-like material. In some cases, pyoderma gangrenosum may require injection with corticosteroids.[28] Treatment may also involve inhibitors of tumor necrosis factor (TNF), a cytokine that promotes cell survival.[35]
Other associations determined between the skin and ulcerative colitis include a skin condition known as hidradenitis suppurativa (HS). This condition represents a chronic process in which follicles become occluded leading to recurring inflammation of nodules and abscesses and even fistulas tunnels in the skin that drain fluid.[36]
Ulcerative colitis may affect the circulatory and endocrine system. UC increases the risk of blood clots in both arteries and veins;[37][38][39] painful swelling of the lower legs can be a sign of deep venous thrombosis, while difficulty breathing may be a result of pulmonary embolism (blood clots in the lungs). The risk of blood clots is about threefold higher in individuals with IBD.[38] The risk of venous thromboembolism is high in ulcerative colitis due to hypercoagulability from inflammation, especially with active or extensive disease.[37] Additional risk factors may include surgery, hospitalization, pregnancy, the use of corticosteroids and tofacitinib, a JAK inhibitor.[37]
Osteoporosis may occur related to systemic inflammation or prolonged steroid use in the treatment of UC, which increases the risk of bone fractures.[18] Clubbing, a deformity of the ends of the fingers, may occur.[18] Amyloidosis may occur, especially with severe and poorly controlled disease, which usually presents with protein in the urine (proteinuria) and nephritic syndrome.[18]
Ulcerative colitis has a significant association with primary sclerosing cholangitis (PSC), a progressive inflammatory disorder of small and large bile ducts. Up to 70-90% of people with primary sclerosing cholangitis have ulcerative colitis.[34] As many as 5% of people with ulcerative colitis may progress to develop primary sclerosing cholangitis.[28][40] PSC is more common in men, and often begins between 30 and 40 years of age.[28] It can present asymptomatically or exhibit symptoms of itchiness (pruritis) and fatigue. Other symptoms include systemic signs such as fever and night sweats. Such symptoms are often associated with a bacterial episodic version of PSC. Upon physical exam, one may discern enlarged liver contours (hepatomegaly) or enlarged spleen (splenomegaly) as well as areas of excoriation. Yellow coloring of the skin, or jaundice, may also be present due to excess of bile byproduct buildup (bilirubin) from the biliary tract. In diagnosis, lab results often reveal a pattern indicative of biliary disease (cholestatic pattern). This is often displayed by markedly elevated alkaline phosphatase levels and milder or no elevation in liver enzyme levels. Xray results often show bile ducts with thicker walls, areas of dilation or narrowing.[41]
In some cases, primary sclerosing cholangitis occurs several years before the bowel symptoms of ulcerative colitis develop.[34] PSC does not parallel the onset, extent, duration, or activity of the colonic inflammation in ulcerative colitis.[34] In addition, colectomy does not have an impact on the course of primary sclerosing cholangitis in individuals with UC.[34] PSC is associated with an increased risk of colorectal cancer and cholangiocarcinoma (bile duct cancer).[34][28] PSC is a progressive condition, and may result in cirrhosis of the liver.[28] No specific therapy has been proven to affect the long term course of PSC.[28]
Ulcerative colitis is an autoimmune disease characterized by T-cells infiltrating the colon.[43] No direct causes for UC are known, but factors such as genetics, environment, and an overactive immune system play a role.[1] UC is associated with comorbidities that produce symptoms in many areas of the body outside the digestive system.
A genetic component to the cause of UC can be hypothesized based on aggregation of UC in families, variation of prevalence between different ethnicities, genetic markers and linkages.[44] In addition, the identical twin concordance rate is 10%, whereas the dizygotic twin concordance rate is only 3%.[44][45] Between 8 and 14% of people with ulcerative colitis have a family history of inflammatory bowel disease.[12] In addition, people with a first degree relative with UC have a four-fold increase in their risk of developing the disease.[12]
Twelve regions of the genome may be linked to UC, including, in the order of their discovery, chromosomes 16, 12, 6, 14, 5, 19, 1, and 3,[46] but none of these loci has been consistently shown to be at fault, suggesting that the disorder is influenced by multiple genes. For example, chromosome band 1p36 is one such region thought to be linked to inflammatory bowel disease.[47] Some of the putative regions encode transporter proteins such as OCTN1 and OCTN2. Other potential regions involve cell scaffolding proteins such as the MAGUK family. Human leukocyte antigen associations may even be at work. In fact, this linkage on chromosome 6 may be the most convincing and consistent of the genetic candidates.[46]
Multiple autoimmune disorders are associated with ulcerative colitis, including celiac disease,[48] psoriasis,[49] lupus erythematosus,[50] rheumatoid arthritis,[51] episcleritis, and scleritis.[32] Ulcerative colitis is also associated with acute intermittent porphyria.[52]
Many hypotheses have been raised for environmental factors contributing to the pathogenesis of ulcerative colitis, including diet, breastfeeding and medications. Breastfeeding may have a protective effect in the development of ulcerative colitis.[53][54] One study of isotretinoin found a small increase in the rate of UC.[55]
As the colon is exposed to many dietary substances which may encourage inflammation, dietary factors have been hypothesized to play a role in the pathogenesis of both ulcerative colitis and Crohn's disease. However, research does not show a link between diet and the development of ulcerative colitis. Few studies have investigated such an association; one study showed no association of refined sugar on the number of people affected of ulcerative colitis.[56] High intake of unsaturated fat and vitamin B6 may enhance the risk of developing ulcerative colitis.[57] Other identified dietary factors that may influence the development and/or relapse of the disease include meat protein and alcoholic beverages.[58][59] Specifically, sulfur has been investigated as being involved in the cause of ulcerative colitis, but this is controversial.[60] Sulfur restricted diets have been investigated in people with UC and animal models of the disease. The theory of sulfur as an etiological factor is related to the gut microbiota and mucosal sulfide detoxification in addition to the diet.[61][62][63]
As a result of a class-action lawsuit and community settlement with DuPont, three epidemiologists conducted studies on the population surrounding a chemical plant that was exposed to PFOA at levels greater than in the general population. The studies concluded that there was an association between PFOA exposure and six health outcomes, one of which being ulcerative colitis.[64]
Levels of sulfate-reducing bacteria tend to be higher in persons with ulcerative colitis, which could indicate higher levels of hydrogen sulfide in the intestine. An alternative theory suggests that the symptoms of the disease may be caused by toxic effects of the hydrogen sulfide on the cells lining the intestine.[65]
Infection by mycobacterium avium, subspecies paratuberculosis, has been proposed as the ultimate cause of both ulcerative colitis and Crohn's disease.[66]
An increased amount of colonic sulfate-reducing bacteria has been observed in some people with ulcerative colitis, resulting in higher concentrations of the toxic gas hydrogen sulfide. Human colonic mucosa is maintained by the colonic epithelial barrier and immune cells in the lamina propria (see intestinal mucosal barrier). N-butyrate, a short-chain fatty acid, gets oxidized through the beta oxidation pathway into carbon dioxide and ketone bodies. It has been shown that N-butyrate helps supply nutrients to this epithelial barrier. Studies have proposed that hydrogen sulfide plays a role in impairing this beta-oxidation pathway by interrupting the short chain acetyl-CoA dehydrogenase, an enzyme within the pathway. Furthermore, it has been suggested that the protective benefit of smoking in ulcerative colitis is due to the hydrogen cyanide from cigarette smoke reacting with hydrogen sulfide to produce the non-toxic isothiocyanate, thereby inhibiting sulfides from interrupting the pathway.[68] An unrelated study suggested that the sulfur contained in red meats and alcohol may lead to an increased risk of relapse for people in remission.[65]
Other proposed mechanisms driving the pathophysiology of ulcerative colitis involve an abnormal immune response to the normal gut microbiota. This involves abnormal activity of antigen presenting cells (APCs) including dendritic cells and macrophages. Normally, dendritic cells and macrophages patrol the intestinal epithelium and phagocytose (engulf and destroy) pathogenic microorganisms and present parts of the microorganism as antigens to T-cells to stimulate differentiation and activation of the T-cells.[13] However, in ulcerative colitis, aberrant activity of dendritic cells and macrophages results in them phagocytosing bacteria of the normal gut microbiome. After ingesting the microbiome bacterium, the APCs release the cytokine TNFα which stimulates inflammatory signaling and recruits inflammatory cells to the intestines, leading to the inflammation that is characteristic of ulcerative colitis.[13] The TNF inhibitors, including infliximab, adalimumab and golimumab, are used to inhibit this step during the treatment of ulcerative colitis.[13] After phagocytosing the microbe, the APCs then enter the mesenteric lymph nodes where they present antigens to naive T-cells while also releasing the pro-inflammatory cytokines IL-12 and IL-23 which lead to T cell differentiation into Th1 and Th17 T-cells.[13] IL-12 and IL-23 signaling is blocked by the biologic ustekinumab and IL-23 is blocked by guselkumab, mirikizumab and risankizumab, medications that are used in the treatment of ulcerative colitis.[13] From the mesenteric lymph node, the T-cells then enter the intestinal lymphatic venule which provides transport to the intestinal epithelium where they mediate further inflammation characteristic of ulcerative colitis.[13] The T-cells exit the lymphatic venule via the adhesion protein mucosal vascular addressin cell adhesion molecule 1 MAdCAM-1, the ulcerative colitis biologic treatment vedolizumab inhibits T-cell migration out of the lymphatic venules by blocking binding to MAdCAM-1.[13] While the medications ozanimod and etrasimod inhibit the sphingosine-1-phosphate receptor to prevent T-cell migration into the efferent lymphatic venules.[13] Once the mature Th1 and Th17 T-cells exit the efferent lymphatic venule, they travel to the intestinal mucosa and cause further inflammation. T-cell mediated inflammation is thought to be driven by the JAK-STAT intracellular T-cell signaling pathway, leading to the transcription, translation and release of inflammatory cytokines. This T-cell JAK-STAT signaling is inhibited by the medications tofacitinib, filgotinib and upadacitinib which are used in the treatment of ulcerative colitis.[13]
The initial diagnostic workup for ulcerative colitis consists of a complete history and physical examination, assessment of signs and symptoms, laboratory tests and endoscopy.[69] Severe UC can exhibit high erythrocyte sedimentation rate (ESR), decreased albumin (a protein produced by the liver), and various changes in electrolytes. As discussed previously, UC patients often also display elevated alkaline phosphatase. Inflammation in the intestine may also cause higher levels of fecal calprotectin or lactoferrin.[70]
Specific testing may include the following:[19][71]
Although ulcerative colitis is a disease of unknown causation, inquiry should be made as to unusual factors believed to trigger the disease.[19]
The simple clinical colitis activity index was created in 1998 and is used to assess the severity of symptoms.[72]
The best test for diagnosis of ulcerative colitis remains endoscopy, which is examination of the internal surface of the bowel using a flexible camera. Initially, a flexible sigmoidoscopy may be completed to establish the diagnosis.[73] The physician may elect to limit the extent of the initial exam if severe colitis is encountered to minimize the risk of perforation of the colon. However, a complete colonoscopy with entry into the terminal ileum should be performed to rule out Crohn's disease, and assess extent and severity of disease.[73] Endoscopic findings in ulcerative colitis include: erythema (redness of the mucosa), friability of the mucosa, superficial ulceration, and loss of the vascular appearance of the colon. When present, ulcerations may be confluent. Pseudopolyps may be observed.[74]
Ulcerative colitis is usually continuous from the rectum, with the rectum almost universally being involved. Perianal disease is rare. The degree of involvement endoscopically ranges from proctitis (rectal inflammation) to left sided colitis (extending to descending colon), to extensive colitis (extending proximal to descending colon).[14]
Biopsies of the mucosa are taken during endoscopy to confirm the diagnosis of UC and differentiate it from Crohn's disease, which is managed differently clinically. Histologic findings in ulcerative colitis includes: distortion of crypt architecture, crypt abscesses, and inflammatory cells in the mucosa (lymphocytes, plasma cells, and granulocytes).[28] Unlike the transmural inflammation seen in Crohn's disease, the inflammation of ulcerative colitis is limited to the mucosa.[28]
Blood and stool tests serve primarily to assess disease severity, level of inflammation and rule out causes of infectious colitis.  All individuals with suspected ulcerative colitis should have stool testing to rule out infection.[12]
A complete blood count may demonstrate anemia, leukocytosis, or thrombocytosis.[12] Anemia may be caused by inflammation or bleeding. Chronic blood loss may lead to iron deficiency as a cause for anemia, particularly microcytic anemia (small red blood cells), which can be evaluated with a serum ferritin, iron, total iron-binding capacity and transferrin saturation. Anemia may be due to a complication of treatment from azathioprine, which can cause low blood counts,[75] or sulfasalazine, which can result in folate deficiency. Thiopurine metabolites (from azathioprine) and a folate level can help.[76]
UC may cause high levels of inflammation throughout the body, which may be quantified with serum inflammatory markers, such as CRP and ESR.  However, elevated inflammatory markers are not specific for UC and elevations are commonly seen in other conditions, including infection. In addition, inflammatory markers are not uniformly elevated in people with ulcerative colitis.  Twenty five percent of individuals with confirmed inflammation on endoscopic evaluation have a normal CRP level.[19] Serum albumin may also be low related to inflammation, in addition to loss of protein in the GI tract associated with bleeding and colitis. Low serum levels of vitamin D are associated with UC, although the significance of this finding is unclear.[77]
Specific antibody markers may be elevated in ulcerative colitis. Specifically, perinuclear antineutrophil cytoplasmic antibodies (pANCA) are found in 70 percent of cases of UC.[19] Antibodies against Saccharomyces cerevisiae may be present, but are more often positive in Crohn's disease compared with ulcerative colitis.  However, due to poor accuracy of these serolologic tests, they are not helpful in the diagnostic evaluation of possible inflammatory bowel disease.[19][28]
Several stool tests may help quantify the extent of inflammation present in the colon and rectum.  Fecal calprotectin is elevated in inflammatory conditions affecting the colon, and is useful in distinguishing irritable bowel syndrome (noninflammatory) from a flare in inflammatory bowel disease.[19] Fecal calprotectin is 88% sensitive and 79% specific for the diagnosis of ulcerative colitis.[19] If the fecal calprotectin is low, the likelihood of inflammatory bowel disease are less than 1 percent.[12] Lactoferrin is an additional nonspecific marker of intestinal inflammation.[78]
Overall, imaging tests, such as x-ray or CT scan, may be helpful in assessing for complications of ulcerative colitis, such as perforation or toxic megacolon. Bowel ultrasound (US) is a cost-effective, well-tolerated, non-invasive and readily available tool for the management of patients with inflammatory bowel disease (IBD), including UC, in clinical practice.[79] Some studies demonstrated that bowel ultrasound is an accurate tool for assessing disease activity in people with ulcerative colitis.[80][81] Imaging is otherwise of limited use in diagnosing ulcerative colitis.[12][28] Magnetic resonance imaging (MRI) is necessary to diagnose underlying PSC.[28]
Abdominal xray is often the test of choice and may display nonspecific findings in cases of mild or moderate ulcerative colitis. In circumstances of severe UC, radiographic findings may include thickening of the mucosa, often termed "thumbprinting", which indicates swelling due to fluid displacement (edema). Other findings may include colonic dilation and stool buildup evidencing constipation.[70]
Similar to xray, in mild ulcerative colitis, double contrast barium enema often shows nonspecific findings. Conversely, barium enema may display small buildups of barium in microulcerations. Severe UC can be characterized by various polyps, colonic shortening, loss of haustrae (the small bulging pouches in the colon),and narrowing of the colon. It is important to note that barium enema should not be conducted in patients exhibiting very severe symptoms as this may slow or stop stool passage through the colon causing ileus and toxic megacolon.[70]
Other methods of imaging include computed tomography (CT) and magnetic resonance imaging (MRI). Both may depict colonic wall thickening but have decreased ability to find early signs of wall changes when compared to barium enema. In cases of severe ulcerative colitis, however, they often exhibit equivalent ability to detect colonic changes.[70]
Doppler ultrasound is the last means of imaging that may be used. Similar to the imaging methods mentioned earlier, this may show some thickened bowel wall layers. In severe cases, this may show thickening in all bowel wall layers (transmural thickness).[70]
Several conditions may present in a similar manner as ulcerative colitis, and should be excluded. Such conditions include: Crohn's disease, infectious colitis, nonsteroidal anti-inflammatory drug enteropathy, and irritable bowel syndrome. Alternative causes of colitis should be considered, such as ischemic colitis (inadequate blood flow to the colon), radiation colitis (if prior exposure to radiation therapy), or chemical colitis. Pseudomembranous colitis may occur due to Clostridioides difficile infection following administration of antibiotics. Entamoeba histolytica is a protozoan parasite that causes intestinal inflammation. A few cases have been misdiagnosed as UC with poor outcomes occurring due to the use of corticosteroids.[82]
The most common disease that mimics the symptoms of ulcerative colitis is Crohn's disease, as both are inflammatory bowel diseases that can affect the colon with similar symptoms. It is important to differentiate these diseases since their courses and treatments may differ. In some cases, however, it may not be possible to tell the difference, in which case the disease is classified as indeterminate colitis.[83] Crohn's disease can be distinguished from ulcerative colitis in several ways. Characteristics that indicate Crohn's include evidence of disease around the anus (perianal disease). This includes anal fissures and abscesses as well as fistulas, which are abnormal connections between various bodily structures.[84]
Infectious colitis is another condition that may present in similar manner to ulcerative colitis. Endoscopic findings are also oftentimes similar. One can discern whether a patient has infectious colitis by employing tissue cultures and stool studies. Biopsy of the colon is another beneficial test but is more invasive.
Other forms of colitis that may present similarly include radiation and diversion colitis. Radiation colitis occurs after irradiation and often affects the rectum or sigmoid colon, similar to ulcerative colitis. Upon histology radiation colitis may indicate eosinophilic infiltrates, abnormal epithelial cells, or fibrosis. Diversion colitis, on the other hand, occurs after portions of bowel loops have been removed. Histology in this condition often shows increased growth of lymphoid tissue.
In patients who have undergone transplantation, graft versus host disease may also be a differential diagnosis. This response to transplantation often causes prolonged diarrhea if the colon is affected. Typical symptoms also include rash. Involvement of the upper gastrointestinal tract may lead to difficulty swallowing or ulceration. Upon histology, graft versus host disease may present with crypt cell necrosis and breakdown products within the crypts themselves.[85]
Standard treatment for ulcerative colitis depends on the extent of involvement and disease severity. The goal is to induce remission initially with medications, followed by the administration of maintenance medications to prevent a relapse. The concept of induction of remission and maintenance of remission is very important. The medications used to induce and maintain a remission somewhat overlap, but the treatments are different. Physicians first direct treatment to inducing remission, which involves relief of symptoms and mucosal healing of the colon's lining, and then longer-term treatment to maintain remission and prevent complications.[91]
For acute stages of the disease, a low fiber diet may be recommended.[92][93][94]
Ulcerative colitis can be treated with a number of medications, including 5-ASA drugs such as sulfasalazine and mesalazine. Corticosteroids such as prednisone can also be used due to their immunosuppressive and short-term healing properties, but because their risks outweigh their benefits, they are not used long-term in treatment. Immunosuppressive medications such as azathioprine and biological agents such as infliximab and adalimumab are given only if people cannot achieve remission with 5-ASA and corticosteroids. Infliximab, ustekinumab, or vedolizumab are recommended in those with moderate or severe disease.[95]
A formulation of budesonide was approved by the U.S. Food and Drug Administration (FDA) for treatment of active ulcerative colitis in January 2013.[96][97] In 2018, tofacitinib was approved for treatment of moderately to severely active ulcerative colitis in the United States, the first oral medication indicated for long term use in this condition.[98] The evidence on methotrexate does not show a benefit in producing remission in people with ulcerative colitis.[99] Cyclosporine is effective for severe UC[95] and tacrolimus has also shown benefits.[100][101][102][103]
Etrasimod (Velsipity) was approved for medical use in the United States in October 2023.[104]
Sulfasalazine has been a major agent in the therapy of mild to moderate ulcerative colitis for over 50 years. In 1977, it was shown that 5-aminosalicylic acid (5-ASA, mesalazine/mesalamine) was the therapeutically active component in sulfasalazine.[105]  Many 5-ASA drugs have been developed with the aim of delivering the active compound to the large intestine to maintain therapeutic efficacy but with reduction of the side effects associated with the sulfapyridine moiety in sulfasalazine.  Oral 5-ASA drugs are particularly effective in inducing and in maintaining remission in mild to moderate ulcerative colitis.[106][107]  Rectal suppository, foam or liquid enema formulations of 5-ASA are used for colitis affecting the rectum, sigmoid or descending colon, and have been shown to be effective especially when combined with oral treatment.[108]
Biologic treatments such as the TNF inhibitors infliximab, adalimumab, and golimumab are commonly used to treat people with UC who are no longer responding to corticosteroids. Tofacitinib and vedolizumab can also produce good clinical remission and response rates in UC.[8] Biologics may be used early in treatment (step down approach), or after other treatments have failed to induce remission (step up approach); the strategy should be individualized.[109]
Unlike aminosalicylates, biologics can cause serious side effects such as an increased risk of developing extra-intestinal cancers,[110] heart failure; and weakening of the immune system, resulting in a decreased ability of the immune system to clear infections and reactivation of latent infections such as tuberculosis. For this reason, people on these treatments are closely monitored and are often tested for hepatitis and tuberculosis annually.[111][112]
Etrasimod, a once-daily oral sphingosine 1-phosphate (S1P) receptor modulator that selectively activates S1P receptor subtypes 1, 4, and 5 with no detectable activity on S1P 2 or 3, is in development for treatment of immune-mediated diseases, including ulcerative colitis, and was shown in 2 randomized trials to be effective and well tolerated as induction and maintenance therapy in patients with moderately to severely active ulcerative colitis.[113]
Unlike Crohn's disease, ulcerative colitis has a lesser chance of affecting smokers than non-smokers.[114][115] In select individuals with a history of previous tobacco use, resuming low dose smoking may improve signs and symptoms of active ulcerative colitis,[116] but it is not recommended due to the overwhelmingly negative health effects of tobacco.[117] Studies using a transdermal nicotine patch have shown clinical and histological improvement.[118] In one double-blind, placebo-controlled study conducted in the United Kingdom, 48.6% of people with UC who used the nicotine patch, in conjunction with their standard treatment, showed complete resolution of symptoms. Another randomized, double-blind, placebo-controlled, single-center clinical trial conducted in the United States showed that 39% of people who used the patch showed significant improvement, versus 9% of those given a placebo.[119] However, nicotine therapy is generally not recommended due to side effects and inconsistent results.[120][121][122]
The gradual loss of blood from the gastrointestinal tract, as well as chronic inflammation, often leads to anemia, and professional guidelines suggest routinely monitoring for anemia with blood tests repeated every three months in active disease and annually in quiescent disease.[123] Adequate disease control usually improves anemia of chronic disease, but iron deficiency anemia should be treated with iron supplements. The form in which treatment is administered depends both on the severity of the anemia and on the guidelines that are followed. Some advise that parenteral iron be used first because people respond to it more quickly, it is associated with fewer gastrointestinal side effects, and it is not associated with compliance issues.[124] Others require oral iron to be used first, as people eventually respond and many will tolerate the side effects.[123][125]
Many patients affected by ulcerative colitis need immunosuppressant therapies, which may be associated with a higher risk of contracting opportunistic infectious diseases.[126]
Many of these potentially harmful diseases, such as Hepatitis B, Influenza, chickenpox, herpes zoster virus, pneumococcal pneumonia, or human papilloma virus, can be prevented by vaccines. Each drug used in the treatment of IBD should be classified according to the degree of immunosuppression induced in the patient. Several guidelines suggest investigating patients’ vaccination status before starting any treatment and performing vaccinations against vaccine preventable diseases when required.[127][128] 
Compared to the rest of the population, patients affected by IBD are known to be at higher risk of contracting some vaccine-preventable diseases.[129] Patients treated with Janus kinase inhibitor showed higher risk of  Shingles.[130]  Nevertheless, despite the increased risk of infections, vaccination rates in IBD patients are known to be suboptimal and may also be lower than vaccination rates in the general population.[131][132]
Unlike in Crohn's disease, the gastrointestinal aspects of ulcerative colitis can generally be cured by surgical removal of the large intestine, though extraintestinal symptoms may persist. This procedure is necessary in the event of: exsanguinating hemorrhage, frank perforation, or documented or strongly suspected carcinoma. Surgery is also indicated for people with severe colitis or toxic megacolon. People with symptoms that are disabling and do not respond to drugs may wish to consider whether surgery would improve the quality of life.[14]
The removal of the entire large intestine, known as a proctocolectomy, results in a permanent ileostomy – where a stoma is created by pulling the terminal ileum through the abdomen. Intestinal contents are emptied into a removable ostomy bag which is secured around the stoma using adhesive.[136]
Another surgical option for ulcerative colitis that is affecting most of the large bowel is called the ileal pouch-anal anastomosis (IPAA). This is a two- or three-step procedure. In a three-step procedure, the first surgery is a sub-total colectomy, in which the large bowel is removed, but the rectum remains in situ, and a temporary ileostomy is made. The second step is a proctectomy and formation of the ileal pouch (commonly known as a "j-pouch"). This involves removing the large majority of the remaining rectal stump and creating a new "rectum" by fashioning the end of the small intestine into a pouch and attaching it to the anus. After this procedure, a new type of ileostomy is created (known as a loop ileostomy) to allow the anastomoses to heal. The final surgery is a take-down procedure where the ileostomy is reversed and there is no longer the need for an ostomy bag. When done in two steps, a proctocolectomy – removing both the colon and rectum – is performed alongside the pouch formation and loop ileostomy. The final step is the same take-down surgery as in the three-step procedure. Time taken between each step can vary, but typically a six- to twelve-month interval is recommended between the first two steps, and a minimum of two to three months is required between the formation of the pouch and the ileostomy take-down.[14]
While the ileal pouch procedure removes the need for an ostomy bag, it does not restore normal bowel function. In the months following the final operation, patients typically experience 8–15 bowel movements a day. Over time this number decreases, with many patients reporting four-six bowel movements after one year post-op. While many patients have success with this procedure, there are a number of known complications. Pouchitis, inflammation of the ileal pouch resulting in symptoms similar to ulcerative colitis, is relatively common. Pouchitis can be acute, remitting, or chronic however treatment using antibiotics, steroids, or biologics can be highly effective. Other complications include fistulas, abscesses, and pouch failure. Depending on the severity of the condition, pouch revision surgery may need to be performed. In some cased the pouch may need to be de-functioned or removed and an ileostomy recreated.[137][138]
The risk of cancer arising from an ileal pouch anal anastomosis is low.[139] However, annual surveillance with pouchoscopy may be considered in individuals with risk factors for dysplasia, such as a history of dysplasia or colorectal cancer, a history of PSC, refractory pouchitis, and severely inflamed atrophic pouch mucosa.[139]
In a number of randomized clinical trials, probiotics have demonstrated the potential to be helpful in the treatment of ulcerative colitis. Specific types of probiotics such as Escherichia coli Nissle have been shown to induce remission in some people for up to a year.[140]
A Cochrane review of controlled trials using various probiotics found low-certainty evidence that probiotic supplements may increase the probability of clinical remission.[141] People receiving probiotics were 73% more likely to experience disease remission and over 2x as likely to report improvement in symptoms compared to those receiving a placebo, with no clear difference in minor or serious adverse effects.[141] Although there was no clear evidence of greater remission when probiotic supplements were compared with 5‐aminosalicylic acid treatment as a monotherapy, the likelihood of remission was 22% higher if probiotics were used in combination with 5-aminosalicylic acid therapy.[141]
It is unclear whether probiotics help to prevent future relapse in people with stable disease activity, either as a monotherapy or combination therapy.[142]
Fecal microbiota transplant involves the infusion of human probiotics through fecal enemas. Ulcerative colitis typically requires a more prolonged bacteriotherapy treatment than Clostridium difficile infection to be successful, possibly due to the time needed to heal the ulcerated epithelium. The response of ulcerative colitis is potentially very favorable with one study reporting 67.7% of people experiencing complete remission.[143] Other studies found a benefit from using fecal microbiota transplantation.[144][145][146]
A variety of alternative medicine therapies have been used for ulcerative colitis, with inconsistent results. Curcumin (turmeric) therapy, in conjunction with taking the medications mesalamine or sulfasalazine, may be effective and safe for maintaining remission in people with quiescent ulcerative colitis.[147][148] The effect of curcumin therapy alone on quiescent ulcerative colitis is unknown.[148]
Treatments using cannabis or cannabis oil are uncertain. So far, studies have not determined its effectiveness and safety.[149]
Many interventions have been considered to manage abdominal pain in people with ulcerative colitis, including FODMAPs diet, relaxation training, yoga, kefir diet and stellate ganglion block treatment. It is unclear whether any of these are safe or effective at improving pain or reducing anxiety and depression.[150]
Diet can play a role in symptoms of patients with ulcerative colitis.[151]
The most avoided foods by patients are spicy foods, dairy products, alcohol, fruits and vegetables and carbonated beverages; these foods are mainly avoided during remission and to prevent relapse. In some cases, especially in the flares period, the dietary restrictions of these patients can be very severe and can lead to a compromised nutritional state. Some patients tend to eliminate gluten spontaneously, despite not having a definite diagnosis of Coeliac disease, because they believe that gluten can exacerbate gastrointestinal symptoms.[152]
Many studies found that patients with IBD reported a higher frequency of depressive and anxiety disorders than the general population, and most studies confirm that women with IBD are more likely than men to develop affective disorders and show that up to 65% of them may have depression disorder and anxiety disorder.[153][154]
Poor prognostic factors include: age < 40 years upon diagnosis, extensive colitis, severe colitis on endoscopy, prior hospitalization, elevated CRP and low serum albumin.[19]
People with ulcerative colitis usually have an intermittent course, with periods of disease inactivity alternating with "flares" of disease. People with proctitis or left-sided colitis usually have a more benign course: only 15% progress proximally with their disease, and up to 20% can have sustained remission in the absence of any therapy. A subset of people experience a course of disease progress rapidly. In these cases, there is usually a failure to respond to medication and surgery often is performed within the first few years of disease onset.[155][156] People with more extensive disease are less likely to sustain remission, but the rate of remission is independent of the severity of the disease.[157] Several risk factors are associated with eventual need for colectomy, including: prior hospitalization for UC, extensive colitis, need for systemic steroids, young age at diagnosis, low serum albumin, elevated inflammatory markers (CRP & ESR), and severe inflammation seen during colonoscopy.[95][19] Surgical removal of the large intestine is necessary in some cases.[19]
The risk of colorectal cancer is significantly increased in people with ulcerative colitis after ten years if involvement is beyond the splenic flexure. People with backwash ileitis might have an increased risk for colorectal carcinoma.[158] Those people with only proctitis usually have no increased risk.[19] It is recommended that people have screening colonoscopies with random biopsies to look for dysplasia after eight years of disease activity, at one to two year intervals.[159]
People with ulcerative colitis are at similar[160] or perhaps slightly increased overall risk of death compared with the background population.[161] However, the distribution of causes-of-death differs from the general population.[160] Specific risk factors may predict worse outcomes and a higher risk of mortality in people with ulcerative colitis, including: C. difficile infection[19] and cytomegalovirus infection (due to reactivation).[162]
Together with Crohn's disease, about 11.2 million people were affected as of 2015[update].[163] Each year it newly occurs in 1 to 20 per 100,000 people, and 5 to 500 per 100,000 individuals are affected.[7][9] The disease is more common in North America and Europe than other regions.[9] Often it begins in people aged 15 to 30 years, or among those over 60.[1] Males and females appear to be affected in equal proportions.[7] It has also become more common since the 1950s.[7][9] Together, ulcerative colitis and Crohn's disease affect about a million people in the United States.[164] With appropriate treatment the risk of death appears the same as that of the general population.[3] The first description of ulcerative colitis occurred around the 1850s.[9]
Together with Crohn's disease, about 11.2 million people were affected as of 2015[update].[163] Each year, ulcerative colitis newly occurs in 1 to 20 per 100,000 people (incidence), and there are a total of 5–500 per 100,000 individuals with the disease (prevalence).[7][9] In 2015, a worldwide total of 47,400 people died due to inflammatory bowel disease (UC and Crohn's disease).[6] The peak onset is between 30 and 40 years of age,[12] with a second peak of onset occurring in the 6th decade of life.[165] Ulcerative colitis is equally common among men and women.[12][7] With appropriate treatment the risk of death appears similar to that of the general population.[3] UC has become more common since the 1950s.[7][9]
The geographic distribution of UC and Crohn's disease is similar worldwide,[166] with the highest number of new cases a year of UC found in Canada, New Zealand and the United Kingdom.[167] The disease is more common in North America and Europe than other regions.[9] In general, higher rates are seen in northern locations compared to southern locations in Europe[168] and the United States.[169] UC is more common in western Europe compared with eastern Europe.[170]  Worldwide, the prevalence of UC varies from 2 to 299 per 100,000 people.[5] Together, ulcerative colitis and Crohn's disease affect about a million people in the United States.[164]
As with Crohn's disease, the rates of UC are greater among Ashkenazi Jews and decreases progressively in other persons of Jewish descent, non-Jewish Caucasians, Africans, Hispanics, and Asians.[22] Appendectomy prior to age 20 for appendicitis[171] and current tobacco use[172] are protective against development of UC.[12] However, former tobacco use is associated with a higher risk of developing the disease.[172][12]
As of 2004[update], the number of new cases of UC in the United States was between 2.2 and 14.3 per 100,000 per year.[173] The number of people affected in the United States in 2004 was between 37 and 246 per 100,000.[173]
In Canada, between 1998 and 2000, the number of new cases per year was 12.9 per 100,000 population or 4,500 new cases. The number of people affected was estimated to be 211 per 100,000 or 104,000.[174]
In the United Kingdom 10 per 100,000 people newly develop the condition a year while the number of people affected is 243 per 100,000. Approximately 146,000 people in the United Kingdom have been diagnosed with UC.[175]
The term ulcerative colitis was first used by Samuel Wilks in 1859. The term entered general medical vocabulary afterwards in 1888 with William Hale-White publishing a report of various cases of "ulcerative colitis".[176]
Ulcerative Colitis was the first subtype of IBD to be identified.[176]
Helminthic therapy using the whipworm Trichuris suis has been shown in a randomized control trial from Iowa to show benefit in people with ulcerative colitis.[177] The therapy tests the hygiene hypothesis which argues that the absence of helminths in the colons of people in the developed world may lead to inflammation. Both helminthic therapy and fecal microbiota transplant induce a characteristic Th2 white cell response in the diseased areas, which was unexpected given that ulcerative colitis was thought to involve Th2 overproduction.[177]
Alicaforsen is a first generation antisense oligodeoxynucleotide designed to bind specifically to the human ICAM-1 messenger RNA through Watson-Crick base pair interactions in order to subdue expression of ICAM-1.[178] ICAM-1 propagates an inflammatory response promoting the extravasation and activation of leukocytes (white blood cells) into inflamed tissue.[178] Increased expression of ICAM-1 has been observed within the inflamed intestinal mucosa of ulcerative colitis patients, where ICAM-1 over production correlated with disease activity.[179] This suggests that ICAM-1 is a potential therapeutic target in the treatment of ulcerative colitis.[180]
Gram positive bacteria present in the lumen could be associated with extending the time of relapse for ulcerative colitis.[181]
A series of drugs in development looks to disrupt the inflammation process by selectively targeting an ion channel in the inflammation signaling cascade known as KCa3.1.[182] In a preclinical study in rats and mice, inhibition of KCa3.1 disrupted the production of Th1 cytokines IL-2 and TNF-α and decreased colon inflammation as effectively as sulfasalazine.[182]
Neutrophil extracellular traps[183] and the resulting degradation of the extracellular matrix[184] have been reported in the colon mucosa in ulcerative colitis patients in clinical remission, indicating the involvement of the innate immune system in the etiology.[183]
Fexofenadine, an antihistamine drug used in treatment of allergies, has shown promise in a combination therapy in some studies.[185][186] Opportunely, low gastrointestinal absorption (or high absorbed drug gastrointestinal secretion) of fexofenadine results in higher concentration at the site of inflammation. Thus, the drug may locally decrease histamine secretion by involved gastrointestinal mast cells and alleviate the inflammation.[186]
There is evidence that etrolizumab is effective for ulcerative colitis, with phase 3 trials underway as of 2016.[8][187][188][189] Etrolizumab is a humanized monoclonal antibody that targets he β7 subunit of integrins α4β7 and αEβ7. Etrolizumab decreases lymphocytes trafficking, similar to vedolizumab (another integrin antagonist).
A type of leukocyte apheresis, known as granulocyte and monocyte adsorptive apheresis, still requires large-scale trials to determine whether or not it is effective.[190] Results from small trials have been tentatively positive.[191]

Narcolepsy is a chronic neurological disorder that involves a decreased ability to regulate sleep–wake cycles.[1] Symptoms often include periods of excessive daytime sleepiness and brief involuntary sleep episodes. [1] Narcolepsy paired with cataplexy is evidenced to be an autoimmune disorder.[3] These experiences of cataplexy can be brought on by strong emotions. Less commonly, there may be vivid hallucinations or an inability to move (sleep paralysis) while falling asleep or waking up. People with narcolepsy tend to sleep about the same number of hours per day as people without, but the quality of sleep tends to be lessened.[1]
Narcolepsy is a clinical syndrome of hypothalamic disorder,[4] but the exact cause of narcolepsy is unknown, with potentially several causes.[5] In up to 10% of cases, there is a family history of the disorder. Often, those affected have low levels of the neuropeptide orexin, which may be due to an autoimmune disorder triggered in genetically susceptible individuals by infection with H1N1 influenza.[6] In rare cases, narcolepsy can be caused by traumatic brain injury, tumors, or other diseases affecting the parts of the brain that regulate wakefulness or REM sleep. Diagnosis is typically based on the symptoms and sleep studies, after ruling out other potential causes. Excessive daytime sleepiness can also be caused by other sleep disorders such as sleep apnea, major depressive disorder, anemia, heart failure, drinking alcohol and not getting enough sleep. The accompanying cataplexy may be mistaken for seizures.[1]
While there is no cure, a number of lifestyle changes and medications may help. Lifestyle changes include taking regular short naps and sleep hygiene. Medications used include modafinil, sodium oxybate and methylphenidate. While initially effective, tolerance to the benefits may develop over time. Tricyclic antidepressants and selective serotonin reuptake inhibitors (SSRIs) may improve cataplexy.[1]
Estimates of frequency range from 0.2 to 600 per 100,000 people in various countries.[2] The condition often begins in childhood, with males and females being affected equally. Untreated narcolepsy increases the risk of motor vehicle collisions and falls.[1]
Narcolepsy generally occurs anytime between early childhood and 50 years of age, and most commonly between 15 and 36 years of age. However, it may also rarely appear at any time outside of this range.[7]
There are two main characteristics of narcolepsy: excessive daytime sleepiness and abnormal REM sleep.[8] Excessive daytime sleepiness occurs even after adequate night time sleep. A person with narcolepsy is likely to become drowsy or fall asleep, often at inappropriate or undesired times and places, or just be very tired throughout the day. Narcoleptics may not be able to experience the amount of restorative deep sleep that healthy people experience due to abnormal REM regulation – they are not "over-sleeping". Narcoleptics typically have higher REM sleep density than non-narcoleptics, but also experience more REM sleep without atonia.[9] Many narcoleptics have sufficient REM sleep, but do not feel refreshed or alert throughout the day.[10] This can feel like living their entire lives in a constant state of sleep deprivation.[medical citation needed]
Excessive sleepiness can vary in severity, and it appears most commonly during monotonous situations that do not require much interaction.[10] Daytime naps may occur with little warning and may be physically irresistible. These naps can occur several times a day. They are typically refreshing, but only for a few hours or less. Vivid dreams may be experienced on a regular basis, even during very brief naps. Drowsiness may persist for prolonged periods or remain constant. In addition, night-time sleep may be fragmented, with frequent awakenings. A second prominent symptom of narcolepsy is abnormal REM sleep. Narcoleptics are unique in that they enter into the REM phase of sleep in the beginnings of sleep, even when sleeping during the day.[8]
The classic symptoms of the disorder, often referred to as the "tetrad of narcolepsy", are cataplexy, sleep paralysis, hypnagogic hallucinations, and excessive daytime sleepiness.[11] Other symptoms may include automatic behaviors and night-time wakefulness.[8][12][13] These symptoms may not occur in all people with narcolepsy.
In most cases, the first symptom of narcolepsy to appear is excessive and overwhelming daytime sleepiness. The other symptoms may begin alone or in combination months or years after the onset of the daytime naps. There are wide variations in the development, severity, and order of appearance of cataplexy, sleep paralysis, and hypnagogic hallucinations in individuals. Only about 20 to 25 percent of people with narcolepsy experience all four symptoms. The excessive daytime sleepiness generally persists throughout life, but sleep paralysis and hypnagogic hallucinations may not.
Many people with narcolepsy also have insomnia for extended periods of time. The excessive daytime sleepiness and cataplexy often become severe enough to cause serious problems in a person's social, personal, and professional life. Normally, when an individual is awake, brain waves show a regular rhythm. When a person first falls asleep, the brain waves become slower and less regular, which is called non-rapid eye movement (NREM) sleep. After about an hour and a half of NREM sleep, the brain waves begin to show a more active pattern again, called REM sleep (rapid eye movement sleep), when most remembered dreaming occurs. Associated with the EEG-observed waves during REM sleep, muscle atonia is present called REM atonia.[medical citation needed]
In narcolepsy, the order and length of NREM and REM sleep periods are disturbed, with REM sleep occurring at sleep onset instead of after a period of NREM sleep. Also, some aspects of REM sleep that normally occur only during sleep, like lack of muscular control, sleep paralysis, and vivid dreams, occur at other times in people with narcolepsy. For example, the lack of muscular control can occur during wakefulness in a cataplexy episode; it is said that there is an intrusion of REM atonia during wakefulness. Sleep paralysis and vivid dreams can occur while falling asleep or waking up. Simply put, the brain does not pass through the normal stages of dozing and deep sleep but goes directly into (and out of) rapid eye movement (REM) sleep.[medical citation needed]
As a consequence night time sleep does not include as much deep sleep, so the brain tries to "catch up" during the day, hence excessive daytime sleepiness. People with narcolepsy may visibly fall asleep at unpredicted moments (such motions as head bobbing are common). People with narcolepsy fall quickly into what appears to be very deep sleep, and they wake up suddenly and can be disoriented when they do (dizziness is a common occurrence). They have very vivid dreams, which they often remember in great detail. People with narcolepsy may dream even when they only fall asleep for a few seconds. Along with vivid dreaming, people with narcolepsy are known to have audio or visual hallucinations prior to falling asleep or before waking up.[16]
Narcoleptics can gain excess weight; children can gain 20 to 40 pounds (9.1 to 18.1 kg) when they first develop narcolepsy; in adults the body-mass index is about 15% above average.[17][18]
The exact cause of narcolepsy is unknown, and it may be caused by several distinct factors.[1][5] The mechanism involves the loss of orexin-releasing neurons within the lateral hypothalamus (about 70,000 neurons[19]).[20][21]
Some researches indicated that people with type 1 narcolepsy (narcolepsy with cataplexy) have a lower level of orexin (hypocretin), which is a chemical contributing to the regulation of wakefulness and REM sleep.[22] It also acts as a neurotransmitter to enable nerve cells to communicate.[7]
In up to 10% of cases there is a family history of the disorder. Family history is more common in narcolepsy with cataplexy.[1] There is a strong link with certain genetic variants,[20] which may make T-cells susceptible to react to the orexin-releasing neurons (autoimmunity)[23] after being stimulated by infection with H1N1 influenza.[6] In addition to genetic factors, low levels of orexin peptides have been correlated with a history of infection, diet, contact with toxins such as pesticides, and brain injuries due to head trauma, brain tumors or strokes.[8][20]
The primary genetic factor that has been strongly implicated in the development of narcolepsy involves an area of chromosome 6 known as the human leukocyte antigen (HLA) complex.[20][24] Specific variations in HLA genes are strongly correlated with the presence of narcolepsy (HLA DQB1*06:02, frequently in combination with HLA DRB1*15:01);[20] however, these variations are not required for the condition to occur and sometimes occur in individuals without narcolepsy.[20][25] These genetic variations in the HLA complex are thought to increase the risk of an auto-immune response to orexin-releasing neurons in the lateral hypothalamus.[20][21][25]
The allele HLA-DQB1*06:02 of the human gene HLA-DQB1 was reported in more than 90% of people with narcolepsy, and alleles of other HLA genes such as HLA-DQA1*01:02 have been linked. A 2009 study found a strong association with polymorphisms in the TRAC gene locus (dbSNP IDs rs1154155, rs12587781, and rs1263646).[19] A 2013 review article reported additional but weaker links to the loci of the genes TNFSF4 (rs7553711), Cathepsin H (rs34593439), and P2RY11-DNMT1 (rs2305795).[26] Another gene locus that has been associated with narcolepsy is EIF3G (rs3826784).[27]
Type 1 narcolepsy is caused by hypocretin/orexin neuronal loss. T-cells have been demonstrated to be cross-reactive to both a particular piece of the hemagglutinin flu protein of the pandemic 2009 H1N1 and the amidated terminal ends of the secreted hypocretin peptides.[6]
Genes associated with narcolepsy mark the particular HLA heterodimer (DQ0602) involved in presentation of these antigens and modulate expression of the specific T cell receptor segments (TRAJ24 and TRBV4-2) involved in T cell receptor recognition of these antigens, suggesting causality.[6]
A link between GlaxoSmithKline's H1N1 flu vaccine Pandemrix and narcolepsy has been found in both children and adults.[28] In 2010, Finland's National Institute of Health and Welfare recommended that Pandemrix vaccinations be suspended pending further investigation into narcolepsy.[29][30] In 2018, it was demonstrated that T-cells stimulated by Pandemrix were cross-reactive by molecular mimicry with part of the hypocretin peptide, the loss of which is associated with type I narcolepsy.[6]
Orexin, otherwise known as hypocretin, is a neuropeptide that acts within the brain to regulate appetite and wakefulness as well as a number of other cognitive and physiological processes.[20][31][32] Loss of these orexin-producing neurons causes narcolepsy and most individuals with narcolepsy have a reduced number of these neurons in their brains.[20][21][25] Selective destruction of the HCRT/OX neurons with preservation of proximate structures suggests a highly specific autoimmune pathophysiology.[33] Cerebrospinal fluid HCRT-1/OX-A is undetectable in up to 95% of patients with type 1 narcolepsy.[33]
The system which regulates sleep, arousal, and transitions between these states in humans is composed of three interconnected subsystems: the orexin projections from the lateral hypothalamus, the reticular activating system, and the ventrolateral preoptic nucleus.[21] In narcoleptic individuals, these systems are all associated with impairments due to a greatly reduced number of hypothalamic orexin projection neurons and significantly fewer orexin neuropeptides in cerebrospinal fluid and neural tissue, compared to non-narcoleptic individuals.[21] Those with narcolepsy generally experience the REM stage of sleep within five minutes of falling asleep, while people who do not have narcolepsy (unless they are significantly sleep deprived)[34] do not experience REM until after a period of slow-wave sleep, which lasts for about the first hour or so of a sleep cycle.[1]
The neural control of normal sleep states and the relationship to narcolepsy are only partially understood. In humans, narcoleptic sleep is characterized by a tendency to go abruptly from a waking state to REM sleep with little or no intervening non-REM sleep. The changes in the motor and proprioceptive systems during REM sleep have been studied in both human and animal models. During normal REM sleep, spinal and brainstem alpha motor neuron hyperpolarization produces almost complete atonia of skeletal muscles via an inhibitory descending reticulospinal pathway. Acetylcholine may be one of the neurotransmitters involved in this pathway. In narcolepsy, the reflex inhibition of the motor system seen in cataplexy has features normally seen only in normal REM sleep.[1]
The third edition of the International Classification of Sleep Disorders (ICSD-3) differentiates between narcolepsy with cataplexy (type 1) and narcolepsy without cataplexy (type 2), while the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) uses the diagnosis of narcolepsy to refer to type 1 narcolepsy only. The DSM-5 refers to narcolepsy without cataplexy as hypersomnolence disorder.[35] The most recent edition of the International Classification of Diseases, ICD-11, currently identifies three types of narcolepsy: type 1 narcolepsy, type 2 narcolepsy, and unspecified narcolepsy.[36]
ICSD-3 diagnostic criteria posits that the individual must experience "daily periods of irrepressible need to sleep or daytime lapses into sleep" for both subtypes of narcolepsy.[35] This symptom must last for at least three months. For a diagnosis of type 1 narcolepsy, the person must present with either cataplexy, a mean sleep latency of less than 8 minutes, and two or more sleep-onset REM periods (SOREMPs), or they must present with a hypocretin-1 concentration of less than 110 pg/mL.[35] A diagnosis of type 2 narcolepsy requires a mean sleep latency of less than 8 minutes, two or more SOREMPs, and a hypocretin-1 concentration of more than 110 pg/mL. In addition, the hypersomnolence and sleep latency findings cannot be better explained by other causes.[35]
DSM-5 narcolepsy criteria requires that the person to display recurrent periods of "an irrepressible need to sleep, lapsing into sleep, or napping" for at least three times a week over a period of three months.[35] The individual must also display one of the following: cataplexy, hypocretin-1 concentration of less than 110 pg/mL, REM sleep latency of less than 15 minutes, or a multiple sleep latency test (MSLT) showing sleep latency of less than 8 minutes and two or more SOREMPs.[35] For a diagnosis of hypersomnolence disorder, the individual must present with excessive sleepiness despite at least 7 hours of sleep as well as either recurrent lapses into daytime sleep, nonrestorative sleep episodes of 9 or more hours, or difficulty staying awake after awakening. In addition, the hypersomnolence must occur at least three times a week for a period of three months, and must be accompanied by significant distress or impairment. It also cannot be explained by another sleep disorder, coexisting mental or medical disorders, or medication.[37]
Diagnosis is relatively easy when all the symptoms of narcolepsy are present, but if the sleep attacks are isolated and cataplexy is mild or absent, diagnosis is more difficult. Three tests that are commonly used in diagnosing narcolepsy are polysomnography (PSG), the multiple sleep latency test (MSLT), and the Epworth Sleepiness Scale (ESS). These tests are usually performed by a sleep specialist.[38]
Polysomnography involves the continuous recording of sleep brain waves and a number of nerve and muscle functions during night time sleep. When tested, people with narcolepsy fall asleep rapidly, enter REM sleep early, and may often awaken during the night. The polysomnogram also helps to detect other possible sleep disorders that could cause daytime sleepiness.[citation needed]
The Epworth Sleepiness Scale is a brief questionnaire that is administered to determine the likelihood of the presence of a sleep disorder, including narcolepsy.[citation needed]
The multiple sleep latency test is performed after the person undergoes an overnight sleep study. The person will be asked to sleep once every 2 hours, and the time it takes for them to do so is recorded. Most individuals will fall asleep within 5 to 8 minutes, as well as display REM sleep faster than non-narcoleptic people.[citation needed]
Measuring orexin levels in a person's cerebrospinal fluid sampled in a spinal tap may help in diagnosing narcolepsy, with abnormally low levels serving as an indicator of the disorder.[39] This test can be useful when MSLT results are inconclusive or difficult to interpret.[40]
People with narcolepsy can be substantially helped, but not cured currently. However, the technology exists in early form such as experiments in using the prepro-orexin transgene via gene editing restored normal function in mice models by making other neurons produce orexin after the original set have been destroyed, or replacing the missing orexinergic neurons with hypocretin stem cell transplantation, are both steps in that direction for fixing the biology effectively permanently once applied in humans.[41][42] Additionally effective ideal non-gene editing and chemical-drug methods involve hypocretin treatments methods such as future drugs like hypocretin agonists (such as danavorexton)[43] or hypocretin replacement, in the form of hypocretin 1 given intravenous (injected into the veins), intracisternal (direct injection into the brain), and intranasal (sprayed through the nose), the latter being low in efficacy, at the low amount used in current experiments but may be effective at very high doses in the future.[44][45]
General strategies like people and family education, sleep hygiene and medication compliance, and discussion of safety issues for example driving license can be useful. Potential side effects of medication can also be addressed.[10] Regular follow-up is useful to be able to monitor the response to treatment, to assess the presence of other sleep disorders like obstructive sleep apnea, and to discuss psychosocial issues.[10]
In many cases, planned regular short naps can reduce the need for pharmacological treatment of the EDS, but only improve symptoms for a short duration. A 120-minute nap provided benefit for 3 hours in the person's alertness whereas a 15-minute nap provided no benefit.[46] Daytime naps are not a replacement for night time sleep. Ongoing communication between the health care provider, person, and their family members is important for optimal management of narcolepsy.
The main treatment of excessive daytime sleepiness in narcolepsy is central nervous system stimulants such as methylphenidate, amphetamine, dextroamphetamine, modafinil, and armodafinil. In late 2007 an alert for severe adverse skin reactions to modafinil was issued by the FDA.[47] Pemoline was previously used but was withdrawn due to toxicity.[48]
Another drug that is used is atomoxetine, a non-stimulant and a norepinephrine reuptake inhibitor (NRI), which has no addiction liability or recreational effects.[48] Other NRIs like viloxazine and reboxetine have also been used in the treatment of narcolepsy.[48] Additional related medications include mazindol and selegiline.[48]
Another FDA-approved treatment option for narcolepsy is sodium oxybate,[49] also known as sodium gamma-hydroxybutyrate (GHB). It can be used for cataplexy associated with narcolepsy and excessive daytime sleepiness associated with narcolepsy.[49][50][10][51] Several studies also showed that sodium oxybate is effective to treat cataplexy.[10]
Solriamfetol is a new molecule indicated for narcolepsy of type 1 and 2.[52] Solriamfetol works by inhibiting the reuptake of the monoamines via the interaction with both the dopamine transporter and the norepinephrine transporter. This mechanism differs from that of the wake-promoting agents modafinil and armodafinil. These are thought to bind primarily at the dopamine transporter to inhibit the reuptake of dopamine. Solriamfetol also differs from amphetamines as it does not promote the release of norepinephrine in the brain.[53]
Narcolepsy has sometimes been treated with selective serotonin reuptake inhibitors and tricyclic antidepressants, such as clomipramine, imipramine, or protriptyline, as well as other drugs that suppress REM sleep.[54] Venlafaxine, an antidepressant which blocks the reuptake of serotonin and norepinephrine, has shown usefulness in managing symptoms of cataplexy,[55] but it has notable side effects including sleep disruption.[56] The antidepressant class is used mainly for the treatment of cataplexy; for people with narcolepsy without cataplexy these are usually not used.[57]
Common behavioral treatments for childhood narcolepsy include improved sleep hygiene, scheduled naps, and physical exercise.[58]
Many medications are used in treating adults and may be used to treat children. These medications include central nervous system stimulants such as methylphenidate, modafinil, amphetamine, and dextroamphetamine.[59] Other medications, such as sodium oxybate[52] or atomoxetine, may also be used to counteract sleepiness. Medications such as sodium oxybate, venlafaxine, fluoxetine, and clomipramine may be prescribed if the child presents with cataplexy.[60]
Estimates of frequency range from 0.2 per 100,000 in Israel to 600 per 100,000 in Japan.[2] These differences may be due to how the studies were conducted or the populations themselves.[2]
In the United States, narcolepsy is estimated to affect as many as 200,000 Americans, but fewer than 50,000 are diagnosed. The prevalence of narcolepsy is about 1 per 2,000 persons.[61] Narcolepsy is often mistaken for depression, epilepsy, the side effects of medications, poor sleeping habits or recreational drug use, making misdiagnosis likely.[citation needed] While narcolepsy symptoms are often confused with depression, there is a link between the two disorders. Research studies have mixed results on co-occurrence of depression in people with narcolepsy, as the numbers quoted by different studies are anywhere between 6% and 50%.[62]
Narcolepsy can occur in both men and women at any age, although typical symptom onset occurs in adolescence and young adulthood. There is about a ten-year delay in diagnosing narcolepsy in adults.[18] Cognitive, educational, occupational, and psychosocial problems associated with the excessive daytime sleepiness of narcolepsy have been documented. For these to occur in the crucial teen years when education, development of self-image, and development of occupational choice are taking place is especially devastating. While cognitive impairment does occur, it may only be a reflection of the excessive daytime somnolence.[63]
In 2015, it was reported that the British Department of Health was paying for sodium oxybate medication at a cost of £12,000 a year for 80 people who are taking legal action over problems linked to the use of the Pandemrix swine flu vaccine. Sodium oxybate is not available to people with narcolepsy through the National Health Service.[64]
The term "narcolepsy" is from the French narcolepsie.[65] The French term was first used in 1880 by Jean-Baptiste-Édouard Gélineau, who used the Greek νάρκη (narkē), meaning "numbness", and λῆψις (lepsis) meaning "attack".[65]
It remains to be seen whether H3 antagonists (i.e., compounds such as pitolisant that promote the release of the wakefulness-promoting molecule amine histamine) will be particularly useful as wake-promoting agents.[66] However, usage now does exist in various nations such as in France, United Kingdom's (NHS as of September 2016[update][67][68][69][70]) after being given marketing authorisation by European Commission on the advice of the European Medicines Agency and in the United States by the approval of the Food and Drug Administration (FDA) as of August 2019[update].[71]
Given the possible role of hyper-active GABAA receptors in the primary hypersomnias (narcolepsy and idiopathic hypersomnia), medications that could counteract this activity are being studied to test their potential to improve sleepiness. These currently include clarithromycin and flumazenil.[72][73]
Flumazenil is the only GABAA receptor antagonist on the market as of January 2013, and it is currently manufactured only as an intravenous formulation. Given its pharmacology, researchers consider it to be a promising medication in the treatment of primary hypersomnias. Results of a small, double-blind, randomized, controlled clinical trial were published in November 2012. This research showed that flumazenil provides relief for most people whose CSF contains the unknown "somnogen" that enhances the function of GABAA receptors, making them more susceptible to the sleep-inducing effect of GABA. For one person, daily administration of flumazenil by sublingual lozenge and topical cream has proven effective for several years.[72][74] A 2014 case report also showed improvement in primary hypersomnia symptoms after treatment with a continuous subcutaneous flumazenil infusion.[75] The supply of generic flumazenil was initially thought to be too low to meet the potential demand for treatment of primary hypersomnias.[76] However, this scarcity has eased, and dozens of people are now being treated with flumazenil off-label.[77]
In a test tube model, clarithromycin (an antibiotic approved by the FDA for the treatment of infections) was found to return the function of the GABA system to normal in people with primary hypersomnias. Investigators therefore treated a few people with narcolepsy with off-label clarithromycin, and most felt their symptoms improved with this treatment. In order to help further determine whether clarithromycin is truly beneficial for the treatment of narcolepsy and idiopathic hypersomnia, a small, double-blind, randomized, controlled clinical trial was completed in 2012.[73] "In this pilot study, clarithromycin improved subjective sleepiness in GABA-related hypersomnia. Larger trials of longer duration are warranted."[78] In 2013, a retrospective review evaluating longer-term clarithromycin use showed efficacy in a large percentage of people with GABA-related hypersomnia.[79] "It is important to note that the positive effect of clarithromycin is secondary to a benzodiazepine antagonist-like effect, not its antibiotic effects, and treatment must be maintained."[66]
Orexin-A (a.k.a. hypocretin-1) has been shown to be strongly wake-promoting in animal models, but it does not cross the blood–brain barrier. The first line treatment for narcolepsy, modafinil, has been found to interact indirectly with the orexin system. It is also likely that an orexin receptor agonist will be found and developed for the treatment of hypersomnia.[66] One such agent which is currently in clinical trials is danavorexton.[80][81]
Abnormally low levels of acylcarnitine have been observed in people with narcolepsy.[82] These same low levels have been associated with primary hypersomnia in general in mouse studies. "Mice with systemic carnitine deficiency exhibit a higher frequency of fragmented wakefulness and rapid eye movement (REM) sleep, and reduced locomotor activity." Administration of acetyl-L-carnitine was shown to improve these symptoms in mice.[83] A subsequent human trial found that people with narcolepsy given L-carnitine spent less total time in daytime sleep than people who were given a placebo.[84]
Animal studies try to mimic the disorder in humans by either modifying the Hypocretin/Orexin receptors or by eliminating this peptide.[85] An orexin deficit caused by the degeneration of hypothalamic neurons is suggested to be one of the causes of narcolepsy.[86] More recent clinical studies on both animals and humans have also revealed that hypocretin is involved in other functions beside regulation of wakefulness and sleep. These functions include autonomic regulation, emotional processing, reward learning behaviour or energy homeostasis. In studies where the concentration of the hypocretin was measured under different circumstances, it was observed that the hypocretin levels increased with the positive emotion, anger or social interaction but stayed low during sleep or during pain experience.[87]
The most reliable and valid animal models developed are the canine (narcoleptic dogs) and the rodent (orexin-deficient mice) ones which helped investigating the narcolepsy and set the focus on the role of orexin in this disorder.[86]
Dogs, as well as other species like cats or horses, can also exhibit spontaneous narcolepsy with similar symptoms as the ones reported in humans. The attacks of cataplexy in dogs can involve partial or full collapse.[86] Narcolepsy with cataplexy was identified in a few breeds like Labrador retrievers or Doberman pinschers where it was investigated the possibility to inherit this disorder in the autosomal recessive mode.[88] According to [85] a reliable canine model for narcolepsy would be the one in which the narcoleptic symptoms are the result of a mutation in the gene HCRT 2. The animals affected exhibited excessive daytime sleepiness with a reduced state of vigilance and severe cataplexy resulted after palatable food and interactions with the owners or with other animals.[85]
Mice that are genetically engineered to lack orexin genes demonstrate many similarities to human narcolepsy. During nocturnal hours, when mice are normally present, those lacking orexin demonstrated murine cataplexy and displayed brain and muscle electrical activity similar to the activity present during REM and NREM sleep. This cataplexy is able to be triggered through social interaction, wheel running, and ultrasonic vocalizations. Upon awakening, the mice also display behavior consistent with excessive daytime sleepiness.[86]
Mouse models have also been used to test whether the lack of orexin neurons is correlated with narcolepsy. Mice whose orexin neurons have been ablated have shown sleep fragmentation, SOREMPs, and obesity.[86]
Rat models have been used to demonstrate the association between orexin deficiency and narcoleptic symptoms. Rats who lost the majority of their orexinergic neurons exhibited multiple SOREMPs as well as less wakefulness during nocturnal hours, shortened REM latency, and brief periods of cataplexy.[86]


Alzheimer's disease (AD) is a neurodegenerative disease that usually starts slowly and progressively worsens,[2] and is the cause of 60–70% of cases of dementia.[2][10] The most common early symptom is difficulty in remembering recent events.[1] As the disease advances, symptoms can include problems with language, disorientation (including easily getting lost), mood swings, loss of motivation, self-neglect, and behavioral issues.[2] As a person's condition declines, they often withdraw from family and society.[11] Gradually, bodily functions are lost, ultimately leading to death. Although the speed of progression can vary, the typical life expectancy following diagnosis is three to nine years.[needs update][8][12]
The cause of Alzheimer's disease is poorly understood.[11] There are many environmental and genetic risk factors associated with its development. The strongest genetic risk factor is from an allele of APOE.[13][14] Other risk factors include a history of head injury, clinical depression, and high blood pressure.[1] The disease process is largely associated with amyloid plaques, neurofibrillary tangles, and loss of neuronal connections in the brain.[15] A probable diagnosis is based on the history of the illness and cognitive testing, with medical imaging and blood tests to rule out other possible causes.[5][16] Initial symptoms are often mistaken for normal brain aging.[11] Examination of brain tissue is needed for a definite diagnosis, but this can only take place after death.[17] Good nutrition, physical activity, and engaging socially are known to be of benefit generally in aging, and may help in reducing the risk of cognitive decline and Alzheimer's.[15]
No treatments can stop or reverse its progression, though some may temporarily improve symptoms.[2] Affected people become increasingly reliant on others for assistance, often placing a burden on caregivers.[18] The pressures can include social, psychological, physical, and economic elements.[18] Exercise programs may be beneficial with respect to activities of daily living and can potentially improve outcomes.[19] Behavioral problems or psychosis due to dementia are often treated with antipsychotics, but this is not usually recommended, as there is little benefit and an increased risk of early death.[20][21]
As of 2020, there were approximately 50 million people worldwide with Alzheimer's disease.[9] It most often begins in people over 65 years of age, although up to 10% of cases are early-onset impacting those in their 30s to mid-60s.[17][4] It affects about 6% of people 65 years and older,[11] and women more often than men.[22] The disease is named after German psychiatrist and pathologist Alois Alzheimer, who first described it in 1906.[23] Alzheimer's financial burden on society is large, with an estimated global annual cost of US$1 trillion.[9] It is ranked as the seventh leading cause of death in the United States.[15]
The course of Alzheimer's is generally described in three stages, with a progressive pattern of cognitive and functional impairment.[24][17] The three stages are described as early or mild, middle or moderate, and late or severe.[24] The disease is known to target the hippocampus which is associated with memory, and this is responsible for the first symptoms of memory impairment. As the disease progresses so does the degree of memory impairment.[15]
The first symptoms are often mistakenly attributed to aging or stress.[25] Detailed neuropsychological testing can reveal mild cognitive difficulties up to eight years before a person fulfills the clinical criteria for diagnosis of Alzheimer's disease.[26] These early symptoms can affect the most complex activities of daily living.[27] The most noticeable deficit is short term memory loss, which shows up as difficulty in remembering recently learned facts and inability to acquire new information.[26]
Subtle problems with the executive functions of attentiveness, planning, flexibility, and abstract thinking, or impairments in semantic memory (memory of meanings, and concept relationships) can also be symptomatic of the early stages of Alzheimer's disease.[26] Apathy and depression can be seen at this stage, with apathy remaining as the most persistent symptom throughout the course of the disease.[28][29] Mild cognitive impairment (MCI) is often found to be a transitional stage between normal aging and dementia. MCI can present with a variety of symptoms, and when memory loss is the predominant symptom, it is termed amnestic MCI and is frequently seen as a prodromal stage of Alzheimer's disease.[30] Amnestic MCI has a greater than 90% likelihood of being associated with Alzheimer's.[31]
In people with Alzheimer's disease, the increasing impairment of learning and memory eventually leads to a definitive diagnosis. In a small percentage, difficulties with language, executive functions, perception (agnosia), or execution of movements (apraxia) are more prominent than memory problems.[32] Alzheimer's disease does not affect all memory capacities equally. Older memories of the person's life (episodic memory), facts learned (semantic memory), and implicit memory (the memory of the body on how to do things, such as using a fork to eat or how to drink from a glass) are affected to a lesser degree than new facts or memories.[33][34]
Language problems are mainly characterised by a shrinking vocabulary and decreased word fluency, leading to a general impoverishment of oral and written language.[32][35] In this stage, the person with Alzheimer's is usually capable of communicating basic ideas adequately.[32][35][36] While performing fine motor tasks such as writing, drawing, or dressing, certain movement coordination and planning difficulties (apraxia) may be present, but they are commonly unnoticed.[32] As the disease progresses, people with Alzheimer's disease can often continue to perform many tasks independently, but may need assistance or supervision with the most cognitively demanding activities.[32]
Progressive deterioration eventually hinders independence, with subjects being unable to perform most common activities of daily living.[32] Speech difficulties become evident due to an inability to recall vocabulary, which leads to frequent incorrect word substitutions (paraphasias). Reading and writing skills are also progressively lost.[32][36] Complex motor sequences become less coordinated as time passes and Alzheimer's disease progresses, so the risk of falling increases.[32] During this phase, memory problems worsen, and the person may fail to recognise close relatives.[32] Long-term memory, which was previously intact, becomes impaired.[32]
Behavioral and neuropsychiatric changes become more prevalent. Common manifestations are wandering, irritability and emotional lability, leading to crying, outbursts of unpremeditated aggression, or resistance to caregiving.[32] Sundowning can also appear.[37] Approximately 30% of people with Alzheimer's disease develop illusionary misidentifications and other delusional symptoms.[32] Subjects also lose insight of their disease process and limitations (anosognosia).[32] Urinary incontinence can develop.[32] These symptoms create stress for relatives and caregivers, which can be reduced by moving the person from home care to other long-term care facilities.[32][38]
During the final stage, known as the late-stage or severe stage, there is complete dependence on caregivers.[15][24][32] Language is reduced to simple phrases or even single words, eventually leading to complete loss of speech.[32][36] Despite the loss of verbal language abilities, people can often understand and return emotional signals. Although aggressiveness can still be present, extreme apathy and exhaustion are much more common symptoms. People with Alzheimer's disease will ultimately not be able to perform even the simplest tasks independently; muscle mass and mobility deteriorates to the point where they are bedridden and unable to feed themselves. The cause of death is usually an external factor, such as infection of pressure ulcers or pneumonia, not the disease itself.[32] In some cases, there is a paradoxical lucidity immediately before death, where there is an unexpected recovery of mental clarity.[39]
Alzheimer's disease is believed to occur when abnormal amounts of amyloid beta (Aβ), accumulating extracellularly as amyloid plaques and tau proteins, or intracellularly as neurofibrillary tangles, form in the brain, affecting neuronal functioning and connectivity, resulting in a progressive loss of brain function.[40][41] This altered protein clearance ability is age-related, regulated by brain cholesterol,[42] and associated with other neurodegenerative diseases.[43][44]
The cause for most Alzheimer's cases is still mostly unknown,[9] except for 1–2% of cases where deterministic genetic differences have been identified.[13] Several competing hypotheses attempt to explain the underlying cause; the most predominant hypothesis is the amyloid beta (Aβ) hypothesis.[9]
The oldest hypothesis, on which most drug therapies are based, is the cholinergic hypothesis, which proposes that Alzheimer's disease is caused by reduced synthesis of the neurotransmitter acetylcholine.[9] The loss of cholinergic neurons noted in the limbic system and cerebral cortex, is a key feature in the progression of Alzheimer's.[30] The 1991 amyloid hypothesis postulated that extracellular amyloid beta (Aβ) deposits are the fundamental cause of the disease.[45][46] Support for this postulate comes from the location of the gene for the amyloid precursor protein (APP) on chromosome 21, together with the fact that people with trisomy 21 (Down syndrome) who have an extra gene copy almost universally exhibit at least the earliest symptoms of Alzheimer's disease by 40 years of age.[7] A specific isoform of apolipoprotein, APOE4, is a major genetic risk factor for Alzheimer's disease.[10] While apolipoproteins enhance the breakdown of beta amyloid, some isoforms are not very effective at this task (such as APOE4), leading to excess amyloid buildup in the brain.[47]
Late-onset Alzheimer's is about 70% heritable.[48][49] Genetic models in 2020 predict Alzheimer's disease with 90% accuracy.[50] Most cases of Alzheimer's are not familial, and so they are termed sporadic Alzheimer's disease.[medical citation needed] Most cases of sporadic Alzheimer's disease are late onset, developing after the age of 65 years.[51]
The strongest genetic risk factor for sporadic Alzheimer's disease is APOEε4.[14] APOEε4 is one of four alleles of apolipoprotein E (APOE). APOE plays a major role in lipid-binding proteins in lipoprotein particles and the ε4 allele disrupts this function.[52] Between 40 and 80% of people with Alzheimer's disease possess at least one APOEε4 allele.[53] The APOEε4 allele increases the risk of the disease by three times in heterozygotes and by 15 times in homozygotes.[54] Like many human diseases, environmental effects and genetic modifiers result in incomplete penetrance. For example, Nigerian Yoruba people do not show the relationship between dose of APOEε4 and incidence or age-of-onset for Alzheimer's disease seen in other human populations.[55][56]
Only 1–2% of Alzheimer's cases are inherited due to autosomal dominant effects, as Alzheimer's is highly polygenic. When the disease is caused by autosomal dominant variants, it is known as early onset familial Alzheimer's disease, which is rarer and has a faster rate of progression.[13] Less than 5% of sporadic Alzheimer's disease have an earlier onset,[13] and early-onset Alzheimer's is about 90% heritable.[48][49] FAD usually implies multiple persons affected in one or more generation.[medical citation needed][57]
Early onset familial Alzheimer's disease can be attributed to mutations in one of three genes: those encoding amyloid-beta precursor protein (APP) and presenilins PSEN1 and PSEN2.[31] Most mutations in the APP and presenilin genes increase the production of a small protein called amyloid beta (Aβ)42, which is the main component of amyloid plaques.[58] Some of the mutations merely alter the ratio between Aβ42 and the other major forms—particularly Aβ40—without increasing Aβ42 levels in the brain.[59] Two other genes associated with autosomal dominant Alzheimer's disease are ABCA7 and SORL1.[60]
Alleles in the TREM2 gene have been associated with a three to five times higher risk of developing Alzheimer's disease.[61]
A Japanese pedigree of familial Alzheimer's disease was found to be associated with a deletion mutation of codon 693 of APP.[62] This mutation and its association with Alzheimer's disease was first reported in 2008,[63] and is known as the Osaka mutation. Only homozygotes with this mutation have an increased risk of developing Alzheimer's disease. This mutation accelerates Aβ oligomerization but the proteins do not form the amyloid fibrils that aggregate into amyloid plaques, suggesting that it is the Aβ oligomerization rather than the fibrils that may be the cause of this disease. Mice expressing this mutation have all the usual pathologies of Alzheimer's disease.[64]
The tau hypothesis proposes that tau protein abnormalities initiate the disease cascade.[46] In this model, hyperphosphorylated tau begins to pair with other threads of tau as paired helical filaments. Eventually, they form neurofibrillary tangles inside nerve cell bodies.[65] When this occurs, the microtubules disintegrate, destroying the structure of the cell's cytoskeleton which collapses the neuron's transport system.[66]
A number of studies connect the misfolded amyloid beta and tau proteins associated with the pathology of Alzheimer's disease, as bringing about oxidative stress that leads to chronic inflammation.[67] Sustained inflammation (neuroinflammation) is also a feature of other neurodegenerative diseases including Parkinson's disease, and ALS.[68] Spirochete infections have also been linked to dementia.[9] DNA damages accumulate in AD brains; reactive oxygen species may be the major source of this DNA damage.[69]
Sleep disturbances are seen as a possible risk factor for inflammation in Alzheimer's disease. Sleep problems have been seen as a consequence of Alzheimer's disease but studies suggest that they may instead be a causal factor. Sleep disturbances are thought to be linked to persistent inflammation.[70]
The cellular homeostasis of biometals such as ionic copper, iron, and zinc is disrupted in Alzheimer's disease, though it remains unclear whether this is produced by or causes the changes in proteins.[9][71] Smoking is a significant Alzheimer's disease risk factor.[1] Systemic markers of the innate immune system are risk factors for late-onset Alzheimer's disease.[72] Exposure to air pollution may be a contributing factor to the development of Alzheimer's disease.[9]

Retrogenesis is a medical hypothesis that just as the fetus goes through a process of neurodevelopment beginning with neurulation and ending with myelination, the brains of people with Alzheimer's disease go through a reverse neurodegeneration process starting with demyelination and death of axons (white matter) and ending with the death of grey matter.[73] Likewise the hypothesis is, that as infants go through states of cognitive development, people with Alzheimer's disease go through the reverse process of progressive cognitive impairment.[74]
The association with celiac disease is unclear, with a 2019 study finding no increase in dementia overall in those with CD, while a 2018 review found an association with several types of dementia including Alzheimer's disease.[75][76]
According to one theory, dysfunction of oligodendrocytes and their associated myelin during aging contributes to axon damage, which in turn generates in amyloid production and tau hyper-phosphorylation.[77][78]
Alzheimer's disease is characterised by loss of neurons and synapses in the cerebral cortex and certain subcortical regions. This loss results in gross atrophy of the affected regions, including degeneration in the temporal lobe and parietal lobe, and parts of the frontal cortex and cingulate gyrus.[79] Degeneration is also present in brainstem nuclei particularly the locus coeruleus in the pons.[80] Studies using MRI and PET have documented reductions in the size of specific brain regions in people with Alzheimer's disease as they progressed from mild cognitive impairment to Alzheimer's disease, and in comparison with similar images from healthy older adults.[81][82]
Both Aβ plaques and neurofibrillary tangles are clearly visible by microscopy in brains of those with Alzheimer's disease,[83] especially in the hippocampus.[84] However, Alzheimer's disease may occur without neurofibrillary tangles in the neocortex.[85] Plaques are dense, mostly insoluble deposits of beta-amyloid peptide and cellular material outside and around neurons. Tangles (neurofibrillary tangles) are aggregates of the microtubule-associated protein tau which has become hyperphosphorylated and accumulate inside the cells themselves. Although many older individuals develop some plaques and tangles as a consequence of aging, the brains of people with Alzheimer's disease have a greater number of them in specific brain regions such as the temporal lobe.[86] Lewy bodies are not rare in the brains of people with Alzheimer's disease.[87]
Alzheimer's disease has been identified as a protein misfolding disease, a proteopathy, caused by the accumulation of abnormally folded amyloid beta protein into amyloid plaques, and tau protein into neurofibrillary tangles in the brain.[88] Plaques are made up of small peptides, 39–43 amino acids in length, called amyloid beta (Aβ). Amyloid beta is a fragment from the larger amyloid-beta precursor protein (APP) a transmembrane protein that penetrates the neuron's membrane. APP is critical to neuron growth, survival, and post-injury repair.[89][90] In Alzheimer's disease, gamma secretase and beta secretase act together in a proteolytic process which causes APP to be divided into smaller fragments.[91] One of these fragments gives rise to fibrils of amyloid beta, which then form clumps that deposit outside neurons in dense formations known as amyloid plaques.[83][92]
Alzheimer's disease is also considered a tauopathy due to abnormal aggregation of the tau protein. Every neuron has a cytoskeleton, an internal support structure partly made up of structures called microtubules. These microtubules act like tracks, guiding nutrients and molecules from the body of the cell to the ends of the axon and back. A protein called tau stabilises the microtubules when phosphorylated, and is therefore called a microtubule-associated protein. In Alzheimer's disease, tau undergoes chemical changes, becoming hyperphosphorylated; it then begins to pair with other threads, creating neurofibrillary tangles and disintegrating the neuron's transport system.[93] Pathogenic tau can also cause neuronal death through transposable element dysregulation.[94] Necroptosis has also been reported as a mechanism of cell death in brain cells affected with tau tangles.[95][96]
Exactly how disturbances of production and aggregation of the beta-amyloid peptide give rise to the pathology of Alzheimer's disease is not known.[97][98] The amyloid hypothesis traditionally points to the accumulation of beta-amyloid peptides as the central event triggering neuron degeneration. Accumulation of aggregated amyloid fibrils, which are believed to be the toxic form of the protein responsible for disrupting the cell's calcium ion homeostasis, induces programmed cell death (apoptosis).[99] It is also known that Aβ selectively builds up in the mitochondria in the cells of Alzheimer's-affected brains, and it also inhibits certain enzyme functions and the utilisation of glucose by neurons.[100]
Iron dyshomeostasis is linked to disease progression, an iron-dependent form of regulated cell death called ferroptosis could be involved. Products of lipid peroxidation are also elevated in AD brain compared with controls.[101]
Various inflammatory processes and cytokines may also have a role in the pathology of Alzheimer's disease. Inflammation is a general marker of tissue damage in any disease, and may be either secondary to tissue damage in Alzheimer's disease or a marker of an immunological response.[102] There is increasing evidence of a strong interaction between the neurons and the immunological mechanisms in the brain. Obesity and systemic inflammation may interfere with immunological processes which promote disease progression.[103]
Alterations in the distribution of different neurotrophic factors and in the expression of their receptors such as the brain-derived neurotrophic factor (BDNF) have been described in Alzheimer's disease.[104][105]
Alzheimer's disease (AD) can only be definitively diagnosed with autopsy findings; in the absence of autopsy, clinical diagnoses of AD are "possible" or "probable", based on other findings.[106][107][108] Up to 23% of those clinically diagnosed with AD may be misdiagnosed and may have pathology suggestive of another condition with symptoms that mimic those of AD.[107]
AD is usually clinically diagnosed based on the person's medical history, history from relatives, and behavioral observations. The presence of characteristic neurological and neuropsychological features and the absence of alternative conditions supports the diagnosis.[needs update][109][110] Advanced medical imaging with computed tomography (CT) or magnetic resonance imaging (MRI), and with single-photon emission computed tomography (SPECT) or positron emission tomography (PET), can be used to help exclude other cerebral pathology or subtypes of dementia.[111] Moreover, it may predict conversion from prodromal stages (mild cognitive impairment) to Alzheimer's disease.[112] FDA-approved radiopharmaceutical diagnostic agents used in PET for Alzheimer's disease are florbetapir (2012), flutemetamol (2013), florbetaben (2014), and flortaucipir (2020).[113] Because many insurance companies in the United States do not cover this procedure, its use in clinical practice is largely limited to clinical trials as of 2018[update].[114]
Assessment of intellectual functioning including memory testing can further characterise the state of the disease.[1] Medical organizations have created diagnostic criteria to ease and standardise the diagnostic process for practising physicians. Definitive diagnosis can only be confirmed with post-mortem evaluations when brain material is available and can be examined histologically for senile plaques and neurofibrillary tangles.[114][115]
There are three sets of criteria for the clinical diagnoses of the spectrum of Alzheimer's disease: the 2013 fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5); the National Institute on Aging-Alzheimer's Association (NIA-AA) definition as revised in 2011; and the International Working Group criteria as revised in 2010.[31][114] Three broad time periods, which can span decades, define the progression of Alzheimer's disease from the preclinical phase, to mild cognitive impairment (MCI), followed by Alzheimer's disease dementia.[116]
Eight intellectual domains are most commonly impaired in AD—memory, language, perceptual skills, attention, motor skills, orientation, problem solving and executive functional abilities, as listed in the fourth text revision of the DSM (DSM-IV-TR).[117]
The DSM-5 defines criteria for probable or possible Alzheimer's for both major and mild neurocognitive disorder.[118][119][108] Major or mild neurocognitive disorder must be present along with at least one cognitive deficit for a diagnosis of either probable or possible AD.[118][120] For major neurocognitive disorder due to Alzheimer's disease, probable Alzheimer's disease can be diagnosed if the individual has genetic evidence of Alzheimer's[121] or if two or more acquired cognitive deficits, and a functional disability that is not from another disorder, are present.[122] Otherwise, possible Alzheimer's disease can be diagnosed as the diagnosis follows an atypical route.[123] For mild neurocognitive disorder due to Alzheimer's, probable Alzheimer's disease can be diagnosed if there is genetic evidence, whereas possible Alzheimer's disease can be met if all of the following are present: no genetic evidence, decline in both learning and memory, two or more cognitive deficits, and a functional disability not from another disorder.[118][124]
The NIA-AA criteria are used mainly in research rather than in clinical assessments.[125] They define Alzheimer's disease through three major stages: preclinical, mild cognitive impairment (MCI), and Alzheimer's dementia.[126][127] Diagnosis in the preclinical stage is complex and focuses on asymptomatic individuals;[127][128] the latter two stages describe individuals experiencing symptoms.[127] The core clinical criteria for MCI is used along with identification of biomarkers,[129] predominantly those for neuronal injury (mainly tau-related) and amyloid beta deposition.[125][127] The core clinical criteria itself rests on the presence of cognitive impairment[127] without the presence of comorbidities.[130][131] The third stage is divided into probable and possible Alzheimer's disease dementia.[131] In probable Alzheimer's disease dementia there is steady impairment of cognition over time and a memory-related or non-memory-related cognitive dysfunction.[131] In possible Alzheimer's disease dementia, another causal disease such as cerebrovascular disease is present.[131]
Neuropsychological tests including cognitive tests such as the mini–mental state examination (MMSE), the Montreal Cognitive Assessment (MoCA) and the Mini-Cog are widely used to aid in diagnosis of the cognitive impairments in AD.[132] These tests may not always be accurate, as they lack sensitivity to mild cognitive impairment, and can be biased by language or attention problems;[132] more comprehensive test arrays are necessary for high reliability of results, particularly in the earliest stages of the disease.[133][134]
Further neurological examinations are crucial in the differential diagnosis of Alzheimer's disease and other diseases.[25] Interviews with family members are used in assessment; caregivers can supply important information on daily living abilities and on the decrease in the person's mental function.[135] A caregiver's viewpoint is particularly important, since a person with Alzheimer's disease is commonly unaware of their deficits.[136] Many times, families have difficulties in the detection of initial dementia symptoms and may not communicate accurate information to a physician.[137]
Supplemental testing can rule out other potentially treatable diagnoses and help avoid misdiagnoses.[138] Common supplemental tests include blood tests, thyroid function tests, as well as tests to assess vitamin B12 levels, rule out neurosyphilis and rule out metabolic problems (including tests for kidney function, electrolyte levels and for diabetes).[138] MRI or CT scans might also be used to rule out other potential causes of the symptoms – including tumors or strokes.[132] Delirium and depression can be common among individuals and are important to rule out.[139]
Psychological tests for depression are used, since depression can either be concurrent with Alzheimer's disease (see Depression of Alzheimer disease), an early sign of cognitive impairment,[140] or even the cause.[141][142]
Due to low accuracy, the C-PIB-PET scan is not recommended as an early diagnostic tool or for predicting the development of Alzheimer's disease when people show signs of mild cognitive impairment (MCI).[143] The use of 18F-FDG PET scans, as a single test, to identify people who may develop Alzheimer's disease is not supported by evidence.[144]
There are no disease-modifying treatments available to cure Alzheimer's disease and because of this, AD research has focused on interventions to prevent the onset and progression.[145] There is no evidence that supports any particular measure in preventing Alzheimer's,[1] and studies of measures to prevent the onset or progression have produced inconsistent results. Epidemiological studies have proposed relationships between an individual's likelihood of developing AD and modifiable factors, such as medications, lifestyle, and diet. There are some challenges in determining whether interventions for Alzheimer's disease act as a primary prevention method, preventing the disease itself, or a secondary prevention method, identifying the early stages of the disease.[146] These challenges include duration of intervention, different stages of disease at which intervention begins, and lack of standardization of inclusion criteria regarding biomarkers specific for Alzheimer's disease.[146] Further research is needed to determine factors that can help prevent Alzheimer's disease.[146]
Cardiovascular risk factors, such as hypercholesterolaemia, hypertension, diabetes, and smoking, are associated with a higher risk of onset and worsened course of AD.[147][148] The use of statins to lower cholesterol may be of benefit in Alzheimer's.[149] Antihypertensive and antidiabetic medications in individuals without overt cognitive impairment may decrease the risk of dementia by influencing cerebrovascular pathology.[1][150] More research is needed to examine the relationship with Alzheimer's disease specifically; clarification of the direct role medications play versus other concurrent lifestyle changes (diet, exercise, smoking) is needed.[1]
Depression is associated with an increased risk for Alzheimer's disease; management with antidepressants may provide a preventative measure.[151]
Historically, long-term usage of non-steroidal anti-inflammatory drugs (NSAIDs) were thought to be associated with a reduced likelihood of developing Alzheimer's disease as it reduces inflammation; however, NSAIDs do not appear to be useful as a treatment.[114] Additionally, because women have a higher incidence of Alzheimer's disease than men, it was once thought that estrogen deficiency during menopause was a risk factor. However, there is a lack of evidence to show that hormone replacement therapy (HRT) in menopause decreases risk of cognitive decline.[152]
Plant-made metallochaperones could be a novel approach for the treatment of Alzheimer's disease.[153]
Certain lifestyle activities, such as physical and cognitive exercises, higher education and occupational attainment, cigarette smoking, stress, sleep, and the management of other comorbidities, including diabetes and hypertension, may affect the risk of developing Alzheimer's.[151]
Physical exercise is associated with a decreased rate of dementia,[154] and is effective in reducing symptom severity in those with AD.[155] Memory and cognitive functions can be improved with aerobic exercises including brisk walking three times weekly for forty minutes.[156] It may also induce neuroplasticity of the brain.[157] Participating in mental exercises, such as reading, crossword puzzles, and chess have shown a potential to be preventative.[151] Meeting the WHO recommendations for physical activity is associated with a lower risk of AD.[158]
Higher education and occupational attainment, and participation in leisure activities, contribute to a reduced risk of developing Alzheimer's,[159] or of delaying the onset of symptoms. This is compatible with the cognitive reserve theory, which states that some life experiences result in more efficient neural functioning providing the individual a cognitive reserve that delays the onset of dementia manifestations.[159] Education delays the onset of Alzheimer's disease syndrome without changing the duration of the disease.[160]
Cessation in smoking may reduce risk of developing Alzheimer's' disease, specifically in those who carry APOE ɛ4 allele.[161][151] The increased oxidative stress caused by smoking results in downstream inflammatory or neurodegenerative processes that may increase risk of developing AD.[162] Avoidance of smoking, counseling and pharmacotherapies to quit smoking are used, and avoidance of environmental tobacco smoke is recommended.[151]
Alzheimer's disease is associated with sleep disorders but the precise relationship is unclear.[163][164] It was once thought that as people get older, the risk of developing sleep disorders and AD independently increase, but research is examining whether sleep disorders may increase the prevalence of AD.[163] One theory is that the mechanisms to increase clearance of toxic substances, including Aβ, are active during sleep.[163][165] With decreased sleep, a person is increasing Aβ production and decreasing Aβ clearance, resulting in Aβ accumulation.[166][163][164] Receiving adequate sleep (approximately 7–8 hours) every night has become a potential lifestyle intervention to prevent the development of AD.[151]
Stress is a risk factor for the development of Alzheimer's.[151] The mechanism by which stress predisposes someone to development of Alzheimer's is unclear, but it is suggested that lifetime stressors may affect a person's epigenome, leading to an overexpression or under expression of specific genes.[167] Although the relationship of stress and Alzheimer's is unclear, strategies to reduce stress and relax the mind may be helpful strategies in preventing the progression or Alzheimer's disease.[168] Meditation, for instance, is a helpful lifestyle change to support cognition and well-being, though further research is needed to assess long-term effects.[157]
There is no cure for Alzheimer's disease; available treatments offer relatively small symptomatic benefits but remain palliative in nature.[9][169] Treatments can be divided into pharmaceutical, psychosocial, and caregiving.
Medications used to treat the cognitive symptons of Alzheimer's disease rather than the underlying cause include: four acetylcholinesterase inhibitors (tacrine, rivastigmine, galantamine, and donepezil) and memantine, an NMDA receptor antagonist. The acetylcholinesterase inhibitors are intended for those with mild to severe Alzheimer's, whereas memantine is intended for those with moderate or severe Alzheimer's disease.[114] The benefit from their use is small.[170][171][172][10]
Reduction in the activity of the cholinergic neurons is a well-known feature of Alzheimer's disease.[173] Acetylcholinesterase inhibitors are employed to reduce the rate at which acetylcholine (ACh) is broken down, thereby increasing the concentration of ACh in the brain and combating the loss of ACh caused by the death of cholinergic neurons.[174] There is evidence for the efficacy of these medications in mild to moderate Alzheimer's disease,[175][170] and some evidence for their use in the advanced stage.[170] The use of these drugs in mild cognitive impairment has not shown any effect in a delay of the onset of Alzheimer's disease.[176] The most common side effects are nausea and vomiting, both of which are linked to cholinergic excess. These side effects arise in approximately 10–20% of users, are mild to moderate in severity, and can be managed by slowly adjusting medication doses.[177] Less common secondary effects include muscle cramps, decreased heart rate (bradycardia), decreased appetite and weight, and increased gastric acid production.[175]
Glutamate is an excitatory neurotransmitter of the nervous system, although excessive amounts in the brain can lead to cell death through a process called excitotoxicity which consists of the overstimulation of glutamate receptors. Excitotoxicity occurs not only in Alzheimer's disease, but also in other neurological diseases such as Parkinson's disease and multiple sclerosis.[178] Memantine is a noncompetitive NMDA receptor antagonist first used as an anti-influenza agent. It acts on the glutamatergic system by blocking NMDA receptors and inhibiting their overstimulation by glutamate.[178][179] Memantine has been shown to have a small benefit in the treatment of moderate to severe Alzheimer's disease.[180] Reported adverse events with memantine are infrequent and mild, including hallucinations, confusion, dizziness, headache and fatigue.[181][182] The combination of memantine and donepezil[183] has been shown to be "of statistically significant but clinically marginal effectiveness".[184]
An extract of Ginkgo biloba known as EGb 761 has been used for treating Alzheimer's and other neuropsychiatric disorders.[185] Its use is approved throughout Europe.[186] The World Federation of Biological Psychiatry guidelines lists EGb 761 with the same weight of evidence (level B) given to acetylcholinesterase inhibitors and memantine. EGb 761 is the only one that showed improvement of symptoms in both Alzheimer's disease and vascular dementia. EGb 761 may have a role either on its own or as an add-on if other therapies prove ineffective.[185] A 2016 review concluded that the quality of evidence from clinical trials on Ginkgo biloba has been insufficient to warrant its use for treating Alzheimer's disease.[187]
Atypical antipsychotics are modestly useful in reducing aggression and psychosis in people with Alzheimer's disease, but their advantages are offset by serious adverse effects, such as stroke, movement difficulties or cognitive decline.[188] When used in the long-term, they have been shown to associate with increased mortality.[189] Stopping antipsychotic use in this group of people appears to be safe.[190]
Psychosocial interventions are used as an adjunct to pharmaceutical treatment and can be classified within behavior-, emotion-, cognition- or stimulation-oriented approaches.[needs update][191]
Behavioral interventions attempt to identify and reduce the antecedents and consequences of problem behaviors. This approach has not shown success in improving overall functioning,[192] but can help to reduce some specific problem behaviors, such as incontinence.[193] There is a lack of high quality data on the effectiveness of these techniques in other behavior problems such as wandering.[194][195] Music therapy is effective in reducing behavioral and psychological symptoms.[196]
Emotion-oriented interventions include reminiscence therapy, validation therapy, supportive psychotherapy, sensory integration, also called snoezelen, and simulated presence therapy. A Cochrane review has found no evidence that this is effective.[197] Reminiscence therapy (RT) involves the discussion of past experiences individually or in group, many times with the aid of photographs, household items, music and sound recordings, or other familiar items from the past. A 2018 review of the effectiveness of RT found that effects were inconsistent, small in size and of doubtful clinical significance, and varied by setting.[198] Simulated presence therapy (SPT) is based on attachment theories and involves playing a recording with voices of the closest relatives of the person with Alzheimer's disease. There is partial evidence indicating that SPT may reduce challenging behaviors.[199]
The aim of cognition-oriented treatments, which include reality orientation and cognitive retraining, is the reduction of cognitive deficits. Reality orientation consists of the presentation of information about time, place, or person to ease the understanding of the person about its surroundings and his or her place in them. On the other hand, cognitive retraining tries to improve impaired capacities by exercising mental abilities. Both have shown some efficacy improving cognitive capacities.[200]
Stimulation-oriented treatments include art, music and pet therapies, exercise, and any other kind of recreational activities. Stimulation has modest support for improving behavior, mood, and, to a lesser extent, function. Nevertheless, as important as these effects are, the main support for the use of stimulation therapies is the change in the person's routine.[191]
Since Alzheimer's has no cure and it gradually renders people incapable of tending to their own needs, caregiving is essentially the treatment and must be carefully managed over the course of the disease.
During the early and moderate stages, modifications to the living environment and lifestyle can increase safety and reduce caretaker burden.[201][202] Examples of such modifications are the adherence to simplified routines, the placing of safety locks, the labeling of household items to cue the person with the disease or the use of modified daily life objects.[191][203][204] If eating becomes problematic, food will need to be prepared in smaller pieces or even puréed.[205] When swallowing difficulties arise, the use of feeding tubes may be required. In such cases, the medical efficacy and ethics of continuing feeding is an important consideration of the caregivers and family members.[206][207] The use of physical restraints is rarely indicated in any stage of the disease, although there are situations when they are necessary to prevent harm to the person with Alzheimer's disease or their caregivers.[191]
During the final stages of the disease, treatment is centred on relieving discomfort until death, often with the help of hospice.[208]
Diet may be a modifiable risk factor for the development of Alzheimer's disease. The Mediterranean diet, and the DASH diet are both associated with less cognitive decline. A different approach has been to incorporate elements of both of these diets into one known as the MIND diet.[209] Studies of individual dietary components, minerals and supplements are conflicting as to whether they prevent AD or cognitive decline.[209]
The early stages of Alzheimer's disease are difficult to diagnose. A definitive diagnosis is usually made once cognitive impairment compromises daily living activities, although the person may still be living independently. The symptoms will progress from mild cognitive problems, such as memory loss through increasing stages of cognitive and non-cognitive disturbances, eliminating any possibility of independent living, especially in the late stages of the disease.[32]
Life expectancy of people with Alzheimer's disease is reduced.[210] The normal life expectancy for 60 to 70 years old is 23 to 15 years; for 90 years old it is 4.5 years.[211] Following Alzheimer's disease diagnosis it ranges from 7 to 10 years for those in their 60s and early 70s (a loss of 13 to 8 years), to only about 3 years or less (a loss of 1.5 years) for those in their 90s.[210]
Fewer than 3% of people live more than fourteen years after diagnosis.[212] Disease features significantly associated with reduced survival are an increased severity of cognitive impairment, decreased functional level, history of falls, and disturbances in the neurological examination. Other coincident diseases such as heart problems, diabetes, or history of alcohol abuse are also related with shortened survival.[213][214][215] While the earlier the age at onset the higher the total survival years, life expectancy is particularly reduced when compared to the healthy population among those who are younger.[216] Men have a less favourable survival prognosis than women.[needs update][212][3]
Pneumonia and dehydration are the most frequent immediate causes of death brought by Alzheimer's disease, while cancer is a less frequent cause of death than in the general population.[needs update][3]
Two main measures are used in epidemiological studies: incidence and prevalence. Incidence is the number of new cases per unit of person-time at risk (usually number of new cases per thousand person-years); while prevalence is the total number of cases of the disease in the population at any given time.
Regarding incidence, cohort longitudinal studies (studies where a disease-free population is followed over the years) provide rates between 10 and 15 per thousand person-years for all dementias and 5–8 for Alzheimer's disease,[217][218] which means that half of new dementia cases each year are Alzheimer's disease. Advancing age is a primary risk factor for the disease and incidence rates are not equal for all ages: every 5 years after the age of 65, the risk of acquiring the disease approximately doubles, increasing from 3 to as much as 69 per thousand person years.[217][218] Females with Alzheimer's disease are more common than males, but this difference is likely due to women's longer life spans. When adjusted for age, both sexes are affected by Alzheimer's at equal rates.[10] In the United States, the risk of dying from Alzheimer's disease in 2010 was 26% higher among the non-Hispanic white population than among the non-Hispanic black population, and the Hispanic population had a 30% lower risk than the non-Hispanic white population.[219] However, much Alzheimer's research remains to be done in minority groups, such as the African American and the Hispanic/Latino populations.[220][221] Studies have shown that these groups are underrepresented in clinical trials and do not have the same risk of developing Alzheimer's when carrying certain genetic risk factors (i.e. APOE4), compared to their caucasian counterparts.[221][222][223]
The prevalence of Alzheimer's disease in populations is dependent upon factors including incidence and survival. Since the incidence of Alzheimer's disease increases with age, prevalence depends on the mean age of the population for which prevalence is given. In the United States in 2020, Alzheimer's dementia prevalence was estimated to be 5.3% for those in the 60–74 age group, with the rate increasing to 13.8% in the 74–84 group and to 34.6% in those greater than 85.[224] Prevalence rates in some less developed regions around the globe are lower.[225][226] As the incidence and prevalence are steadily increasing, the prevalence itself is projected to triple by 2050.[clarification needed][227] As of 2020, 50 million people globally have AD, with this number expected to increase to 152 million by 2050.[9]
The ancient Greek and Roman philosophers and physicians associated old age with increasing dementia.[23] It was not until 1901 that German psychiatrist Alois Alzheimer identified the first case of what became known as Alzheimer's disease, named after him, in a fifty-year-old woman he called Auguste D. He followed her case until she died in 1906 when he first reported publicly on it.[228] During the next five years, eleven similar cases were reported in the medical literature, some of them already using the term Alzheimer's disease.[23] The disease was first described as a distinctive disease by Emil Kraepelin after suppressing some of the clinical (delusions and hallucinations) and pathological features (arteriosclerotic changes) contained in the original report of Auguste D.[229] He included Alzheimer's disease, also named presenile dementia by Kraepelin, as a subtype of senile dementia in the eighth edition of his Textbook of Psychiatry, published on 15 July, 1910.[230]
For most of the 20th century, the diagnosis of Alzheimer's disease was reserved for individuals between the ages of 45 and 65 who developed symptoms of dementia. The terminology changed after 1977 when a conference on Alzheimer's disease concluded that the clinical and pathological manifestations of presenile and senile dementia were almost identical, although the authors also added that this did not rule out the possibility that they had different causes.[231] This eventually led to the diagnosis of Alzheimer's disease independent of age.[232] The term senile dementia of the Alzheimer type (SDAT) was used for a time to describe the condition in those over 65, with classical Alzheimer's disease being used to describe those who were younger. Eventually, the term Alzheimer's disease was formally adopted in medical nomenclature to describe individuals of all ages with a characteristic common symptom pattern, disease course, and neuropathology.[233]
The National Institute of Neurological and Communicative Disorders and Stroke (NINCDS) and the Alzheimer's Disease and Related Disorders Association (ADRDA, now known as the Alzheimer's Association) established the most commonly used NINCDS-ADRDA Alzheimer's Criteria for diagnosis in 1984,[234] extensively updated in 2007.[235][138] These criteria require that the presence of cognitive impairment, and a suspected dementia syndrome, be confirmed by neuropsychological testing for a clinical diagnosis of possible or probable Alzheimer's disease. A histopathologic confirmation including a microscopic examination of brain tissue is required for a definitive diagnosis. Good statistical reliability and validity have been shown between the diagnostic criteria and definitive histopathological confirmation.[236]
Dementia, and specifically Alzheimer's disease, may be among the most costly diseases for societies worldwide.[237] As populations age, these costs will probably increase and become an important social problem and economic burden.[238] Costs associated with AD include direct and indirect medical costs, which vary between countries depending on social care for a person with AD.[237][239][240] Direct costs include doctor visits, hospital care, medical treatments, nursing home care, specialized equipment, and household expenses.[237][238] Indirect costs include the cost of informal care and the loss in productivity of informal caregivers.[238]
In the United States as of 2019[update], informal (family) care is estimated to constitute nearly three-fourths of caregiving for people with AD at a cost of US$234 billion per year and approximately 18.5 billion hours of care.[237] The cost to society worldwide to care for individuals with AD is projected to increase nearly ten-fold, and reach about US$9.1 trillion by 2050.[239]
Costs for those with more severe dementia or behavioral disturbances are higher and are related to the additional caregiving time to provide physical care.[240]
Individuals with Alzheimer's will require assistance in their lifetime, and care will most likely come in the form of a full-time caregiver which is often a role that is taken on by the spouse or a close relative. Caregiving tends to include physical and emotional burdens as well as time and financial strain at times on the person administering the aid.[241][242] Alzheimer's disease is known for placing a great burden on caregivers which includes social, psychological, physical, or economic aspects.[18][243][244] Home care is usually preferred by both those people with Alzheimer's disease as well as their families.[245] This option also delays or eliminates the need for more professional and costly levels of care.[245][246] Nevertheless, two-thirds of nursing home residents have dementias.[191]
Dementia caregivers are subject to high rates of physical and mental disorders.[247] Factors associated with greater psychosocial problems of the primary caregivers include having an affected person at home, the carer being a spouse, demanding behaviors of the cared person such as depression, behavioral disturbances, hallucinations, sleep problems or walking disruptions and social isolation.[248][249] Regarding economic problems, family caregivers often give up time from work to spend 47 hours per week on average with the person with Alzheimer's disease, while the costs of caring for them are high. Direct and indirect costs of caring for somebody with Alzheimer's average between $18,000 and $77,500 per year in the United States, depending on the study.[250][242]
Cognitive behavioral therapy and the teaching of coping strategies either individually or in group have demonstrated their efficacy in improving caregivers' psychological health.[18][251]
Alzheimer's disease has been portrayed in films such as: Iris (2001), based on John Bayley's memoir of his wife Iris Murdoch;[252] The Notebook (2004), based on Nicholas Sparks's 1996 novel of the same name;[253] A Moment to Remember (2004); Thanmathra (2005);[254] Memories of Tomorrow (Ashita no Kioku) (2006), based on Hiroshi Ogiwara's novel of the same name;[255] Away from Her (2006), based on Alice Munro's short story The Bear Came over the Mountain;[256] Still Alice (2014), about a Columbia University professor who has early onset Alzheimer's disease, based on Lisa Genova's 2007 novel of the same name and featuring Julianne Moore in the title role. Documentaries on Alzheimer's disease include Malcolm and Barbara: A Love Story (1999) and Malcolm and Barbara: Love's Farewell (2007), both featuring Malcolm Pointon.[257][258][259]
Alzheimer's disease has also been portrayed in music by English musician the Caretaker in releases such as Persistent Repetition of Phrases (2008), An Empty Bliss Beyond This World (2011), and Everywhere at the End of Time (2016–2019).[260][261][262] Paintings depicting the disorder include the late works by American artist William Utermohlen, who drew self-portraits from 1995 to 2000 as an experiment of showing his disease through art.[263][264]
Additional research on the lifestyle effect may provide insight into neuroimaging biomarkers and better understanding of the mechanisms causing both Alzheimer's disease and early-onset AD.[265]
Alzheimer's disease is associated with neuroinflammation and loss of function of microglia, the resident immune cells of the central nervous system.[266] Microglia become progressively dysfunctional following exposure to amyloid plaques, and exposure to pro-inflammatory cytokines (e.g., TNFα, IL-1β, IL-12) has been hypothesized to sustain this dysfunction. Aberrant synaptic pruning via microglial phagocytosis may also contribute to AD pathology.[267] The complement system, which is involved in some forms of typical microglial pruning during development,[268] is implicated in animal models of AD by way of dysregulation of the activation (e.g. C1q; C3b) and terminal (e.g. MAC) pathways in synapses with proximity to amyloid plaques.[269]
Antibodies may have the ability to alter the disease course by targeting amyloid beta with immunotherapy medications such as donanemab, aducanumab, and lecanemab.[270][271][272] Aducanumab was approved by the US Food and Drug Administration (FDA) in 2021 using the accelerated approval process, although the approval generated controversy and more evidence is needed to address administration, safety, and effectiveness.[273][274][275][276] It has less effectiveness in people who already had severe Alzheimer's symptoms.[277] Lecanemab was also approved via the FDA accelerated approval process,[278][279][280] and was converted to traditional approval in July 2023 after further testing, along with the addition of a black box warning about amyloid-related imaging abnormalities.[281][282] Anti-amyloid drugs also cause brain shrinkage.[283]
Specific medications that may reduce the risk or progression of Alzheimer's disease have been studied.[284] The research trials investigating medications generally impact Aβ plaques, inflammation, APOE, neurotransmitter receptors, neurogenesis, growth factors or hormones.[284][285][286]
Machine learning algorithms with electronic health records are being studied as a way to predict AD earlier.[287]
Creutzfeldt–Jakob disease

Migraine (UK: /ˈmiːɡreɪn/, US: /ˈmaɪ-/)[11][12] is a genetically influenced complex neurological disorder characterized by episodes of moderate-to-severe headache, most often unilateral and generally associated with nausea and light and sound sensitivity.[1] Other characterizing symptoms may include nausea, vomiting, cognitive dysfunction, allodynia, and dizziness. Exacerbation of headache symptoms during physical activity is another distinguishing feature.[13] Up to one-third of migraine sufferers experience aura: a premonitory period of sensory disturbance widely accepted to be caused by cortical spreading depression at the onset of a migraine attack.[13] Although primarily considered to be a headache disorder, migraine is highly heterogenous in its clinical presentation and is better thought of as a spectrum disease rather than a distinct clinical entity.[14] Disease burden can range from episodic discrete attacks, consisting of as little as several lifetime attacks, to chronic disease.[14][15]
Migraine is believed to be caused by a mixture of environmental and genetic factors that influence the excitation and inhibition of nerve cells in the brain.[3] An older "vascular hypothesis" postulated that the headache of migraine is produced by vasodilation and aura by vasoconstriction, but this mechanism has been disproven.[16] The accepted hypothesis suggests that multiple primary neuronal impairments lead to a series of intracranial and extracranial changes, triggering a physiological cascade that leads to migraine symptomatology.[17]
Initial recommended treatment for acute attacks is with over-the-counter analgesics (pain medication) such as ibuprofen and paracetamol (acetaminophen) for headache, antiemetics (anti-nausea medication) for nausea, and the avoidance of triggers.[9] Specific medications such as triptans, ergotamines, or CGRP inhibitors may be used in those experiencing headaches that are refractory to simple pain medications.[18] For individuals who experience four or more attacks per month, or could otherwise benefit from prevention, prophylactic medication is recommended.[19] Commonly prescribed prophylactic medications include beta blockers like propranolol, anticonvulsants like sodium valproate, antidepressants like amitriptyline, and other off-label classes of medications.[8] Preventive medications inhibit migraine pathophysiology through various mechanisms, such as blocking calcium and sodium channels, blocking gap junctions, and inhibiting matrix metalloproteinases, among other mechanisms.[20][21] Nonpharmacological preventative therapies include nutritional supplementation, dietary interventions, sleep improvement, and aerobic exercise.[22]
Globally, approximately 15% of people are affected by migraine.[10] In the Global Burden of Disease Study, conducted in 2010, migraines ranked as the third-most prevalent disorder in the world.[23] It most often starts at puberty and is worst during middle age.[1] As of 2016[update], it is one of the most common causes of disability.[24] An early description consistent with migraines is contained in the Ebers Papyrus, written around 1500 BCE in ancient Egypt.[25] The word migraine is from the Greek ἡμικρᾱνίᾱ (hēmikrāníā), 'pain in half of the head',[26] from ἡμι- (hēmi-), 'half' and κρᾱνίον (krāníon), 'skull'.[27]
Migraine typically presents with self-limited, recurrent severe headache associated with autonomic symptoms.[5][28] About 15–30% of people living with migraine experience episodes with aura,[9][29] and they also frequently experience episodes without aura.[30] The severity of the pain, duration of the headache, and frequency of attacks are variable.[5] A migraine attack lasting longer than 72 hours is termed status migrainosus.[31] There are four possible phases to a migraine attack, although not all the phases are necessarily experienced:[13]
Migraine is associated with major depression, bipolar disorder, anxiety disorders, and obsessive–compulsive disorder. These psychiatric disorders are approximately 2–5 times more common in people without aura, and 3–10 times more common in people with aura.[32]
Prodromal or premonitory symptoms occur in about 60% of those with migraines,[2][33] with an onset that can range from two hours to two days before the start of pain or the aura.[34] These symptoms may include a wide variety of phenomena,[35] including altered mood, irritability, depression or euphoria, fatigue, craving for certain food(s), stiff muscles (especially in the neck), constipation or diarrhea, and sensitivity to smells or noise.[33] This may occur in those with either migraine with aura or migraine without aura.[36] Neuroimaging indicates the limbic system and hypothalamus as the origin of prodromal symptoms in migraine.[37]
Aura is a transient focal neurological phenomenon that occurs before or during the headache.[2] Aura appears gradually over a number of minutes (usually occurring over 5–60 minutes) and generally lasts less than 60 minutes.[38][39] Symptoms can be visual, sensory or motoric in nature, and many people experience more than one.[40] Visual effects occur most frequently: they occur in up to 99% of cases and in more than 50% of cases are not accompanied by sensory or motor effects.[40] If any symptom remains after 60 minutes, the state is known as persistent aura.[41]
Visual disturbances often consist of a scintillating scotoma (an area of partial alteration in the field of vision which flickers and may interfere with a person's ability to read or drive).[2] These typically start near the center of vision and then spread out to the sides with zigzagging lines which have been described as looking like fortifications or walls of a castle.[40] Usually the lines are in black and white but some people also see colored lines.[40] Some people lose part of their field of vision known as hemianopsia while others experience blurring.[40]
Sensory aura are the second most common type; they occur in 30–40% of people with auras.[40] Often a feeling of pins-and-needles begins on one side in the hand and arm and spreads to the nose–mouth area on the same side.[40] Numbness usually occurs after the tingling has passed with a loss of position sense.[40] Other symptoms of the aura phase can include speech or language disturbances, world spinning, and less commonly motor problems.[40] Motor symptoms indicate that this is a hemiplegic migraine, and weakness often lasts longer than one hour unlike other auras.[40] Auditory hallucinations or delusions have also been described.[42]
Classically the headache is unilateral, throbbing, and moderate to severe in intensity.[38] It usually comes on gradually[38] and is aggravated by physical activity during a migraine attack.[13] However, the effects of physical activity on migraine are complex, and some researchers have concluded that, while exercise can trigger migraine attacks, regular exercise may have a prophylactic effect and decrease frequency of attacks.[43] The feeling of pulsating pain is not in phase with the pulse.[44] In more than 40% of cases, however, the pain may be bilateral (both sides of the head), and neck pain is commonly associated with it.[45] Bilateral pain is particularly common in those who have migraine without aura.[2] Less commonly pain may occur primarily in the back or top of the head.[2] The pain usually lasts 4 to 72 hours in adults;[38] however, in young children frequently lasts less than 1 hour.[46] The frequency of attacks is variable, from a few in a lifetime to several a week, with the average being about one a month.[47][48]
The pain is frequently accompanied by nausea, vomiting, sensitivity to light, sensitivity to sound, sensitivity to smells, fatigue, and irritability.[2] Many thus seek a dark and quiet room.[49] In a basilar migraine, a migraine with neurological symptoms related to the brain stem or with neurological symptoms on both sides of the body,[50] common effects include a sense of the world spinning, light-headedness, and confusion.[2] Nausea occurs in almost 90% of people, and vomiting occurs in about one-third.[49] Other symptoms may include blurred vision, nasal stuffiness, diarrhea, frequent urination, pallor, or sweating.[51] Swelling or tenderness of the scalp may occur as can neck stiffness.[51] Associated symptoms are less common in the elderly.[52]
Sometimes, aura occurs without a subsequent headache.[40] This is known in modern classification as a typical aura without headache, or acephalgic migraine in previous classification, or commonly as a silent migraine.[53][54] However, silent migraine can still produce debilitating symptoms, with visual disturbance, vision loss in half of both eyes, alterations in color perception, and other sensory problems, like sensitivity to light, sound, and odors, and aura sudden outbreak without headache can be scary.[55] It can last from 15 to 30 minutes, usually no longer than 60 minutes, and it can recur or appear as an isolated event.[54]
The migraine postdrome could be defined as that constellation of symptoms occurring once the acute headache has settled.[56] Many report a sore feeling in the area where the migraine was, and some report impaired thinking for a few days after the headache has passed. The person may feel tired or "hung over" and have head pain, cognitive difficulties, gastrointestinal symptoms, mood changes, and weakness.[57] According to one summary, "Some people feel unusually refreshed or euphoric after an attack, whereas others note depression and malaise."[58][unreliable medical source?]
The underlying causes of migraines are unknown.[59] However, they are believed to be related to a mix of environmental and genetic factors.[3] They run in families in about two-thirds of cases[5] and rarely occur due to a single gene defect.[60] While migraines were once believed to be more common in those of high intelligence, this does not appear to be true.[61] A number of psychological conditions are associated, including depression, anxiety, and bipolar disorder.[62]
Studies of twins indicate a 34% to 51% genetic influence of likelihood to develop migraine.[3] This genetic relationship is stronger for migraine with aura than for migraines without aura.[30] A number of specific variants of genes increase the risk by a small to moderate amount.[60]
Single gene disorders that result in migraines are rare.[60] One of these is known as familial hemiplegic migraine, a type of migraine with aura, which is inherited in an autosomal dominant fashion.[63][64] Four genes have been shown to be involved in familial hemiplegic migraine.[65] Three of these genes are involved in ion transport.[65] The fourth is the axonal protein PRRT2, associated with the exocytosis complex.[65] Another genetic disorder associated with migraine is CADASIL syndrome or cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy.[2] One meta-analysis found a protective effect from angiotensin converting enzyme polymorphisms on migraine.[66] The TRPM8 gene, which codes for a cation channel, has been linked to migraines.[67]
Migraine may be induced by triggers, with some reporting it as an influence in a minority of cases[5] and others the majority.[68] Many things such as fatigue, certain foods, alcohol, and weather have been labeled as triggers; however, the strength and significance of these relationships are uncertain.[68][69] Most people with migraines report experiencing triggers.[70] Symptoms may start up to 24 hours after a trigger.[5]
Common triggers quoted are stress, hunger, and fatigue (these equally contribute to tension headaches).[68] Psychological stress has been reported as a factor by 50 to 80% of people.[71] Migraine has also been associated with post-traumatic stress disorder and abuse.[72] Migraine episodes are more likely to occur around menstruation.[71] Other hormonal influences, such as menarche, oral contraceptive use, pregnancy, perimenopause, and menopause, also play a role.[73] These hormonal influences seem to play a greater role in migraine without aura.[61] Migraine episodes typically do not occur during the second and third trimesters of pregnancy, or following menopause.[2]
Between 12% and 60% of people report foods as triggers.[74][75]
There are many reports[76][77][78][79][80] that tyramine – which is naturally present in chocolate, alcoholic beverages, most cheeses, processed meats, and other foods – can trigger migraine symptoms in some individuals. Monosodium glutamate (MSG) has been reported as a trigger for migraine,[81] but a systematic review concluded that "a causal relationship between MSG and headache has not been proven... It would seem premature to conclude that the MSG present in food causes headache".[82]
A 2009 review on potential triggers in the indoor and outdoor environment concluded that while there were insufficient studies to confirm environmental factors as causing migraine, "migraineurs worldwide consistently report similar environmental triggers".[83]
Migraine is believed to be primarily a neurological disorder,[84][85] while others believe it to be a neurovascular disorder with blood vessels playing the key role, although evidence does not support this completely.[86][87][88][89] Others believe both are likely important.[90][91][92][93] One theory is related to increased excitability of the cerebral cortex and abnormal control of pain neurons in the trigeminal nucleus of the brainstem.[94]
Cortical spreading depression, or spreading depression according to Leão, is a burst of neuronal activity followed by a period of inactivity, which is seen in those with migraines with aura.[95] There are a number of explanations for its occurrence, including activation of NMDA receptors leading to calcium entering the cell.[95] After the burst of activity, the blood flow to the cerebral cortex in the area affected is decreased for two to six hours.[95] It is believed that when depolarization travels down the underside of the brain, nerves that sense pain in the head and neck are triggered.[95]
The exact mechanism of the head pain which occurs during a migraine episode is unknown.[96] Some evidence supports a primary role for central nervous system structures (such as the brainstem and diencephalon),[97] while other data support the role of peripheral activation (such as via the sensory nerves that surround blood vessels of the head and neck).[96] The potential candidate vessels include dural arteries, pial arteries and extracranial arteries such as those of the scalp.[96] The role of vasodilatation of the extracranial arteries, in particular, is believed to be significant.[98]
Adenosine, a neuromodulator, may be involved.[99] Released after the progressive cleavage of adenosine triphosphate (ATP), adenosine acts on adenosine receptors to put the body and brain in a low activity state by dilating blood vessels and slowing the heart rate, such as before and during the early stages of sleep. Adenosine levels have been found to be high during migraine attacks.[99][100] Caffeine's role as an inhibitor of adenosine may explain its effect in reducing migraine.[101] Low levels of the neurotransmitter serotonin, also known as 5-hydroxytryptamine (5-HT), are also believed to be involved.[102]
Calcitonin gene-related peptides (CGRPs) have been found to play a role in the pathogenesis of the pain associated with migraine, as levels of it become elevated during an attack.[9][44]
The diagnosis of a migraine is based on signs and symptoms.[5] Neuroimaging tests are not necessary to diagnose migraine, but may be used to find other causes of headaches in those whose examination and history do not confirm a migraine diagnosis.[103] It is believed that a substantial number of people with the condition remain undiagnosed.[5]
The diagnosis of migraine without aura, according to the International Headache Society, can be made according the "5, 4, 3, 2, 1 criteria," which is as follows:[13]
If someone experiences two of the following: photophobia, nausea, or inability to work or study for a day, the diagnosis is more likely.[104] In those with four out of five of the following: pulsating headache, duration of 4–72 hours, pain on one side of the head, nausea, or symptoms that interfere with the person's life, the probability that this is a migraine attack is 92%.[9] In those with fewer than three of these symptoms, the probability is 17%.[9]
Migraine was first comprehensively classified in 1988.[30] The International Headache Society updated their classification of headaches in 2004.[13] A third version was published in 2018.[105] According to this classification, migraine is a primary headache disorder along with tension-type headaches and cluster headaches, among others.[106]
Migraine is divided into seven subclasses (some of which include further subdivisions):[citation needed]
The diagnosis of abdominal migraine is controversial.[108] Some evidence indicates that recurrent episodes of abdominal pain in the absence of a headache may be a type of migraine[108][109] or are at least a precursor to migraines.[30] These episodes of pain may or may not follow a migraine-like prodrome and typically last minutes to hours.[108] They often occur in those with either a personal or family history of typical migraine.[108] Other syndromes that are believed to be precursors include cyclical vomiting syndrome and benign paroxysmal vertigo of childhood.[30]
Other conditions that can cause similar symptoms to a migraine headache include temporal arteritis, cluster headaches, acute glaucoma, meningitis and subarachnoid hemorrhage.[9] Temporal arteritis typically occurs in people over 50 years old and presents with tenderness over the temple, cluster headache presents with one-sided nose stuffiness, tears and severe pain around the orbits, acute glaucoma is associated with vision problems, meningitis with fevers, and subarachnoid hemorrhage with a very fast onset.[9] Tension headaches typically occur on both sides, are not pounding, and are less disabling.[9]
Those with stable headaches that meet criteria for migraines should not receive neuroimaging to look for other intracranial disease.[110][111][112] This requires that other concerning findings such as papilledema (swelling of the optic disc) are not present. People with migraines are not at an increased risk of having another cause for severe headaches.[citation needed]
Preventive treatments of migraine include medications, nutritional supplements, lifestyle alterations, and surgery. Prevention is recommended in those who have headaches more than two days a week, cannot tolerate the medications used to treat acute attacks, or those with severe attacks that are not easily controlled.[9] Recommended lifestyle changes include stopping tobacco use and reducing behaviors that interfere with sleep.[113]
The goal is to reduce the frequency, painfulness, and duration of migraine episodes, and to increase the effectiveness of abortive therapy.[114][115] Another reason for prevention is to avoid medication overuse headache. This is a common problem and can result in chronic daily headache.[116][117]
Preventive migraine medications are considered effective if they reduce the frequency or severity of the migraine attacks by at least 50%.[118] Due to few medications being approved specifically for the preventative treatment of migraine headaches; many medications such as beta-blockers, anticonvulsive agents such as topiramate or sodium valproate, antidepressants such as amitriptyline and calcium channel blockers such as flunarizine are used off label for the preventative treatment of migraine headaches.[39] Guidelines are fairly consistent in rating the anticonvulsants topiramate and divalproex/sodium valproate, and the beta blockers propranolol and metoprolol as having the highest level of evidence for first-line use for migraine prophylaxis in adults.[119][120] Propranolol and topiramate have the best evidence in children; however, evidence only supports short-term benefit as of 2020.[113][121]
The beta blocker timolol is also effective for migraine prevention and in reducing migraine attack frequency and severity.[119] While beta blockers are often used for first-line treatment, other antihypertensives also have a proven efficiency in migraine prevention, namely the calcium channel blocker verapamil[122] and the angiotensin receptor blocker candesartan.[123][124]
Tentative evidence also supports the use of magnesium supplementation.[125] Increasing dietary intake may be better.[126] Recommendations regarding effectiveness varied for the anticonvulsants gabapentin and pregabalin. Frovatriptan is effective for prevention of menstrual migraine.[119]
The antidepressants amitriptyline and venlafaxine are probably also effective.[127] Angiotensin inhibition by either an angiotensin-converting enzyme inhibitor or angiotensin II receptor antagonist may reduce attacks.[128]
Medications in the anti-calcitonin gene-related peptide, including eptinezumab, erenumab, fremanezumab, and galcanezumab, appear to decrease the frequency of migraines by one to two per month.[129]
Acupuncture has a small effect in reducing migraine frequency, compared to sham acupuncture, a practice where needles are placed randomly or do not penetrate the skin.[130] Physiotherapy, massage and relaxation, and chiropractic manipulation might be as effective as propranolol or topiramate in the prevention of migraine headaches; however, the research had some problems with methodology.[131][132] Another review, however, found evidence to support spinal manipulation to be poor and insufficient to support its use.[133]
Tentative evidence supports the use of stress reduction techniques such as cognitive behavioral therapy, biofeedback, and relaxation techniques.[71] Regular physical exercise may decrease the frequency.[43] Numerous psychological approaches have been developed that are aimed at preventing or reducing the frequency of migraine in adults including educational approaches, relaxation techniques, assistance in developing coping strategies, strategies to change the way one thinks of a migraine attack, and strategies to reduce symptoms.[134] Other strategies include: progressive muscle relaxation, biofeedback, behavioral training, acceptance and commitment therapy, and mindfulness-based interventions.[135] The medical evidence supporting the effectiveness of these types of psychological approaches is very limited.[134]
Among alternative medicines, butterbur has the best evidence for its use.[136][137] Unprocessed butterbur contains chemicals called pyrrolizidine alkaloids (PAs) which can cause liver damage, however there are versions that are PA free.[138] In addition, butterbur may cause allergic reactions in people who are sensitive to plants such as ragweed.[139] There is tentative evidence that coenzyme Q10 reduces migraine frequency.[140]
Feverfew has traditionally been used as a treatment for fever, headache and migraine, women's conditions such as difficulties in labour and regulation of menstruation, relief of stomach ache, toothache and insect bites. During the last decades, it has mainly been used for headache and as a preventive treatment for migraine.[141] The plant parts used for medicinal use are the dried leaves or the dried aerial parts. Several historical data supports feverfew's traditional medicinal uses.[142] In addition, several clinical studies have been performed assessing the efficacy and safety of feverfew monotherapy in the prevention of migraine.[143] The majority of the clinical trials favoured feverfew over placebo. The data also suggest that feverfew is associated with only mild and transient adverse effects. The frequency of migraine was positively affected after treatment with feverfew. Reduction of migraine severity was also reported after intake of feverfew and incidence of nausea and vomiting decreased significantly. No effect of feverfew was reported in one study.[143]
There is tentative evidence for melatonin as an add-on therapy for prevention and treatment of migraine.[144][145] The data on melatonin are mixed and certain studies have had negative results.[144] The reasons for the mixed findings are unclear but may stem from differences in study design and dosage.[144] Melatonin's possible mechanisms of action in migraine are not completely clear, but may include improved sleep, direct action on melatonin receptors in the brain, and anti-inflammatory properties.[144][146]
Medical devices, such as biofeedback and neurostimulators, have some advantages in migraine prevention, mainly when common anti-migraine medications are contraindicated or in case of medication overuse. Biofeedback helps people be conscious of some physiological parameters so as to control them and try to relax and may be efficient for migraine treatment.[147][148] Neurostimulation uses noninvasive or implantable neurostimulators similar to pacemakers for the treatment of intractable chronic migraine with encouraging results for severe cases.[149][150] A transcutaneous electrical nerve stimulator and a transcranial magnetic stimulator are approved in the United States for the prevention of migraines.[151][152] There is also tentative evidence for transcutaneous electrical nerve stimulation decreases the frequency of migraines.[153] Migraine surgery, which involves decompression of certain nerves around the head and neck, may be an option in certain people who do not improve with medications.[154]
There are three main aspects of treatment: trigger avoidance, acute symptomatic control, and medication for prevention.[5] Medications are more effective if used earlier in an attack.[5] The frequent use of medications may result in medication overuse headache, in which the headaches become more severe and more frequent.[13] This may occur with triptans, ergotamines, and analgesics, especially opioid analgesics.[13] Due to these concerns simple analgesics are recommended to be used less than three days per week at most.[155]
Recommended initial treatment for those with mild to moderate symptoms are simple analgesics such as nonsteroidal anti-inflammatory drugs (NSAIDs) or the combination of paracetamol (also known as acetaminophen), aspirin, and caffeine, although caffeine overuse can be a contributor to migraine chronification as well as a migraine trigger for many patients.[9][156] Several NSAIDs, including diclofenac and ibuprofen, have evidence to support their use.[157][158] Aspirin (900 to 1000 mg) can relieve moderate to severe migraine pain, with an effectiveness similar to sumatriptan.[159] Ketorolac is available in intravenous and intramuscular formulations.[9]
Paracetamol, either alone or in combination with metoclopramide, is another effective treatment with a low risk of adverse effects.[160] Intravenous metoclopramide is also effective by itself.[161][162] In pregnancy, paracetamol and metoclopramide are deemed safe as are NSAIDs until the third trimester.[9]
Naproxen by itself may not be effective as a stand-alone medicine to stop a migraine headache as it is only weakly better than a placebo medication in clinical trials.[163]
Triptans such as sumatriptan are medications used to stop an active migraine headache (an abortive medication). Triptans are the initially recommended treatments for those with moderate to severe pain from an acute migraine headache or those with milder symptoms who do not respond to simple analgesics.[9] Triptans have been shown to be effective for both pain and nausea in up to 75% of people.[164][165] There are different methods or routes of administration to take sumatriptan including oral (by mouth), injectable (subcutaneous), rectal, nasal spray, and oral dissolving tablets.[5][166][167][168] For people with migraine symptoms such as nausea or vomiting, taking the abortive medicine by mouth or through the nose may be difficult. All route of administration have been shown to be effective at reducing migraine symptoms, however, nasal and injectable subcutaneous administration may result in more side effects.[168][167] The adverse effects associated with rectal administration have not been well studied.[166] Some people may find that they respond to one type of sumatriptan better than another.[9]
Most side effects are mild, including flushing; however, rare cases of myocardial ischemia have occurred.[5] They are thus not recommended for people with cardiovascular disease,[9] who have had a stroke, or have migraines that are accompanied by neurological problems.[169][unreliable medical source?] In addition, triptans should be prescribed with caution for those with risk factors for vascular disease.[169][unreliable medical source?] While historically not recommended in those with basilar migraines there is no specific evidence of harm from their use in this population to support this caution.[50] Triptans are not addictive, but may cause medication-overuse headaches if used more than 10 days per month.[170]
Sumatriptan does not prevent other migraine headaches from starting in the future.[167] For increased effectiveness at stopping migraine symptoms, a combined therapy that includes sumatriptan and naproxen may be suggested.[171]
Calcitonin gene-related peptide receptor antagonists (CGRP) target calcitonin gene-related peptide or its receptor to prevent migraine headaches or reduce their severity.[39] CGRP is a signaling molecule as well as a potent vasodilator that is involved in the development of a migraine headache.[39] There are four injectable monoclonal antibodies that target CGRP or its receptor (eptinezumab, erenumab, fremanezumab, and galcanezumab) and the medications have demonstrated efficacy in the preventative treatment of episodic and chronic migraine headaches in phase III randomized clinical trials.[39]
Zavegepant was approved for medical use in the United States in March 2023.[172][173][174]
Ergotamine and dihydroergotamine are older medications still prescribed for migraines, the latter in nasal spray and injectable forms.[5][175] They appear equally effective to the triptans[176] and experience adverse effects that typically are benign.[177] In the most severe cases, such as those with status migrainosus, they appear to be the most effective treatment option.[177] They can cause vasospasm including coronary vasospasm and are contraindicated in people with coronary artery disease.[178]
Magnesium is recognized as an inexpensive, over-the-counter supplement which can be part of a multimodal approach to migraine reduction. Some studies have shown to be effective in both preventing and treating migraine in intravenous form.[179] The intravenous form reduces attacks as measured in approximately 15–45 minutes, 120 minutes, and 24-hour time periods, magnesium taken orally alleviates the frequency and intensity of migraines.[180][181]
Intravenous metoclopramide, intravenous prochlorperazine, or intranasal lidocaine are other potential options.[9][162] Metoclopramide or prochlorperazine are the recommended treatment for those who present to the emergency department.[9][162] Haloperidol may also be useful in this group.[162][175] A single dose of intravenous dexamethasone, when added to standard treatment of a migraine attack, is associated with a 26% decrease in headache recurrence in the following 72 hours.[182] Spinal manipulation for treating an ongoing migraine headache is not supported by evidence.[133] It is recommended that opioids and barbiturates not be used due to questionable efficacy, addictive potential, and the risk of rebound headache.[9][39] There is tentative evidence that propofol may be useful if other measures are not effective.[183]
Occipital nerve stimulation, may be effective but has the downsides of being cost-expensive and has a significant amount of complications.[184]
There is modest evidence for the effectiveness of non-invasive neuromodulatory devices, behavioral therapies and acupuncture in the treatment of migraine headaches.[39] There is little to no evidence for the effectiveness of physical therapy, chiropractic manipulation and dietary approaches to the treatment of migraine headaches.[39] Behavioral treatment of migraine headaches may be helpful for those who may not be able to take medications (for example pregnant women).[39]
Feverfew is registered as a traditional herbal medicine in the Nordic countries under the brand name Glitinum, only powdered feverfew is approved in the Herbal community monograph issued by European Medicines Agency (EMA).
Ibuprofen helps decrease pain in children with migraines and is the initially recommended treatment.[185][186] Paracetamol does not appear to be effective in providing pain relief.[185] Triptans are effective, though there is a risk of causing minor side effects like taste disturbance, nasal symptoms, dizziness, fatigue, low energy, nausea, or vomiting.[185][187] Ibuprofen should be used less than half the days in a month and triptans less than a third of the days in a month to decrease the risk of medication overuse headache.[186]
Topiramate and botulinum toxin (Botox) have evidence in treating chronic migraine.[127][188] Botulinum toxin has been found to be useful in those with chronic migraine but not those with episodic ones.[189][190] The anti-CGRP monoclonal antibody erenumab was found in one study to decrease chronic migraines by 2.4 days more than placebo.[191]
"Migraine exists on a continuum of different attack frequencies and associated levels of disability."[192] For those with occasional, episodic migraine, a "proper combination of drugs for prevention and treatment of migraine attacks" can limit the disease's impact on patients' personal and professional lives.[193] But fewer than half of people with migraine seek medical care and more than half go undiagnosed and undertreated.[194] "Responsive prevention and treatment of migraine is incredibly important" because evidence shows "an increased sensitivity after each successive attack, eventually leading to chronic daily migraine in some individuals."[193] Repeated migraine results in "reorganization of brain circuitry," causing "profound functional as well as structural changes in the brain."[195] "One of the most important problems in clinical migraine is the progression from an intermittent, self-limited inconvenience to a life-changing disorder of chronic pain, sensory amplification, and autonomic and affective disruption. This progression, sometimes termed chronification in the migraine literature, is common, affecting 3% of migraineurs in a given year, such that 8% of migraineurs have chronic migraine in any given year." Brain imagery reveals that the electrophysiological changes seen during an attack become permanent in people with chronic migraine; "thus, from an electrophysiological point of view, chronic migraine indeed resembles a never-ending migraine attack."[195] Severe migraine ranks in the highest category of disability, according to the World Health Organization, which uses objective metrics to determine disability burden for the authoritative annual Global Burden of Disease report. The report classifies severe migraine alongside severe depression, active psychosis, quadriplegia, and terminal-stage cancer.[196]
Migraine with aura appears to be a risk factor for ischemic stroke[197] doubling the risk.[198] Being a young adult, being female, using hormonal birth control, and smoking further increases this risk.[197] There also appears to be an association with cervical artery dissection.[199] Migraine without aura does not appear to be a factor.[200] The relationship with heart problems is inconclusive with a single study supporting an association.[197] Migraine does not appear to increase the risk of death from stroke or heart disease.[201] Preventative therapy of migraines in those with migraine with aura may prevent associated strokes.[202] People with migraine, particularly women, may develop higher than average numbers of white matter brain lesions of unclear significance.[203]
Migraine is common, with around 33% of women and 18% of men affected at some point in their lifetime.[204] Onset can be at any age, but prevalence rises sharply around puberty, and remains high until declining after age 50.[204] Before puberty, boys and girls are equally impacted, with around 5% of children experiencing migraines. From puberty onwards, women experience migraines at greater rates than men. From age 30 to 50, up to 4 times as many women experience migraines as men.[204]
Worldwide, migraine affects nearly 15% or approximately one billion people.[10] In the United States, about 6% of men and 18% of women experience a migraine attack in a given year, with a lifetime risk of about 18% and 43% respectively.[5] In Europe, migraines affect 12–28% of people at some point in their lives with about 6–15% of adult men and 14–35% of adult women getting at least one yearly.[205] Rates of migraine are slightly lower in Asia and Africa than in Western countries.[61][206] Chronic migraine occurs in approximately 1.4 to 2.2% of the population.[207]
In women, migraine without aura are more common than migraine with aura; however in men the two types occur with similar frequency.[61]
During perimenopause symptoms often get worse before decreasing in severity.[208] While symptoms resolve in about two-thirds of the elderly, in 3–10% they persist.[52]
An early description consistent with migraine is contained in the Ebers papyrus, written around 1500 BCE in ancient Egypt.[25] In 200 BCE, writings from the Hippocratic school of medicine described the visual aura that can precede the headache and a partial relief occurring through vomiting.[209]
A second-century description by Aretaeus of Cappadocia divided headaches into three types: cephalalgia, cephalea, and heterocrania.[210] Galen of Pergamon used the term hemicrania (half-head), from which the word migraine was eventually derived.[210] He also proposed that the pain arose from the meninges and blood vessels of the head.[209] Migraine was first divided into the two now used types – migraine with aura (migraine ophthalmique) and migraine without aura (migraine vulgaire) in 1887 by Louis Hyacinthe Thomas, a French Librarian.[209] The mystical visions of Hildegard von Bingen, which she described as "reflections of the living light", are consistent with the visual aura experienced during migraines.[211]
Trepanation, the deliberate drilling of holes into a skull, was practiced as early as 7,000 BCE.[25] While sometimes people survived, many would have died from the procedure due to infection.[212] It was believed to work via "letting evil spirits escape".[213] William Harvey recommended trepanation as a treatment for migraines in the 17th century.[214] The association between trepanation and headaches in ancient history may simply be a myth or unfounded speculation that originated several centuries later. In 1913, the world-famous American physician William Osler misinterpreted the French anthropologist and physician Paul Broca's words about a set of children's skulls from the Neolithic age that he found during the 1870s. These skulls presented no evident signs of fractures that could justify this complex surgery for mere medical reasons. Trepanation was probably born of superstitions, to remove "confined demons" inside the head, or to create healing or fortune talismans with the bone fragments removed from the skulls of the patients. However, Osler wanted to make Broca's theory more palatable to his modern audiences, and explained that trepanation procedures were used for mild conditions such as "infantile convulsions headache and various cerebral diseases believed to be caused by confined demons."[215]
While many treatments for migraine have been attempted, it was not until 1868 that use of a substance which eventually turned out to be effective began.[209] This substance was the fungus ergot from which ergotamine was isolated in 1918.[216] Methysergide was developed in 1959 and the first triptan, sumatriptan, was developed in 1988.[216] During the 20th century with better study-design, effective preventive measures were found and confirmed.[209]
Migraine is a significant source of both medical costs and lost productivity. It has been estimated that migraine is the most costly neurological disorder in the European Community, costing more than €27 billion per year.[217] In the United States, direct costs have been estimated at $17 billion, while indirect costs – such as missed or decreased ability to work – is estimated at $15 billion.[218] Nearly a tenth of the direct cost is due to the cost of triptans.[218] In those who do attend work during a migraine attack, effectiveness is decreased by around a third.[217] Negative impacts also frequently occur for a person's family.[217]
Transcranial magnetic stimulation shows promise,[9][219] as does transcutaneous supraorbital nerve stimulation.[220] There is preliminary evidence that a ketogenic diet may help prevent episodic and long-term migraine.[221][222]
While no definitive proof has been found linking migraine to gender, statistical data indicates that women may be more prone to having migraine, showing migraine incidence three times higher among women than men.[223][224] The Society for Women's Health Research has also mentioned hormonal influences, mainly estrogen, as having a considerable role in provoking migraine pain. Studies and research related to the gender dependencies of migraine are still ongoing, and conclusions have yet to be achieved.[225][226][227]
Creutzfeldt–Jakob disease

Prostate cancer is the uncontrolled growth of cells in the prostate, a gland in the male reproductive system below the bladder. Early prostate cancer usually causes no symptoms. As the tumor grows, it can damage nearby organs causing erectile dysfunction, blood in the urine or semen, and trouble urinating. Some tumors eventually spread to other areas of the body, particularly the bones and lymph nodes. There, tumors cause severe bone pain, leg weakness or paralysis, and eventually death.
Most cases of prostate cancer are detected after screening tests – typically blood tests for levels of prostate-specific antigen (PSA) – indicate unusual growth of prostate tissue. A definitive diagnosis requires a biopsy of the prostate. If cancer is present, the pathologist assigns a Gleason score of 6 to 10, with higher scores representing a more dangerous tumor. Medical imaging is performed to look for cancer spread outside the prostate. Based on the Gleason score, PSA levels, and imaging results, a cancer case is assigned a stage 1 to 4. Higher stage signifies a more advanced, more dangerous disease.
Most prostate tumors remain small and cause no health problems. These are managed with active surveillance, monitoring the tumor with regular tests to ensure it has not grown. Tumors more likely to be dangerous can be destroyed with radiation therapy or surgically removed by radical prostatectomy. Those whose cancer spreads beyond the prostate, are treated with hormone therapy that reduces levels of the androgens (male sex hormones) that prostate cells need in order to survive. Eventually cancer cells grow resistant to this treatment. This most-advanced stage of the disease, called castration-resistant prostate cancer, is treated with continued hormone therapy alongside the chemotherapy drug docetaxel. Prostate cancer prognosis depends on how far the cancer has spread at diagnosis. Most men are diagnosed with tumors confined to the prostate; 99% of them survive more than 10 years from their diagnoses. Tumors that have metastasized to distant body sites are most dangerous, with five-year survival rates of 30–40%.
The risk of developing prostate cancer increases with age; the average age of diagnosis is 67. Those with a family history of prostate cancer are more likely to have prostate cancer themselves. Each year 1.2 million cases of prostate cancer are diagnosed, and 350,000 die of the disease,[2] making it the second-leading cause of cancer and cancer death in men. One in eight men is diagnosed with prostate cancer in his lifetime, while one in forty dies of the disease.[3] Prostate tumors were initially thought to be rare, with an 1893 report describing just 50 cases in the medical literature. As surgery became more common, prostate tumors were found in surgical specimens from enlarged prostates. Surgery and radiation therapy methods to treat the disease were developed over the course of the 20th century. Major work describing prostate tumors' need for male sex hormones, and the subsequent development of hormone therapies for prostate cancer, earned Charles B. Huggins the 1966 Nobel Prize in Physiology or Medicine, and Andrzej W. Schally the 1977 Nobel Prize in Physiology or Medicine.
Most cases of prostate cancer are diagnosed through screening tests, when tumors are too small to cause any symptoms.[4] As a tumor grows beyond the prostate, it can damage nearby organs causing erectile dysfunction, blood in the urine or semen, or trouble urinating – often frequent urination and slow or weak urine stream.[4] More than half of men over age 50 experience some form of urination problem,[5] typically due to issues other than prostate cancer such as benign prostatic hyperplasia (non-cancerous enlargement of the prostate).[4]
Advanced prostate tumors often metastasize to nearby bones of the pelvis and back; there they can cause fatigue, unexplained weight loss, and back or bone pain that does not improve with rest.[6] Metastases can damage the bones around them, and around a quarter of those with metastatic prostate cancer develop a bone fracture.[7] Growing metastases can also compress the spinal cord causing weakness in the legs and feet, or limb paralysis.[8][9]
Prostate cancer screening searches for tumors in those without symptoms.[10] This is typically done through blood tests for levels of the protein prostate-specific antigen (PSA), which are elevated in those with enlarged prostates, whether due to prostate cancer or benign prostatic hyperplasia.[11][10][12] The average person's blood has around 1 nanogram (ng) of PSA per milliliter (mL) of blood tested.[13] Those with PSA levels below average are very unlikely to develop dangerous prostate cancer over the next 8 to 10 years.[13] Men with PSA above 3 ng/mL are at increased risk; 30% will have prostate cancer, and 10% a high-grade cancer that requires treatment.[14] Those with PSA levels above 4 ng/mL are often referred for a prostate biopsy; however, even for this high risk group the majority of biopsies are negative for prostate cancer.[15] Men with higher than average PSA levels are often recommended to repeat the blood test four to six weeks later, as PSA levels can fluctuate unrelated to prostate cancer.[16] Those with elevated PSA may undergo secondary screening blood tests that measure subtypes of PSA and other blood proteins to better predict the likelihood that a person will develop aggressive prostate cancer; 4Kscore, Prostate Health Index, ExoDx Prostate Test, and SelectMDx are all available for this purpose.[17]
Several large studies have found that men screened for prostate cancer have a reduced risk of dying from the disease;[18] however, detection of cancer cases that would not have otherwise impacted health can cause anxiety, and lead to unneeded biopsies and treatments.[10] Major national health organizations offer differing recommendations, attempting to balance the benefits of early diagnosis with the potential harms of treating people whose tumors are unlikely to impact health.[10] Most medical guidelines recommend that men in good health and at high risk of prostate cancer (due to age, family history, ethnicity, or prior evidence of high blood PSA levels) be counseled on the risks and benefits of PSA testing, and be offered access to screening tests.[10] Uptake of screening varies by geography – more than 80% of men are screened in the US and Western Europe, 20% of men in Japan, and screening is rare in regions with low human development index.[18]
Men suspected of having prostate cancer may undergo several tests to help assess the prostate. One common procedure is the digital rectal examination, in which a doctor inserts a lubricated finger into the rectum to feel the nearby prostate.[19][20] Tumors feel like stiff, irregularly shaped lumps against the rest of the prostate. Hardening of the prostate can also be due to benign prostatic hyperplasia; around 20–25% of those with abnormal findings on their rectal exams have prostate cancer.[21]
A diagnosis of prostate cancer requires a biopsy of the prostate. Prostate biopsies are typically taken by a needle passing through the rectum or perineum, guided by transrectal ultrasound imaging, magnetic resonance imaging (MRI), or a combination of the two.[22][20] Ten to twelve samples are taken from several regions of the prostate to improve the chances of finding any tumors.[20] Biopsies are examined under a microscope by a pathologist, who determines the type and extent of cancerous cells present. Cancers are first classified based on their appearance under a microscope. Over 95% of prostate cancers are classified as adenocarcinomas (resembling gland tissue), with the rest largely squamous-cell carcinoma (resembling squamous cells, a type of epithelial cell) and transitional cell carcinoma (resembling transitional cells).[23]
Next tumor samples are graded based on how much the tumor tissue differs from normal prostate tissue; the more different the tumor appears, the faster the tumor is likely to grow. The Gleason grading system is commonly used, where the pathologist assigns a number from 1 (most similar to healthy prostate tissue) to 5 (least similar) for the most common pattern observed under the microscope, then does the same for the second-most common pattern. The sum of these two numbers is the Gleason score.[23] The total scores of 2 through 5 are no longer commonly used in practice, making the lowest score 6, and the highest score 10. Scores are commonly grouped into Gleason grade groups: a score of 6 or lower is Gleason grade group 1; a score of 7 with the first number (from the most common pattern) 3 and the second number 4 is grade group 2; the reverse – first number 4, second number 3 – is grade group 3; a score of 8 is grade group 4; a score of 9 or 10 is grade group 5.[23] Higher Gleason scores and higher grade groups represent cancer cases likely to be more aggressive with worse prognosis.[23]
Extent of cancer spread is assessed by MRI or PSMA scan – a positron emission tomography (PET) imaging technique where a radioactive label that binds the prostate protein prostate-specific membrane antigen is used to detect metastases distant from the prostate.[24][20] CT scans may also be used, but are less able to detect spread outside the prostate than MRI. Bone scintigraphy is used to test for spread of cancer to bones.[24]
After diagnosis, the tumor is staged to determine the extent of its growth and spread. Prostate cancer is typically staged using the American Joint Committee on Cancer's (AJCC) three-component TNM system, with scores assigned for the extent of the tumor (T), spread to any lymph nodes (N), and the presence of metastases (M).[25] Scores of T1 and T2 represent tumors that remain within the prostate: T1 is for tumors not detectable by imaging or digital rectal exam; T2 is for tumors detectable by imaging or rectal exam, but still confined within the prostate.[26] T3 is for tumors that grow beyond the prostate – T3a for tumors with any extension outside the prostate; T3b for tumors that invade the adjacent seminal vesicles. T4 is for tumors that have grown into organs beyond the seminal vesicles.[26] The N and M scores are binary (yes or no). N1 represents any spread to the nearby lymph nodes. M1 represents any metastases to other body sites.[26]
The AJCC then combines the TNM scores, Gleason grade group, and results of the PSA blood test to categorize cancer cases into one of four stages, and their subdivisions. Cancer cases with localized tumors (T1 or T2), no spread (N0 and M0), Gleason grade group 1, and PSA less than 10 ng/mL are designated stage I. Those with localized tumors and PSA between 10 and 20 ng/mL are desigated stage II – subdivided into IIA for Gleason grade group 1, IIB for grade group 2, and IIC for grade group 3 or 4. Stage III is the designation for any of three higher risk factors: IIIA is for a PSA level about 20 ng/mL; IIIB is for T3 or T4 tumors; IIIC is for a Gleason grade group of 5. Stage IV is for cancers that have spread to lymph nodes (N1, stage IVA) or other organs (M1, stage IVB).[25]
The United Kingdom National Institute for Health and Care Excellence recommends a five-stage system based on disease prognosis called the Cambridge Prognostic Group, with prognostic groups CPG 1 to CPG 5.[27] CPG 1 is the same as AJCC stage I. Cases with localized tumors (T1 or T2) and either Gleason grade group 2 or higher PSA levels (10 to 20 ng/mL) are designated CPG 2. CPG 3 represents either Gleason grade group 3, or the combination of the CPG 2 criteria. CPG 4 is similar to AJCC stage 3 – any of Gleason grade group 4, PSA levels above 20 ng/mL, or a tumor that has grown beyond the prostate (T3). CPG 5 is for the highest risk cases: either a T4 tumor, Gleason grade group 5, or any two of the CPG 4 criteria.[28]
No drug or vaccine is approved by regulatory agencies for the prevention of prostate cancer. Several studies have shown 5α-reductase inhibitors to reduce the total incidence of prostate cancer; however, it remains unclear whether they reduce cases of dangerous disease.[29]
Treatment of prostate cancer varies based on how advanced the cancer is, the risk it may spread, and the affected person's health and personal preferences.[30] Those with localized disease at low risk for spread are often more likely to be harmed by the side effects of treatment than the disease itself, and so are monitored regularly by active surveillance – repeat testing for a worsening of their disease.[31] Those at higher risk may receive treatment to eliminate the tumor – typically prostatectomy (surgery to remove the prostate) or radiation therapy, sometimes alongside hormone therapy.[32] Those with metastatic disease are additionally treated with chemotherapy, as well as additional radiation or other agents to alleviate the symptoms of metastatic tumors.[32] Throughout the treatment course, blood PSA levels are monitored to assess the effectiveness of treatments, and whether the disease is advancing.[33]
Men diagnosed with low-risk cases of prostate cancer often defer treatment and are monitored regularly for cancer progression by active surveillance.[31] Active surveillance involves monitoring the tumor for growth at fixed intervals by PSA tests (around every six months), digital rectal exam (annually), and MRI or repeat biopsies (every one to three years).[31] This program continues until increases in PSA levels, Gleason grade, or tumor size indicate a higher-risk tumor that may require intervention.[34] At least half of men remain on active surveillance, never requiring more direct treatment for their prostate tumors.[35]
Those who elect to have therapy typically receive radiation therapy or a prostatectomy.[36] Radiation can be delivered by intensity-modulated radiation therapy (IMRT), which allows for high doses (greater than 80 Gy) to be delivered to the prostate with relatively little radiation to other organs, or by brachytherapy, where a radioactive source is surgically placed next to the prostate.[37][38] IMRT is given over several sessions, with treatments repeated five days per week for several weeks. Brachytherapy is typically performed in a single session, with the radioactive source permanently implanted into the prostate, where it expends its radioactivity within the next few months.[39] With either technique, radiation damage to nearby organs can increase the risk of subsequent bladder cancer and cause erectile dysfunction, infertility, and gastronintestinal problems: diarrhea, bloody stools, fecal incontinence, and pain.[40]
Radical prostatectomy aims to surgically remove the cancerous part of the prostate, along with the seminal vesicles, and parts of the vas deferens.[41] This is typically done by robot-assisted surgery, where robotic tools inserted through small holes in the abdomen allow a surgeon to make small and exact movements during surgery.[42] This method results in shorter hospital stays, less blood loss, and fewer complications than traditional open surgery.[42] In places where robot-assisted surgery is unavailable, prostatectomy can be performed laparoscopically (using a camera and hand tools through small holes in the abdomen), or through traditional open surgery with an incision above the penis (retropubic approach) or below the scrotum (perineal approach).[43][42] The four approaches result in similar rates of cancer control.[43] Damage to nearby tissue during surgery can result in erectile dysfunction and urinary incontinence. Erectile dysfunction is more likely in those who are older or had previous erectile issues.[43] Incontinence is more common in those who are older and have shorter urethras.[43] Both for cancer progression outcomes and surgical side effects, the skill and experience of the individual surgeon doing the procedure are among the greatest determinants of success.[43]
Radiotherapy and surgery result in similar outcomes with respect to bowel, erectile and urinary function after five years.[44] After prostatectomy, PSA levels drop rapidly, reaching very low or undetectable levels within two months. Radiotherapy also substantially reduces PSA levels, but more slowly and less completely, with PSA levels reaching their nadir two years after radiotherapy.[45] After either treatment, PSA levels are  monitored regularly. Up to half of those treated will eventually have a rise in PSA levels, suggesting the tumor or small metastases are growing again.[46] People with high or rising PSA levels are often offered another round of radiation therapy directed at the former tumor site. This reduces risk for further progression by 75%.[47] Those suspected of metastases can undergo PET scanning with sensitive radiotracers C-11 choline, F-18 fluciclovine, and F-18 or Ga-68 attached to a PSMA-targeting drug, each of which is able to detect small metastases more sensitively than alternative imaging methods.[48][47]
For those with metastatic disease, the standard of care is androgen deprivation therapy, drugs that reduce levels of androgens (male sex hormones) that prostate cells require in order to grow.[49] Various drugs are used to lower androgen levels by blocking the synthesis or action of testosterone, the primary androgen. The first line of treatment is typically GnRH agonists like leuprolide, goserelin, triptorelin, or leuprolide mesylate by injection monthly or less frequently if needed.[50][49] GnRH agonists cause a brief rise in testosterone levels at treatment initiation, which can worsen disease in people with significant symptoms of metastases.[51] In these people, GnRH antagonists like degarelix or relugolix are given instead, and can also rapidly reduce testosterone levels.[51] Hormone therapy halts tumor growth in more than 95% of those treated,[52] with PSA levels returning to normal in up to 70%.[53]
Despite reduced testosterone levels, metastatic prostate tumors eventually continue to grow – manifested by rising blood PSA levels, and metastases to nearby bones.[54][55] This is the most advanced stage of the disease, called castration-resistant prostate cancer (CRPC). CRPC is typically treated with systemic chemotherapy alongside continued hormone therapy. The standard of care is docetaxel given every three weeks.[56] Most CRPC cases still rely on androgen signaling, and so antiandrogen drugs are often used, namely the androgen receptor antagonists enzalutamide, apalutamide, and darolutamide, as well as the testosterone production inhibitor abiraterone acetate.[54][57] Those whose cancer becomes resistant to docetaxel may receive the second-generation taxane drug cabazitaxel.[54] An alternative is the cell therapy procedure Sipuleucel-T, where the affected person's immune cells are removed, treated to more effectively target prostate cancer cells, and re-injected into the same person.[54][57]
Bone metastases – present in around 85% of those with metastatic prostate cancer – are the primary cause of symptoms and death from metastatic prostate cancer.[58][59] Those with constant pain are prescribed nonsteroidal anti-inflammatory drugs.[60] However, people with bone metastases often experience "breakthrough pain", sudden bursts of severe pain that resolve within around 15 minutes, before pain medications can take effect.[60] Single sites of pain can be treated with external beam radiation therapy to shrink nearby tumors.[61] More dispersed bone pain can be treated with radioactive compounds that disproportionately accumulate in bone, like radium-223 and samarium-153-EDTMP, which help reduce the size of bone tumors. Similarly, the systemic chemotherapeutics used for metastatic prostate cancer can reduce pain as they shrink tumors.[61] Other bone modifying agents like zoledronic acid and denosumab can reduce prostate cancer bone pain, even though they have little effect on tumor size.[61] Metastases compress the spinal cord in up to 12% of those with metastatic prostate cancer causing pain, weakness, numbness, and paralysis.[62][63] Inflammation in the spine can be treated with high-dose steroids, as well as surgery and radiotherapy to shrink spinal tumors and relieve pressure on the spinal cord.[62][63]
Those with advanced prostate cancer often suffer fatigue, lethargy, and a generalized weakness. This is caused in part by gastrointestinal problems, with loss of appetite, weight loss, nausea, and constipation all common. These are typically treated with appetite-increasing drugs – megestrol acetate or corticosteroids – antiemetics, or treatments that focus on underlying gastrointestinal issues.[64] General weakness can also be caused by anemia, itself caused by a combination of the disease itself, poor nutrition, and damage to the bone marrow from cancer treatments or bone metastases.[65] Anemia can be improved various ways depending on the cause, or can be addressed directly with blood transfusions.[65] Organ damage and metastases in the lymph nodes can lead to uncomfortable accumulation of fluid (called lymphedema) in the genitals or lower limbs. These swellings can be extremely painful, curtailing an affected person's ability to urinate, have sex, or walk normally. Lymphedema can be treated by applying pressure to aid drainage, surgically draining pooled fluid, and cleaning and treating nearby damaged skin.[66]
People with prostate cancer are around twice as likely to experience anxiety or depression compared to those without cancer.[67] When added to normal prostate cancer treatments, psychological interventions such as psychoeducation and cognitive behavioral therapy can help reduce anxiety, depression, and general distress.[68]
As those severely ill with metastatic prostate cancer near the end of their lives, most experience confusion and may hallucinate or have trouble recognizing loved ones.[69][70] Confusion is caused by various conditions, including kidney failure, sepsis, dehydration, and as a side effect of various drugs, especially opioids.[69] Most people sleep for long periods, and some feel drowsy when awake.[70] Restlessness is also common, sometimes caused by physical discomfort from constipation or urinary retention, sometimes caused by anxiety.[70] In their last few days, affected men's breathing may become shallow and slow, with long pauses between breaths. Breathing may be accompanied by a rattling noise as fluid lingers in the throat, but this is not uncomfortable for the affected person.[70][71] Their hands and feet may cool to the touch, and skin become blotchy or blue due to weaker blood circulation. Many stop eating and drinking, resulting in dry-feeling mouth, which can be aided by moistening the mouth and lips.[70] The person becomes less and less responsive, and eventually the heart and breathing stop.[71]
The prognosis of diagnosed prostate cancer varies widely based on the cancer's grade and stage at the time of diagnosis; those with lower stage disease have vastly improved prognoses. Around 80% of prostate cancer diagnoses are in men whose cancer is still confined to the prostate. These men often survive long after diagnosis, with as many as 99% still alive 10 years from diagnosis.[72] Men whose cancer has metastasized to a nearby part of the body (around 15% of diagnoses) have poorer prognoses, with five-year survival rates of 60–80%.[73] Those with metastases in distant body sites (around 5% of diagnoses) have relatively poor prognoses, with five-year survival rates of 30–40%.[73]
Those who have low blood PSA levels at diagnosis, and whose tumors have a low Gleason grade and less-advanced clinical stage tend to have better prognoses.[74] After prostatectomy or radiotherapy, those who have a short time between treatment and a subsequent rise in PSA levels, or a rapid rate of PSA level increases are more likely to die from their cancers.[46] Castration-resistant metastatic prostate cancer is incurable,[75] and kills a majority of those whose disease reaches this stage.[54]
Prostate cancer is caused by the accumulation of genetic mutations to the DNA of cells in the prostate. These mutations affect genes involved in cell growth, replication, cell death, and DNA damage repair.[76] Changes to these genes can cause cells in the prostate to grow uncontrollably, resulting in a tumor.[77] Over time, the tumor may grow large enough to invade nearby organs such as the seminal vesicles or bladder.[78] Eventually, tumor cells develop the ability to travel through the lymphatic system to nearby lymph nodes, or through the bloodstream to the bone marrow and (more rarely) other body sites.[79] At these new sites, the cancer cells disrupt normal body function and continue to grow. Metastases cause most of the discomfort associated with prostate cancer, and eventually can kill the affected person.[79]
Most prostate tumors begin in the peripheral zone – the outermost part of the prostate.[80] As cells begin to grow out of control, they form a small clump of disregulated cells called a prostatic intraepithelial neoplasia (PIN).[81] Some PINs continue to grow, forming layers of tissue that stop expressing genes common to their original tissue location – p63, cytokeratin 5, and cytokeratin 14 – and begin expressing genes common to cells that makeup the innermost lining of the pancreatic duct – cytokeratin 8 and cytokeratin 18.[80] These multilayered PINs also often overexpress the gene AMACR, which is associated with prostate cancer progression.[80]
Particularly large PINs can eventually grow into tumors. This is commonly accompanied by large-scale changes to the genome, with chromosome sequences being rearranged or copied repeatedly. Some genomic alterations are partiuclarly common in early prostate cancer, namely gene fusion between TMPRSS2 and the oncogene ERG (up to 60% of prostate tumors), mutations that disable SPOP (up to 15% of tumors), and mutations that hyperactivate FOXA1 (up to 5% of tumors).[80]
Metastatic prostate cancer tends to have more genetic mutations than localized disease.[82] Many of these mutations are in genes that protect from DNA damage, such as p53 (mutated in 8% of localized tumors, more than 27% of metastatic ones) and RB1 (1% of localized tumors, more than 5% of metastatic ones).[82] Similarly mutations in the DNA repair-related genes BRCA2 and ATM are rare in localized disease but found in at least 7% and 5% of metastatic disease cases respectively.[82]
The transition from castrate-sensitive to castrate-resistant prostate cancer is also accompanied by the acquisition of various gene mutations. In castrate-resistant disease, more than 70% of tumors have mutations in the androgen receptor signaling pathway – amplifications and gain-of-function mutations in the receptor gene itself, amplification of its activators (e.g. FOXA1), or inactivating mutations in its negative regulators (e.g. ZBTB16 and NCOR1).[82] These androgen receptor disruptions are only found in up to 6% of biopsies of castrate-sensitive metastatic disease.[82] Similarly, deletions of the tumor suppressor PTEN are harbored by 12–17% of castrate-sensitive tumors, but over 40% of castrate-resistant tumors.[82] Less commonly, tumors have aberrant activation of the Wnt signaling pathway via disruption of members APC (9% of tumors) or CTNNB1 (4% of tumors); or  dysregulation of the PI3K pathway via PI3KCA/PI3KCB mutations (6% of tumors) or AKT1 (2% of tumors).[82]
Prostate cancer is the second-most frequently diagnosed cancer in men, and the second-most frequent cause of cancer death in men (after lung cancer).[2][3] Around 1.2 million new cases of prostate cancer are diagnosed each year, and 350,000 men die of the disease.[2] One in eight men are diagnosed with prostate cancer in their lifetime, and around one in forty die of the disease.[3] Rates of prostate cancer rise with age. Due to this, prostate cancer rates are generally higher in parts of the world with higher life expectancy, which also tend to be areas with higher gross domestic product and higher human development index.[2] Australia, Europe, North America, New Zealand, and parts of South America have the highest incidence. South Asia, Central Asia, and sub-Saharan Africa have the lowest incidence of prostate cancer; though incidence is increasing in these regions at among the fastest rates in the world.[2] Prostate cancer is the most diagnosed cancer in men in over half of the world's countries, and the leading cause of cancer death in men in around a quarter of countries.[83]
Prostate cancer is rare in those under 40 years old.[84] The overwhelming majority of cases are diagnosed in those over 60 years,[2] with the average person diagnosed at 67.[85] The average person who dies from prostate cancer is 77.[85] Only a minority of prostate cancer cases are ever diagnosed. Autopsies of men who died at various ages have shown cancer in the prostates of over 40% of men over age 50. Incidence rises with age, and nearly 70% of men autopsied at age 80–89 had cancer in their prostates.[86]
Prostate cancer is more common in some families. Men with an affected first-degree relative (father or brother) have more than twice the risk of developing prostate cancer, and those with two first-degree relatives have a five-fold greater risk compared with men with no family history.[87] Increased risk also runs in some ethnic groups, with men of African and African-Caribbean ancestry at particularly high risk – having prostate cancer at higher rates, and having more-aggressive prostate cancers that develop at earlier ages.[88] Large genome-wide association studies have identified over 100 gene variants associated with increased prostate cancer risk.[89] The greatest risk increase is associated with variations in BRCA2 (up to an eight-fold increased risk) and HOXB13 (three-fold increased risk), both of which are involved in repairing DNA damage.[89] Variants in other genes involved in DNA damage repair have also been associated with an increased risk of developing prostate cancer – particularly early-onset prostate cancer – including BRCA1, ATM, NBS1, MSH2, MSH6, PMS2, CHEK2, RAD51D, and PALB2.[89] Additionally, variants in the genome near the oncogene MYC are associated with increased risk.[89] As are single-nucleotide polymorphisms in the vitamin D receptor common in African-Americans, and in the androgen receptor, CYP3A4, and CYP17 involved in testosterone synthesis and signaling.[87] Together known gene variants are estimated to cause around 25% of prostate cancer cases, including 40% of early-onset prostate cancers.[87]
Men who are taller are at a slightly increased risk for developing prostate cancer, as are men who are obese.[90] High levels of blood cholesterol are also associated with increased prostate cancer risk; consequently, those who take the cholesterol-lowering drugs, statins, have a reduced risk of advanced prostate cancer.[91] Chronic inflammation can cause various cancers. Potential links between infection (or other sources of inflammation) and prostate cancer have been studied but none definitively found, with one large study finding no link between prostate cancer and a history of gonorrhea, syphilis, chlamydia, or infection with various human papillomaviruses.[92]
Regular vigorous exercise may reduce one's chance of developing advanced prostate cancer, as can several dietary interventions.[93] Those with a diet rich in cruciferous vegetables, fish, genistein, or lycopene (found in tomatoes) are at a reduced risk of symptomatic prostate cancer.[87][94] Conversely, those who consume high levels of dietary fats, polycyclic aromatic hydrocarbons (from cooking red meats), or calcium may be at an increased risk of developing advanced prostate cancer.[87][95] Several dietary supplements have been studied and found not to impact prostate cancer risk, including selenium, vitamin C, vitamin D, and vitamin E.[29][95]
A tumor in the prostate was first described in 1817 by the English surgeon George Langstaff, following the autopsy of a man who had died at age 68 with lower-body pain and urinary issues.[96][97] In 1853, London Hospital surgeon John Adams described another prostate tumor from a man who had died with urinary issues; Adams had a pathologist examine the tumor, providing the first histologically confirmed case of a cancerous tumor in the prostate.[96][98] The disease was initially thought to be uncommon as it was rarely distinguished from other causes of urinary obstruction.[99] An 1893 report found only 50 cases described in the medical literature.[100] Around the turn of the 19th century, prostate surgery to relieve urinary obstruction became more common, allowing surgeons and pathologists to examine the removed prostate tissue. Two studies around the time found cancer in as many as 10% of surgical specimens, suggesting prostate cancer was a fairly common cause of prostate enlargement.[100]
For much of the 20th century, the primary therapy for prostate cancer was surgery to remove the prostate. Perineal prostatectomy was first performed in 1904 by Hugh H. Young at Johns Hopkins Hospital.[101][102] Young's method became the widespread standard, initially done primarily to relieve symptoms of urinary blockage.[101] In 1931 a new surgical method, transurethral resection of the prostate, became available, replacing perineal prostatectomy for symptomatic relief of obstruction.[100] In 1945, Terence Millin described a retropubic prostatectomy approach, which provided easier access to pelvic lymph nodes to assist in staging the extent of disease, and was easier for surgeons to learn.[101] This was improved upon by Patrick C. Walsh's 1983 description of a retropubic prostatectomy approach that avoided damage to the nerves near the prostate, preserving erectile function.[101][103]
Radiation therapy for prostate cancer was used occasionally in the early 20th century, with radium implanted into the urethra or rectum to reduce the tumor size and associated symptoms.[104] In the 1950s the advent of more powerful radiation machines allowed for external beam radiotherapy to reach the prostate. By the 1960s, this was often combined with hormone therapy to improve the potency of therapy.[104] In the 1970s, Willet Whitmore pioneered an open surgery technique where needles of Iodine-125 were placed directly into the prostate. This was improved upon by Henrik H. Holm in 1983 by using transcrectal ultrasound to guide the implantation of radioactive material.[104]
The observation that the testicles (and the hormones they secrete) influence prostate size was made as early as the late 18th century via castration experiments in animals. However, occasional experimentation over the next century bore mixed results, likely due to the inability to separate prostate tumors from prostates enlarged due to benign prostatic hyperplasia. In 1941, Charles B. Huggins and Clarence V. Hodges published two studies using surgical castration or oral estrogen to reduce androgen levels and improve prostate cancer symptoms. Huggins was awarded the 1966 Nobel Prize in Physiology or Medicine for this discovery, the first systemic therapy for prostate cancer.[105][106] In the 1960s, large studies showed estrogen therapy to be as effective as surgical castration at treating prostate cancer, but that those on estrogen therapy were at increased risk of suffering blood clots.[105] Through the 1980s, Andrzej W. Schally's studies of GnRH led to the development of GnRH agonists, which were found to be as effective as estrogen without the increased risk of clotting.[105][107] Schally was awarded the 1977 Nobel Prize in Physiology or Medicine for his work on GnRH and prostate cancer.[105]
Systemic chemotherapy for prostate cancer has been studied since the 1950s, with clinical trials failing to show benefits in most people who receive the drug.[108] In 1996, the US Food and Drug Administration approved the systemic chemotherapy mitoxantrone for those with castration-resistant prostate cancer based on trials showing that it improved symptoms even though it failed to enhance survival.[109] In 2004, docetaxel was approved as the first chemotherapy to increase survival in those with castration-resistant prostate cancer.[109] After additional trials in 2015, docetaxel use was extended to those with castration-sensitive prostate cancer.[110]
Prostate cancer screening and awareness have been widely promoted since the early 2000s by Prostate Cancer Awareness Month in September and Movember in November.[111] Analyses of internet searches and social media posts suggest neither event changes the level of prostate cancer interest or discussion, in contrast to the more established Breast Cancer Awareness Month.[111][112] A light blue ribbon is used to promote prostate cancer awareness.
Prostate cancer is a major topic of ongoing research, with the U.S. National Cancer Institute (NCI, the world's largest funder of cancer research) spending $209 million on prostate cancer research in 2020 – the sixth highest among cancer types.[113] Despite high gross spending, prostate cancer research funding is relatively low for the number of deaths it causes. The NCI spends around $5,700 per prostate cancer death, considerably lower than for brain cancer ($21,000 per death), breast cancer ($13,000 per death) or cancer as a whole ($11,000 per death).[114] A similar trend holds for private nonprofit organizations. Annual revenues of prostate cancer-focused nonprofits rank sixth among cancer types, but prostate cancer nonprofits have lower revenue than would be expected for the number of cases, deaths, and potential years of life lost.[115]
Research into prostate cancer relies on a number of laboratory models to test aspects of the disease. Several prostate immortalized cell lines are widely used, namely the classic lines DU145, PC-3, and LNCaP, as well as more recent cell lines 22Rv1, LAPC-4, VCaP, and MDA-PCa-2a and −2b.[116] Research requiring more complex models of the prostate uses organoids – clusters of prostate cells that can be grown from human prostate tumors or stem cells.[117] Modeling tumor growth and metastasis requires a model organism, typically a mouse. Researchers can either surgically implant human prostate tumors into immunocompromised mice (a technique called a patient derived xenograft),[118] or can induce prostate tumors in mice either with chemical exposure or genetic engineering.[119] These genetically engineered mouse models typically use a Cre recombinase system to disrupt tumor suppressors or activate oncogenes specifically in prostate cells.[120]

Rosacea is a long-term skin condition that typically affects the face.[2][3] It results in redness, pimples, swelling, and small and superficial dilated blood vessels.[2] Often, the nose, cheeks, forehead, and chin are most involved.[3] A red, enlarged nose may occur in severe disease, a condition known as rhinophyma.[3]
The cause of rosacea is unknown.[2] Risk factors are believed to include a family history of the condition.[3] Factors that may potentially worsen the condition include heat, exercise, sunlight, cold, spicy food, alcohol, menopause, psychological stress, or steroid cream on the face.[3] Diagnosis is based on symptoms.[2]
While not curable, treatment usually improves symptoms.[3] Treatment is typically with metronidazole, doxycycline,  minocycline, or tetracycline.[4] When the eyes are affected, azithromycin eye drops may help.[5] Other treatments with tentative benefit include brimonidine cream, ivermectin cream, and isotretinoin.[4] Dermabrasion or laser surgery may also be used.[3] The use of sunscreen is typically recommended.[3]
Rosacea affects between 1% and 10% of people.[2] Those affected are most often 30 to 50 years old and female.[2] People with paler skin or European ancestry are more frequently affected.[2] The condition was described in The Canterbury Tales in the 1300s, and possibly as early as the 200s BC by Theocritus.[6][7]
Rosacea typically begins with reddening (flushing) of the skin in symmetrical patches near the center of the face.[9] Common signs can depend on age and sex: flushing and red swollen patches are common in the young, small and visible dilated blood vessels in older individuals, and swelling of the nose is common in men.[9] Other signs include lumps on the skin (papules or pustules) and swelling of the face.[9] Many people experience stinging or burning pain and rarely itching.[9]
Skin problems tend to be aggravated by particular trigger factors, that differ for different people. Common triggers are ultraviolet light, heat, cold, or certain foods or beverages.[9]
Erythematotelangiectatic rosacea[10] rosacea (also known as "vascular rosacea"[10]) is characterized by prominent history of prolonged (over 10 minutes) flushing reaction to various stimuli, such as emotional stress, hot drinks, alcohol, spicy foods, exercise, cold or hot weather, or hot baths and showers.[11]
In glandular rosacea, men with thick sebaceous skin predominate, a disease in which the papules are edematous, and the pustules are often 0.5 to 1.0 cm in size, with nodulocystic lesions often present.[11]
The exact cause of rosacea is unknown.[2] Triggers that cause episodes of flushing and blushing play a part in its development. Exposure to temperature extremes, strenuous exercise, heat from sunlight, severe sunburn, stress, anxiety, cold wind, and moving to a warm or hot environment from a cold one, such as heated shops and offices during the winter, can each cause the face to become flushed.[2] Certain foods and drinks can also trigger flushing, such as alcohol, foods and beverages containing caffeine (especially hot tea and coffee), foods high in histamines, and spicy foods.[12]
Medications and topical irritants have also been known to trigger rosacea flares. Some acne and wrinkle treatments reported to cause rosacea include microdermabrasion and chemical peels, as well as high dosages of isotretinoin, benzoyl peroxide, and tretinoin.
Steroid-induced rosacea is caused by the use of topical steroids.[13] These steroids are often prescribed for seborrheic dermatitis. Dosage should be slowly decreased and not immediately stopped to avoid a flare-up.
In 2007, Richard Gallo and colleagues noticed that patients with rosacea had high levels of  cathelicidin, an antimicrobial peptide,[14] and elevated levels of stratum corneum tryptic enzymes (SCTEs). Antibiotics have been used in the past to treat rosacea, but they may only work because they inhibit some SCTEs.[14]
Studies of rosacea and Demodex mites have revealed that some people with rosacea have increased numbers of the mite,[12] especially those with steroid-induced rosacea. Demodex folliculitis (demodicidosis, also known as "mange" in animals) is a condition that may have a "rosacea-like" appearance.[15]
A 2007, National Rosacea Society-funded study demonstrated that Demodex folliculorum mites may be a cause or exacerbating factor in rosacea.[16] The researchers identified Bacillus oleronius as distinct bacterium associated with Demodex mites. When analyzing blood samples using a peripheral blood mononuclear cell proliferation assay, they discovered that B. oleronius stimulated an immune system response in 79 percent of 22 patients with subtype 2 (papulopustular) rosacea, compared with only 29% of 17 subjects without the disorder. They concluded, "The immune response results in inflammation, as evident in the papules (bumps) and pustules (pimples) of subtype 2 rosacea. This suggests that the B. oleronius bacteria found in the mites could be responsible for the inflammation associated with the condition."[16]
Small intestinal bacterial overgrowth (SIBO) was demonstrated to have greater prevalence in rosacea patients and treating it with locally acting antibiotics led to rosacea lesion improvement in two studies. Conversely in rosacea patients who were SIBO negative, antibiotic therapy had no effect.[17] The effectiveness of treating SIBO in rosacea patients may suggest that gut bacteria play a role in the pathogenesis of rosacea lesions.
Most people with rosacea have only mild redness and are never formally diagnosed or treated.  No test for rosacea is known. In many cases, simple visual inspection by a trained health-care professional is sufficient for diagnosis.  In other cases, particularly when pimples or redness on less-common parts of the face is present, a trial of common treatments is useful for confirming a suspected diagnosis. The disorder can be confused or co-exist with acne vulgaris or seborrheic dermatitis.  The presence of a rash on the scalp or ears suggests a different or co-existing diagnosis because rosacea is primarily a facial diagnosis, although it may occasionally appear in these other areas.
Four rosacea subtypes exist,[19] and a patient may have more than one subtype:[20]: 176 
Variants of rosacea include:[25]: 689 
The type of rosacea a person has informs the choice of treatment.[26] Mild cases are often not treated at all, or are simply covered up with normal cosmetics.
Therapy for the treatment of rosacea is not curative, and is best measured in terms of reduction in the amount of facial redness and inflammatory lesions, a decrease in the number, duration, and intensity of flares, and concomitant symptoms of itching, burning, and tenderness. The two primary modalities of rosacea treatment are topical and oral antibiotic agents.[27] Laser therapy has also been classified as a form of treatment.[27] While medications often produce a temporary remission of redness within a few weeks, the redness typically returns shortly after treatment is suspended.  Long-term treatment, usually 1–2 years, may result in permanent control of the condition for some patients.[27][28] Lifelong treatment is often necessary, although some cases resolve after a while and go into a permanent remission.[28]  Other cases, if left untreated, worsen over time.[29] Some people have also reported better results after changing diet. This is not confirmed by medical studies, even though some studies relate the histamine production to outbreak of rosacea.[30]
Certain behavioral changes may improve the symptoms of rosacea or help to prevent exacerbations. Keeping a symptoms diary to document potential symptom triggers and avoiding those triggers is recommended.[24] Common exacerbating triggers include ultraviolet light and irritant cosmetics, therefore it is recommended that those with rosacea wear sunscreen (with a sun factor protection (SPF) of 30 or greater) and avoid cosmetics.[24] If using cosmetics or makeup is desired, then oil free foundation and concealer should be used.[24] Skin astringents, products that can dry the skin and impair the skin barrier, including products with alcohol, menthol, peppermint, camphor, or eucalyptus oil, should generally be avoided. People should avoid using exfoliating skin scrubs, cosmetics or soaps containing sodium laureth sulfate, or waterproof makeup to the affected area as these products can compromise the skin barrier protection and be difficult to remove.[24] Using soap-free cleansers and non-oily moisturizers are preferred if used on the affected area. Many skin care products have been specifically formulated for those with sensitive skin or for those with conditions such as rosacea.[24] Ocular rosacea may be treated with daily gentle eyelid washing using warm water, and artificial tears to lubricate the eye.[24]
Managing pre-trigger events such as prolonged exposure to cool environments can directly influence warm-room flushing.[31]
Medications with good evidence include topical ivermectin and azelaic acid creams and brimonidine, and doxycycline and isotretinoin by mouth.[32] Lesser evidence supports topical metronidazole cream and tetracycline by mouth.[32] Isotretinoin and tetracycline antibiotics, which may be used in more severe cases of inflammatory rosacea, are absolutely contraindicated in women who are pregnant, may become pregnant or lactating as they are highly teratogenic (associated with birth defects). Contraception is required for women of child bearing age who are using these medications.[24]
Metronidazole is thought to act through anti-inflammatory mechanisms, while azelaic acid is thought to decrease cathelicidin production. Oral antibiotics of the tetracycline class such as doxycycline, minocycline, and oxytetracycline are also commonly used and thought to reduce papulopustular lesions through anti-inflammatory actions rather than through their antibacterial capabilities.[12]
Using alpha-hydroxy acid peels may help relieve redness caused by irritation, and reduce papules and pustules associated with rosacea.[33]
Oral Beta-blockers are often used for those with flushing due to rosacea. These include nadolol, propanolol or carvedilol. The possible adverse reactions of the oral beta-blockers include low blood pressure, low heart rate or dizziness.[24] The oral α-2 adrenergic receptor agonist clonidine can also be used for flushing symptoms.[24] The flushing and blushing that typically accompany rosacea may also be treated with the topical application of alpha agonists such as brimonidine which has vasoconstrictor activity and achieves maximal symptom improvement 3-6 hours after application, other topicals used for flushing or erythema include oxymetazoline or xylometazoline.[12]
A review found that ivermectin was more effective than alternatives for treatment of papulopustular acne rosacea.[34][35] An ivermectin cream has been approved by the FDA, as well as in Europe, for the treatment of inflammatory lesions of rosacea. The treatment is based upon the hypothesis that parasitic mites of the genus Demodex play a role in rosacea.[36] In a clinical study, ivermectin reduced lesions by 83% over 4 months, as compared to 74% under a metronidazole standard therapy.[37]  Quassia amara extract at 4% demonstrated to have clinical efficacy for rosacea.[38] When compared to metronidazole 0.75% as usual care in a randomized, double-blinded clinical trial, Quassia amara extract at 4% demonstrated earlier onset of action, including improvement in telangiectasia, flushing, and papules. Quassia amara showed a sustained reduction of symptoms at 42 days of treatment.[39]
Cyclosporin eye drops have been shown to reduce symptoms in those with ocular rosacea. Cyclosporin should not be used in those with an active ocular infection.[24] Other options include topical metronidazole cream or topical fusidic acid applied to the eyelids, or oral doxycycline in more severe cases of ocular rosacea. If papules and pustules persist, then sometimes isotretinoin can be prescribed.[40]
Evidence for the use of laser and intense pulsed-light therapy in rosacea is poor.[41]
The highly visible nature of rosacea symptoms are often psychologically challenging for those affected. People with rosacea can experience issues with self-esteem, socializing, and changes to their thoughts, feelings, and coping mechanisms.[9]
Rosaceae affects around 5% of people worldwide.[9] Incidence varies by ethnicity, and is particularly prevalent in those with Celtic heritage.[9] Men and women are equally likely to develop rosacea.[9]

Sleep apnea, also spelled sleep apnoea, is a sleep disorder in which pauses in breathing or periods of shallow breathing during sleep occur more often than normal.[1] Each pause can last for a few seconds to a few minutes and they happen many times a night.[1] In the most common form, this follows loud snoring.[2] A choking or snorting sound may occur as breathing resumes.[1] Because the disorder disrupts normal sleep, those affected may experience sleepiness or feel tired during the day.[1] In children, it may cause hyperactivity or problems in school.[2]
Sleep apnea may be either obstructive sleep apnea (OSA), in which breathing is interrupted by a blockage of air flow, central sleep apnea (CSA), in which regular unconscious breath simply stops, or a combination of the two.[1] OSA is the most common form.[1] OSA has four key contributors; these include a narrow, crowded, or collapsible upper airway, an ineffective pharyngeal dilator muscle function during sleep, airway narrowing during sleep, and unstable control of breathing (high loop gain).[10][11] It is often a chronic condition. Other risk factors include being overweight,[12] a family history of the condition, allergies, and enlarged tonsils.[6] Some people with sleep apnea are unaware they have the condition.[1] In many cases it is first observed by a family member.[1] Sleep apnea is often diagnosed with an overnight sleep study.[8] For a diagnosis of sleep apnea, more than five episodes per hour must occur.[13]
In CSA, the basic neurological controls for breathing rate malfunction and fail to give the signal to inhale, causing the individual to miss one or more cycles of breathing. If the pause in breathing is long enough, the percentage of oxygen in the circulation can drop to a lower than normal level (hypoxaemia) and the concentration of carbon dioxide can build to a higher than normal level (hypercapnia).[14] In turn, these conditions of hypoxia and hypercapnia will trigger additional effects on the body (such as Cheyne-Stokes Respiration).[15] Brain cells need constant oxygen to live, and if the level of blood oxygen goes low enough for long enough, brain damage and even death can occur.
A systemic disorder, sleep apnea is associated with a wide array of effects, including increased risk of car accidents, hypertension, cardiovascular disease, myocardial infarction, stroke, atrial fibrillation, insulin resistance, higher incidence of cancer, and neurodegeneration.[12] The exact effects of the condition depend on how severe the apnea is and on the individual characteristics of the person having the apnea.
Treatment may include lifestyle changes, mouthpieces, breathing devices, and surgery.[1] Effective lifestyle changes may include avoiding alcohol, losing weight, stopping smoking, and sleeping on one's side.[16] Breathing devices include the use of a CPAP machine.[17] With proper use, CPAP improves outcomes.[18] Evidence suggests that CPAP may improve sensitivity to insulin, blood pressure, and sleepiness.[19][20][21] Long term compliance, however, is an issue with more than half of people not appropriately using the device.[18][22] In 2017, only 15% of potential patients in developed countries used CPAP machines, while in developing countries well under 1% of potential patients used CPAP.[23] Without treatment, sleep apnea may increase the risk of heart attack, stroke, diabetes, heart failure, irregular heartbeat, obesity, and motor vehicle collisions.[1]
Alzheimer's disease and severe obstructive sleep apnea are connected[24] because there is an increase in the protein beta-amyloid as well as white-matter damage.  These are the main indicators of Alzheimer's, which in this case comes from the lack of proper rest or poorer sleep efficiency resulting in neurodegeneration.[3][25][26] Having sleep apnea in mid-life brings a higher likelihood of developing Alzheimer's in older age, and if one has Alzheimer's then one is also more likely to have sleep apnea.[9] This is demonstrated by cases of sleep apnea even being misdiagnosed as dementia.[27] With the use of treatment through CPAP, there is a reversible risk factor in terms of the amyloid proteins.  This usually restores brain structure and diminishes cognitive impairment.[28][29][30]
OSA is a common sleep disorder. A large analysis in 2019 of the estimated prevalence of OSA found that OSA affects 936 million—1 billion people between the ages of 30–69 globally, or roughly every 1 in 10 people, and up to 30% of the elderly.[31] Sleep apnea is somewhat more common in men than women, roughly a 2:1 ratio of men to women, and in general more people are likely to have it with older age and obesity.[5]
People with sleep apnea have problems with excessive daytime sleepiness (EDS) and impaired alertness.[32] OSA may increase risk for driving accidents and work-related accidents. If OSA is not treated, people are at increased risk of other health problems, such as diabetes.
Due to the disruption in daytime cognitive state, behavioral effects may be present. These can include moodiness, belligerence, as well as a decrease in attentiveness and energy.[33] These effects may become intractable, leading to depression.[34]
There is evidence that the risk of diabetes among those with moderate or severe sleep apnea is higher.[35] Finally, because there are many factors that could lead to some of the effects previously listed, some people are not aware that they have sleep apnea and are either misdiagnosed or ignore the symptoms altogether.[32]
Sleep apnea can affect people regardless of sex, race, or age. However, risk factors include:[citation needed]
Alcohol, sedatives and tranquilizers may also promote sleep apnea by relaxing throat muscles. People who smoke tobacco have sleep apnea at three times the rate of people who have never done so.[36]
Central sleep apnea is more often associated with any of the following risk factors:
High blood pressure is very common in people with sleep apnea.[37]
When breathing is paused, carbon dioxide builds up in the bloodstream. Chemoreceptors in the bloodstream note the high carbon dioxide levels. The brain is signaled to awaken the person, which clears the airway and allows breathing to resume. Breathing normally will restore oxygen levels and the person will fall asleep again.[38] This carbon dioxide build-up may be due to the decrease of output of the brainstem regulating the chest wall or pharyngeal muscles, which causes the pharynx to collapse.[39] People with sleep apnea experience reduced or no slow-wave sleep and spend less time in REM sleep.[39]
OSA is a serious medical condition. Daytime fatigue and sleepiness, cardiovascular problems and eye problems are considered potential complications of OSA. OSA may also be a risk factor of COVID-19. People with OSA have a higher risk of developing severe complications of COVID-19.[40]
Despite this[which?] medical consensus, the variety of apneic events (e.g., hypopnea vs apnea, central vs obstructive), the variability of patients' physiologies, and the inherent shortcomings and variability of equipment and methods, this field is subject to debate.[41]
Within this context, the definition of an event depends on several factors (e.g., patient's age) and account for this variability through a multi-criteria decision rule described in several, sometimes conflicting, guidelines.[42][43]
Oximetry, which may be performed over one or several nights in a person's home, is a simpler, but less reliable alternative to a polysomnography. The test is recommended only when requested by a physician and should not be used to test those without symptoms.[44] Home oximetry may be effective in guiding prescription for automatically self-adjusting continuous positive airway pressure.[45][46]
There are three types of sleep apnea. OSA accounts for 84%, CSA for 0.9%, and 15% of cases are mixed.[47]
Obstructive sleep apnea (OSA) is the most common category of sleep-disordered breathing. The muscle tone of the body ordinarily relaxes during sleep, and at the level of the throat, the human airway is composed of collapsible walls of soft tissue that can obstruct breathing. Mild occasional sleep apnea, such as many people experience during an upper respiratory infection, may not be significant, but chronic severe obstructive sleep apnea requires treatment to prevent low blood oxygen (hypoxemia), sleep deprivation, and other complications.
Individuals with low muscle-tone and soft tissue around the airway (e.g., because of obesity) and structural features that give rise to a narrowed airway are at high risk for obstructive sleep apnea. The elderly are more likely to have OSA than young people. Men are more likely to develop sleep apnea than women and children are, though it is not uncommon in the last two population groups.[48]
The risk of OSA rises with increasing body weight, active smoking and age. In addition, patients with diabetes or "borderline" diabetes have up to three times the risk of having OSA.
Common symptoms include loud snoring, restless sleep, and sleepiness during the daytime. Diagnostic tests include home oximetry or polysomnography in a sleep clinic.
Some treatments involve lifestyle changes, such as avoiding alcohol or muscle relaxants, losing weight, and quitting smoking. Many people benefit from sleeping at a 30-degree elevation of the upper body[49] or higher, as if in a recliner. Doing so helps prevent the gravitational collapse of the airway. Lateral positions (sleeping on a side), as opposed to supine positions (sleeping on the back), are also recommended as a treatment for sleep apnea,[50][51][52] largely because the gravitational component is smaller in the lateral position. Some people benefit from various kinds of oral appliances such as the Mandibular advancement splint to keep the airway open during sleep. Continuous positive airway pressure (CPAP) is the most effective treatment for severe obstructive sleep apnea, but oral appliances are considered a first-line approach equal to CPAP for mild to moderate sleep apnea, according to the American Academy of Sleep Medicine (AASM) parameters of care.[53] There are also surgical procedures to remove and tighten tissue and widen the airway.
Snoring is a common finding in people with this syndrome. Snoring is the turbulent sound of air moving through the back of the mouth, nose, and throat. Although not everyone who snores is experiencing difficulty breathing, snoring in combination with other risk factors has been found to be highly predictive of OSA.[54] The loudness of the snoring is not indicative of the severity of obstruction, however. If the upper airways are tremendously obstructed, there may not be enough air movement to make much sound. Even the loudest snoring does not mean that an individual has sleep apnea syndrome. The sign that is most suggestive of sleep apneas occurs when snoring stops.
Up to 78% of genes associated with habitual snoring also increase the risk for OSA.[55]
Other indicators include (but are not limited to): hypersomnolence, obesity (BMI ≥ 30), large neck circumference—16 in (410 mm) in women, 17 in (430 mm) in men — enlarged tonsils and large tongue volume, micrognathia, morning headaches, irritability/mood-swings/depression, learning and/or memory difficulties, and sexual dysfunction.
The term "sleep-disordered breathing" is commonly used in the U.S. to describe the full range of breathing problems during sleep in which not enough air reaches the lungs (hypopnea and apnea). Sleep-disordered breathing is associated with an increased risk of cardiovascular disease, stroke, high blood pressure, arrhythmias, diabetes, and sleep deprived driving accidents.[56][57][58][59] When high blood pressure is caused by OSA, it is distinctive in that, unlike most cases of high blood pressure (so-called essential hypertension), the readings do not drop significantly when the individual is sleeping.[60] Stroke is associated with obstructive sleep apnea.[61]
Obstructive sleep apnea is associated with problems in daytime functioning, such as daytime sleepiness, motor vehicle crashes, psychological problems, decreased cognitive functioning, and reduced quality of life.[62] Other associated problems include cerebrovascular diseases (hypertension, coronary artery disease, and stroke) and diabetes.[62] These problems could be, at least in part, caused by risk factors of OSA.[62]
In pure central sleep apnea or Cheyne–Stokes respiration, the brain's respiratory control centers are imbalanced during sleep.[63] Blood levels of carbon dioxide, and the neurological feedback mechanism that monitors them, do not react quickly enough to maintain an even respiratory rate, with the entire system cycling between apnea and tachypnea, even during wakefulness. The sleeper stops breathing and then starts again. There is no effort made to breathe during the pause in breathing: there are no chest movements and no struggling. After the episode of apnea, breathing may be faster (tachypnea) for a period of time, a compensatory mechanism to blow off retained waste gases and absorb more oxygen.
While sleeping, a normal individual is "at rest" as far as cardiovascular workload is concerned. Breathing is regular in a healthy person during sleep, and oxygen levels and carbon dioxide levels in the bloodstream stay fairly constant. Any sudden drop in oxygen or excess of carbon dioxide (even if tiny) strongly stimulates the brain's respiratory centers to breathe.
In any person, hypoxia and hypercapnia have certain common effects on the body.[64] The heart rate will increase, unless there are such severe co-existing problems with the heart muscle itself or the autonomic nervous system that makes this compensatory increase impossible. The more translucent areas of the body will show a bluish or dusky cast from cyanosis, which is the change in hue that occurs owing to lack of oxygen in the blood ("turning blue"). Overdoses of drugs that are respiratory depressants (such as heroin, and other opiates) kill by damping the activity of the brain's respiratory control centers. In central sleep apnea, the effects of sleep alone can remove the brain's mandate for the body to breathe.
Some people with sleep apnea have a combination of both types; its prevalence ranges from 0.56% to 18%. The condition is generally detected when obstructive sleep apnea is treated with CPAP and central sleep apnea emerges. The exact mechanism of the loss of central respiratory drive during sleep in OSA is unknown but is most likely related to incorrect settings of the CPAP treatment and other medical conditions the person has.[66]
The treatment of obstructive sleep apnea is different than that of central sleep apnea. Treatment often starts with behavioral therapy and some people may be suggested to try a continuous positive airway pressure device. Many people are told to avoid alcohol, sleeping pills, and other sedatives, which can relax throat muscles, contributing to the collapse of the airway at night.[67] The evidence supporting one treatment option compared to another for a particular person is not clear.[68]
More than half of people with obstructive sleep apnea have some degree of positional obstructive sleep apnea, meaning that it gets worse when they sleep on their backs.[69]  Sleeping on their sides is an effective and cost-effective treatment for positional obstructive sleep apnea.[69]
For moderate to severe sleep apnea, the most common treatment is the use of a continuous positive airway pressure (CPAP) or automatic positive airway pressure (APAP) device.[67][70] These splint the person's airway open during sleep by means of pressurized air. The person typically wears a plastic facial mask, which is connected by a flexible tube to a small bedside CPAP machine.[67]
Although CPAP therapy is effective in reducing apneas and less expensive than other treatments, some people find it uncomfortable. Some complain of feeling trapped, having chest discomfort, and skin or nose irritation. Other side effects may include dry mouth, dry nose, nosebleeds, sore lips and gums.[71]
Whether or not it decreases the risk of death or heart disease is controversial with some reviews finding benefit and others not.[18][72][68] This variation across studies might be driven by low rates of compliance—analyses of those who use CPAP for at least four hours a night suggests a decrease in cardiovascular events.[73]
Excess body weight is thought to be an important cause of sleep apnea.[74] People who are overweight have more tissues in the back of their throat which can restrict the airway, especially when sleeping.[75] In weight loss studies of overweight individuals, those who lose weight show reduced apnea frequencies and improved apnoea–hypopnoea index (AHI).[74][76] Weight loss effective enough to relieve obesity hypoventilation syndrome (OHS) must be 25–30% of body weight. For some obese people, it can be difficult to achieve and maintain this result without bariatric surgery.[77]
In children, orthodontic treatment to expand the volume of the nasal airway, such as nonsurgical rapid palatal expansion is common. The procedure has been found to significantly decrease the AHI and lead to long-term resolution of clinical symptoms.[78][79]
Since the palatal suture is fused in adults, regular RPE using tooth-borne expanders cannot be performed. Mini-implant assisted rapid palatal expansion (MARPE) has been recently developed as a non-surgical option for the transverse expansion of the maxilla in adults. This method increases the volume of the nasal cavity and nasopharynx, leading to increased airflow and reduced respiratory arousals during sleep.[80][81] Changes are permanent with minimal complications.
Several surgical procedures (sleep surgery) are used to treat sleep apnea, although they are normally a third line of treatment for those who reject or are not helped by CPAP treatment or dental appliances.[18] Surgical treatment for obstructive sleep apnea needs to be individualized to address all anatomical areas of obstruction.
Often, correction of the nasal passages needs to be performed in addition to correction of the oropharynx passage. Septoplasty and turbinate surgery may improve the nasal airway,[82] but has been found to be ineffective at reducing respiratory arousals during sleep.[83]
Tonsillectomy and uvulopalatopharyngoplasty (UPPP or UP3) are available to address pharyngeal obstruction.
The "Pillar" device is a treatment for snoring and obstructive sleep apnea; it is thin, narrow strips of polyester. Three strips are inserted into the roof of the mouth (the soft palate) using a modified syringe and local anesthetic, in order to stiffen the soft palate. This procedure addresses one of the most common causes of snoring and sleep apnea — vibration or collapse of the soft palate. It was approved by the FDA for snoring in 2002 and for obstructive sleep apnea in 2004. A 2013 meta-analysis found that "the Pillar implant has a moderate effect on snoring and mild-to-moderate obstructive sleep apnea" and that more studies with high level of evidence were needed to arrive at a definite conclusion; it also found that the polyester strips work their way out of the soft palate in about 10% of the people in whom they are implanted.[84]
Base-of-tongue advancement by means of advancing the genial tubercle of the mandible, tongue suspension, or hyoid suspension (aka hyoid myotomy and suspension or hyoid advancement) may help with the lower pharynx.
Other surgery options may attempt to shrink or stiffen excess tissue in the mouth or throat, procedures done at either a doctor's office or a hospital. Small shots or other treatments, sometimes in a series, are used for shrinkage, while the insertion of a small piece of stiff plastic is used in the case of surgery whose goal is to stiffen tissues.[67]
Maxillomandibular advancement (MMA) is considered the most effective surgery for people with sleep apnea, because it increases the posterior airway space (PAS).[85] However, health professionals are often unsure as to who should be referred for surgery and when to do so: some factors in referral may include failed use of CPAP or device use; anatomy which favors rather than impedes surgery; or significant craniofacial abnormalities which hinder device use.[86]
Several inpatient and outpatient procedures use sedation. Many drugs and agents used during surgery to relieve pain and to depress consciousness remain in the body at low amounts for hours or even days afterwards. In an individual with either central, obstructive or mixed sleep apnea, these low doses may be enough to cause life-threatening irregularities in breathing or collapses in a patient's airways.[87] Use of analgesics and sedatives in these patients postoperatively should therefore be minimized or avoided.
Surgery on the mouth and throat, as well as dental surgery and procedures, can result in postoperative swelling of the lining of the mouth and other areas that affect the airway. Even when the surgical procedure is designed to improve the airway, such as tonsillectomy and adenoidectomy or tongue reduction, swelling may negate some of the effects in the immediate postoperative period. Once the swelling resolves and the palate becomes tightened by postoperative scarring, however, the full benefit of the surgery may be noticed.
A person with sleep apnea undergoing any medical treatment must make sure their doctor and anesthetist are informed about the sleep apnea. Alternative and emergency procedures may be necessary to maintain the airway of sleep apnea patients.[88]
Diaphragm pacing, which involves the rhythmic application of electrical impulses to the diaphragm, has been used to treat central sleep apnea.[89][90]
In April 2014, the U.S. Food and Drug Administration granted pre-market approval for use of an upper airway stimulation system in people who cannot use a continuous positive airway pressure device. The Inspire Upper Airway Stimulation system senses respiration and applies mild electrical stimulation during inspiration, which pushes the tongue slightly forward to open the airway.[91]
There is currently insufficient evidence to recommend any medication for OSA.[92] This may result in part because people with sleep apnea have tended to be treated as a single group in clinical trials. Identifying specific physiological factors underlying sleep apnea makes it possible to test drugs specific to those causal factors: airway narrowing, impaired muscle activity, low arousal threshold for waking, and unstable breathing control.[10][93] 
Those who experience low waking thresholds may benefit from eszopiclone, a sedative typically used to treat insomnia.[10][94] The  antidepressant desipramine may stimulate upper airway muscles and lessen pharyngeal collapsibility in people who have limited muscle function in their airways.[10][95]
There is limited evidence for medication, but 2012 AASM guidelines suggested that acetazolamide "may be considered" for the treatment of central sleep apnea; zolpidem and triazolam may also be considered for the treatment of central sleep apnea,[96] but "only if the patient does not have underlying risk factors for respiratory depression".[92][70] 
Low doses of oxygen are also used as a treatment for hypoxia but are discouraged due to side effects.[97][98][99]
An oral appliance, often referred to as a mandibular advancement splint, is a custom-made mouthpiece that shifts the lower jaw forward and opens the bite slightly, opening up the airway. These devices can be fabricated by a general dentist. Oral appliance therapy (OAT) is usually successful in patients with mild to moderate obstructive sleep apnea.[100][101] While CPAP is more effective for sleep apnea than oral appliances, oral appliances do improve sleepiness and quality of life and are often better tolerated than CPAP.[101]
Nasal EPAP is a bandage-like device placed over the nostrils that uses a person's own breathing to create positive airway pressure to prevent obstructed breathing.[102]
Oral pressure therapy uses a device that creates a vacuum in the mouth, pulling the soft palate tissue forward. It has been found useful in about 25 to 37% of people.[103][104]
Death could occur from untreated OSA due to lack of oxygen to the body.[71]
There is increasing evidence that sleep apnea may lead to liver function impairment, particularly fatty liver diseases (see steatosis).[33][105][106][107]
It has been revealed that people with OSA show tissue loss in brain regions that help store memory, thus linking OSA with memory loss.[108] Using magnetic resonance imaging (MRI), the scientists discovered that people with sleep apnea have mammillary bodies that are about 20% smaller, particularly on the left side. One of the key investigators hypothesized that repeated drops in oxygen lead to the brain injury.[109]
The immediate effects of central sleep apnea on the body depend on how long the failure to breathe endures. At worst, central sleep apnea may cause sudden death. Short of death, drops in blood oxygen may trigger seizures, even in the absence of epilepsy. In people with epilepsy, the hypoxia caused by apnea may trigger seizures that had previously been well controlled by medications.[65] In other words, a seizure disorder may become unstable in the presence of sleep apnea. In adults with coronary artery disease, a severe drop in blood oxygen level can cause angina, arrhythmias, or heart attacks (myocardial infarction). Longstanding recurrent episodes of apnea, over months and years, may cause an increase in carbon dioxide levels that can change the pH of the blood enough to cause a respiratory acidosis.
The Wisconsin Sleep Cohort Study estimated in 1993 that roughly one in every 15 Americans was affected by at least moderate sleep apnea.[110][111] It also estimated that in middle-age as many as 9% of women and 24% of men were affected, undiagnosed and untreated.[74][110][111]
The costs of untreated sleep apnea reach further than just health issues. It is estimated that in the U.S., the average untreated sleep apnea patient's annual health care costs $1,336 more than an individual without sleep apnea. This may cause $3.4 billion/year in additional medical costs. Whether medical cost savings occur with treatment of sleep apnea remains to be determined.[112]
Sleep disorders including sleep apnea have become an important health issue in the United States. Twenty-two million Americans have been estimated to have sleep apnea, with 80% of moderate and severe OSA cases undiagnosed.[113]
OSA can occur at any age, but it happens more frequently in men who are over 40 and overweight.[113]
A type of CSA was described in the German myth of Ondine's curse where the person when asleep would forget to breathe.[114] The clinical picture of this condition has long been recognized as a character trait, without an understanding of the disease process. The term "Pickwickian syndrome" that is sometimes used for the syndrome was coined by the famous early 20th-century physician William Osler, who must have been a reader of Charles Dickens. The description of Joe, "the fat boy" in Dickens's novel The Pickwick Papers, is an accurate clinical picture of an adult with obstructive sleep apnea syndrome.[115]
The early reports of obstructive sleep apnea in the medical literature described individuals who were very severely affected, often presenting with severe hypoxemia, hypercapnia and congestive heart failure.
The management of obstructive sleep apnea was improved with the introduction of continuous positive airway pressure (CPAP), first described in 1981 by Colin Sullivan and associates in Sydney, Australia.[116] The first models were bulky and noisy, but the design was rapidly improved and by the late 1980s, CPAP was widely adopted. The availability of an effective treatment stimulated an aggressive search for affected individuals and led to the establishment of hundreds of specialized clinics dedicated to the diagnosis and treatment of sleep disorders. Though many types of sleep problems are recognized, the vast majority of patients attending these centers have sleep-disordered breathing. Sleep apnea awareness day is April 18 in recognition of Colin Sullivan.[117]

Kidney stone disease, also known as renal calculus disease, nephrolithiasis or urolithiasis, is a crystallopathy where a solid piece of material (renal calculus) develops in the urinary tract.[2] Renal calculi typically form in the kidney and leave the body in the urine stream.[2] A small calculus may pass without causing symptoms.[2] If a stone grows to more than 5 millimeters (0.2 inches), it can cause blockage of the ureter, resulting in sharp and severe pain in the lower back or abdomen.[2][7] A calculus may also result in blood in the urine, vomiting, or painful urination.[2] About half of people who have had a renal calculus are likely to have another within ten years.[8]
Most calculi form by a combination of genetics and environmental factors.[2] Risk factors include high urine calcium levels, obesity, certain foods, some medications, calcium supplements, hyperparathyroidism, gout and not drinking enough fluids.[2][8] Calculi form in the kidney when minerals in urine are at high concentration.[2] The diagnosis is usually based on symptoms, urine testing, and medical imaging.[2] Blood tests may also be useful.[2] Calculi are typically classified by their location: nephrolithiasis (in the kidney), ureterolithiasis (in the ureter), cystolithiasis (in the bladder), or by what they are made of (calcium oxalate, uric acid, struvite, cystine).[2]
In those who have had renal calculi, drinking fluids is a way to prevent them. Drinking fluids such that more than two liters of urine are produced per day is recommended.[4] If fluid intake alone is not effective to prevent renal calculi, the medications thiazide diuretic, citrate, or allopurinol may be suggested.[4] Soft drinks containing phosphoric acid (typically colas) should be avoided.[4] When a calculus causes no symptoms, no treatment is needed.[2] For those with symptoms, pain control is usually the first measure, using medications such as nonsteroidal anti-inflammatory drugs or opioids.[7][9] Larger calculi may be helped to pass with the medication tamsulosin[10] or may require procedures such as extracorporeal shock wave lithotripsy, ureteroscopy, or percutaneous nephrolithotomy.[2]
Renal calculi have affected humans throughout history with a description of surgery to remove them dating from as early as 600 BCE in ancient India by Sushruta.[1] Between 1% and 15% of people globally are affected by renal calculi at some point in their lives.[8][11] In 2015, 22.1 million cases occurred,[5] resulting in about 16,100 deaths.[6] They have become more common in the Western world since the 1970s.[8][12] Generally, more men are affected than women.[2][11] The prevalence and incidence of the disease rises worldwide and continues to be challenging for patients, physicians, and healthcare systems alike. In this context, epidemiological studies are striving to elucidate the worldwide changes in the patterns and the burden of the disease and identify modifiable risk factors that contribute to the development of renal calculi.[13]
The hallmark of a stone that obstructs the ureter or renal pelvis is excruciating, intermittent pain that radiates from the flank to the groin or to the inner thigh.[14] This is due to the transfer of referred pain signals from the lower thoracic splanchnic nerves to the lumbar splanchnic nerves as the stone passes down from the kidney or proximal ureter to the distal ureter. This pain, known as renal colic, is often described as one of the strongest pain sensations known.[15] Renal colic caused by kidney stones is commonly accompanied by urinary urgency, restlessness, hematuria, sweating, nausea, and vomiting. It typically comes in waves lasting 20 to 60 minutes caused by peristaltic contractions of the ureter as it attempts to expel the stone.[14]
The embryological link between the urinary tract, the genital system, and the gastrointestinal tract is the basis of the radiation of pain to the gonads, as well as the nausea and vomiting that are also common in urolithiasis.[16] Postrenal azotemia and hydronephrosis can be observed following the obstruction of urine flow through one or both ureters.[17]
Pain in the lower-left quadrant can sometimes be confused with diverticulitis because the sigmoid colon overlaps the ureter, and the exact location of the pain may be difficult to isolate due to the proximity of these two structures.
Dehydration from low fluid intake is a major factor in stone formation.[14][18] Individuals living in warm climates are at higher risk due to increased fluid loss.[19] Obesity, immobility, and sedentary lifestyles are other leading risk factors.[19]
High dietary intake of animal protein,[14] sodium, sugars including honey, refined sugars, fructose and high fructose corn syrup,[20] and excessive consumption of fruit juices may increase the risk of kidney stone formation due to increased uric acid excretion and elevated urinary oxalate levels (whereas tea, coffee, wine and beer may decrease the risk).[19][18]
Kidney stones can result from an underlying metabolic condition, such as distal renal tubular acidosis,[21] Dent's disease,[22] hyperparathyroidism,[23] primary hyperoxaluria,[24] or medullary sponge kidney. 3–20% of people who form kidney stones have medullary sponge kidney.[25][26]
Kidney stones are more common in people with Crohn's disease;[27] Crohn's disease is associated with hyperoxaluria and malabsorption of magnesium.[28]
A person with recurrent kidney stones may be screened for such disorders. This is typically done with a 24-hour urine collection. The urine is analyzed for features that promote stone formation.[17]
Calcium is one component of the most common type of human kidney stones, calcium oxalate. Some studies suggest that people who take calcium or vitamin D as a dietary supplement have a higher risk of developing kidney stones.[29][30]  In the United States, kidney stone formation was used as an indicator of excess calcium intake by the Reference Daily Intake committee for calcium in adults.[31]
In the early 1990s, a study conducted for the Women's Health Initiative in the US found that postmenopausal women who consumed 1000 mg of supplemental calcium and 400 international units of vitamin D per day for seven years had a 17% higher risk of developing kidney stones than subjects taking a placebo.[29] The Nurses' Health Study also showed an association between supplemental calcium intake and kidney stone formation.[30]
Unlike supplemental calcium, high intakes of dietary calcium do not appear to cause kidney stones and may actually protect against their development.[30][29] This is perhaps related to the role of calcium in binding ingested oxalate in the gastrointestinal tract. As the amount of calcium intake decreases, the amount of oxalate available for absorption into the bloodstream increases; this oxalate is then excreted in greater amounts into the urine by the kidneys. In the urine, oxalate is a very strong promoter of calcium oxalate precipitation—about 15 times stronger than calcium.
A 2004 study found that diets low in calcium are associated with a higher overall risk for kidney stone formation.[32] For most individuals, other risk factors for kidney stones, such as high intakes of dietary oxalates and low fluid intake, play a greater role than calcium intake.[33]
Calcium is not the only electrolyte that influences the formation of kidney stones. For example, by increasing urinary calcium excretion, high dietary sodium may increase the risk of stone formation.[30]
Drinking fluoridated tap water may increase the risk of kidney stone formation by a similar mechanism, though further epidemiologic studies are warranted to determine whether fluoride in drinking water is associated with an increased incidence of kidney stones.[34] High dietary intake of potassium appears to reduce the risk of stone formation because potassium promotes the urinary excretion of citrate, an inhibitor of calcium crystal formation.[35]
Kidney stones are more likely to develop, and to grow larger, if a person has low dietary magnesium. Magnesium inhibits stone formation.[36]
Diets in Western nations typically contain a large proportion of animal protein. Eating animal protein creates an acid load that increases urinary excretion of calcium and uric acid and reduced citrate. Urinary excretion of excess sulfurous amino acids (e.g., cysteine and methionine), uric acid, and other acidic metabolites from animal protein acidifies the urine, which promotes the formation of kidney stones.[37] Low urinary-citrate excretion is also commonly found in those with a high dietary intake of animal protein, whereas vegetarians tend to have higher levels of citrate excretion.[30] Low urinary citrate, too, promotes stone formation.[37]
The evidence linking vitamin C supplements with an increased rate of kidney stones is inconclusive.[38][39] The excess dietary intake of vitamin C might increase the risk of calcium-oxalate stone formation.[40] The link between vitamin D intake and kidney stones is also tenuous.
Excessive vitamin D supplementation may increase the risk of stone formation by increasing the intestinal absorption of calcium; correction of a deficiency does not.[30]
When the urine becomes supersaturated (when the urine solvent contains more solutes than it can hold in solution) with one or more calculogenic (crystal-forming) substances, a seed crystal may form through the process of nucleation.[25] Heterogeneous nucleation (where there is a solid surface present on which a crystal can grow) proceeds more rapidly than homogeneous nucleation (where a crystal must grow in a liquid medium with no such surface), because it requires less energy. Adhering to cells on the surface of a renal papilla, a seed crystal can grow and aggregate into an organized mass. Depending on the chemical composition of the crystal, the stone-forming process may proceed more rapidly when the urine pH is unusually high or low.[41]
Supersaturation of the urine with respect to a calculogenic compound is pH-dependent. For example, at a pH of 7.0, the solubility of uric acid in urine is 158 mg/100 mL. Reducing the pH to 5.0 decreases the solubility of uric acid to less than 8 mg/100 mL. The formation of uric-acid stones requires a combination of hyperuricosuria (high urine uric-acid levels) and low urine pH; hyperuricosuria alone is not associated with uric-acid stone formation if the urine pH is alkaline.[42] Supersaturation of the urine is a necessary, but not a sufficient, condition for the development of any urinary calculus.[25] Supersaturation is likely the underlying cause of uric acid and cystine stones, but calcium-based stones (especially calcium oxalate stones) may have a more complex cause.[43]
While supersaturation of urine may lead to crystalluria, it does not necessarily promote the formation of a kidney stone because the particle may not reach the sufficient size needed for renal attachment.[44][45] On the other hand, Randall's plaques, which were first identified by Alexander Randall in 1937,[46] are calcium phosphate deposits that form in the papillary interstitium and are thought to be the nidus required for stone development.[47] In addition to Randall's plugs, which form in the Duct of Bellini, these structures can generate reactive oxygen species that further enhance stone formation.[48]
Some bacteria have roles in promoting stone formation. Specifically, urease-positive bacteria, such as Proteus mirabilis can produce the enzyme urease, which converts urea to ammonia and carbon dioxide.[49] This increases the urinary pH and promotes struvite stone formation. Additionally, non-urease producing bacteria can provide bacteria components that can promote calcium oxalate crystallization, though this mechanism is poorly understood.[50][51]
Normal urine contains chelating agents, such as citrate, that inhibit the nucleation, growth, and aggregation of calcium-containing crystals. Other endogenous inhibitors include calgranulin (an S-100 calcium-binding protein), Tamm–Horsfall protein, glycosaminoglycans, uropontin (a form of osteopontin), nephrocalcin (an acidic glycoprotein), prothrombin F1 peptide, and bikunin (uronic acid-rich protein). The biochemical mechanisms of action of these substances have not yet been thoroughly elucidated. However, when these substances fall below their normal proportions, stones can form from an aggregation of crystals.[52]
Sufficient dietary intake of magnesium and citrate inhibits the formation of calcium oxalate and calcium phosphate stones; in addition, magnesium and citrate operate synergistically to inhibit kidney stones. The efficacy of magnesium in subduing stone formation and growth is dose-dependent.[30][36][53]
Hypocitraturia or low urinary-citrate excretion (variably defined as less than 320 mg/day) can be a contributing cause of kidney stones in up to 2/3 of cases. The protective role of citrate is linked to several mechanisms; citrate reduces urinary supersaturation of calcium salts by forming soluble complexes with calcium ions and by inhibiting crystal growth and aggregation. Therapy with potassium citrate  is commonly prescribed in clinical practice to increase urinary citrate and to reduce stone formation rates. Alkali citrate is also used to increase urine citrate levels. It can be prescribed or found over-the-counter in pill, liquid or powder form.[54][55]
Diagnosis of kidney stones is made on the basis of information obtained from the history, physical examination, urinalysis, and radiographic studies.[56] Clinical diagnosis is usually made on the basis of the location and severity of the pain, which is typically colicky in nature (comes and goes in spasmodic  waves). Pain in the back occurs when calculi produce an obstruction in the kidney.[57] Physical examination may reveal fever and tenderness at the costovertebral angle on the affected side.[56]
Calcium-containing stones are relatively radiodense, and they can often be detected by a traditional radiograph of the abdomen that includes the kidneys, ureters, and bladder (KUB film).[58] KUB radiograph, although useful in monitoring size of stone or passage of stone in stone formers, might not be useful in the acute setting due to low sensitivity.[59] Some 60% of all renal stones are radiopaque.[60][61] In general, calcium phosphate stones have the greatest density, followed by calcium oxalate and magnesium ammonium phosphate stones. Cystine calculi are only faintly radiodense, while uric acid stones are usually entirely radiolucent.[62]
In people with a history of stones, those who are less than 50 years of age and are presenting with the symptoms of stones without any concerning signs do not require helical CT scan imaging.[63] A CT scan is also not typically recommended in children.[64]
Otherwise a noncontrast helical CT scan with 5 millimeters (0.2 in) sections is the diagnostic method to use to detect kidney stones and confirm the diagnosis of kidney stone disease.[16][56][60][65][7] Near all stones are detectable on CT scans with the exception of those composed of certain drug residues in the urine,[58] such as from indinavir.
Where a CT scan is unavailable, an intravenous pyelogram may be performed to help confirm the diagnosis of urolithiasis. This involves intravenous injection of a contrast agent followed by a KUB film. Uroliths present in the kidneys, ureters, or bladder may be better defined by the use of this contrast agent. Stones can also be detected by a retrograde pyelogram, where a similar contrast agent is injected directly into the distal ostium of the ureter (where the ureter terminates as it enters the bladder).[60]
Renal ultrasonography can sometimes be useful, because it gives details about the presence of hydronephrosis, suggesting that the stone is blocking the outflow of urine.[58] Radiolucent stones, which do not appear on KUB, may show up on ultrasound imaging studies. Other advantages of renal ultrasonography include its low cost and absence of radiation exposure. Ultrasound imaging is useful for detecting stones in situations where X-rays or CT scans are discouraged, such as in children or pregnant women.[66] Despite these advantages, renal ultrasonography in 2009 was not considered a substitute for noncontrast helical CT scan in the initial diagnostic evaluation of urolithiasis.[65] The main reason for this is that, compared with CT, renal ultrasonography more often fails to detect small stones (especially ureteral stones) and other serious disorders that could be causing the symptoms.[14]
On the contrary, a 2014 study suggested that ultrasonography should be used as the initial diagnostic imaging test, with further imaging studies be performed at the discretion of the physician on the basis of clinical judgment, and using ultrasonography rather than CT as an initial diagnostic test results in less radiation exposure and equally good outcome.[67]
Bilateral kidney stones can be seen on this KUB radiograph. There are phleboliths in the pelvis, which can be misinterpreted as bladder stones.
Axial CT scan of abdomen without contrast, showing a 3-mm stone (marked by an arrow) in the left proximal ureter
Renal ultrasonograph of a stone located at the pyeloureteric junction with accompanying hydronephrosis.
Measurement of a 5.6 mm large kidney stone in soft tissue versus skeletal CT window.
Laboratory investigations typically carried out include[56][65][58][68]
By far, the most common type of kidney stones worldwide contains calcium. For example, calcium-containing stones represent about 80% of all cases in the United States; these typically contain calcium oxalate either alone or in combination with calcium phosphate in the form of apatite or brushite.[25][52] Factors that promote the precipitation of oxalate crystals in the urine, such as primary hyperoxaluria, are associated with the development of calcium oxalate stones.[24] The formation of calcium phosphate stones is associated with conditions such as hyperparathyroidism[23] and renal tubular acidosis.[73]
Oxaluria is increased in patients with certain gastrointestinal disorders including inflammatory bowel disease such as Crohn's disease or in patients who have undergone resection of the small bowel or small-bowel bypass procedures. Oxaluria is also increased in patients who consume increased amounts of oxalate (found in vegetables and nuts). Primary hyperoxaluria is a rare autosomal recessive condition that usually presents in childhood.[74]
Calcium oxalate crystals can come in two varieties. Calcium oxalate monohydrate can appear as 'dumbbells' or as long ovals that resemble the individual posts in a picket fence. Calcium oxalate dihydrate have a tetragonal “envelope” appearance.[74]
About 10–15% of urinary calculi are composed of struvite (ammonium magnesium phosphate, NH4MgPO4·6H2O).[75] Struvite stones (also known as "infection stones," urease, or triple-phosphate stones) form most often in the presence of infection by urea-splitting bacteria. Using the enzyme urease, these organisms metabolize urea into ammonia and carbon dioxide. This alkalinizes the urine, resulting in favorable conditions for the formation of struvite stones. Proteus mirabilis, Proteus vulgaris, and Morganella morganii are the most common organisms isolated; less common organisms include Ureaplasma urealyticum and some species of Providencia, Klebsiella, Serratia, and Enterobacter. These infection stones are commonly observed in people who have factors that predispose them to urinary tract infections, such as those with spinal cord injury and other forms of neurogenic bladder, ileal conduit urinary diversion, vesicoureteral reflux, and obstructive uropathies. They are also commonly seen in people with underlying metabolic disorders, such as idiopathic hypercalciuria, hyperparathyroidism, and gout. Infection stones can grow rapidly, forming large calyceal staghorn (antler-shaped) calculi requiring invasive surgery such as percutaneous nephrolithotomy for definitive treatment.[75]
Struvite stones (triple-phosphate/magnesium ammonium phosphate) have a 'coffin lid' morphology by microscopy.[74]
About 5–10% of all stones are formed from uric acid.[21] People with certain metabolic abnormalities, including obesity,[30] may produce uric acid stones. They also may form in association with conditions that cause hyperuricosuria (an excessive amount of uric acid in the urine) with or without hyperuricemia (an excessive amount of uric acid in the serum). They may also form in association with disorders of acid/base metabolism where the urine is excessively acidic (low pH), resulting in precipitation of uric acid crystals. A diagnosis of uric acid urolithiasis is supported by the presence of a radiolucent stone in the face of persistent urine acidity, in conjunction with the finding of uric acid crystals in fresh urine samples.[76]
As noted above (section on calcium oxalate stones), people with inflammatory bowel disease (Crohn's disease, ulcerative colitis) tend to have hyperoxaluria and form oxalate stones. They also have a tendency to form urate stones. Urate stones are especially common after colon resection.
Uric acid stones appear as pleomorphic crystals, usually diamond-shaped.  They may also look like squares or rods which are polarizable.[74]
People with certain rare inborn errors of metabolism have a propensity to accumulate crystal-forming substances in their urine. For example, those with cystinuria, cystinosis, and Fanconi syndrome may form stones composed of cystine. Cystine stone formation can be treated with urine alkalinization and dietary protein restriction.  People affected by xanthinuria often produce stones composed of xanthine. People affected by adenine phosphoribosyltransferase deficiency may produce 2,8-dihydroxyadenine stones,[77] alkaptonurics produce homogentisic acid stones, and iminoglycinurics produce stones of glycine, proline, and hydroxyproline.[78][79] Urolithiasis has also been noted to occur in the setting of therapeutic drug use, with crystals of drug forming within the renal tract in some people currently being treated with agents such as indinavir,[80] sulfadiazine,[81] and triamterene.[82]
Urolithiasis refers to stones originating anywhere in the urinary system, including the kidneys and bladder.[16] Nephrolithiasis refers to the presence of such stones in the kidneys. Calyceal calculi are aggregations in either the minor or major calyx, parts of the kidney that pass urine into the ureter (the tube connecting the kidneys to the urinary bladder). The condition is called ureterolithiasis when a calculus is located in the ureter. Stones may also form or pass into the bladder, a condition referred to as bladder stones.[83]
Stones less than 5 mm (0.2 in) in diameter pass spontaneously in up to 98% of cases, while those measuring 5 to 10 mm (0.2 to 0.4 in) in diameter pass spontaneously in less than 53% of cases.[84]
Stones that are large enough to fill out the renal calyces are called staghorn stones and are composed of struvite in a vast majority of cases, which forms only in the presence of urease-forming bacteria. Other forms that can possibly grow to become staghorn stones are those composed of cystine, calcium oxalate monohydrate, and uric acid.[85]
Preventative measures depend on the type of stones. In those with calcium stones, drinking plenty of fluids, thiazide diuretics and citrate are effective as is allopurinol in those with high uric acid levels in urine.[86][87]
Specific therapy should be tailored to the type of stones involved. Diet can have an effect on the development of kidney stones. Preventive strategies include some combination of dietary modifications and medications with the goal of reducing the excretory load of calculogenic compounds on the kidneys.[32][88][89] Dietary recommendations to minimize the formation of kidney stones include:
Maintenance of dilute urine by means of vigorous fluid therapy is beneficial in all forms of kidney stones, so increasing urine volume is a key principle for the prevention of kidney stones. Fluid intake should be sufficient to maintain a urine output of at least 2 litres (68 US fl oz) per day.[87] A high fluid intake may reduce the likelihood of kidney stone recurrence or may increase the time between stone development without unwanted effects.
Calcium binds with available oxalate in the gastrointestinal tract, thereby preventing its absorption into the bloodstream. Reducing oxalate absorption decreases kidney stone risk in susceptible people.[96] Because of this, some doctors recommend increasing dairy intake so that its calcium content will serve as an oxalate binder. Taking calcium citrate tablets during or after meals containing high oxalate foods[97] may be useful if dietary calcium cannot be increased by other means as in those with lactose intolerance. The preferred calcium supplement for people at risk of stone formation is calcium citrate, as opposed to calcium carbonate, because it helps to increase urinary citrate excretion.[89]
Aside from vigorous oral hydration and eating more dietary calcium, other prevention strategies include avoidance of higher doses of supplemental vitamin C (since ascorbate is metabolized to oxalate) and restriction of oxalate-rich foods such as leaf vegetables, rhubarb, soy products and chocolate.[98] However, no randomized, controlled trial of oxalate restriction has been performed to test the hypothesis that oxalate restriction reduces stone formation.[97] Some evidence indicates magnesium intake decreases the risk of symptomatic kidney stones.[98]
The mainstay for medical management of uric acid stones is alkalinization (increasing the pH) of the urine. Uric acid stones are among the few types amenable to dissolution therapy, referred to as chemolysis. Chemolysis is usually achieved through the use of oral medications, although in some cases, intravenous agents or even instillation of certain irrigating agents directly onto the stone can be performed, using antegrade nephrostomy or retrograde ureteral catheters.[42] Acetazolamide is a medication that alkalinizes the urine. In addition to acetazolamide or as an alternative, certain dietary supplements are available that produce a similar alkalinization of the urine. These include alkali citrate, sodium bicarbonate, potassium citrate, magnesium citrate, and Bicitra (a combination of citric acid monohydrate and sodium citrate dihydrate).[99] Aside from alkalinization of the urine, these supplements have the added advantage of increasing the urinary citrate level, which helps to reduce the aggregation of calcium oxalate stones.[42]
Increasing the urine pH to around 6.5 provides optimal conditions for dissolution of uric acid stones. Increasing the urine pH to a value higher than 7.0 may increase the risk of calcium phosphate stone formation, though this concept is controversial since citrate does inhibit calcium phosphate crystallization. Testing the urine periodically with nitrazine paper can help to ensure the urine pH remains in this optimal range. Using this approach, stone dissolution rate can be expected to be around 10 mm (0.4 in) of stone radius per month.[42]
Calcium hydroxide decreases urinary calcium when combined with food rich in oxalic acid such as green leafy vegetables.[100]
One of the recognized medical therapies for prevention of stones is the thiazide and thiazide-like diuretics, such as chlorthalidone or indapamide. These drugs inhibit the formation of calcium-containing stones by reducing urinary calcium excretion.[14] Sodium restriction is necessary for clinical effect of thiazides, as sodium excess promotes calcium excretion. Thiazides work best for renal leak hypercalciuria (high urine calcium levels), a condition in which high urinary calcium levels are caused by a primary kidney defect. Thiazides are useful for treating absorptive hypercalciuria, a condition in which high urinary calcium is a result of excess absorption from the gastrointestinal tract.[52]
For people with hyperuricosuria and calcium stones, allopurinol is one of the few treatments that have been shown to reduce kidney stone recurrences. Allopurinol interferes with the production of uric acid in the liver. The drug is also used in people with gout or hyperuricemia (high serum uric acid levels).[101] Dosage is adjusted to maintain a reduced urinary excretion of uric acid. Serum uric acid level at or below 6 mg/100 mL is often a therapeutic goal. Hyperuricemia is not necessary for the formation of uric acid stones; hyperuricosuria can occur in the presence of normal or even low serum uric acid. Some practitioners advocate adding allopurinol only in people in whom hyperuricosuria and hyperuricemia persist, despite the use of a urine-alkalinizing agent such as sodium bicarbonate or potassium citrate.[42]
Stone size influences the rate of spontaneous stone passage. For example, up to 98% of small stones (less than 5 mm (0.2 in) in diameter) may pass spontaneously through urination within four weeks of the onset of symptoms,[7] but for larger stones (5 to 10 mm (0.2 to 0.4 in) in diameter), the rate of spontaneous passage decreases to less than 53%.[84] Initial stone location also influences the likelihood of spontaneous stone passage. Rates increase from 48% for stones located in the proximal ureter to 79% for stones located at the vesicoureteric junction, regardless of stone size.[84] Assuming no high-grade obstruction or associated infection is found in the urinary tract, and symptoms are relatively mild, various nonsurgical measures can be used to encourage the passage of a stone.[42] Repeat stone formers benefit from more intense management, including proper fluid intake and use of certain medications, as well as careful monitoring.[102]
Management of pain often requires intravenous administration of NSAIDs or opioids.[14] NSAIDs appear somewhat better than opioids or paracetamol in those with normal kidney function.[103] Medications by mouth are often effective for less severe discomfort.[66] The use of antispasmodics does not have further benefit.[9]
The use of medications to speed the spontaneous passage of stones in the ureter is referred to as medical expulsive therapy.[104][105] Several agents, including alpha adrenergic blockers (such as tamsulosin) and calcium channel blockers (such as nifedipine), may be effective.[104] Alpha-blockers likely result in more people passing their stones, and they may pass their stones in a shorter time.[105] People taking alpha-blockers may also use less pain medication and may not need to visit the hospital.[105] Alpha-blockers appear to be more effective for larger stones (over 5 mm in size) than smaller stones.[105] However, use of alpha-blockers may be associated with a slight increase in serious, unwanted effects from this medication.[105] A combination of tamsulosin and a corticosteroid may be better than tamsulosin alone.[104] These treatments also appear to be useful in addition to lithotripsy.[7]
Extracorporeal shock wave lithotripsy (ESWL) is a noninvasive technique for the removal of kidney stones. Most ESWL is carried out when the stone is present near the renal pelvis. ESWL involves the use of a lithotriptor machine to deliver externally applied, focused, high-intensity pulses of ultrasonic energy to cause fragmentation of a stone over a period of around 30–60 minutes. Following its introduction in the United States in February 1984, ESWL was rapidly and widely accepted as a treatment alternative for renal and ureteral stones.[106] It is currently used in the treatment of uncomplicated stones located in the kidney and upper ureter, provided the aggregate stone burden (stone size and number) is less than 20 mm (0.8 in) and the anatomy of the involved kidney is normal.[107][108]
For a stone greater than 10 millimetres (0.39 in), ESWL may not help break the stone in one treatment; instead, two or three treatments may be needed. Some 80-85% of simple renal calculi can be effectively treated with ESWL.[7] A number of factors can influence its efficacy, including chemical composition of the stone, presence of anomalous renal anatomy and the specific location of the stone within the kidney, presence of hydronephrosis, body mass index, and distance of the stone from the surface of the skin.[106]
Common adverse effects of ESWL include acute trauma, such as bruising at the site of shock administration, and damage to blood vessels of the kidney.[109][110] In fact, the vast majority of people who are treated with a typical dose of shock waves using currently accepted treatment settings are likely to experience some degree of acute kidney injury.[106] ESWL-induced acute kidney injury is dose-dependent (increases with the total number of shock waves administered and with the power setting of the lithotriptor) and can be severe,[106] including internal bleeding and subcapsular hematomas. On rare occasions, such cases may require blood transfusion and even lead to acute kidney failure. Hematoma rates may be related to the type of lithotriptor used; hematoma rates of less than 1% and up to 13% have been reported for different lithotriptor machines.[110] Recent studies show reduced acute tissue injury when the treatment protocol includes a brief pause following the initiation of treatment, and both improved stone breakage and a reduction in injury when ESWL is carried out at slow shock wave rate.[106]
In addition to the aforementioned potential for acute kidney injury, animal studies suggest these acute injuries may progress to scar formation, resulting in loss of functional renal volume.[109][110] Recent prospective studies also indicate elderly people are at increased risk of developing new-onset hypertension following ESWL. In addition, a retrospective case-control study published by researchers from the Mayo Clinic in 2006 has found an increased risk of developing diabetes mellitus and hypertension in people who had undergone ESWL, compared with age and gender-matched people who had undergone nonsurgical treatment. Whether or not acute trauma progresses to long-term effects probably depends on multiple factors that include the shock wave dose (i.e., the number of shock waves delivered, rate of delivery, power setting, acoustic characteristics of the particular lithotriptor, and frequency of retreatment), as well as certain intrinsic predisposing pathophysiologic risk factors.[106]
To address these concerns, the American Urological Association established the Shock Wave Lithotripsy Task Force to provide an expert opinion on the safety and risk-benefit ratio of ESWL. The task force published a white paper outlining their conclusions in 2009. They concluded the risk-benefit ratio remains favorable for many people.[106] The advantages of ESWL include its noninvasive nature, the fact that it is technically easy to treat most upper urinary tract calculi, and that, at least acutely, it is a well-tolerated, low-morbidity treatment for the vast majority of people. However, they recommended slowing the shock wave firing rate from 120 pulses per minute to 60 pulses per minute to reduce the risk of renal injury and increase the degree of stone fragmentation.[106]
Alpha-blockers are sometimes prescribed after shock wave lithotripsy to help the pieces of the stone leave the person's body.[111] By relaxing muscles and helping to keep blood vessels open, alpha blockers may relax the ureter muscles to allow the kidney stone fragments to pass. When compared to usual care or placebo treatment, alpha blockers may lead to faster clearing of stones, a reduced need for extra treatment and fewer unwanted effects.[111] They may also clear kidney stones in more adults than the standard shock wave lithotripsy procedure. The unwanted effects associated with alpha blockers are hospital emergency visits and return to hospital for stone-related issues, but these effects were more common in adults who did not receive alpha-blockers as a part of their treatment.[111]
Most stones under 5 mm (0.2 in) pass spontaneously.[32][7] Prompt surgery may, nonetheless, be required in persons with only one working kidney, bilateral obstructing stones, a urinary tract infection and thus, it is presumed, an infected kidney, or intractable pain.[112] Beginning in the mid-1980s, less invasive treatments such as extracorporeal shock wave lithotripsy, ureteroscopy, and percutaneous nephrolithotomy began to replace open surgery as the modalities of choice for the surgical management of urolithiasis.[7]  More recently, flexible ureteroscopy has been adapted to facilitate retrograde nephrostomy creation for percutaneous nephrolithotomy.  This approach is still under investigation, though early results are favorable.[113] Percutaneous nephrolithotomy or, rarely, anatrophic nephrolithotomy, is the treatment of choice for large or complicated stones (such as calyceal staghorn calculi) or stones that cannot be extracted using less invasive procedures.[56][7]
Ureteroscopy has become increasingly popular as flexible and rigid fiberoptic ureteroscopes have become smaller. One ureteroscopic technique involves the placement of a ureteral stent (a small tube extending from the bladder, up the ureter and into the kidney) to provide immediate relief of an obstructed kidney. Stent placement can be useful for saving a kidney at risk for postrenal acute kidney failure due to the increased hydrostatic pressure, swelling and infection (pyelonephritis and pyonephrosis) caused by an obstructing stone. Ureteral stents vary in length from 24 to 30 cm (9.4 to 11.8 in) and most have a shape commonly referred to as a "double-J" or "double pigtail", because of the curl at both ends. They are designed to allow urine to flow past an obstruction in the ureter. They may be retained in the ureter for days to weeks as infections resolve and as stones are dissolved or fragmented by ESWL or by some other treatment. The stents dilate the ureters, which can facilitate instrumentation, and they also provide a clear landmark to aid in the visualization of the ureters and any associated stones on radiographic examinations. The presence of indwelling ureteral stents may cause minimal to moderate discomfort, frequency or urgency incontinence, and infection, which in general resolves on removal. Most ureteral stents can be removed cystoscopically during an office visit under topical anesthesia after resolution of urolithiasis.[114] Research is currently uncertain if placing a temporary stent during ureteroscopy leads to different outcomes than not placing a stent in terms of number of hospital visits for post operative problems, short or long term pain, need for narcotic pain medication, risk of UTI, need for a repeat procedure or narrowing of the ureter from scarring.[115]
More definitive ureteroscopic techniques for stone extraction (rather than simply bypassing the obstruction) include basket extraction and ultrasound ureterolithotripsy. Laser lithotripsy is another technique, which involves the use of a holmium:yttrium aluminium garnet (Ho:YAG) laser to fragment stones in the bladder, ureters, and kidneys.[116]
Ureteroscopic techniques are generally more effective than ESWL for treating stones located in the lower ureter, with success rates of 93–100% using Ho:YAG laser lithotripsy.[84] Although ESWL has been traditionally preferred by many practitioners for treating stones located in the upper ureter, more recent experience suggests ureteroscopic techniques offer distinct advantages in the treatment of upper ureteral stones. Specifically, the overall success rate is higher, fewer repeat interventions and postoperative visits are needed, and treatment costs are lower after ureteroscopic treatment when compared with ESWL. These advantages are especially apparent with stones greater than 10 mm (0.4 in) in diameter. However, because ureteroscopy of the upper ureter is much more challenging than ESWL, many urologists still prefer to use ESWL as a first-line treatment for stones of less than 10 mm, and ureteroscopy for those greater than 10 mm in diameter.[84] Ureteroscopy is the preferred treatment in pregnant and morbidly obese people, as well as those with bleeding disorders.[7]
Kidney stones affect all geographical, cultural, and racial groups. The lifetime risk is about 10-15% in the developed world, but can be as high as 20-25% in the Middle East. The increased risk of dehydration in hot climates, coupled with a diet 50% lower in calcium and 250% higher in oxalates compared to Western diets, accounts for the higher net risk in the Middle East.[118] In the Middle East, uric acid stones are more common than calcium-containing stones.[25] The number of deaths due to kidney stones is estimated at 19,000 per year being fairly consistent between 1990 and 2010.[119]
In North America and Europe, the annual number of new cases per year of kidney stones is roughly 0.5%. In the United States, the frequency in the population of urolithiasis has increased from 3.2% to 5.2% from the mid-1970s to the mid-1990s.[21] In the United States, about 9% of the population has had a kidney stone.[2]
The total cost for treating urolithiasis was US$2 billion in 2003.[58] About 65–80% of those with kidney stones are men; most stones in women are due to either metabolic defects (such as cystinuria) or infections in the case of struvite stones.[75][120][19] Urinary tract calculi disorders are more common in men than in women.  Men most commonly experience their first episode between 30 and 40 years of age, whereas for women, the age at first presentation is somewhat later.[75] The age of onset shows a bimodal distribution in women, with episodes peaking at 35 and 55 years.[58] Recurrence rates are estimated at 50% over a 10-year and 75% over 20-year period,[21] with some people experiencing ten or more episodes over the course of a lifetime.[75]
A 2010 review concluded that rates of disease are increasing.[117]
The existence of kidney stones was first recorded thousands of years ago, with various explanations given; Joseph Glanville's Saducismus Triumphatus, for example, gives a detailed description of Abraham Mechelburg's voiding of small stones through his penis' virga, attributing the issue to witchcraft.[121]
In 1901, a stone discovered in the pelvis of an ancient Egyptian mummy was dated to 4,800 BC.
Medical texts from ancient Mesopotamia, India, China, Persia, Greece, and Rome all mentioned calculous disease. Part of the Hippocratic Oath suggests there were practicing surgeons in ancient Greece to whom physicians were to defer for lithotomies, or the surgical removal of stones. The Roman medical treatise De Medicina by Aulus Cornelius Celsus contained a description of lithotomy,[122] and this work served as the basis for this procedure until the 18th century.[123]
Examples of people who had kidney stone disease include Napoleon I, Epicurus, Napoleon III, Peter the Great, Louis XIV, George IV, Oliver Cromwell, Lyndon B. Johnson, Benjamin Franklin, Michel de Montaigne, Francis Bacon, Isaac Newton, Samuel Pepys, William Harvey, Herman Boerhaave, and Antonio Scarpa.[124]
New techniques in lithotomy began to emerge starting in 1520, but the operation remained risky. After Henry Jacob Bigelow popularized the technique of litholapaxy in 1878,[125] the mortality rate dropped from about 24% to 2.4%. However, other treatment techniques continued to produce a high level of mortality, especially among inexperienced urologists.[123][124] In 1980, Dornier MedTech introduced extracorporeal shock wave lithotripsy for breaking up stones via acoustical pulses, and this technique has since come into widespread use.[106]
The term renal calculus is from the Latin rēnēs, meaning "kidneys", and calculus, meaning "pebble". Lithiasis (stone formation) in the kidneys is called nephrolithiasis (/ˌnɛfroʊlɪˈθaɪəsɪs/), from nephro-, meaning kidney, + -lith, meaning stone, and -iasis, meaning disorder. A distinction between nephrolithiasis and urolithiasis can be made because not all urinary stones (uroliths) form in the kidney; they can also form in the bladder. But the distinction is often clinically irrelevant (with similar disease process and treatment either way) and the words are thus often used loosely as synonyms.
Although kidney stones do not often occur in children, the incidence is increasing.[126] These stones are in the kidney in two thirds of reported cases, and in the ureter in the remaining cases. Older children are at greater risk independent of whether or not they are male or female.[127]
As with adults, most pediatric kidney stones are predominantly composed of calcium oxalate; struvite and calcium phosphate stones are less common. Calcium oxalate stones in children are associated with high amounts of calcium, oxalate, and magnesium in acidic urine.[128]
Treatment of kidney stones in children is similar to treatments for adults, including shock wave lithotripsy, medication, and treatment using scope through the bladder, kidney or skin.[129] Of these treatments, research is uncertain if shock waves are more effective than medication or a scope through the bladder, but it is likely less successful than a scope through skin into the kidney.[129] When going in with a scope through the kidney, a regular and a mini-sized scope likely have similar success rates of stone removal. Alpha-blockers, a type of medication, may increase the successful removal of kidney stones when compared with a placebo and without ibuprofen.[129]
Metabolic syndrome and its associated diseases of obesity and diabetes as general risk factors for kidney stone disease are under research  to determine if urinary excretion of calcium, oxalate and urate are higher than in people with normal weight or underweight, and if diet and physical activity have roles.[130][131] Dietary, fluid intake, and lifestyle factors remain major topics for research on prevention of kidney stones, as of 2017.[132]
The gut microbiota has been explored as a contributing factor for stone disease, indicating that some bacteria may be different in people forming kidney stones.[133] One bacterium, Oxalobacter formigenes, is potentially beneficial for mitigating calcium oxalate stones because of its ability to metabolize oxalate as its sole carbon source,[134] but 2018 research suggests that it is instead part of a network of oxalate degrading bacteria.[135] Additionally, one study found that oral antibiotic use, which alters the gut microbiota,[136] can increase the odds of a person developing a kidney stone.[137]
Among ruminants, uroliths more commonly cause problems in males than in females; the sigmoid flexure of the ruminant male urinary tract is more likely to obstruct passage. Early-castrated males are at greater risk, because of lesser urethral diameter.[138]
Low Ca:P intake ratio is conducive to phosphatic (e.g. struvite) urolith formation.[138]  Incidence among wether lambs can be minimized by maintaining a dietary Ca:P intake ratio of 2:1.[138][139]
Alkaline (higher) pH favors formation of carbonate and phosphate calculi. For domestic ruminants, dietary cation: anion balance is sometimes adjusted to assure a slightly acidic urine pH, for prevention of calculus formation.[138]
Differing generalizations regarding effects of pH on formation of silicate uroliths may be found.[138][140]  In this connection, it may be noted that under some circumstances, calcium carbonate accompanies silica in siliceous uroliths.[141]
Pelleted feeds may be conducive to formation of phosphate uroliths, because of increased urinary phosphorus excretion.  This is attributable to lower saliva production where pelleted rations containing finely ground constituents are fed.  With less blood phosphate partitioned into saliva, more tends to be excreted in urine.[142]  (Most saliva phosphate is fecally excreted.[143])
Oxalate uroliths can occur in ruminants, although such problems from oxalate ingestion may be relatively uncommon.  Ruminant urolithiasis associated with oxalate ingestion has been reported.[144]   However, no renal tubular damage or visible deposition of calcium oxalate crystals in kidneys was found in yearling wether sheep fed diets containing soluble oxalate at 6.5 percent of dietary dry matter for about 100 days.[145]
Conditions limiting water intake can result in stone formation.[146]
Various surgical interventions, e.g.  amputation of the urethral process at its base near the glans penis in male ruminants, perineal urethrostomy, or tube cystostomy may be considered for relief of obstructive urolithiasis.[146]


Cystic fibrosis (CF) is a rare[6][7] genetic disorder that affects mostly the lungs, but also the pancreas, liver, kidneys, and intestine.[1][8] The hallmark feature of CF is the accumulation of thick mucus in different organs. Long-term issues include difficulty breathing and coughing up mucus as a result of frequent lung infections.[1] Other signs and symptoms may include sinus infections, poor growth, fatty stool, clubbing of the fingers and toes, and infertility in most males.[1] Different people may have different degrees of symptoms.[1]
Cystic fibrosis is inherited in an autosomal recessive manner.[1] It is caused by the presence of mutations in both copies (alleles) of the gene encoding the cystic fibrosis transmembrane conductance regulator (CFTR) protein.[1] Those with a single working copy are carriers and otherwise mostly healthy.[3] CFTR is involved in the production of sweat, digestive fluids, and mucus.[9] When the CFTR is not functional, secretions that are usually thin instead become thick.[10] The condition is diagnosed by a sweat test and genetic testing.[1] The sweat test measures sodium concentration, as people with cystic fibrosis have abnormally salty sweat, which can often be tasted by parents kissing their children. Screening of infants at birth takes place in some areas of the world.[1]
There is no known cure for cystic fibrosis.[3] Lung infections are treated with antibiotics which may be given intravenously, inhaled, or by mouth.[1] Sometimes, the antibiotic azithromycin is used long-term.[1] Inhaled hypertonic saline and salbutamol may also be useful.[1] Lung transplantation may be an option if lung function continues to worsen.[1] Pancreatic enzyme replacement and fat-soluble vitamin supplementation are important, especially in the young.[1] Airway clearance techniques such as chest physiotherapy may have some short-term benefit, but long-term effects are unclear.[11] The average life expectancy is between 42 and 50 years in the developed world,[5][12] with a median of 40.7 years.[13] Lung problems are responsible for death in 80% of people with cystic fibrosis.[1]
CF is most common among people of Northern European ancestry, for whom it affects about 1 out of 3,000 newborns,[1] and among which around 1 out of 25 people is a carrier.[3] It is least common in Africans and Asians, though it does occur in all races.[1] It was first recognized as a specific disease by Dorothy Andersen in 1938, with descriptions that fit the condition occurring at least as far back as 1595.[8] The name "cystic fibrosis" refers to the characteristic fibrosis and cysts that form within the pancreas.[8][14]
Cystic fibrosis typically manifests early in life. Newborns and infants with cystic fibrosis tend to have frequent, large, greasy stools (a result of malabsorption) and are underweight for their age.[15] 15–20% of newborns have their small intestine blocked by meconium, often requiring surgery to correct.[15] Newborns occasionally have neonatal jaundice due to blockage of the bile ducts.[15] Children with cystic fibrosis lose excessive salt in their sweat, and parents often notice salt crystallizing on the skin, or a salty taste when they kiss their child.[15]
The primary cause of morbidity and death in people with cystic fibrosis is progressive lung disease, which eventually leads to respiratory failure.[16] This typically begins as a prolonged respiratory infection that continues until treated with antibiotics.[16] Chronic infection of the respiratory tract is nearly universal in people with cystic fibrosis, with Pseudomonas aeruginosa, fungi, and mycobacteria all increasingly common over time.[17] Inflammation of the upper airway results in frequent runny nose and nasal obstruction. Nasal polyps are common, particularly in children and teenagers.[16] As the disease progresses, people tend to have shortness of breath, and a chronic cough that produces sputum.[16] Breathing problems make it increasingly challenging to exercise, and prolonged illness causes those affected to be underweight for their age.[16] In late adolescence or adulthood, people begin to develop severe signs of lung disease: wheezing, digital clubbing, cyanosis, coughing up blood, pulmonary heart disease, and collapsed lung (atelectasis or pneumothorax).[16]
In rare cases, cystic fibrosis can manifest itself as a coagulation disorder. Vitamin K is normally absorbed from breast milk, formula, and later, solid foods. This absorption is impaired in some CF patients. Young children are especially sensitive to vitamin K malabsorptive disorders because only a very small amount of vitamin K crosses the placenta, leaving the child with very low reserves and limited ability to absorb vitamin K from dietary sources after birth. Because clotting factors II, VII, IX, and X are vitamin K–dependent, low levels of vitamin K can result in coagulation problems. Consequently, when a child presents with unexplained bruising, a coagulation evaluation may be warranted to determine whether an underlying disease is present.[18]
Lung disease results from clogging of the airways due to mucus build-up, decreased mucociliary clearance, and resulting inflammation.[19][20] In later stages, changes in the architecture of the lung, such as pathology in the major airways (bronchiectasis), further exacerbate difficulties in breathing. Other signs include high blood pressure in the lung (pulmonary hypertension), heart failure, difficulties getting enough oxygen to the body (hypoxia), and respiratory failure requiring support with breathing masks, such as bilevel positive airway pressure machines or ventilators.[21] Staphylococcus aureus, Haemophilus influenzae, and Pseudomonas aeruginosa are the three most common organisms causing lung infections in CF patients.[20]: 1254  In addition, opportunistic infection due to Burkholderia cepacia complex can occur, especially through transmission from patient to patient.[22]
In addition to typical bacterial infections, people with CF more commonly develop other types of lung diseases. Among these is allergic bronchopulmonary aspergillosis, in which the body's response to the common fungus Aspergillus fumigatus causes worsening of breathing problems. Another is infection with Mycobacterium avium complex, a group of bacteria related to tuberculosis, which can cause lung damage and do not respond to common antibiotics.[23]
Mucus in the paranasal sinuses is equally thick and may also cause blockage of the sinus passages, leading to infection. This may cause facial pain, fever, nasal drainage, and headaches. Individuals with CF may develop overgrowth of the nasal tissue (nasal polyps) due to inflammation from chronic sinus infections.[24] Recurrent sinonasal polyps can occur in 10% to 25% of CF patients.[20]: 1254  These polyps can block the nasal passages and increase breathing difficulties.[25][26]
Cardiorespiratory complications are the most common causes of death (about 80%) in patients at most CF centers in the United States.[20]: 1254 
Digestive problems are also prevalent in individuals with CF. Approximately 15%-20% of newborns diagnosed with CF experience intestinal blockage (meconium ileus), and other digestive issues may arise due to mucus accumulation in the pancreas.[27] Consequently, there is impaired insulin production, leading to cystic fibrosis-related diabetes mellitus. Moreover, enzyme transport disruption from the pancreas to the intestines results in digestive problems such as recurrent diarrhea or weight loss.[28]
The thick mucus seen in the lungs has a counterpart in thickened secretions from the pancreas, an organ responsible for providing digestive juices that help break down food. These secretions block the exocrine movement of the digestive enzymes into the duodenum and result in irreversible damage to the pancreas, often with painful inflammation (pancreatitis).[29] The pancreatic ducts are totally plugged in more advanced cases, usually seen in older children or adolescents.[20] This causes atrophy of the exocrine glands and progressive fibrosis.[20]
In addition, protrusion of internal rectal membranes (rectal prolapse) is more common, occurring in as many as 10% of children with CF,[20] and it is caused by increased fecal volume, malnutrition, and increased intra–abdominal pressure due to coughing.[30]
Individuals with CF also have difficulties absorbing the fat-soluble vitamins A, D, E, and K.[31]
In addition to the pancreas problems, people with CF experience more heartburn,[31] intestinal blockage by intussusception, and constipation.[32] Older individuals with CF may develop distal intestinal obstruction syndrome, which occurs when feces becomes thick with mucus (inspissated) and can cause bloating, pain, and incomplete or complete bowel obstruction.[33][31]
Exocrine pancreatic insufficiency occurs in the majority (85–90%) of patients with CF.[20]: 1253  It is mainly associated with "severe" CFTR mutations, where both alleles are completely nonfunctional (e.g. ΔF508/ΔF508).[20]: 1253  It occurs in 10–15% of patients with one "severe" and one "mild" CFTR mutation where little CFTR activity still occurs, or where two "mild" CFTR mutations exist.[20]: 1253  In these milder cases, sufficient pancreatic exocrine function is still present so that enzyme supplementation is not required.[20]: 1253  Usually, no other GI complications occur in pancreas-sufficient phenotypes, and in general, such individuals usually have excellent growth and development.[20]: 1254  Despite this, idiopathic chronic pancreatitis can occur in a subset of pancreas-sufficient individuals with CF, and is associated with recurrent abdominal pain and life-threatening complications.[20]
Liver diseases are another common complication in CF patients. The prevalence observed in studies ranged from 18% at age two to 41% at age 12, with no significant increase thereafter.[34] Another study found that males with CF are more prone to liver diseases compared to females, and those with meconium ileus have an increased risk of liver diseases.[35]
Thickened secretions also may cause liver problems in patients with CF. Bile secreted by the liver to aid in digestion may block the bile ducts, leading to liver damage. Impaired digestion or absorption of lipids can result in steatorrhea. Over time, this can lead to scarring and nodularity (cirrhosis). The liver fails to rid the blood of toxins and does not make important proteins, such as those responsible for blood clotting.[36][37] Liver disease is the third-most common cause of death associated with CF.[20]
Around 5–7% of people experience liver damage severe enough to cause symptoms: typically gallstones causing biliary colic.[38]
The pancreas contains the islets of Langerhans, which are responsible for making insulin, a hormone that helps regulate blood glucose. Damage to the pancreas can lead to loss of the islet cells, leading to a type of diabetes unique to those with the disease.[39] This cystic fibrosis-related diabetes shares characteristics of type 1 and type 2 diabetes, and is one of the principal nonpulmonary complications of CF.[40]
Vitamin D is involved in calcium and phosphate regulation. Poor uptake of vitamin D from the diet because of malabsorption can lead to the bone disease osteoporosis in which weakened bones are more susceptible to fractures.[41]
Infertility affects both men and women. At least 97% of men with cystic fibrosis are infertile, but not sterile, and can have children with assisted reproductive techniques.[42] The main cause of infertility in men with cystic fibrosis is congenital absence of the vas deferens (which normally connects the testes to the ejaculatory ducts of the penis), but potentially also by other mechanisms causing no sperm, abnormally shaped sperm, and few sperm with poor motility.[43] Many men found to have congenital absence of the vas deferens during evaluation for infertility have a mild, previously undiagnosed form of CF.[44] While females with CF are generally fertile, around 20% of women with CF have fertility difficulties due to thickened cervical mucus or malnutrition. In severe cases, malnutrition disrupts ovulation and causes a lack of menstruation.[45]
CF is caused by having no functional copies (alleles) of the gene cystic fibrosis transmembrane conductance regulator (CFTR). As of 2018, over 1,900 mutations leading to CF have been described, but only 5 of them have a frequency greater than 1% among patients. The most common mutant allele, ΔF508, is a deletion (Δ signifying deletion) of three nucleotides that results in a loss of the amino-acid residue phenylalanine (F) at the 508th position of the protein.[46][47] This mutant allele is already present in 1 in 20 to 25 people of Northern European ancestry; it accounts for 70% of CF cases worldwide and 90% of cases in the United States; however, over 700 other mutant alleles, some of which represent new mutations, can produce CF.[48] Although most people have two working copies (alleles) of the CFTR gene, only one is needed to prevent cystic fibrosis. CF develops when neither allele can produce a functional CFTR protein. Thus, CF is considered an autosomal recessive disease.[49]
The CFTR gene, found at the q31.2 locus of chromosome 7, is 230,000 base pairs long, and encodes a protein that is 1,480 amino acids long. More specifically, the location is between base pair 117,120,016 and 117,308,718 on the long arm of chromosome 7, region 3, band 1, subband 2, represented as 7q31.2. Structurally, the CFTR is a type of gene known as an ABC gene. The product of this gene (the CFTR protein) is a chloride ion channel important in creating sweat, digestive juices, and mucus. This protein possesses two ATP-hydrolyzing domains, which allows the protein to use energy in the form of ATP. It also contains two domains comprising six alpha helices apiece, which allow the protein to cross the cell membrane. A regulatory binding site on the protein allows activation by phosphorylation, mainly by cAMP-dependent protein kinase.[21] The carboxyl terminal of the protein is anchored to the cytoskeleton by a PDZ domain interaction.[50] The majority of CFTR in lung passages is produced by rare ion-transporting cells that regulate mucus properties.[51]
In addition, the evidence is increasing that genetic modifiers besides CFTR modulate the frequency and severity of the disease. One example is mannan-binding lectin, which is involved in innate immunity by facilitating phagocytosis of microorganisms. Polymorphisms in one or both mannan-binding lectin alleles that result in lower circulating levels of the protein are associated with a threefold higher risk of end-stage lung disease, as well as an increased burden of chronic bacterial infections.[20]
Up to one in 25 individuals of Northern European ancestry is considered a genetic carrier.[52] The disease appears only when two of these carriers have children, as each pregnancy between them has a 25% chance of producing a child with the disease. Although only about one of every 3,000 newborns of the affected ancestry has CF, since the CFTR gene's discovery in 1989, over 2,000 variants have been identified, but only about 700 of these have been recognized as responsible for causing CF.[53] Current tests look for the most common mutations.[52]
The mutant alleles screened by the test vary according to a person's ethnic group or by the occurrence of CF already in the family. More than 10 million Americans, including one in 25 white Americans, are carriers of one mutant allele of the CF gene. CF is present in other races, though not as frequently as in white individuals. About one in 46 Hispanic Americans, one in 65 African Americans, and one in 90 Asian Americans carry a mutation of the CF gene.[52]
The CFTR gene regulates the transport of salts and water through cell membranes, providing instructions for creating a pathway that allows the passage of chloride ions.[54] A mutation in the CFTR gene can impair the normal function of chloride channels, leading to abnormal transport of chloride ions and water, resulting in the formation of thick and abnormal mucus.[55]
Several mutations in the CFTR gene can occur, and different mutations cause different defects in the CFTR protein, sometimes causing a milder or more severe disease. These protein defects are also targets for drugs which can sometimes restore their function. ΔF508-CFTR gene mutation, which occurs in >90% of patients in the U.S., creates a protein that does not fold normally and is not appropriately transported to the cell membrane, resulting in its degradation.[56]
Other mutations result in proteins that are too short (truncated) because production is ended prematurely. Other mutations produce proteins that do not use energy (in the form of ATP) normally, do not allow chloride, iodide, and thiocyanate to cross the membrane appropriately,[57] and degrade at a faster rate than normal. Mutations may also lead to fewer copies of the CFTR protein being produced.[21]
The protein created by this gene is anchored to the outer membrane of cells in the sweat glands, lungs, pancreas, and all other remaining exocrine glands in the body.
The protein spans this membrane and acts as a channel connecting the inner part of the cell (cytoplasm) to the surrounding fluid. This channel is primarily responsible for controlling the movement of halide anions from inside to outside of the cell; however, in the sweat ducts, it facilitates the movement of chloride from the sweat duct into the cytoplasm. When the CFTR protein does not resorb ions in sweat ducts, chloride and thiocyanate[58] released from sweat glands are trapped inside the ducts and pumped to the skin.
Additionally hypothiocyanite, OSCN, cannot be produced by the immune defense system.[59][60] Because chloride is negatively charged, this modifies the electrical potential inside and outside the cell that normally causes cations to cross into the cell. Sodium is the most common cation in the extracellular space. The excess chloride within sweat ducts prevents sodium resorption by epithelial sodium channels and the combination of sodium and chloride creates the salt, which is lost in high amounts in the sweat of individuals with CF. This lost salt forms the basis for the sweat test.[21]
Most of the damage in CF is due to blockage of the narrow passages of affected organs with thickened secretions. These blockages lead to remodeling and infection in the lung, damage by accumulated digestive enzymes in the pancreas, blockage of the intestines by thick feces, etc. Several theories have been posited on how the defects in the protein and cellular function cause the clinical effects. The most current theory suggests that defective ion transport leads to dehydration in the airway epithelia, thickening mucus.[61] In airway epithelial cells, the cilia exist in between the cell's apical surface and mucus in a layer known as airway surface liquid (ASL). The flow of ions from the cell and into this layer is determined by ion channels such as CFTR. CFTR not only allows chloride ions to be drawn from the cell and into the ASL, but it also regulates another channel called ENac, which allows sodium ions to leave the ASL and enter the respiratory epithelium. CFTR normally inhibits this channel, but if the CFTR is defective, then sodium flows freely from the ASL and into the cell.[citation needed]
As water follows sodium, the depth of ASL will be depleted and the cilia will be left in the mucous layer.[62] As cilia cannot effectively move in a thick, viscous environment, mucociliary clearance is deficient and a buildup of mucus occurs, clogging small airways.[63] The accumulation of more viscous, nutrient-rich mucus in the lungs allows bacteria to hide from the body's immune system, causing repeated respiratory infections. The presence of the same CFTR proteins in the pancreatic duct and sweat glands in the skin also cause symptoms in these systems.[citation needed]
The lungs of individuals with cystic fibrosis are colonized and infected by bacteria from an early age. These bacteria, which often spread among individuals with CF, thrive in the altered mucus, which collects in the small airways of the lungs. This mucus leads to the formation of bacterial microenvironments known as biofilms that are difficult for immune cells and antibiotics to penetrate. Viscous secretions and persistent respiratory infections repeatedly damage the lung by gradually remodeling the airways, which makes infection even more difficult to eradicate.[64] The natural history of CF lung infections and airway remodeling is poorly understood, largely due to the immense spatial and temporal heterogeneity both within and between the microbiomes of CF patients.[65]
Over time, both the types of bacteria and their individual characteristics change in individuals with CF. In the initial stage, common bacteria such as S. aureus and H. influenzae colonize and infect the lungs.[20] Eventually, Pseudomonas aeruginosa (and sometimes Burkholderia cepacia) dominates. By 18 years of age, 80% of patients with classic CF harbor P. aeruginosa, and 3.5% harbor B. cepacia.[20] Once within the lungs, these bacteria adapt to the environment and develop resistance to commonly used antibiotics. Pseudomonas can develop special characteristics that allow the formation of large colonies, known as "mucoid" Pseudomonas, which are rarely seen in people who do not have CF.[64] Scientific evidence suggests the interleukin 17 pathway plays a key role in resistance and modulation of the inflammatory response during P. aeruginosa infection in CF.[66] In particular, interleukin 17-mediated immunity plays a double-edged activity during chronic airways infection; on one side, it contributes to the control of P. aeruginosa burden, while on the other, it propagates exacerbated pulmonary neutrophilia and tissue remodeling.[66]
Infection can spread by passing between different individuals with CF.[67] In the past, people with CF often participated in summer "CF camps" and other recreational gatherings.[68][69] Hospitals grouped patients with CF into common areas and routine equipment (such as nebulizers)[70] was not sterilized between individual patients.[71] This led to transmission of more dangerous strains of bacteria among groups of patients. As a result, individuals with CF are now routinely isolated from one another in the healthcare setting, and healthcare providers are encouraged to wear gowns and gloves when examining patients with CF to limit the spread of virulent bacterial strains.[72]
CF patients may also have their airways chronically colonized by filamentous fungi (such as Aspergillus fumigatus, Scedosporium apiospermum, Aspergillus terreus) and/or yeasts (such as Candida albicans); other filamentous fungi less commonly isolated include Aspergillus flavus and Aspergillus nidulans (occur transiently in CF respiratory secretions) and Exophiala dermatitidis and Scedosporium prolificans (chronic airway-colonizers); some filamentous fungi such as Penicillium emersonii and Acrophialophora fusispora are encountered in patients almost exclusively in the context of CF.[73] Defective mucociliary clearance characterizing CF is associated with local immunological disorders. In addition, the prolonged therapy with antibiotics and the use of corticosteroid treatments may also facilitate fungal growth. Although the clinical relevance of the fungal airway colonization is still a matter of debate, filamentous fungi may contribute to the local inflammatory response and therefore to the progressive deterioration of the lung function, as often happens with allergic bronchopulmonary aspergillosis – the most common fungal disease in the context of CF, involving a Th2-driven immune response to Aspergillus species.[73][74]
Diagnosis of CF is initially based on clinical findings indicative of respiratory diseases, various digestive problems, meconium ileus, and more. Definitive diagnosis may involve genetic testing based on family history or chloride concentration testing in sweat, which is relatively high (>60mEq/L) in individuals with CF.
In many localities all newborns are screened for cystic fibrosis within the first few days of life, typically by blood test for high levels of immunoreactive trypsinogen.[75] Newborns with positive tests or those who are otherwise suspected of having cystic fibrosis based on symptoms or family history, then undergo a sweat test. An electric current is used to drive pilocarpine into the skin, stimulating sweating. The sweat is collected and analyzed for salt levels. Having unusually high levels of chloride in the sweat suggests CFTR is dysfunctional; the person is then diagnosed with cystic fibrosis.[76][note 1] Genetic testing is also available to identify the CFTR mutations typically associated with cystic fibrosis. Many laboratories can test for the 30–96 most common CFTR mutations, which can identify over 90% of people with cystic fibrosis.[76]
People with CF have less thiocyanate and hypothiocyanite in their saliva[78] and mucus (Banfi et al.). In the case of milder forms of CF, transepithelial potential difference measurements can be helpful. CF can also be diagnosed by identification of mutations in the CFTR gene.[79]
In many cases, a parent makes the diagnosis because the infant tastes salty.[20] Immunoreactive trypsinogen levels can be increased in individuals who have a single mutated copy of the CFTR gene (carriers) or, in rare instances, in individuals with two normal copies of the CFTR gene. Due to these false positives, CF screening in newborns can be controversial.[80][81]
By 2010 every US state had instituted newborn screening programs[82] and as of 2016[update] 21 European countries had programs in at least some regions.[83]
Women who are pregnant or couples planning a pregnancy can have themselves tested for the CFTR gene mutations to determine the risk that their child will be born with CF. Testing is typically performed first on one or both parents and, if the risk of CF is high, testing on the fetus is performed. The American College of Obstetricians and Gynecologists recommends all people thinking of becoming pregnant be tested to see if they are a carrier.[84]
Because development of CF in the fetus requires each parent to pass on a mutated copy of the CFTR gene and because CF testing is expensive, testing is often performed initially on one parent. If testing shows that parent is a CFTR gene mutation carrier, the other parent is tested to calculate the risk that their children will have CF. CF can result from more than a thousand different mutations.[49] As of 2016[update], typically only the most common mutations are tested for, such as ΔF508.[49] Most commercially available tests look for 32 or fewer different mutations. If a family has a known uncommon mutation, specific screening for that mutation can be performed. Because not all known mutations are found on current tests, a negative screen does not guarantee that a child will not have CF.[85]
During pregnancy, testing can be performed on the placenta (chorionic villus sampling) or the fluid around the fetus (amniocentesis). However, chorionic villus sampling has a risk of fetal death of one in 100 and amniocentesis of one in 200;[86] a recent study has indicated this may be much lower, about one in 1,600.[87]
Economically, for carrier couples of cystic fibrosis, when comparing preimplantation genetic diagnosis (PGD) with natural conception (NC) followed by prenatal testing and abortion of affected pregnancies, PGD provides net economic benefits up to a maternal age around 40 years, after which NC, prenatal testing, and abortion have higher economic benefit.[88]
Treatment for CF is diverse, tailored to different symptoms, and includes various devices, inhalation medications to alleviate respiratory difficulties, oral enzyme supplements to address exocrine pancreatic insufficiency, and, in some cases, surgical interventions for conditions such as meconium ileus.[89] While treatment alleviates symptoms and prevents potential complications, there is currently no cure for the disease. 
The management of CF has improved significantly over the past 70 years. While infants born with it 70 years ago would have been unlikely to live beyond their first year, infants today are likely to live well into adulthood. Advances in the treatment of cystic fibrosis have meant that people with cystic fibrosis can live a fuller life less encumbered by their condition. The cornerstones of management are the proactive treatment of airway infection, encouragement of good nutrition, and an active lifestyle. Pulmonary rehabilitation as a management of CF continues throughout a person's life, and is aimed at maximizing organ function, and therefore the quality of life.[90] Occupational therapists use energy conservation techniques in the rehabilitation process for patients with cystic fibrosis.[91] Examples of energy conservation techniques are ergonomic principles, pursed lip breathing, and diaphragmatic breathing.[92] People with CF tend to have fatigue and dyspnoea due to chronic pulmonary infections, so reducing the amount of energy spent during activities can help people feel better and gain more independence.[91] At best, current treatments delay the decline in organ function.[citation needed] Because of the wide variation in disease symptoms, treatment typically occurs at specialist multidisciplinary centers and is tailored to the individual. Targets for therapy are the lungs, gastrointestinal tract (including pancreatic enzyme supplements), the reproductive organs (including assisted reproductive technology), and psychological support.[93]
The most consistent aspect of therapy in CF is limiting and treating the lung damage caused by thick mucus and infection, with the goal of maintaining quality of life. Intravenous, inhaled, and oral antibiotics are used to treat chronic and acute infections. Mechanical devices and inhalation medications are used to alter and clear the thickened mucus. These therapies, while effective, can be extremely time-consuming. Oxygen therapy at home is recommended in those with significant low oxygen levels.[94] Many people with CF use probiotics, which are thought to be able to correct intestinal dysbiosis and inflammation, but the clinical trial evidence regarding the effectiveness of probiotics for reducing pulmonary exacerbations in people with CF is uncertain.[95]
Many people with CF are on one or more antibiotics at all times, even when healthy, to prophylactically suppress infection.[citation needed] Antibiotics are absolutely necessary whenever pneumonia is suspected or a noticeable decline in lung function is seen, and are usually chosen based on the results of a sputum analysis and the person's past response.[citation needed] This prolonged therapy often necessitates hospitalization and insertion of a more permanent IV such as a peripherally inserted central catheter or Port-a-Cath. Inhaled therapy with antibiotics such as tobramycin, colistin, and aztreonam is often given for months at a time to improve lung function by impeding the growth of colonized bacteria.[96][97][98] Inhaled antibiotic therapy helps lung function by fighting infection, but also has significant drawbacks such as development of antibiotic resistance, tinnitus, and changes in the voice.[99] Inhaled levofloxacin may be used to treat Pseudomonas aeruginosa in people with cystic fibrosis who are infected.[100]
Antibiotics by mouth such as ciprofloxacin or azithromycin are given to help prevent infection or to control ongoing infection.[101] The aminoglycoside antibiotics (e.g. tobramycin) used can cause hearing loss, damage to the balance system in the inner ear or kidney failure with long-term use.[102] To prevent these side-effects, the amount of antibiotics in the blood is routinely measured and adjusted accordingly.[103]
Currently, no reliable clinical trial evidence shows the effectiveness of antibiotics for pulmonary exacerbations in people with cystic fibrosis and Burkholderia cepacia complex[104] or for the use of antibiotics to treat nontuberculous mycobacteria in people with CF.[105]
The early management of Pseudomonas aeruginosa infection is usually suggested using nebulised antibiotics with or without oral antibiotics to remove the bacteria from the person's airways for a period of time.[106] When choosing antibiotics to treat lung infections caused by Pseudomonas aeruginosa in people with cystic fibrosis, it is still unclear whether the choice of antibiotics should be based on the results of testing antibiotics separately (one at a time) or in combination with each other.[107] It is also not clear if these treatment approaches for the Pseudomonas aeruginosa infection improve the person's quality of life or lifespan.[106] The negative side effects of antibiotics for this infection are also not well studied.[106] Intravenous antibiotic therapy to treat Pseudomonas aeruginosa infections has been shown not to be any better than antibiotics taken orally.[106]
Methicillin‐resistant Staphylococcus aureus (MRSA) infections can be dangerous for people with cystic fibrosis and can worsen lung damage leading to more rapid decline. Early treatment with antibiotics is standard; however, further research is needed to determine longer term effects and benefits (3–6 months after the treatment or longer) and survival rates associated with different treatment options.[108]
Factors related to the antibiotics use, the chronicity of the disease, and the emergence of resistant bacteria demand more exploration for different strategies such as antibiotic adjuvant therapy.[109] Antibiotic adjuvant therapy refers to therapeutic approaches that aim to improve the action of antibiotics such a pharmaceutical agents or supplements that impact the virulence of the bacterium or that change the susceptibility of the organism to the antibiotic so that the antibiotics are more effective.[109] There is no strong evidence to recommend specific antibiotic adjuvant therapies such as β‐carotene, nitric oxide, zinc supplements, or KB001‐A.[109]
Aerosolized medications that help loosen secretions include dornase alfa and hypertonic saline.[110] Dornase is a recombinant human deoxyribonuclease, which breaks down DNA in the sputum, thus decreasing its viscosity.[111] Dornase alpha improves lung function and probably decreases the risk of exacerbations but there is insufficient evidence to know if it is more or less effective than other similar medications.[112] Dornase alpha may improve lung function; however, there is no strong evidence that it is better than other hyperosmolar therapies.[112]
Denufosol, an investigational drug, opens an alternative chloride channel, helping to liquefy mucus.[113] Whether inhaled corticosteroids are useful is unclear, but stopping inhaled corticosteroid therapy is safe.[114] There is weak evidence that corticosteroid treatment may cause harm by interfering with growth.[114] Pneumococcal vaccination has not been studied as of 2014[update].[115] As of 2014[update], there is no clear evidence from randomized controlled trials that the influenza vaccine is beneficial for people with cystic fibrosis.[116]
Ivacaftor is a medication taken by mouth for the treatment of CF due to a number of specific mutations responsive to ivacaftor-induced CFTR protein enhancement.[117][118] It improves lung function by about 10%; however, as of 2014[update] it is expensive.[117] The first year it was on the market, the list price was over $300,000 per year in the United States.[117][needs update] In July 2015, the U.S. Food and Drug Administration approved lumacaftor/ivacaftor.[119] In 2018, the FDA approved the combination ivacaftor/tezacaftor; the manufacturer announced a list price of $292,000 per year.[120] Tezacaftor helps move the CFTR protein to the correct position on the cell surface, and is designed to treat people with the F508del mutation.[121]
In 2019, the combination drug elexacaftor/ivacaftor/tezacaftor marketed as Trikafta in the United States, was approved for CF patients over the age of 12.[122][123] In 2021, this was extended to include patients over the age of 6.[124] In Europe this drug was approved in 2020 and marketed as Kaftrio.[125]  It is used in those that have a f508del mutation, which occurs in about 90% of patients with cystic fibrosis.[122][126] According to the Cystic Fibrosis Foundation, "this medicine represents the single greatest therapeutic advancement in the history of CF, offering a treatment for the underlying cause of the disease that could eventually bring modulator therapy to 90 percent of people with CF."[127] In a clinical trial, participants who were administered the combination drug experienced a subsequent 63% decrease in pulmonary exacerbations and a 41.8 mmol/L decrease in sweat chloride concentration.[128] By mitigating a repertoire of symptoms associated with cystic fibrosis, the combination drug significantly improved quality-of-life metrics among patients with the disease as well.[128][127] The combination drug is also known to interact with CYP3A inducers,[129] such as carbamazepine used in the treatment of bipolar disorder, causing elexafaftor/ivacaftor/tezacaftor to circulate in the body at decreased concentrations. As such, concomitant use is not recommended.[130] The list price in the US is going to be $311,000 per year;[131] however, insurance may cover much of the cost of the drug.[132]
Ursodeoxycholic acid, a bile salt, has been used; however, there is insufficient data to show if it is effective.[133]
It is uncertain whether vitamin A or beta-carotene supplementation have any effect on eye and skin problems caused by vitamin A deficiency.[134]
There is no strong evidence that people with cystic fibrosis can prevent osteoporosis by increasing their intake of vitamin D.[135]
For people with vitamin E deficiency and cystic fibrosis, there is evidence that vitamin E supplementation may improve vitamin E levels, although it is still uncertain what effect supplementation has on vitamin E‐specific deficiency disorders or on lung function.[136]
Robust evidence regarding the effects of vitamin K supplementation in people with cystic fibrosis is lacking as of 2020.[137]
Various studies have examined the effects of omega-3 fatty acid supplementation for people with cystic fibrosis but the evidence is uncertain whether it has any benefits or adverse effects.[138]
Several mechanical techniques are used to dislodge sputum and encourage its expectoration. One technique good for short-term airway clearance is chest physiotherapy where a respiratory therapist percusses an individual's chest by hand several times a day, to loosen up secretions. This "percussive effect" can be administered also through specific devices that use chest wall oscillation or intrapulmonary percussive ventilator. Other methods such as biphasic cuirass ventilation, and associated clearance mode available in such devices, integrate a cough assistance phase, as well as a vibration phase for dislodging secretions. These are portable and adapted for home use.[11]
Another technique is positive expiratory pressure physiotherapy that consists of providing a back pressure to the airways during expiration. This effect is provided by devices that consists of a mask or a mouthpiece in which a resistance is applied only on the expiration phase.[139] Operating principles of this technique seems to be the increase of gas pressure behind mucus through collateral ventilation along with a temporary increase in functional residual capacity preventing the early collapse of small airways during exhalation.[140][141]
As lung disease worsens, mechanical breathing support may become necessary. Individuals with CF may need to wear special masks at night to help push air into their lungs. These machines, known as bilevel positive airway pressure (BiPAP) ventilators, help prevent low blood oxygen levels during sleep. Non-invasive ventilators may be used during physical therapy to improve sputum clearance.[142] It is not known if this type of therapy has an impact on pulmonary exacerbations or disease progression.[142] It is not known what role non-invasive ventilation therapy has for improving exercise capacity in people with cystic fibrosis.[142] However, the authors noted that "non‐invasive ventilation may be a useful adjunct to other airway clearance techniques, particularly in people with cystic fibrosis who have difficulty expectorating sputum."[143] During severe illness, a tube may be placed in the throat (a procedure known as a tracheostomy) to enable breathing supported by a ventilator.[144][145]
For children, preliminary studies show massage therapy may help people and their families' quality of life.[146]
Some lung infections require surgical removal of the infected part of the lung. If this is necessary many times, lung function is severely reduced.[147] The most effective treatment options for people with CF who have spontaneous or recurrent pneumothoraces is not clear.[148]
Lung transplantation may become necessary for individuals with CF as lung function and exercise tolerance decline. Although single lung transplantation is possible in other diseases, individuals with CF must have both lungs replaced because the remaining lung might contain bacteria that could infect the transplanted lung. A pancreatic or liver transplant may be performed at the same time to alleviate liver disease and/or diabetes.[149] Lung transplantation is considered when lung function declines to the point where assistance from mechanical devices is required or someone's survival is threatened.[150] According to Merck Manual, "bilateral lung transplantation for severe lung disease is becoming more routine and more successful with experience and improved techniques. Among adults with CF, median survival posttransplant is about 9 years."[151]
Newborns with intestinal obstruction typically require surgery, whereas adults with distal intestinal obstruction syndrome typically do not. Treatment of pancreatic insufficiency by replacement of missing digestive enzymes allows the duodenum to properly absorb nutrients and vitamins that would otherwise be lost in the feces. However, the best dosage and form of pancreatic enzyme replacement is unclear, as are the risks and long-term effectiveness of this treatment.[152]
So far, no large-scale research involving the incidence of atherosclerosis and coronary heart disease in adults with cystic fibrosis has been conducted. This is likely because the vast majority of people with cystic fibrosis do not live long enough to develop clinically significant atherosclerosis or coronary heart disease.[153]
Diabetes is the most common nonpulmonary complication of CF. It mixes features of type 1 and type 2 diabetes, and is recognized as a distinct entity, cystic fibrosis-related diabetes.[40][154] While oral antidiabetic drugs are sometimes used, the recommended treatment is the use of insulin injections or an insulin pump,[155] and, unlike in type 1 and 2 diabetes, dietary restrictions are not recommended.[40] While Stenotrophomonas maltophilia is relatively common in people with cystic fibrosis, the evidence about the effectiveness of antibiotics for S. maltophilia is uncertain.[156]
Bisphosphonates taken by mouth or intravenously can be used to improve the bone mineral density in people with cystic fibrosis, but there are no proof that this reduces fractures or increases survival rates.[157] When taking bisphosphates intravenously, adverse effects such as pain and flu-like symptoms can be an issue.[157] The adverse effects of bisphosphates taken by mouth on the gastrointestinal tract are not known.[157]
Poor growth may be avoided by insertion of a feeding tube for increasing food energy through supplemental feeds or by administration of injected growth hormone.[158]
Sinus infections are treated by prolonged courses of antibiotics. The development of nasal polyps or other chronic changes within the nasal passages may severely limit airflow through the nose, and over time reduce the person's sense of smell. Sinus surgery is often used to alleviate nasal obstruction and to limit further infections. Nasal steroids such as fluticasone propionate are used to decrease nasal inflammation.[159]
Female infertility may be overcome by assisted reproduction technology, particularly embryo transfer techniques. Male infertility caused by absence of the vas deferens may be overcome with testicular sperm extraction, collecting sperm cells directly from the testicles. If the collected sample contains too few sperm cells to likely have a spontaneous fertilization, intracytoplasmic sperm injection can be performed.[160] Third party reproduction is also a possibility for women with CF. Whether taking antioxidants affects outcomes is unclear.[161]
Physical exercise is usually part of outpatient care for people with cystic fibrosis.[162] Aerobic exercise seems to be beneficial for aerobic exercise capacity, lung function and health-related quality of life; however, the quality of the evidence was poor.[162]
Due to the use of aminoglycoside antibiotics, ototoxicity is common. Symptoms may include "tinnitus, hearing loss, hyperacusis, aural fullness, dizziness, and vertigo".[163]
Problems with the gastrointestinal system including constipation and obstruction of the gastrointestinal tract including distal intestinal obstruction syndrome are frequent complications for people with cystic fibrosis.[33] Treatment of gastrointestinal problems is required in order to prevent a complete obstruction, reduce other CF symptoms, and improve the quality of life.[33] While stool softeners, laxatives, and prokinetics (GI-focused treatments) are often suggested, there is no clear consensus from experts at to which approach is the best and comes with the least risks.[33] Mucolytics or systemic treatments aimed at dysfunctional CFTR are also sometimes suggested to improve symptoms.[164]
The prognosis for cystic fibrosis has improved due to earlier diagnosis through screening and better treatment and access to health care. In 1959, the median age of survival of children with CF in the United States was six months.[165]
In 2010, survival is estimated to be 37 years for women and 40 for men.[166] In Canada, median survival increased from 24 years in 1982 to 47.7 in 2007.[167] In the United States those born with CF in 2016 have a predicted life expectancy of 47.7 when cared for in specialty clinics.[168]
In the US, of those with CF who are more than 18 years old as of 2009, 92% had graduated from high school, 67% had at least some college education, 15% were disabled, 9% were unemployed, 56% were single, and 39% were married or living with a partner.[169]
Chronic illnesses can be difficult to manage. CF is a chronic illness that affects the "digestive and respiratory tracts resulting in generalized malnutrition and chronic respiratory infections".[170] The thick secretions clog the airways in the lungs, which often cause inflammation and severe lung infections.[171][172] If it is compromised, it affects the quality of life of someone with CF and their ability to complete such tasks as everyday chores.[citation needed]
According to Schmitz and Goldbeck (2006), CF significantly increases emotional stress on both the individual and the family, "and the necessary time-consuming daily treatment routine may have further negative effects on quality of life".[173] However, Havermans and colleagues (2006) have established that young outpatients with CF who have participated in the Cystic Fibrosis Questionnaire-Revised "rated some quality of life domains higher than did their parents".[174] Consequently, outpatients with CF have a more positive outlook for themselves. As Merck Manual notes, "with appropriate support, most patients can make an age-appropriate adjustment at home and school. Despite myriad problems, the educational, occupational, and marital successes of patients are impressive."[151]
Furthermore, there are many ways to enhance the quality of life in CF patients. Exercise is promoted to increase lung function. Integrating an exercise regimen into the CF patient's daily routine can significantly improve quality of life.[175] No definitive cure for CF is known, but diverse medications are used, such as mucolytics, bronchodilators, steroids, and antibiotics, that have the purpose of loosening mucus, expanding airways, decreasing inflammation, and fighting lung infections, respectively.[176]
Cystic fibrosis is the most common life-limiting autosomal recessive disease among people of European heritage.[178] In the United States, about 30,000 individuals have CF; most are diagnosed by six months of age. In Canada, about 4,000 people have CF.[179] Around 1 in 25 people of European descent, and one in 30 of white Americans,[180] is a carrier of a CF mutation. Although CF is less common in these groups, roughly one in 46 Hispanics, one in 65 Africans, and one in 90 Asians carry at least one abnormal CFTR gene.[181][182] Ireland has the world's highest prevalence of CF, at one in 1353.[183]
Although technically a rare disease, CF is ranked as one of the most widespread life-shortening genetic diseases. It is most common among nations in the Western world. An exception is Finland, where only one in 80 people carries a CF mutation.[184] The World Health Organization states, "In the European Union, one in 2000–3000 newborns is found to be affected by CF".[185] In the United States, one in 3,500 children is born with CF.[186] In 1997, about one in 3,300 white children in the United States was born with CF. In contrast, only one in 15,000 African American children have it, and in Asian Americans, the rate was even lower at one in 32,000.[187]
Cystic fibrosis is diagnosed equally in males and females. For reasons that remain unclear, data have shown that males tend to have a longer life expectancy than females,[188][189] though recent studies suggest this gender gap may no longer exist, perhaps due to improvements in health care facilities.[190][191] A recent study from Ireland identified a link between the female hormone estrogen and worse outcomes in CF.[192]
The distribution of CF alleles varies among populations. The frequency of ΔF508 carriers has been estimated at one in 200 in northern Sweden, one in 143 in Lithuanians, and one in 38 in Denmark. No ΔF508 carriers were found among 171 Finns and 151 Saami people.[193] ΔF508 does occur in Finland, but it is a minority allele there. CF is known to occur in only 20 families (pedigrees) in Finland.[194]
The ΔF508 mutation is estimated to have occurred up to 52,000 years ago.[195] Numerous hypotheses have been advanced as to why such a lethal allele has persisted and spread in the human population. Other common autosomal recessive diseases such as sickle-cell anemia have been found to protect carriers from other diseases, an evolutionary trade-off known as heterozygote advantage. Resistance to the following have all been proposed as possible sources of heterozygote advantage:
CF is supposed to have appeared about 3,000 BC because of migration of peoples, gene mutations, and new conditions in nourishment.[205] Although the entire clinical spectrum of CF was not recognized until the 1930s, certain aspects of CF were identified much earlier. Indeed, literature from Germany and Switzerland in the 18th century warned "Wehe dem Kind, das beim Kuß auf die Stirn salzig schmeckt, es ist verhext und muss bald sterben" ("Woe to the child who tastes salty from a kiss on the forehead, for he is bewitched and soon must die"), recognizing the association between the salt loss in CF and illness.[205]
In the 19th century, Carl von Rokitansky described a case of fetal death with meconium peritonitis, a complication of meconium ileus associated with CF. Meconium ileus was first described in 1905 by Karl Landsteiner.[205] In 1936, Guido Fanconi described a connection between celiac disease, cystic fibrosis of the pancreas, and bronchiectasis.[206]
In 1938, Dorothy Hansine Andersen published an article, "Cystic Fibrosis of the Pancreas and Its Relation to Celiac Disease: a Clinical and Pathological Study", in the American Journal of Diseases of Children. She was the first to describe the characteristic cystic fibrosis of the pancreas and to correlate it with the lung and intestinal disease prominent in CF.[14] She also first hypothesized that CF was a recessive disease and first used pancreatic enzyme replacement to treat affected children. In 1952, Paul di Sant'Agnese discovered abnormalities in sweat electrolytes; a sweat test was developed and improved over the next decade.[207]
The first linkage between CF and another marker (paraoxonase) was found in 1985 by Hans Eiberg, indicating that only one locus exists for CF.[208] In 1988, the first mutation for CF, ΔF508, was discovered by Francis Collins, Lap-Chee Tsui, and John R. Riordan on the seventh chromosome.[209] Subsequent research has found over 1,000 different mutations that cause CF.[210]
Because mutations in the CFTR gene are typically small, classical genetics techniques had been unable to accurately pinpoint the mutated gene.[211] Using protein markers, gene-linkage studies were able to map the mutation to chromosome 7. Chromosome walking and chromosome jumping techniques were then used to identify and sequence the gene.[211] In 1989, Lap-Chee Tsui led a team of researchers at the Hospital for Sick Children in Toronto that discovered the gene responsible for CF.[212] CF represents a classic example of how a human genetic disorder was elucidated strictly by the process of forward genetics.[211][213]
People with CF may be listed in a disease registry that allows researchers and doctors to track health results and identify candidates for clinical trials.[214]
Gene therapy has been explored as a potential cure for CF. Results from clinical trials have shown limited success as of 2016[update], and using gene therapy as routine therapy is not suggested.[215] A small study published in 2015 found a small benefit.[216]
The focus of much CF gene therapy research is aimed at trying to place a normal copy of the CFTR gene into affected cells. Transferring the normal CFTR gene into the affected epithelial cells would result in the production of functional CFTR protein in all target cells, without adverse reactions or an inflammation response. To prevent the lung manifestations of CF, only 5–10% the normal amount of CFTR gene expression is needed.[217] Multiple approaches have been tested for gene transfer, such as liposomes and viral vectors in animal models and clinical trials. However, both methods were found to be relatively inefficient treatment options,[218] mainly because very few cells take up the vector and express the gene, so the treatment has little effect. Additionally, problems have been noted in cDNA recombination, such that the gene introduced by the treatment is rendered unusable.[219] There has been a functional repair in culture of CFTR by CRISPR/Cas9 in intestinal stem cell organoids of cystic fibrosis patients.[220]
Phage therapy is being studied for multidrug resistant bacteria in people with CF.[221][222]
A number of small molecules that aim at compensating various mutations of the CFTR gene are under development. CFTR modulator therapies have been used in place of other types of genetic therapies. These therapies focus on the expression of a genetic mutation instead of the mutated gene itself. Modulators are split into two classes: potentiators and correctors. Potentiators act on the CFTR ion channels that are embedded in the cell membrane, and these types of drugs help open up the channel to allow transmembrane flow. Correctors are meant to assist in the transportation of nascent proteins, a protein that is formed by ribosomes before it is morphed into a specific shape, to the cell surface to be implemented into the cell membrane.[223]
Most target the transcription stage of genetic expression. One approach has been to try and develop medication that get the ribosome to overcome the stop codon and produce a full-length CFTR protein. About 10% of CF results from a premature stop codon in the DNA, leading to early termination of protein synthesis and truncated proteins. These drugs target nonsense mutations such as G542X, which consists of the amino acid glycine in position 542 being replaced by a stop codon. Aminoglycoside antibiotics interfere with protein synthesis and error-correction. In some cases, they can cause the cell to overcome a premature stop codon by inserting a random amino acid, thereby allowing expression of a full-length protein. Future research for these modulators is focused on the cellular targets that can be effected by a change in a gene's expression. Otherwise, genetic therapy will be used as a treatment when modulator therapies do not work given that 10% of people with cystic fibrosis are not affected by these drugs.[224]
Elexacaftor/ivacaftor/tezacaftor was approved in the United States in 2019 for cystic fibrosis.[225] This combination of previously developed medicines is able to treat up to 90% of people with cystic fibrosis.[223][225] This medications restores some effectiveness of the CFTR protein so that it can work as an ion channel on the cell's surface.[226]
It has previously been shown that inter-species interactions are an important contributor to the pathology of CF lung infections. Examples include the production of antibiotic degrading enzymes such as β-lactamases and the production of metabolic by-products such as short-chain fatty acids (SCFAs) by anaerobic species, which can enhance the pathogenicity of traditional pathogens such as Pseudomonas aeruginosa.[227] Due to this, it has been suggested that the direct alteration of CF microbial community composition and metabolic function would provide an alternative to traditional antibiotic therapies.[65]
Antisense therapy is being researched to treat a subset of mutations which have limited or no response to CFTR modulators.[228] Such mutations fall into two classes: splicing (e.g., c.3718-2477C>T) and nonsense (e.g., G542X, W1282X), both of which result in very low expression of CFTR protein, although the protein itself is usually unaffected. This is contrary to the more common mutations such as ΔF508 which have normal CFTR expression but in a non-functional form. Modulators serve only to correct these aberrant proteins and are of little to no benefit in the case of insufficient expression. Antisense oligonucleotides (ASOs) can solve this problem through the promotion of mRNA degradation or by changing pre-mRNA splicing, nonsense-mediated mRNA decay, or translation, thus increasing CFTR expression.
Arthritis is a term often used to mean any disorder that affects joints.[2] Symptoms generally include joint pain and stiffness.[2] Other symptoms may include redness, warmth, swelling, and decreased range of motion of the affected joints.[2][3] In some types of arthritis, other organs are also affected.[7] Onset can be gradual or sudden.[6]
There are over 100 types of arthritis.[10][5][6] The most common forms are osteoarthritis (degenerative joint disease) and rheumatoid arthritis.[7] Osteoarthritis usually occurs with age and affects the fingers, knees, and hips.[7][11] Rheumatoid arthritis is an autoimmune disorder that often affects the hands and feet.[7] Other types include gout, lupus, fibromyalgia, and septic arthritis.[7][12] They are all types of rheumatic disease.[2]
Treatment may include resting the joint and alternating between applying ice and heat.[7][13] Weight loss and exercise may also be useful.[7][14] Recommended medications may depend on the form of arthritis.[15][9] These may include pain medications such as ibuprofen and paracetamol (acetaminophen).[9] In some circumstances, a joint replacement may be useful.[7]
Osteoarthritis affects more than 3.8% of people, while rheumatoid arthritis affects about 0.24% of people.[16] Gout affects about 1–2% of the Western population at some point in their lives.[17] In Australia about 15% of people are affected by arthritis,[18] while in the United States more than 20% have a type of arthritis.[12][19] Overall the disease becomes more common with age.[12] Arthritis is a common reason that people miss work and can result in a decreased quality of life.[9] The term is derived from arthr- (meaning 'joint') and -itis (meaning 'inflammation').[20][21]
There are several diseases where joint pain is primary, and is considered the main feature. Generally when a person has "arthritis" it means that they have one of these diseases, which include:
Joint pain can also be a symptom of other diseases. In this case, the arthritis is considered to be secondary to the main disease; these include:

An undifferentiated arthritis is an arthritis that does not fit into well-known clinical disease categories, possibly being an early stage of a definite rheumatic disease.[51]
Pain, which can vary in severity, is a common symptom in virtually all types of arthritis.[53][54] Other symptoms include swelling, joint stiffness, redness, and aching around the joint(s).[2] Arthritic disorders like lupus and rheumatoid arthritis can affect other organs in the body, leading to a variety of symptoms.[12] Symptoms may include:[2]
It is common in advanced arthritis for significant secondary changes to occur. For example, arthritic symptoms might make it difficult for a person to move around and/or exercise, which can lead to secondary effects, such as:
These changes, in addition to the primary symptoms, can have a huge impact on quality of life.
Arthritis is the most common cause of disability in the United States. More than 20 million individuals with arthritis have severe limitations in function on a daily basis.[12] Absenteeism and frequent visits to the physician are common in individuals who have arthritis. Arthritis can make it difficult for individuals to be physically active and some become home bound.[citation needed]
It is estimated that the total cost of arthritis cases is close to $100 billion of which almost 50% is from lost earnings. Each year, arthritis results in nearly 1 million hospitalizations and close to 45 million outpatient visits to health care centers.[55]
Decreased mobility, in combination with the above symptoms, can make it difficult for an individual to remain physically active, contributing to an increased risk of obesity, high cholesterol or vulnerability to heart disease.[56] People with arthritis are also at increased risk of depression, which may be a response to numerous factors, including fear of worsening symptoms.[57]
There are common risk factors that increase a person's chance of developing arthritis later in adulthood. Some of these are modifiable while others are not.[58] Smoking has been linked to an increased susceptibility of developing arthritis, particularly rheumatoid arthritis.[59]
Diagnosis is made by clinical examination from an appropriate health professional, and may be supported by other tests such as radiology and blood tests, depending on the type of suspected arthritis.[62] All arthritides potentially feature pain. Pain patterns may differ depending on the arthritides and the location. Rheumatoid arthritis is generally worse in the morning and associated with stiffness lasting over 30 minutes.[63] However, in the early stages, patients may have no symptoms after a warm shower. Osteoarthritis, on the other hand, tends to be associated with morning stiffness which eases relatively quickly with movement and exercise. In the aged and children, pain might not be the main presenting feature; the aged patient simply moves less, the infantile patient refuses to use the affected limb.[citation needed]
Elements of the history of the disorder guide diagnosis. Important features are speed and time of onset, pattern of joint involvement, symmetry of symptoms, early morning stiffness, tenderness, gelling or locking with inactivity, aggravating and relieving factors, and other systemic symptoms. It may include checking joints, observing movements, examination of skin for rashes or nodules and symptoms of pulmonary inflammation. Physical examination may confirm the diagnosis or may indicate systemic disease. Radiographs are often used to follow progression or help assess severity.[citation needed]
Blood tests and X-rays of the affected joints often are performed to make the diagnosis. Screening blood tests are indicated if certain arthritides are suspected. These might include: rheumatoid factor, antinuclear factor (ANF), extractable nuclear antigen, and specific antibodies.[citation needed]
Rheumatoid arthritis patients often have high erythrocyte sedimentation rate (ESR, also known as sed rate) or C-reactive protein (CRP) levels, which indicates the presence of an inflammatory process in the body. Anti-cyclic citrullinated peptide (anti-CCP) antibodies and rheumatoid factor (RF) are two more common blood tests. Positive results indicate the risk of rheumatoid arthritis, while negative results help rule out this autoimmune condition. 

Imaging tests like X-rays, MRI scans or Ultrasounds used to diagnose and monitor arthritis. Other imaging tests for rheumatoid arthritis that may be considered include computed tomography (CT) scanning, positron emission tomography (PET) scanning, bone scanning, and dual-energy X-ray absorptiometry (DEXA).[64]
Osteoarthritis is the most common form of arthritis.[65] It affects humans and other animals, notably dogs, but also occurs in cats and horses. It can affect both the larger and the smaller joints of the body. In humans, this includes the hands, wrists, feet, back, hip, and knee. In dogs, this includes the elbow, hip, stifle (knee), shoulder, and back. The disease is essentially one acquired from daily wear and tear of the joint; however, osteoarthritis can also occur as a result of injury. Osteoarthritis begins in the cartilage and eventually causes the two opposing bones to erode into each other. The condition starts with minor pain during physical activity, but soon the pain can be continuous and even occur while in a state of rest. The pain can be debilitating and prevent one from doing some activities. In dogs, this pain can significantly affect quality of life and may include difficulty going up and down stairs, struggling to get up after lying down, trouble walking on slick floors, being unable to hop in and out of vehicles, difficulty jumping on and off furniture, and behavioral changes (e.g., aggression, difficulty squatting to toilet).[66] Osteoarthritis typically affects the weight-bearing joints, such as the back, knee and hip. Unlike rheumatoid arthritis, osteoarthritis is most commonly a disease of the elderly. The strongest predictor of osteoarthritis is increased age, likely due to the declining ability of chondrocytes to maintain the structural integrity of cartilage.[67] More than 30 percent of women have some degree of osteoarthritis by age 65. Other risk factors for osteoarthritis include prior joint trauma, obesity, and a sedentary lifestyle.[68]
Rheumatoid arthritis (RA) is a disorder in which the body's own immune system starts to attack body tissues.[70] The attack is not only directed at the joint but to many other parts of the body. In rheumatoid arthritis, most damage occurs to the joint lining and cartilage which eventually results in erosion of two opposing bones. RA often affects joints in the fingers, wrists, knees and elbows, is symmetrical (appears on both sides of the body), and can lead to severe deformity in a few years if not treated. RA occurs mostly in people aged 20 and above. In children, the disorder can present with a skin rash, fever, pain, disability, and limitations in daily activities.[71] With earlier diagnosis and aggressive treatment, many individuals can lead a better quality of life than if going undiagnosed for long after RA's onset.[72][73] The risk factors with the strongest association for developing rheumatoid arthritis are the female sex, a family history of rheumatoid arthritis, age, obesity, previous joint damage from an injury, and exposure to tobacco smoke.[74][75]
Bone erosion is a central feature of rheumatoid arthritis. Bone continuously undergoes remodeling by actions of bone resorbing osteoclasts and bone forming osteoblasts. One of the main triggers of bone erosion in the joints in rheumatoid arthritis is inflammation of the synovium, caused in part by the production of pro-inflammatory cytokines and receptor activator of nuclear factor kappa B ligand (RANKL), a cell surface protein present in Th17 cells and osteoblasts.[76] Osteoclast activity can be directly induced by osteoblasts through the RANK/RANKL mechanism.[77]
Lupus is a common collagen vascular disorder that can be present with severe arthritis. Other features of lupus include a skin rash, extreme photosensitivity, hair loss, kidney problems, lung fibrosis and constant joint pain.[78]
Gout is caused by deposition of uric acid crystals in the joints, causing inflammation. There is also an uncommon form of gouty arthritis caused by the formation of rhomboid crystals of calcium pyrophosphate known as pseudogout. In the early stages, the gouty arthritis usually occurs in one joint, but with time, it can occur in many joints and be quite crippling. The joints in gout can often become swollen and lose function. Gouty arthritis can become particularly painful and potentially debilitating when gout cannot successfully be treated.[79] When uric acid levels and gout symptoms cannot be controlled with standard gout medicines that decrease the production of uric acid (e.g., allopurinol) or increase uric acid elimination from the body through the kidneys (e.g., probenecid), this can be referred to as refractory chronic gout.[80]
Infectious arthritis is another severe form of arthritis. It presents with sudden onset of chills, fever and joint pain. The condition is caused by bacteria elsewhere in the body. Infectious arthritis must be rapidly diagnosed and treated promptly to prevent irreversible joint damage.[84]
Psoriasis can develop into psoriatic arthritis. With psoriatic arthritis, most individuals develop the skin problem first and then the arthritis. The typical features are continuous joint pains, stiffness and swelling. The disease does recur with periods of remission but there is no cure for the disorder. A small percentage develop a severely painful and destructive form of arthritis which destroys the small joints in the hands and can lead to permanent disability and loss of hand function.[85]
There is no known cure for arthritis and rheumatic diseases. Treatment options vary depending on the type of arthritis and include physical therapy, exercise and diet, orthopedic bracing, and oral and topical medications.[2][86] Joint replacement surgery may be required to repair damage, restore function, or relieve pain.[2]
In general, studies have shown that physical exercise of the affected joint can noticeably improve long-term pain relief. Furthermore, exercise of the arthritic joint is encouraged to maintain the health of the particular joint and the overall body of the person.[87]
Individuals with arthritis can benefit from both physical and occupational therapy. In arthritis the joints become stiff and the range of movement can be limited. Physical therapy has been shown to significantly improve function, decrease pain, and delay the need for surgical intervention in advanced cases.[88] Exercise prescribed by a physical therapist has been shown to be more effective than medications in treating osteoarthritis of the knee. Exercise often focuses on improving muscle strength, endurance and flexibility. In some cases, exercises may be designed to train balance. Occupational therapy can provide assistance with activities. Assistive technology is a tool used to aid a person's disability by reducing their physical barriers by improving the use of their damaged body part, typically after an amputation. Assistive technology devices can be customized to the patient or bought commercially.[89]
There are several types of medications that are used for the treatment of arthritis. Treatment typically begins with medications that have the fewest side effects with further medications being added if insufficiently effective.[90]
Depending on the type of arthritis, the medications that are given may be different. For example, the first-line treatment for osteoarthritis is acetaminophen (paracetamol) while for inflammatory arthritis it involves non-steroidal anti-inflammatory drugs (NSAIDs) like ibuprofen. Opioids and NSAIDs may be less well tolerated.[91] However, topical NSAIDs may have better safety profiles than oral NSAIDs. For more severe cases of osteoarthritis, intra-articular corticosteroid injections may also be considered.[92]
The drugs to treat rheumatoid arthritis (RA) range from corticosteroids to monoclonal antibodies given intravenously. Due to the autoimmune nature of RA, treatments may include not only pain medications and anti-inflammatory drugs, but also another category of drugs called disease-modifying antirheumatic drugs (DMARDs). csDMARDs, TNF biologics and tsDMARDs are specific kinds of DMARDs that are recommended for treatment.[93] Treatment with DMARDs is designed to slow down the progression of RA by initiating an adaptive immune response, in part by CD4+ T helper (Th) cells, specifically Th17 cells.[94] Th17 cells are present in higher quantities at the site of bone destruction in joints and produce inflammatory cytokines associated with inflammation, such as interleukin-17 (IL-17).[76]
A number of rheumasurgical interventions have been incorporated in the treatment of arthritis since the 1950s. Arthroscopic surgery for osteoarthritis of the knee provides no additional benefit to optimized physical and medical therapy.[95]
People with hand arthritis can have trouble with simple activities of daily living tasks (ADLs), such as turning a key in a lock or opening jars, as these activities can be cumbersome and painful. There are adaptive aids or assistive devices (ADs) available to help with these tasks,[96] but they are generally more costly than conventional products with the same function. It is now possible to 3-D print adaptive aids, which have been released as open source hardware to reduce patient costs.[97][98] Adaptive aids can significantly help arthritis patients and the vast majority of those with arthritis need and use them.[99]
Further research is required to determine if transcutaneous electrical nerve stimulation (TENS) for knee osteoarthritis is effective for controlling pain.[100]
Low level laser therapy may be considered for relief of pain and stiffness associated with arthritis.[101] Evidence of benefit is tentative.[102]
Pulsed electromagnetic field therapy (PEMFT) has tentative evidence supporting improved functioning but no evidence of improved pain in osteoarthritis.[103] The FDA has not approved PEMFT for the treatment of arthritis. In Canada, PEMF devices are legally licensed by Health Canada for the treatment of pain associated with arthritic conditions.[104]
Arthritis is predominantly a disease of the elderly, but children can also be affected by the disease.[105] Arthritis is more common in women than men at all ages and affects all races, ethnic groups and cultures. In the United States a CDC survey based on data from 2013 to 2015 showed 54.4 million (22.7%) adults had self-reported doctor-diagnosed arthritis, and 23.7 million (43.5% of those with arthritis) had arthritis-attributable activity limitation (AAAL). With an aging population, this number is expected to increase. Adults with co-morbid conditions, such as heart disease, diabetes, and obesity, were seen to have a higher than average prevalence of doctor-diagnosed arthritis (49.3%, 47.1%, and 30.6% respectively).[106]
Disability due to musculoskeletal disorders increased by 45% from 1990 to 2010. Of these, osteoarthritis is the fastest increasing major health condition.[107] Among the many reports on the increased prevalence of musculoskeletal conditions, data from Africa are lacking and underestimated. A systematic review assessed the prevalence of arthritis in Africa and included twenty population-based and seven hospital-based studies.[108] The majority of studies, twelve, were from South Africa. Nine studies were well-conducted, eleven studies were of moderate quality, and seven studies were conducted poorly. The results of the systematic review were as follows:[citation needed]
Evidence of osteoarthritis and potentially inflammatory arthritis has been discovered in dinosaurs.[109][110] The first known traces of human arthritis date back as far as 4500 BC. In early reports, arthritis was frequently referred to as the most common ailment of prehistoric peoples.[111] It was noted in skeletal remains of Native Americans found in Tennessee and parts of what is now Olathe, Kansas. Evidence of arthritis has been found throughout history, from Ötzi, a mummy (c. 3000 BC) found along the border of modern Italy and Austria, to the Egyptian mummies c. 2590 BC.[112]
In 1715, William Musgrave published the second edition of his most important medical work, De arthritide symptomatica, which concerned arthritis and its effects.[113] Augustin Jacob Landré-Beauvais, a 28-year-old resident physician at Salpêtrière Asylum in France was the first person to describe the symptoms of rheumatoid arthritis. Though Landré-Beauvais' classification of rheumatoid arthritis as a relative of gout was inaccurate, his dissertation encouraged others to further study the disease.[114]
The term is derived from arthr- (from Ancient Greek: ἄρθρον, romanized: árthron, lit. 'joint') and -itis (from -ῖτις, -îtis, lit. 'pertaining to'), the latter suffix having come to be associated with inflammation.
The word arthritides is the plural form of arthritis, and denotes the collective group of arthritis-like conditions.[115]

Gastroesophageal reflux disease (GERD) or gastro-oesophageal reflux disease (GORD)  is one of the upper gastrointestinal chronic diseases in which stomach content persistently and regularly flows up into the esophagus, resulting in symptoms and/or complications.[6][7][10] Symptoms include dental corrosion, dysphagia, heartburn, odynophagia, regurgitation, non-cardiac chest pain, extraesophageal symptoms such as chronic cough, hoarseness, reflux-induced laryngitis, or asthma.[10] In the long term, and when not treated, complications such as esophagitis, esophageal stricture, and Barrett's esophagus may arise.[6]
Risk factors include obesity, pregnancy, smoking, hiatal hernia, and taking certain medications. Medications that may cause or worsen the disease include benzodiazepines, calcium channel blockers, tricyclic antidepressants, NSAIDs, and certain asthma medicines. Acid reflux is due to poor closure of the lower esophageal sphincter, which is at the junction between the stomach and the esophagus. Diagnosis among those who do not improve with simpler measures may involve gastroscopy, upper GI series, esophageal pH monitoring, or esophageal manometry.[6]
Treatment options include lifestyle changes, medications, and sometimes surgery for those who do not improve with the first two measures. Lifestyle changes include not lying down for three hours after eating, lying down on the left side, raising the pillow or bedhead height, losing weight, and stopping smoking.[6][11] Foods that may precipitate GERD symptoms  include coffee, alcohol, chocolate, fatty foods, acidic foods, and spicy foods.[12] Medications include antacids, H2 receptor blockers, proton pump inhibitors, and prokinetics.[6][9]
In the Western world, between 10 and 20% of the population is affected by GERD.[9] It is highly prevalent in North America with 18% to 28% of the population suffering from the condition.[13] Occasional gastroesophageal reflux without troublesome symptoms or complications is even more common.[6] The classic symptoms of GERD were first described in 1925, when Friedenwald and Feldman commented on heartburn and its possible relationship to a hiatal hernia.[14] In 1934 gastroenterologist Asher Winkelstein described reflux and attributed the symptoms to stomach acid.[15]
The most common symptoms of GERD in adults are an acidic taste in the mouth, regurgitation, and heartburn.[16] Less common symptoms include pain with swallowing/sore throat, increased salivation (also known as water brash), nausea,[17] chest pain, coughing, and globus sensation.[18] The acid reflux can induce asthma attack symptoms like shortness of breath, cough, and wheezing in those with underlying asthma.[18]
GERD sometimes causes injury to the esophagus. These injuries may include one or more of the following:
GERD sometimes causes injury of the larynx (LPR).[21][22] Other complications can include aspiration pneumonia.[23]
GERD may be difficult to detect in infants and children since they cannot describe what they are feeling and indicators must be observed. Symptoms may vary from typical adult symptoms. GERD in children may cause repeated vomiting, effortless spitting up, coughing, and other respiratory problems, such as wheezing. Inconsolable crying, refusing food, crying for food and then pulling off the bottle or breast only to cry for it again, failure to gain adequate weight, bad breath, and burping are also common. Children may have one symptom or many; no single symptom is universal in all children with GERD.
Of the estimated 4 million babies born in the US each year, up to 35% of them may have difficulties with reflux in the first few months of their lives, known as 'spitting up'.[24] About 90% of infants will outgrow their reflux by their first birthday.[25]
Acid reflux into the mouth can cause breakdown of the enamel, especially on the inside surface of the teeth. A dry mouth, acid or burning sensation in the mouth, bad breath and redness of the palate may occur.[27] Less common symptoms of GERD include difficulty in swallowing, water brash, chronic cough, hoarse voice, nausea and vomiting.[26]
Signs of enamel erosion are the appearance of a smooth, silky-glazed, sometimes dull, enamel surface with the absence of perikymata, together with intact enamel along the gum margin.[28] It will be evident in people with restorations as tooth structure typically dissolves much faster than the restorative material, causing it to seem as if it "stands above" the surrounding tooth structure.[29]
GERD may lead to Barrett's esophagus, a type of intestinal metaplasia,[20] which is in turn a precursor condition for esophageal cancer. The risk of progression from Barrett's to dysplasia is uncertain, but is estimated at 20% of cases.[30] Due to the risk of chronic heartburn progressing to Barrett's, EGD every five years is recommended for people with chronic heartburn, or who take drugs for chronic GERD.[31]
A small amount of acid reflux is typical even in healthy people (as with infrequent and minor heartburn), but gastroesophageal reflux becomes gastroesophageal reflux disease when signs and symptoms develop into a recurrent problem. Frequent acid reflux is due to poor closure of the lower esophageal sphincter, which is at the junction between the stomach and the esophagus.[6]
Factors that can contribute to GERD:
Factors that have been linked with GERD, but not conclusively:
In 1999, a review of existing studies found that, on average, 40% of GERD patients also had H. pylori infection.[39] The eradication of H. pylori can lead to an increase in acid secretion,[40] leading to the question of whether H. pylori-infected GERD patients are any different from non-infected GERD patients. A double-blind study, reported in 2004, found no clinically significant difference between these two types of patients with regard to the subjective or objective measures of disease severity.[41]
The diagnosis of GERD is usually made when typical symptoms are present.[42] Reflux can be present in people without symptoms and the diagnosis requires both symptoms or complications and reflux of stomach content.[43]
Other investigations may include esophagogastroduodenoscopy (EGD). Barium swallow X-rays should not be used for diagnosis.[42] Esophageal manometry is not recommended for use in the diagnosis, being recommended only prior to surgery.[42] Ambulatory esophageal pH monitoring may be useful in those who do not improve after PPIs and is not needed in those in whom Barrett's esophagus is seen.[42] Investigation for H. pylori is not usually needed.[42]
The current gold standard for diagnosis of GERD is esophageal pH monitoring. It is the most objective test to diagnose the reflux disease and allows monitoring GERD patients in their response to medical or surgical treatment. One practice for diagnosis of GERD is a short-term treatment with proton-pump inhibitors, with improvement in symptoms suggesting a positive diagnosis. Short-term treatment with proton-pump inhibitors may help predict abnormal 24-hour pH monitoring results among patients with symptoms suggestive of GERD.[44]
Endoscopy, the examination of the stomach with a fibre-optic scope, is not routinely needed if the case is typical and responds to treatment.[42] It is recommended when people either do not respond well to treatment or have alarm symptoms, including dysphagia, anemia, blood in the stool (detected chemically), wheezing, weight loss, or voice changes.[42] Some physicians advocate either once-in-a-lifetime or 5- to 10-yearly endoscopy for people with longstanding GERD, to evaluate the possible presence of dysplasia or Barrett's esophagus.[45]
Biopsies performed during gastroscopy may show:
Reflux changes that are not erosive in nature lead to "nonerosive reflux disease".
Severity may be documented with the Johnson-DeMeester's scoring system:[47]
0 – None
1 – Minimal – occasional episodes
2 – Moderate – medical therapy visits
3 – Severe – interference with daily activities
Other causes of chest pain such as heart disease should be ruled out before making the diagnosis.[42] Another kind of acid reflux, which causes respiratory and laryngeal signs and symptoms, is called laryngopharyngeal reflux (LPR) or extraesophageal reflux disease (EERD). Unlike GERD, LPR rarely produces heartburn, and is sometimes called silent reflux.[48] Differential diagnosis of GERD can also include dyspepsia, peptic ulcer disease, esophageal and gastric cancer, and food allergies.[49]
The treatments for GERD may include food choices, lifestyle changes, medications, and possibly surgery. Initial treatment is frequently with a proton-pump inhibitor such as omeprazole.[42] In some cases, a person with GERD symptoms can manage them by taking over-the-counter drugs.[50][51][52] This is often safer and less expensive than taking prescription drugs.[50] Some guidelines recommend trying to treat symptoms with an H2 antagonist before using a proton-pump inhibitor because of cost and safety concerns.[50]
Medical nutrition therapy plays an essential role in managing the symptoms of the disease by preventing reflux, preventing pain and irritation, and decreasing gastric secretions.[10]
Some foods such as chocolate, mint, high-fat food, and alcohol have been shown to relax the lower esophageal sphincter, increasing the risk of reflux.[10] Weight loss is recommended for the overweight or obese, as well as avoidance of  bedtime snacks or lying down immediately after meals (meals should occur at least 2–3 hours before bedtime), elevation of the head of the bed on 6-inch blocks, avoidance of smoking, and avoidance of tight clothing that increases pressure in the stomach. It may be beneficial to avoid spices, citrus juices, tomatoes and soft drinks, and to consume small frequent meals and drink liquids between meals.[43][10][53] Some evidence suggests that reduced sugar intake and increased fiber intake can help.[54][43] Although moderate exercise may improve symptoms in people with GERD, vigorous exercise may worsen them.[55] Breathing exercises may relieve GERD symptoms.[56]
The primary medications used for GERD are proton-pump inhibitors, H2 receptor blockers and antacids with or without alginic acid.[9] The use of acid suppression therapy is a common response to GERD symptoms and many people get more of this kind of treatment than their case merits.[50][57][58][52][51][59] The overuse of acid suppression is a problem because of the side effects and costs.[50][58][52][51][59]
Proton-pump inhibitors (PPIs), such as omeprazole, are the most effective, followed by H2 receptor blockers, such as ranitidine.[43] If a once-daily PPI is only partially effective they may be used twice a day.[43] They should be taken one half to one hour before a meal.[42] There is no significant difference between PPIs.[42] When these medications are used long term, the lowest effective dose should be taken.[43] They may also be taken only when symptoms occur in those with frequent problems.[42] H2 receptor blockers lead to roughly a 40% improvement.[60]
The evidence for antacids is weaker with a benefit of about 10% (NNT=13) while a combination of an antacid and alginic acid (such as Gaviscon) may improve symptoms by 60% (NNT=4).[60] Metoclopramide (a prokinetic) is not recommended either alone or in combination with other treatments due to concerns around adverse effects.[9][43] The benefit of the prokinetic mosapride is modest.[9]
Sucralfate has similar effectiveness to H2 receptor blockers; however, sucralfate needs to be taken multiple times a day, thus limiting its use.[9] Baclofen, an agonist of the GABAB receptor, while effective, has similar issues of needing frequent dosing in addition to greater adverse effects compared to other medications.[9]
The standard surgical treatment for severe GERD is the Nissen fundoplication. In this procedure, the upper part of the stomach is wrapped around the lower esophageal sphincter to strengthen the sphincter and prevent acid reflux and to repair a hiatal hernia.[61] It is recommended only for those who do not improve with PPIs.[42] Quality of life is improved in the short term compared to medical therapy, but there is uncertainty in the benefits of surgery versus long-term medical management with proton pump inhibitors.[62] When comparing different fundoplication techniques, partial posterior fundoplication surgery is more effective than partial anterior fundoplication surgery,[63] and partial fundoplication has better outcomes than total fundoplication.[64]
Esophagogastric dissociation is an alternative procedure that is sometimes used to treat neurologically impaired children with GERD.[65][66] Preliminary studies have shown it may have a lower failure rate[67] and a lower incidence of recurrent reflux.[66]
In 2012 the U.S. Food and Drug Administration (FDA) approved a device called the LINX, which consists of a series of metal beads with magnetic cores that are placed surgically around the lower esophageal sphincter, for those with severe symptoms that do not respond to other treatments. Improvement of GERD symptoms is similar to those of the Nissen fundoplication, although there is no data regarding long-term effects. Compared to Nissen fundoplication procedures, the procedure has shown a reduction in complications such as gas bloat syndrome that commonly occur.[68] Adverse responses include difficulty swallowing, chest pain, vomiting, and nausea. Contraindications that would advise against use of the device are patients who are or may be allergic to titanium, stainless steel, nickel, or ferrous iron materials. A warning advises that the device should not be used by patients who could be exposed to, or undergo, magnetic resonance imaging (MRI) because of serious injury to the patient and damage to the device.[69]
Some patients who are at an increased surgical risk or do not tolerate PPIs[70] may qualify for a more recently developed incisionless procedure known as a TIF transoral incisionless fundoplication.[71] Benefits of this procedure may last for up to six years.[72]
GERD is a common condition that develops during pregnancy, but usually resolves after delivery.[73] The severity of symptoms tend to increase throughout the pregnancy.[73] In pregnancy, dietary modifications and lifestyle changes may be attempted, but often have little effect. Some lifestyle changes that can be implemented are elevating the head of the bed, eating small portions of food at regularly scheduled intervals, reduce fluid intake with a meal, avoid eating 3 hours before bedtime, and refrain from lying down after eating.[73] Calcium-based antacids are recommended if these changes are not effective, aluminum- and magnesium hydroxide -based antacids are also safe.[73] Antacids that contain sodium bicarbonate or magnesium trisilicate should be avoided in pregnancy.[73] Sucralfate has been studied in pregnancy and proven to be safe [73] as is ranitidine[74] and PPIs.[75]
Babies may see relief with smaller, more frequent feedings, more frequent burping during feedings, holding the baby in an upright position 30 minutes after feeding, keeping the baby's head elevated while laying on the back, removing milk and soy from the mother's diet or feeding the baby milk protein-free formula.[76] They may also be treated with medicines such as ranitidine or proton pump inhibitors.[77] Proton pump inhibitors however have not been found to be effective in this population and there is a lack of evidence for safety.[78] The role of an Occupational Therapist with an infant with GERD includes positioning during and after feeding.[79] One technique used is called "the log roll technique" which is practiced when changing an infant's clothing or diapers.[79] Placing an infant on their back while having their legs lifted is not recommended since it causes the acid to flow back up the esophagus.[79] Instead, the occupational therapist would suggest rolling the child on the side, keeping the shoulders and hips aligned to avoid acid rising up the baby's esophagus.[79] Another technique used is feeding the baby on their side with an upright position instead of lying flat on their back.[79] The final positioning technique used for infants is to keep them on their tummy or upright for 20 minutes after feeding.[79][80]
In Western populations, GERD affects approximately 10% to 20% of the population and 0.4% newly develop the condition.[9] For instance, an estimated 3.4 million to 6.8 million Canadians have GERD. The prevalence rate of GERD in developed nations is also tightly linked with age, with adults aged 60 to 70 being the most commonly affected.[81] In the United States 20% of people have symptoms in a given week and 7% every day.[9] No data supports sex predominance with regard to GERD.[82]
An obsolete treatment is vagotomy ("highly selective vagotomy"), the surgical removal of vagus nerve branches that innervate the stomach lining. This treatment has been largely replaced by medication. Vagotomy by itself tended to worsen contraction of the pyloric sphincter of the stomach, and delayed stomach emptying. Historically, vagotomy was combined with pyloroplasty or gastroenterostomy to counter this problem.[83]
A number of endoscopic devices have been tested to treat chronic heartburn.

Hypothyroidism (also called underactive thyroid, low thyroid or hypothyreosis) is a disorder of the endocrine system in which the thyroid gland does not produce enough thyroid hormones.[3] It can cause a number of symptoms, such as poor ability to tolerate cold, a feeling of tiredness, constipation, slow heart rate,  depression, and weight gain.[3] Occasionally there may be swelling of the front part of the neck due to goitre.[3] Untreated cases of hypothyroidism during pregnancy can lead to delays in growth and intellectual development in the baby or congenital iodine deficiency syndrome.[5]
Worldwide, too little iodine in the diet is the most common cause of hypothyroidism.[8][9] Hashimoto's thyroiditis is the most common cause of hypothyroidism in countries with sufficient dietary iodine.[3] Less common causes include previous treatment with radioactive iodine, injury to the hypothalamus or the anterior pituitary gland, certain medications, a lack of a functioning thyroid at birth, or previous thyroid surgery.[3][10] The diagnosis of hypothyroidism, when suspected, can be confirmed with blood tests measuring thyroid-stimulating hormone (TSH) and thyroxine levels.[3]
Salt iodization has prevented hypothyroidism in many populations.[7] Thyroid hormone replacement with levothyroxine treats hypothyroidism.[3] Medical professionals adjust the dose according to symptoms and normalization of the thyroxine and TSH levels.[3] Thyroid medication is safe in pregnancy.[3] Although an adequate amount of dietary iodine is important, too much may worsen specific forms of hypothyroidism.[3]
Worldwide about one billion people are estimated to be iodine-deficient; however, it is unknown how often this results in hypothyroidism.[11] In the United States, hypothyroidism occurs in 0.3–0.4% of people.[8] Subclinical hypothyroidism, a milder form of hypothyroidism characterized by normal thyroxine levels and an elevated TSH level, is thought to occur in 4.3–8.5% of people in the United States.[8] Hypothyroidism is more common in women than in men.[3] People over the age of 60 are more commonly affected.[3] Dogs are also known to develop hypothyroidism, as are cats and horses, albeit more rarely.[12] The word hypothyroidism is from  Greek hypo- 'reduced',  thyreos 'shield', and eidos 'form'.[13]
People with hypothyroidism often have no or only mild symptoms. Numerous symptoms and signs are associated with hypothyroidism and can be related to the underlying cause, or a direct effect of having not enough thyroid hormones.[14][15] Hashimoto's thyroiditis may present with the mass effect of a goitre (enlarged thyroid gland).[14] In middle-aged women, the symptoms may be mistaken for those of menopause.[16]
Delayed relaxation after testing the ankle jerk reflex is a characteristic sign of hypothyroidism and is associated with the severity of the hormone deficit.[8]
Myxedema coma is a rare but life-threatening state of extreme hypothyroidism. It may occur in those with established hypothyroidism when they develop an acute illness. Myxedema coma can be the first presentation of hypothyroidism. People with myxedema coma typically have a low body temperature without shivering, confusion, a slow heart rate and reduced breathing effort. There may be physical signs suggestive of hypothyroidism, such as skin changes or enlargement of the tongue.[18]
Even mild or subclinical hypothyroidism leads to possible infertility and an increased risk of miscarriage.[19][20] Hypothyroidism in early pregnancy, even with limited or no symptoms, may increase the risk of pre-eclampsia, offspring with lower intelligence,[21][22][23][24] and the risk of infant death around the time of birth.[19][20][25] Women are affected by hypothyroidism in 0.3–0.5% of pregnancies.[25] Subclinical hypothyroidism during pregnancy is associated with gestational diabetes, low birth-weight, placental abruption, and the birth of the baby before 37 weeks of pregnancy.[21][26][27][28]
Newborn children with hypothyroidism may have normal birth weight and height (although the head may be larger than expected and the posterior fontanelle may be open). Some may have drowsiness, decreased muscle tone, poor weight gain, a hoarse-sounding cry, feeding difficulties, constipation, an enlarged tongue, umbilical hernia, dry skin, a decreased body temperature, and jaundice.[29] A goiter is rare, although it may develop later in children who have a thyroid gland that does not produce functioning thyroid hormone.[29] A goitre may also develop in children growing up in areas with iodine deficiency.[30] Normal growth and development may be delayed, and not treating infants may lead to an intellectual impairment (IQ 6–15 points lower in severe cases). Other problems include the following: difficulty with large scale and fine motor skills and coordination, reduced muscle tone, squinting, decreased attention span, and delayed speaking.[29] Tooth eruption may be delayed.[31]
In older children and adolescents, the symptoms of hypothyroidism may include fatigue, cold intolerance, sleepiness, muscle weakness, constipation, a delay in growth, overweight for height, pallor, coarse and thick skin, increased body hair, irregular menstrual cycles in girls, and delayed puberty. Signs may include delayed relaxation of the ankle reflex and a slow heartbeat.[29] A goiter may be present with a completely enlarged thyroid gland;[29] sometimes only part of the thyroid is enlarged and it can be knobby.[32]
Thyroid hormone abnormalities are common in major psychiatric disorders including bipolar disorder; clinical research has shown there is a high rate of thyroid dysfunction in mood disorders and schizophrenia-spectrum disorders, concluding that there is a case for screening for the latter among people with thyroid illness.[33]
Hypothyroidism is caused by inadequate function of the gland itself (primary hypothyroidism), inadequate stimulation by thyroid-stimulating hormone from the pituitary gland (secondary hypothyroidism), or inadequate release of thyrotropin-releasing hormone from the brain's hypothalamus (tertiary hypothyroidism).[8][34] Primary hypothyroidism is about a thousandfold more common than central hypothyroidism.[10] Central hypothyroidism is the name used for secondary and tertiary, since hypothalamus and pituitary gland are at the center of thyroid hormone control.
Iodine deficiency is the most common cause of primary hypothyroidism and endemic goitre worldwide.[8][9] In areas of the world with sufficient dietary iodine, hypothyroidism is most commonly caused by the autoimmune disease Hashimoto's thyroiditis (chronic autoimmune thyroiditis).[8][9] Hashimoto's may be associated with a goitre. It is characterized by infiltration of the thyroid gland with T lymphocytes and autoantibodies against specific thyroid antigens such as thyroid peroxidase, thyroglobulin and the TSH receptor.[8]
A more uncommon cause of hypothyroidism is estrogen dominance. It is one of the most common hormone imbalance problems in women, but is not always link with hypothyroidism[35] There are three different types of estrogens: estrone (E1), estradiol (E2), and estriol (E3), with estradiol being the most potent of the three[35][failed verification] Women with estrogen dominance have estradiol levels of 115 pg/ml on day 3 of their cycle. However, estrogen dominance is not just about an over abundance of estradiol, but is more likely correlated with an imbalance between estradiol and progesterone. Women who have too much unopposed estrogen, which is estrogen that does not have enough counterbalancing progesterone in their bodies, commonly have unbalanced thyroid levels, in addition to excess growths within their uteri.[36]
Estradiol disrupts thyroid hormone production because high blood levels of estrogen signal the liver to increase the production of thyroid-binding globulin (TBG). This is an inhibitor protein that binds to the thyroid hormone, reducing the amount of T3 and T4 available for use by cells.[37] Without T3 and T4, the body's cellular function begins to slow down.
After women give birth, about 5% develop postpartum thyroiditis which can occur up to nine months afterwards.[38] This is characterized by a short period of hyperthyroidism followed by a period of hypothyroidism; 20–40% remain permanently hypothyroid.[38]
Autoimmune thyroiditis (Hashimoto's) is associated with other immune-mediated diseases such as diabetes mellitus type 1, pernicious anemia, myasthenia gravis, celiac disease, rheumatoid arthritis and systemic lupus erythematosus.[8] It may occur as part of autoimmune polyendocrine syndrome (type 1 and type 2).[8]
Iatrogenic hypothyroidism can be surgical (a result of thyroidectomy, usually for thyroid nodules or cancer) or following radioiodine ablation (usually for Graves' disease).
Thyroid hormone is required for the normal functioning of numerous tissues in the body. In healthy individuals, the thyroid gland predominantly secretes thyroxine (T4), which is converted into triiodothyronine (T3) in other organs by the selenium-dependent enzyme iodothyronine deiodinase.[41] Triiodothyronine binds to the thyroid hormone receptor in the nucleus of cells, where it stimulates the turning on of particular genes and the production of specific proteins.[42] Additionally, the hormone binds to integrin αvβ3 on the cell membrane, thereby stimulating the sodium–hydrogen antiporter and processes such as formation of blood vessels and cell growth.[42] In blood, almost all thyroid hormone (99.97%) are bound to plasma proteins such as thyroxine-binding globulin; only the free unbound thyroid hormone is biologically active.[8]
The thyroid gland is the only source of thyroid hormone in the body; the process requires iodine and the amino acid tyrosine. Iodine in the bloodstream is taken up by the gland and incorporated into thyroglobulin molecules. The process is controlled by the thyroid-stimulating hormone (TSH, thyrotropin), which is secreted by the pituitary. Not enough iodine, or not enough TSH, can result in decreased production of thyroid hormones.[34]
The hypothalamic–pituitary–thyroid axis plays a key role in maintaining thyroid hormone levels within normal limits. Production of TSH by the anterior pituitary gland is stimulated in turn by thyrotropin-releasing hormone (TRH), released from the hypothalamus. Production of TSH and TRH is decreased by thyroxine by a negative feedback process. Not enough TRH, which is uncommon, can lead to not enough TSH and thereby to not enough thyroid hormone production.[10]
Pregnancy leads to marked changes in thyroid hormone physiology. The gland is increased in size by 10%, thyroxine production is increased by 50%, and iodine requirements are increased. Many women have normal thyroid function but have immunological evidence of thyroid autoimmunity (as evidenced by autoantibodies) or are iodine deficient, and develop evidence of hypothyroidism before or after giving birth.[43]
Laboratory testing of thyroid stimulating hormone levels in the blood is considered the best initial test for hypothyroidism; a second TSH level is often obtained several weeks later for confirmation.[44] Levels may be abnormal in the context of other illnesses, and TSH testing in hospitalized people is discouraged unless thyroid dysfunction is strongly suspected[8] as the cause of the acute illness.[16] An elevated TSH level indicates that the thyroid gland is not producing enough thyroid hormone, and free T4 levels are then often obtained.[8][16][32] Measuring T3 is discouraged by the AACE in the assessment for hypothyroidism.[8]  In England and Wales, the National Institute for Health and Care Excellence (NICE) recommends routine T4 testing in children, and T3 testing in both adults and children if central hypothyroidism is suspected and the TSH is low.[16] There are a number of symptom rating scales for hypothyroidism; they provide a degree of objectivity but have limited use for diagnosis.[8]
Many cases of hypothyroidism are associated with mild elevations in creatine kinase and liver enzymes in the blood. They typically return to normal when hypothyroidism has been fully treated.[8] Levels of cholesterol, low-density lipoprotein and lipoprotein (a) can be elevated;[8] the impact of subclinical hypothyroidism on lipid parameters is less well-defined.[30]
Very severe hypothyroidism and myxedema coma are characteristically associated with low sodium levels in the blood together with elevations in antidiuretic hormone, as well as acute worsening of kidney function due to a number of causes.[18] In most causes, however, it is unclear if the relationship is causal.[45]
A diagnosis of hypothyroidism without any lumps or masses felt within the thyroid gland does not require thyroid imaging; however, if the thyroid feels abnormal, diagnostic imaging is then recommended.[44] The presence of antibodies against thyroid peroxidase (TPO) makes it more likely that thyroid nodules are caused by autoimmune thyroiditis, but if there is any doubt, a needle biopsy may be required.[8]
If the TSH level is normal or low and serum free T4 levels are low, this is suggestive of central hypothyroidism (not enough TSH or TRH secretion by the pituitary gland or hypothalamus). There may be other features of hypopituitarism, such as menstrual cycle abnormalities and adrenal insufficiency. There might also be symptoms of a pituitary mass such as headaches and vision changes. Central hypothyroidism should be investigated further to determine the underlying cause.[10][44]
In overt primary hypothyroidism, TSH levels are high and T4 and T3 levels are low. Overt hypothyroidism may also be diagnosed in those who have a TSH on multiple occasions of greater than 5mIU/L, appropriate symptoms, and only a borderline low T4.[46] It may also be diagnosed in those with a TSH of greater than 10mIU/L.[46]
Subclinical hypothyroidism is a biochemical diagnosis characterized by an elevated serum TSH level, but with a normal serum free thyroxine level.[47][48][49] The incidence of subclinical hypothyroidism is estimated to be 3-15% and a higher incidence is seen in elderly people, females and those with lower iodine levels.[47] Subclinical hypothyroidism is most commonly caused by autoimmune thyroid diseases, especially Hashimoto's thyroiditis.[50]  The presentation of subclinical hypothyroidism is variable and classic signs and symptoms of hypothyroidism may not be observed.[48] Of people with subclinical hypothyroidism, a proportion will develop overt hypothyroidism each year. In those with detectable antibodies against thyroid peroxidase (TPO), this occurs in 4.3%, while in those with no detectable antibodies, this occurs in 2.6%.[8] In addition to detectable anti-TPO antibodies, other risk factors for conversion from subclinical hypothyroidism to overt hypothyroidism include female sex or in those with higher TSH levels or lower level of normal free T4 levels.[47] Those with subclinical hypothyroidism and detectable anti-TPO antibodies who do not require treatment should have repeat thyroid function tested more frequently (e.g. every 6 months) compared with those who do not have antibodies.[44][47]
During pregnancy, the thyroid gland must produce 50% more thyroid hormone to provide enough thyroid hormone for the developing fetus and the expectant mother.[28]  In pregnancy, free thyroxine levels may be lower than anticipated due to increased binding to thyroid binding globulin and decreased binding to albumin. They should either be corrected for the stage of pregnancy,[43] or total thyroxine levels should be used instead for diagnosis.[8] TSH values may also be lower than normal (particularly in the first trimester) and the normal range should be adjusted for the stage of pregnancy.[8][43]
In pregnancy, subclinical hypothyroidism is defined as a TSH between 2.5 and 10 mIU/L with a normal thyroxine level, while those with TSH above 10 mIU/L are considered to be overtly hypothyroid even if the thyroxine level is normal.[43] Antibodies against TPO may be important in making decisions about treatment, and should, therefore, be determined in women with abnormal thyroid function tests.[8]
Determination of TPO antibodies may be considered as part of the assessment of recurrent miscarriage, as subtle thyroid dysfunction can be associated with pregnancy loss,[8] but this recommendation is not universal,[51] and presence of thyroid antibodies may not predict future outcome.[52]
Hypothyroidism may be prevented in a population by adding iodine to commonly used foods. This public health measure has eliminated endemic childhood hypothyroidism in countries where it was once common. In addition to promoting the consumption of iodine-rich foods such as dairy and fish, many countries with moderate iodine deficiency have implemented universal salt iodization.[53] Encouraged by the World Health Organization,[54] 70% of the world's population across 130 countries are receiving iodized salt. In some countries, iodized salt is added to bread.[53] Despite this, iodine deficiency has reappeared in some Western countries as a result of attempts to reduce salt intake.[53]
Pregnant and breastfeeding women, who require 66% more daily iodine than non-pregnant women, may still not be getting enough iodine.[53][55] The World Health Organization recommends a daily intake of 250 µg for pregnant and breastfeeding women.[56] As many women will not achieve this from dietary sources alone, the American Thyroid Association recommends a 150 µg daily supplement by mouth.[43][57]
Screening for hypothyroidism is performed in the newborn period in many countries, generally using TSH. This has led to the early identification of many cases and thus the prevention of developmental delay.[58] It is the most widely used newborn screening test worldwide.[59] While TSH-based screening will identify the most common causes, the addition of T4 testing is required to pick up the rarer central causes of neonatal hypothyroidism.[29] If T4 determination is included in the screening done at birth, this will identify cases of congenital hypothyroidism of central origin in 1:16,000 to 1:160,000 children. Considering that these children usually have other pituitary hormone deficiencies, early identification of these cases may prevent complications.[10]
In adults, widespread screening of the general population is a matter of debate. Some organizations (such as the United States Preventive Services Task Force) state that evidence is insufficient to support routine screening,[60] while others (such as the American Thyroid Association) recommend either intermittent testing above a certain age in all sexes or only in women.[8] Targeted screening may be appropriate in a number of situations where hypothyroidism is common: other autoimmune diseases, a strong family history of thyroid disease, those who have received radioiodine or other radiation therapy to the neck, those who have previously undergone thyroid surgery, those with an abnormal thyroid examination, those with psychiatric disorders, people taking amiodarone or lithium, and those with a number of health conditions (such as certain heart and skin conditions).[8] Yearly thyroid function tests are recommended in people with Down syndrome, as they are at higher risk of thyroid disease.[61] Guidelines for England and Wales from the National Institute for Health and Care Excellence (NICE) recommend testing for thyroid disease in people with type 1 diabetes and new-onset atrial fibrillation, and suggests testing in those with depression or unexplained anxiety (all ages), in children with abnormal growth, or unexplained change in behaviour or school performance.[16] On diagnosis of autoimmune thyroid disease, NICE also recommends screening for celiac disease.[62]
Most people with hypothyroidism symptoms and confirmed thyroxine deficiency are treated with a synthetic long-acting form of thyroxine, known as levothyroxine (L-thyroxine).[8][15] In young and otherwise healthy people with overt hypothyroidism, a full replacement dose (adjusted by weight) can be started immediately; in the elderly and people with heart disease a lower starting dose is recommended to prevent over supplementation and risk of complications.[8][34][16] Lower doses may be sufficient in those with subclinical hypothyroidism, while people with central hypothyroidism may require a higher than average dose.[8]
Blood free thyroxine and TSH levels are monitored to help determine whether the dose is adequate. This is done 4–8 weeks after the start of treatment or a change in levothyroxine dose. Once the adequate replacement dose has been established, the tests can be repeated after 6 and then 12 months, unless there is a change in symptoms.[8] Normalization of TSH does not mean that other abnormalities associated with hypothyroidism improve entirely, such as elevated cholesterol levels.[63]
In people with central/secondary hypothyroidism, TSH is not a reliable marker of hormone replacement and decisions are based mainly on the free T4 level.[8][10] Levothyroxine is best taken 30–60 minutes before breakfast, or four hours after food,[8] as certain substances such as food and calcium can inhibit the absorption of levothyroxine.[64] There is no direct way of increasing thyroid hormone secretion by the thyroid gland.[15]
Treatment with liothyronine alone has not received enough study to make a recommendation as to its use; due to its shorter half-life it would need to be taken more often than levothyroxine.[8]
Adding liothyronine (synthetic T3) to levothyroxine has been suggested as a measure to provide better symptom control, but this has not been confirmed by studies.[9][15][65] In 2007, the British Thyroid Association stated that combined T4 and T3 therapy carried a higher rate of side effects and no benefit over T4 alone.[15][66] Similarly, American guidelines discourage combination therapy due to a lack of evidence, although they acknowledge that some people feel better when receiving combination treatment.[8] Guidelines by NICE for England and Wales discourage liothyronine.[16]
People with hypothyroidism who do not feel well despite optimal levothyroxine dosing may request adjunctive treatment with liothyronine. A 2012 guideline from the European Thyroid Association recommends that support should be offered with regards to the chronic nature of the disease and that other causes of the symptoms should be excluded. Addition of liothyronine should be regarded as experimental, initially only for a trial period of 3 months, and in a set ratio to the current dose of levothyroxine.[67] The guideline explicitly aims to enhance the safety of this approach and to counter its indiscriminate use.[67]
Desiccated thyroid extract is an animal-based thyroid gland extract,[15] most commonly from pigs. It is a combination therapy, containing forms of T4 and T3.[15] It also contains calcitonin (a hormone produced in the thyroid gland involved in the regulation of calcium levels), T1 and T2; these are not present in synthetic hormone medication.[68] This extract was once a mainstream hypothyroidism treatment, but its use today is unsupported by evidence;[9][15] British Thyroid Association and American professional guidelines discourage its use,[8][66] as does NICE.[16]
There is no evidence of a benefit from treating subclinical hypothyroidism in those who are not pregnant, and there are potential risks of overtreatment.[69] Untreated subclinical hypothyroidism may be associated with a modest increase in the risk of coronary artery disease when the TSH is over 10 mIU/L.[69][70] A 2007 review found no benefit of thyroid hormone replacement except for "some parameters of lipid profiles and left ventricular function".[71] There is no association between subclinical hypothyroidism and an increased risk of bone fractures,[72] nor is there a link with cognitive decline.[73]
American guidelines recommend that treatment should be considered in people with symptoms of hypothyroidism, detectable antibodies against thyroid peroxidase, a history of heart disease or are at an increased risk for heart disease, if the TSH is elevated but below 10 mIU/L.[8] American guidelines further recommend universal treatment (independent of risk factors) in those with TSH levels that are markedly elevated; above 10 mIU/L because of an increased risk of heart failure or death due to cardiovascular disease.[8][47]  NICE recommends that those with a TSH above 10 mIU/L should be treated in the same way as overt hypothyroidism. Those with an elevated TSH but below 10 mIU/L who have symptoms suggestive of hypothyroidism should have a trial of treatment but with the aim to stopping this if the symptoms persist despite normalisation of the TSH.[16]
A recent meta-analysis, however, found an increased risk for cardiovascular death in subclinical hypothyroidism.[74]
Myxedema coma or severe decompensated hypothyroidism usually requires admission to the intensive care, close observation and treatment of abnormalities in breathing, temperature control, blood pressure, and sodium levels. Mechanical ventilation may be required, as well as fluid replacement, vasopressor agents, careful rewarming, and corticosteroids (for possible adrenal insufficiency which can occur together with hypothyroidism). Careful correction of low sodium levels may be achieved with hypertonic saline solutions or vasopressin receptor antagonists.[18] For rapid treatment of the hypothyroidism, levothyroxine or liothyronine may be administered intravenously, particularly if the level of consciousness is too low to be able to safely swallow medication.[18] While administration through a nasogastric tube is possible, this may be unsafe and is discouraged.[18]
In women with known hypothyroidism who become pregnant, it is recommended that serum TSH levels are closely monitored. Levothyroxine should be used to keep TSH levels within the normal range for that trimester. The first trimester normal range is below 2.5 mIU/L and the second and third trimesters normal range is below 3.0 mIU/L.[15][43] Treatment should be guided by total (rather than free) thyroxine or by the free T4 index. Similarly to TSH, the thyroxine results should be interpreted according to the appropriate reference range for that stage of pregnancy.[8] The levothyroxine dose often needs to be increased after pregnancy is confirmed,[8][34][43] although this is based on limited evidence and some recommend that it is not always required; decisions may need to based on TSH levels.[75]
Women with anti-TPO antibodies who are trying to become pregnant (naturally or by assisted means) may require thyroid hormone supplementation even if the TSH level is normal. This is particularly true if they have had previous miscarriages or have been hypothyroid in the past.[8] Supplementary levothyroxine may reduce the risk of preterm birth and possibly miscarriage.[76] The recommendation is stronger in pregnant women with subclinical hypothyroidism (defined as TSH 2.5–10 mIU/L) who are anti-TPO positive, in view of the risk of overt hypothyroidism. If a decision is made not to treat, close monitoring of the thyroid function (every 4 weeks in the first 20 weeks of pregnancy) is recommended.[8][43] If anti-TPO is not positive, treatment for subclinical hypothyroidism is not currently recommended.[43] It has been suggested that many of the aforementioned recommendations could lead to unnecessary treatment, in the sense that the TSH cutoff levels may be too restrictive in some ethnic groups; there may be little benefit from treatment of subclinical hypothyroidism in certain cases.[75]
The effectiveness and safety of using Chinese herbal medicines to treat hypothyroidism is not known.[77]
Worldwide about one billion people are estimated to be iodine deficient; however, it is unknown how often this results in hypothyroidism.[11] In large population-based studies in Western countries with sufficient dietary iodine, 0.3–0.4% of the population have overt hypothyroidism. A larger proportion, 4.3–8.5%, have subclinical hypothyroidism.[8] Undiagnosed hypothyroidism is estimated to affect about 4–7% of community-derived populations in the US and Europe.[78] Of people with subclinical hypothyroidism, 80% have a TSH level below the 10 mIU/L mark regarded as the threshold for treatment.[49] Children with subclinical hypothyroidism often return to normal thyroid function, and a small proportion develops overt hypothyroidism (as predicted by evolving antibody and TSH levels, the presence of celiac disease, and the presence of a goitre).[79]
Women are more likely to develop hypothyroidism than men. In population-based studies, women were seven times more likely than men to have TSH levels above 10 mU/L.[8] 2–4% of people with subclinical hypothyroidism will progress to overt hypothyroidism each year. The risk is higher in those with antibodies against thyroid peroxidase.[8][49] Subclinical hypothyroidism is estimated to affect approximately 2% of children; in adults, subclinical hypothyroidism is more common in the elderly, and in White people.[48] There is a much higher rate of thyroid disorders, the most common of which is hypothyroidism, in individuals with Down syndrome[29][61] and Turner syndrome.[29]
Very severe hypothyroidism and myxedema coma are rare, with it estimated to occur in 0.22 per million people a year.[18] The majority of cases occur in women over 60 years of age, although it may happen in all age groups.[18]
Most hypothyroidism is primary in nature. Central/secondary hypothyroidism affects 1:20,000 to 1:80,000 of the population, or about one out of every thousand people with hypothyroidism.[10]
In 1811, Bernard Courtois discovered iodine was present in seaweed, and iodine intake was linked with goitre size in 1820 by Jean-Francois Coindet.[80] Gaspard Adolphe Chatin proposed in 1852 that endemic goitre was the result of not enough iodine intake, and Eugen Baumann demonstrated iodine in thyroid tissue in 1896.[80]
The first cases of myxedema were recognized in the mid-19th century (the 1870s), but its connection to the thyroid was not discovered until the 1880s when myxedema was observed in people following the removal of the thyroid gland (thyroidectomy).[81]  The link was further confirmed in the late 19th century when people and animals who had had their thyroid removed showed improvement in symptoms with transplantation of animal thyroid tissue.[9]  The severity of myxedema, and its associated risk of mortality and complications, created interest in discovering effective treatments for hypothyroidism.[81]  Transplantation of thyroid tissue demonstrated some efficacy, but recurrences of hypothyroidism was relatively common, and sometimes required multiple repeat transplantations of thyroid tissue.[81]
In 1891, the English physician George Redmayne Murray introduced subcutaneously injected sheep thyroid extract,[82] followed shortly after by an oral formulation.[9][83]  Purified thyroxine was introduced in 1914 and in the 1930s synthetic thyroxine became available, although desiccated animal thyroid extract remained widely used.  Liothyronine was identified in 1952.[9]
Early attempts at titrating therapy for hypothyroidism proved difficult.  After hypothyroidism was found to cause a lower basal metabolic rate, this was used as a marker to guide adjustments in therapy in the early 20th century (around 1915).[81]  However, a low basal metabolic rate was known to be non-specific, also present in malnutrition.[81]  The first laboratory test to be helpful in assessing thyroid status was the serum protein-bound iodine, which came into use around the 1950s.
In 1971, the thyroid stimulating hormone (TSH) radioimmunoassay was developed, which was the most specific marker for assessing thyroid status in patients.[81]  Many people who were being treated based on basal metabolic rate, minimizing hypothyroid symptoms, or based on serum protein-bound iodine, were found to have excessive thyroid hormone.[81]  The following year, in 1972, a T3 radioimmunoassay was developed, and in 1974, a T4 radioimmunoassay was developed.[81]
In veterinary practice, dogs are the species most commonly affected by hypothyroidism. The majority of cases occur as a result of primary hypothyroidism, of which two types are recognized: lymphocytic thyroiditis, which is probably immune-driven and leads to destruction and fibrosis of the thyroid gland, and idiopathic atrophy, which leads to the gradual replacement of the gland by fatty tissue.[12][84] There is often lethargy, cold intolerance, exercise intolerance, and weight gain. Furthermore, skin changes and fertility problems are seen in dogs with hypothyroidism, as well as a number of other symptoms.[84] The signs of myxedema can be seen in dogs, with prominence of skin folds on the forehead, and cases of myxedema coma are encountered.[12] The diagnosis can be confirmed by blood test, as the clinical impression alone may lead to overdiagnosis.[12][84] Lymphocytic thyroiditis is associated with detectable antibodies against thyroglobulin, although they typically become undetectable in advanced disease.[84] Treatment is with thyroid hormone replacement.[12]
Other species that are less commonly affected include cats and horses, as well as other large domestic animals. In cats, hypothyroidism is usually the result of other medical treatment such as surgery or radiation. In young horses, congenital hypothyroidism has been reported predominantly in Western Canada and has been linked with the mother's diet.[12]

Epilepsy is a group of non-communicable neurological disorders characterized by recurrent epileptic seizures.[10][11] An epileptic seizure is the clinical manifestation of an abnormal, excessive, and synchronized electrical discharge in the brain cells called neurons.[1][12] The occurrence of two or more unprovoked seizures defines epilepsy.[13] The occurrence of just one seizure may warrant the definition (set out by the International League Against Epilepsy) in a more clinical usage where recurrence may be able to be prejudged.[10] Epileptic seizures can vary from brief and nearly undetectable periods to long periods of vigorous shaking due to abnormal electrical activity in the brain.[1] These episodes can result in physical injuries, either directly such as broken bones or through causing accidents.[1] In epilepsy, seizures tend to recur and may have no immediate underlying cause.[13]  Isolated seizures that are provoked by a specific cause such as poisoning are not deemed to represent epilepsy.[14] People with epilepsy may be treated differently in various areas of the world and experience varying degrees of social stigma due to the alarming nature of their symptoms.[13]
The underlying mechanism of an epileptic seizure is excessive and abnormal neuronal activity in the cortex of the brain[14] which can be observed in the electroencephalogram (EEG) of an individual. The reason this occurs in most cases of epilepsy is unknown (cryptogenic);[1] some cases occur as the result of brain injury, stroke, brain tumors, infections of the brain, or birth defects through a process known as epileptogenesis.[1][2][3] Known genetic mutations are directly linked to a small proportion of cases.[4][15] The diagnosis involves ruling out other conditions that might cause similar symptoms, such as fainting, and determining if another cause of seizures is present, such as alcohol withdrawal or electrolyte problems.[4] This may be partly done by imaging the brain and performing blood tests.[4] Epilepsy can often be confirmed with an EEG, but a normal test does not rule out the condition.[4]
Epilepsy that occurs as a result of other issues may be preventable.[1] Seizures are controllable with medication in about 69% of cases;[7] inexpensive anti-seizure medications are often available.[1] In those whose seizures do not respond to medication; surgery, neurostimulation or dietary changes may then be considered.[11][5][6][needs update] Not all cases of epilepsy are lifelong, and many people improve to the point that treatment is no longer needed.[1]
As of 2020[update], about 50 million people have epilepsy.[11] Nearly 80% of cases occur in the developing world.[1] In 2015, it resulted in 125,000 deaths, an increase from 112,000 in 1990.[9][16] Epilepsy is more common in older people.[17][18] In the developed world, onset of new cases occurs most frequently in babies and the elderly.[19] In the developing world, onset is more common at the extremes of age – in younger children and in older children and young adults due to differences in the frequency of the underlying causes.[20] About 5–10% of people will have an unprovoked seizure by the age of 80.[21] The chance of experiencing a second seizure within two years after the first is around 40%.[22][23] In many areas of the world, those with epilepsy either have restrictions placed on their ability to drive or are not permitted to drive until they are free of seizures for a specific length of time.[24] The word epilepsy is from Ancient Greek ἐπιλαμβάνειν, "to seize, possess, or afflict".[25]
Epilepsy is characterized by a long-term risk of recurrent epileptic seizures.[26] These seizures may present in several ways depending on the parts of the brain involved and the person's age.[26][27]
The most common type (60%) of seizures are convulsive which involve involuntary muscle contractions.[27] Of these, one-third begin as generalized seizures from the start, affecting both hemispheres of the brain and impairing consciousness.[27] Two-thirds begin as focal seizures (which affect one hemisphere of the brain) which may progress to generalized seizures.[27] The remaining 40% of seizures are non-convulsive. An example of this type is the absence seizure, which presents as a decreased level of consciousness and usually lasts about 10 seconds.[2][28]
Certain experiences, known as auras often precede focal seizures.[29] The seizures can include sensory (visual, hearing, or smell), psychic, autonomic, and motor phenomena depending on which part of the brain is involved.[2] Muscle jerks may start in a specific muscle group and spread to surrounding muscle groups in which case it is known as a Jacksonian march.[30] Automatisms may occur, which are non-consciously generated activities and mostly simple repetitive movements like smacking the lips or more complex activities such as attempts to pick up something.[30]
There are six main types of generalized seizures:
They all involve loss of consciousness and typically happen without warning.
Tonic-clonic seizures occur with a contraction of the limbs followed by their extension and arching of the back which lasts 10–30 seconds (the tonic phase). A cry may be heard due to contraction of the chest muscles, followed by a shaking of the limbs in unison (clonic phase). Tonic seizures produce constant contractions of the muscles. A person often turns blue as breathing is stopped. In clonic seizures there is shaking of the limbs in unison. After the shaking has stopped it may take 10–30 minutes for the person to return to normal; this period is called the "postictal state" or "postictal phase." Loss of bowel or bladder control may occur during a seizure.[32] People experiencing a seizure may bite their tongue, either the tip or on the sides;[33] in tonic-clonic seizure, bites to the sides are more common.[33] Tongue bites are also relatively common in psychogenic non-epileptic seizures.[33] Psychogenic non-epileptic seizures are seizure like behavior without an associated synchronised electrical discharge on EEG and are considered a dissociative disorder.[33]
Myoclonic seizures involve very brief muscle spasms in either a few areas or all over.[34][35] These sometimes cause the person to fall, which can cause injury.[34] Absence seizures can be subtle with only a slight turn of the head or eye blinking with impaired consciousness;[2] typically, the person does not fall over and returns to normal right after it ends.[2] Atonic seizures involve losing muscle activity for greater than one second,[30] typically occurring on both sides of the body.[30] Rarer seizure types can cause involuntary unnatural laughter (gelastic), crying (dyscrastic), or more complex experiences such as déjà vu.[35]
About 6% of those with epilepsy have seizures that are often triggered by specific events and are known as reflex seizures.[36] Those with reflex epilepsy have seizures that are only triggered by specific stimuli.[37] Common triggers include flashing lights and sudden noises.[36] In certain types of epilepsy, seizures happen more often during sleep,[38] and in other types they occur almost only when sleeping.[39] Recently the International League against epilepsy has published new uniform guidelines for the classification of seizures as well as epilepsies along with their cause and comorbidities.[40]
Patients with epilepsy may experience seizure clusters which may be broadly defined as an acute deterioration in seizure control.[41] The prevalence of seizure clusters is uncertain given that studies have used different definitions to define them.[42] However, estimates suggest that the prevalence may range from 5% to 50% of epilepsy patients.[43] Refractory epilepsy patients who have a high seizure frequency are at the greatest risk for having seizure clusters.[44][45][46] Seizure clusters are associated with increased healthcare use, worse quality of life, impaired psychosocial functioning, and possibly increased mortality.[42][47] Benzodiazepines are used as an acute treatment for seizure clusters.[48]
After the active portion of a seizure (the ictal state) there is typically a period of recovery during which there is confusion, referred to as the postictal period, before a normal level of consciousness returns.[29] It usually lasts 3 to 15 minutes[49] but may last for hours.[50] Other common symptoms include feeling tired, headache, difficulty speaking, and abnormal behavior.[50] Psychosis after a seizure is relatively common, occurring in 6–10% of people.[51] Often people do not remember what happened during this time.[50] Localized weakness, known as Todd's paralysis, may also occur after a focal seizure. It would typically last for seconds to minutes but may rarely last for a day or two.[52]
Epilepsy can have adverse effects on social and psychological well-being.[27] These effects may include social isolation, stigmatization, or disability.[27] They may result in lower educational achievement and worse employment outcomes.[27] Learning disabilities are common in those with the condition, and especially among children with epilepsy.[27] The stigma of epilepsy can also affect the families of those with the disorder.[32]
Certain disorders occur more often in people with epilepsy, depending partly on the epilepsy syndrome present. These include depression, anxiety, obsessive–compulsive disorder (OCD),[53] and migraine.[54] Attention deficit hyperactivity disorder (ADHD) affects three to five times more children with epilepsy than children without the condition.[55] ADHD and epilepsy have significant consequences on a child's behavioral, learning, and social development.[56] Epilepsy is also more common in children with autism.[57]
Approximately, one-in-three people with epilepsy have a lifetime history of a psychiatric disorder.[58] There are believed to be multiple causes for this including pathophysiological changes related to the epilepsy itself as well as adverse experiences related to living with epilepsy (e.g., stigma, discrimination).[59] In addition, it is thought that the relationship between epilepsy and psychiatric disorders is not unilateral but rather bidirectional. For example, patients with depression have an increased risk for developing new-onset epilepsy.[60]
The presence of comorbid depression or anxiety in patients with epilepsy is associated with a poorer quality of life, increased mortality, increased healthcare use and a worse response to treatment (including surgical).[61][62][63][64] Anxiety disorders and depression may explain more variability in quality of life than seizure type or frequency.[65] There is evidence that both depression and anxiety disorders are underdiagnosed and undertreated in patients with epilepsy.[66]
Epilepsy can have both genetic and acquired causes, with the interaction of these factors in many cases.[67][68] Established acquired causes include serious brain trauma, stroke, tumours, and brain problems resulting from a previous infection.[67] In about 60% of cases, the cause is unknown.[27][32] Epilepsies caused by genetic, congenital, or developmental conditions are more common among younger people, while brain tumors and strokes are more likely in older people.[27]
Seizures may also occur as a consequence of other health problems;[31] if they occur right around a specific cause, such as a stroke, head injury, toxic ingestion, or metabolic problem, they are known as acute symptomatic seizures and are in the broader classification of seizure-related disorders rather than epilepsy itself.[69][70]
Genetics is believed to be involved in the majority of cases, either directly or indirectly.[15][71] Some epilepsies are due to a single gene defect (1–2%); most are due to the interaction of multiple genes and environmental factors.[15] Each of the single gene defects is rare, with more than 200 in all described.[72] Most genes involved affect ion channels, either directly or indirectly.[67] These include genes for ion channels , enzymes, GABA, and G protein-coupled receptors.[34]
In identical twins, if one is affected, there is a 50–60% chance that the other will also be affected.[15] In non-identical twins, the risk is 15%.[15] These risks are greater in those with generalized rather than focal seizures.[15] If both twins are affected, most of the time they have the same epileptic syndrome (70–90%).[15] Other close relatives of a person with epilepsy have a risk five times that of the general population.[73] Between 1 and 10% of those with Down syndrome and 90% of those with Angelman syndrome have epilepsy.[73]
Phakomatoses, also known as neurocutaneous disorders, are a group of multisystemic diseases that most prominently affect the skin and central nervous system. They are caused by defective development of the embryonic ectodermal tissue that is most often due to a single genetic mutation. The brain, as well as other neural tissue and the skin, are all derived from the ectoderm and thus defective development may result in epilepsy as well as other manifestations such as autism and intellectual disability. Some types of phakomatoses such as tuberous sclerosis complex and Sturge-Weber syndrome have a higher prevalence of epilepsy relative to others such as neurofibromatosis type 1.[74]
Tuberous sclerosis complex is an autosomal dominant disorder that is caused by mutations in either the TSC1 or TSC2 gene and it affects approximately 1 in 6,000–10,000 live births.[75][76] These mutations result in the upregulation of the mechanistic target of rapamycin (mTOR) pathway which leads to the growth of tumors in many organs including the brain, skin, heart, eyes and kidneys.[76] In addition, abnormal mTOR activity is believed to alter neural excitability.[77] The prevalence of epilepsy is estimated to be 80-90%.[74][77] The majority of cases of epilepsy present within the first 3 years of life and are medically refractory.[78] Relatively recent developments for the treatment of epilepsy in TSC patients include mTOR inhibitors, cannabidiol and vigabatrin. Epilepsy surgery is often pursued.
Sturge-Weber syndrome is caused by an activating somatic mutation in the GNAQ gene and it affects approximately 1 in 20,000–50,000 live births.[79] The mutation results in vascular malformations affecting the brain, skin and eyes. The typical presentation includes a facial port-wine birthmark, ocular angiomas and cerebral vascular malformations which are most often unilateral but are bilateral in 15% of cases.[80] The prevalence of epilepsy is 75-100% and is higher in those with bilateral involvement.[80] Seizures typically occur within the first two years of life and are refractory in nearly half of cases.[81] However, high rates of seizure freedom with surgery have been reported in as many as 83%.[82]
Neurofibromatosis type 1 is the most common phakomatoses and occurs in approximately 1 in 3,000 live births.[83] It is caused by autosomal dominant mutations in the Neurofibromin 1 gene. Clinical manifestations are variable but may include hyperpigmented skin marks, hamartomas of the iris called Lisch nodules, neurofibromas, optic pathway gliomas and cognitive impairment. The prevalence of epilepsy is estimated to be 4–7%.[84] Seizures are typically easier to control with anti-seizure medications relative to other phakomatoses but in some refractory cases surgery may need to be pursued.[85]
Epilepsy may occur as a result of several other conditions, including tumors, strokes, head trauma, previous infections of the central nervous system, genetic abnormalities, and as a result of brain damage around the time of birth.[31][32] Of those with brain tumors, almost 30% have epilepsy, making them the cause of about 4% of cases.[73] The risk is greatest for tumors in the temporal lobe and those that grow slowly.[73] Other mass lesions such as cerebral cavernous malformations and arteriovenous malformations have risks as high as 40–60%.[73] Of those who have had a stroke, 6–10% develop epilepsy.[86][87] Risk factors for post-stroke epilepsy include stroke severity, cortical involvement, hemorrhage and early seizures.[88][89] Between 6 and 20% of epilepsy is believed to be due to head trauma.[73] Mild brain injury increases the risk about two-fold while severe brain injury increases the risk seven-fold.[73] In those who have experienced a high-powered gunshot wound to the head, the risk is about 50%.[73]
Some evidence links epilepsy and celiac disease and non-celiac gluten sensitivity, while other evidence does not. There appears to be a specific syndrome that includes coeliac disease, epilepsy, and calcifications in the brain.[90][91] A 2012 review estimates that between 1% and 6% of people with epilepsy have coeliac disease while 1% of the general population has the condition.[91]
The risk of epilepsy following meningitis is less than 10%; it more commonly causes seizures during the infection itself.[73] In herpes simplex encephalitis the risk of a seizure is around 50%[73] with a high risk of epilepsy following (up to 25%).[92][93] A form of an infection with the pork tapeworm (cysticercosis), in the brain, is known as neurocysticercosis, and is the cause of up to half of epilepsy cases in areas of the world where the parasite is common.[73] Epilepsy may also occur after other brain infections such as cerebral malaria, toxoplasmosis, and toxocariasis.[73] Chronic alcohol use increases the risk of epilepsy: those who drink six units of alcohol per day have a 2.5-fold increase in risk.[73] Other risks include Alzheimer's disease, multiple sclerosis, and autoimmune encephalitis.[73] Getting vaccinated does not increase the risk of epilepsy.[73] Malnutrition is a risk factor seen mostly in the developing world, although it is unclear however if it is a direct cause or an association.[20] People with cerebral palsy have an increased risk of epilepsy, with half of people with spastic quadriplegia and spastic hemiplegia having the disease.[94]
Normally brain electrical activity is non-synchronous, as large numbers of neurons do not normally fire at the same time, but rather fire in order as signals travel throughout the brain.[2] Neuron activity is regulated by various factors both within the cell and the cellular environment. Factors within the neuron include the type, number and distribution of ion channels, changes to receptors and changes of gene expression.[95] Factors around the neuron include ion concentrations, synaptic plasticity and regulation of transmitter breakdown by glial cells.[95][96]
The exact mechanism of epilepsy is unknown,[97] but a little is known about its cellular and network mechanisms. However, it is unknown under which circumstances the brain shifts into the activity of a seizure with its excessive synchronization.[98][99] Changes in microRNAs (miRNAs) levels seems to play a leading role. MicroRNAs are a family of small non-coding RNAs that control the expression levels of multiple proteins by decreasing mRNA stability and translation, and could therefore be key regulatory mechanisms and therapeutic targets in epilepsy[100][101]
In epilepsy, the resistance of excitatory neurons to fire during this period is decreased.[11][2] This may occur due to changes in ion channels or inhibitory neurons not functioning properly.[2] This then results in a specific area from which seizures may develop, known as a "seizure focus".[2] Another mechanism of epilepsy may be the up-regulation of excitatory circuits or down-regulation of inhibitory circuits following an injury to the brain.[2][3] These secondary epilepsies occur through processes known as epileptogenesis.[2][3] Failure of the blood–brain barrier may also be a causal mechanism as it would allow substances in the blood to enter the brain.[102]
There is evidence that epileptic seizures are usually not a random event. Seizures are often brought on by factors (also known as triggers) such as stress, excessive alcohol use, flickering light, or a lack of sleep, among others. The term seizure threshold is used to indicate the amount of stimulus necessary to bring about a seizure; this threshold is lowered in epilepsy.[98]
In epileptic seizures a group of neurons begin firing in an abnormal, excessive,[27] and synchronized manner.[2] This results in a wave of depolarization known as a paroxysmal depolarizing shift.[103] Normally, after an excitatory neuron fires it becomes more resistant to firing for a period of time.[2] This is due in part to the effect of inhibitory neurons, electrical changes within the excitatory neuron, and the negative effects of adenosine.[2]
Focal seizures begin in one area of the brain while generalized seizures begin in both hemispheres.[31] Some types of seizures may change brain structure, while others appear to have little effect.[104] Gliosis, neuronal loss, and atrophy of specific areas of the brain are linked to epilepsy but it is unclear if epilepsy causes these changes or if these changes result in epilepsy.[104]
The seizures can be described on different scales, from the cellular level[105] to the whole brain.[106] These are several concomitant factor, which on different scale can "drive" the brain to pathological states and trigger a seizure.
The diagnosis of epilepsy is typically made based on observation of the seizure onset and the underlying cause.[27] An electroencephalogram (EEG) to look for abnormal patterns of brain waves and neuroimaging (CT scan or MRI) to look at the structure of the brain are also usually part of the initial investigations.[27] While figuring out a specific epileptic syndrome is often attempted, it is not always possible.[27] Video and EEG monitoring may be useful in difficult cases.[107]
Epilepsy is a disorder of the brain defined by any of the following conditions:[10]
Furthermore, epilepsy is considered to be resolved for individuals who had an age-dependent epilepsy syndrome but are now past that age or those who have remained seizure-free for the last 10 years, with no seizure medicines for the last 5 years.[10]
This 2014 definition of the International League Against Epilepsy[10] (ILAE) is a clarification of the ILAE 2005 conceptual definition, according to which epilepsy is "a disorder of the brain characterized by an enduring predisposition to generate epileptic seizures and by the neurobiologic, cognitive, psychological, and social consequences of this condition. The definition of epilepsy requires the occurrence of at least one epileptic seizure."[108][109]
It is, therefore, possible to outgrow epilepsy or to undergo treatment that causes epilepsy to be resolved, but with no guarantee that it will not return. In the definition, epilepsy is now called a disease, rather than a disorder. This was a decision of the executive committee of the ILAE, taken because the word "disorder," while perhaps having less stigma than does "disease," also does not express the degree of seriousness that epilepsy deserves.[10]
The definition is practical in nature and is designed for clinical use. In particular, it aims to clarify when an "enduring predisposition" according to the 2005 conceptual definition is present. Researchers, statistically minded epidemiologists, and other specialized groups may choose to use the older definition or a definition of their own devising. The ILAE considers doing so is perfectly allowable, so long as it is clear what definition is being used.[10]
The ILAE definition for one seizure needs an understanding of projecting an enduring predisposition to the generation of epileptic seizures.[10] WHO for instance chooses to just use the traditional definition of two unprovoked seizures.[13]
In contrast to the classification of seizures which focuses on what happens during a seizure, the classification of epilepsies focuses on the underlying causes. When a person is admitted to hospital after an epileptic seizure the diagnostic workup results preferably in the seizure itself being classified (e.g. tonic-clonic) and in the underlying disease being identified (e.g. hippocampal sclerosis).[107] The name of the diagnosis finally made depends on the available diagnostic results and the applied definitions and classifications (of seizures and epilepsies) and its respective terminology.
The International League Against Epilepsy (ILAE) provided a classification of the epilepsies and epileptic syndromes in 1989 as follows:[110]
This classification was widely accepted but has also been criticized mainly because the underlying causes of epilepsy (which are a major determinant of clinical course and prognosis) were not covered in detail.[111] In 2010 the ILAE Commission for Classification of the Epilepsies addressed this issue and divided epilepsies into three categories (genetic, structural/metabolic, unknown cause)[112] that were refined in their 2011 recommendation into four categories and a number of subcategories reflecting recent technologic and scientific advances.[113]
Cases of epilepsy may be organized into epilepsy syndromes by the specific features that are present. These features include the age that seizure begin, the seizure types, EEG findings, among others. Identifying an epilepsy syndrome is useful as it helps determine the underlying causes as well as what anti-seizure medication should be tried.[31][116]
The ability to categorize a case of epilepsy into a specific syndrome occurs more often with children since the onset of seizures is commonly early.[70] Less serious examples are benign rolandic epilepsy (2.8 per 100,000), childhood absence epilepsy (0.8 per 100,000) and juvenile myoclonic epilepsy (0.7 per 100,000).[70] Severe syndromes with diffuse brain dysfunction caused, at least partly, by some aspect of epilepsy, are also referred to as developmental and epileptic encephalopathies. These are associated with frequent seizures that are resistant to treatment and cognitive dysfunction, for instance Lennox–Gastaut syndrome (1–2% of all persons with epilepsy),[117] Dravet syndrome(1: 15000-40000 worldwide[118]), and West syndrome(1–9: 100000[119]).[120] Genetics is believed to play an important role in epilepsies by a number of mechanisms. Simple and complex modes of inheritance have been identified for some of them. However, extensive screening have failed to identify many single gene variants of large effect.[121] More recent exome and genome sequencing studies have begun to reveal a number of de novo gene mutations that are responsible for some epileptic encephalopathies, including CHD2 and SYNGAP1[122][123][124] and DNM1, GABBR2, FASN and RYR3.[125]
Syndromes in which causes are not clearly identified are difficult to match with categories of the current classification of epilepsy. Categorization for these cases was made somewhat arbitrarily.[113] The idiopathic (unknown cause) category of the 2011 classification includes syndromes in which the general clinical features and/or age specificity strongly point to a presumed genetic cause.[113] Some childhood epilepsy syndromes are included in the unknown cause category in which the cause is presumed genetic, for instance benign rolandic epilepsy.[113] Clinical syndromes in which epilepsy is not the main feature (e.g. Angelman syndrome) were categorized symptomatic but it was argued to include these within the category idiopathic.[113] Classification of epilepsies and particularly of epilepsy syndromes will change with advances in research.[113]
An electroencephalogram (EEG) can assist in showing brain activity suggestive of an increased risk of seizures. It is only recommended for those who are likely to have had an epileptic seizure on the basis of symptoms. In the diagnosis of epilepsy, electroencephalography may help distinguish the type of seizure or syndrome present.[126] In children it is typically only needed after a second seizure unless specified by a specialist. It cannot be used to rule out the diagnosis and may be falsely positive in those without the disease.[126] In certain situations it may be useful to perform the EEG while the affected individual is sleeping or sleep deprived.[107]
Diagnostic imaging by CT scan and MRI is recommended after a first non-febrile seizure to detect structural problems in and around the brain.[107] MRI is generally a better imaging test except when bleeding is suspected, for which CT is more sensitive and more easily available.[21] If someone attends the emergency room with a seizure but returns to normal quickly, imaging tests may be done at a later point.[21] If a person has a previous diagnosis of epilepsy with previous imaging, repeating the imaging is usually not needed even if there are subsequent seizures.[107][127]
For adults, the testing of electrolyte, blood glucose and calcium levels is important to rule out problems with these as causes.[107] An electrocardiogram can rule out problems with the rhythm of the heart.[107] A lumbar puncture may be useful to diagnose a central nervous system infection but is not routinely needed.[21] In children additional tests may be required such as urine biochemistry and blood testing looking for metabolic disorders.[107][128] Together with EEG and neuroimaging, genetic testing is becoming one of the most important diagnostic technique for epilepsy, as a diagnosis might be achieved in a relevant proportion of cases with severe epilepsies, both in children and adults.[129] For those with negative genetic testing, in some it might be important to repeat or re-analyze previous genetic studies after 2–3 years.[130]
A high blood prolactin level within the first 20 minutes following a seizure may be useful to help confirm an epileptic seizure as opposed to psychogenic non-epileptic seizure.[131][132] Serum prolactin level is less useful for detecting focal seizures.[133] If it is normal an epileptic seizure is still possible[132] and a serum prolactin does not separate epileptic seizures from syncope.[134] It is not recommended as a routine part of the diagnosis of epilepsy.[107]
Diagnosis of epilepsy can be difficult. A number of other conditions may present very similar signs and symptoms to seizures, including syncope, hyperventilation, migraines, narcolepsy, panic attacks and psychogenic non-epileptic seizures (PNES).[135][136] In particular, syncope can be accompanied by a short episode of convulsions.[137] Nocturnal frontal lobe epilepsy, often misdiagnosed as nightmares, was considered to be a parasomnia but later identified to be an epilepsy syndrome.[138] Attacks of the movement disorder paroxysmal dyskinesia may be taken for epileptic seizures.[139] The cause of a drop attack can be, among many others, an atonic seizure.[136]
Children may have behaviors that are easily mistaken for epileptic seizures but are not. These include breath-holding spells, bedwetting, night terrors, tics and shudder attacks.[136] Gastroesophageal reflux may cause arching of the back and twisting of the head to the side in infants, which may be mistaken for tonic-clonic seizures.[136]
Misdiagnosis is frequent (occurring in about 5 to 30% of cases).[27] Different studies showed that in many cases seizure-like attacks in apparent treatment-resistant epilepsy have a cardiovascular cause.[137][140] Approximately 20% of the people seen at epilepsy clinics have PNES[21] and of those who have PNES about 10% also have epilepsy;[141] separating the two based on the seizure episode alone without further testing is often difficult.[141]
While many cases are not preventable, efforts to reduce head injuries,[7][11] provide good care around the time of birth, and reduce environmental parasites such as the pork tapeworm may be effective.[32] Efforts in one part of Central America to decrease rates of pork tapeworm resulted in a 50% decrease in new cases of epilepsy.[20]
Epilepsy can be dangerous when seizure occurs at certain times. The risk of drowning or being involved in a motor vehicle collision is higher. It is also found that people with epilepsy are more likely to have psychological problems.[142] Other complications include aspiration pneumonia and difficulty learning.[143]
Epilepsy is usually treated with daily medication once a second seizure has occurred,[27][107] while medication may be started after the first seizure in those at high risk for subsequent seizures.[107] Supporting people's self management of their condition may be useful.[144] In drug-resistant cases different management options may be looked at including a special diet, the implantation of a neurostimulator, or neurosurgery.
Rolling people with an active tonic-clonic seizure onto their side and into the recovery position helps prevent fluids from getting into the lungs.[145] Putting fingers, a bite block or tongue depressor in the mouth is not recommended as it might make the person vomit or result in the rescuer being bitten.[29][145] Efforts should be taken to prevent further self-injury.[29] Spinal precautions are generally not needed.[145]
If a seizure lasts longer than 5 minutes or if there are more than two seizures in 5 minutes without a return to a normal level of consciousness between them, it is considered a medical emergency known as status epilepticus.[107][146] This may require medical help to keep the airway open and protected;[107] a nasopharyngeal airway may be useful for this.[145] At home the recommended initial medication for seizure of a long duration is midazolam placed in the nose or mouth.[147] Diazepam may also be used rectally.[147] In hospital, intravenous lorazepam is preferred.[107]
If two doses of benzodiazepines are not effective, other medications such as phenytoin are recommended.[107] ] Convulsive status epilepticus that does not respond to initial treatment typically requires admission to the intensive care unit and treatment with stronger agents such as midazolam infusion, ketamine, thiopentone or Propofol.[107] Most institutions have a preferred pathway or protocol to be used in a seizure emergency like status epilepticus.[107] These protocols have been found to be effective in reducing time to delivery of treatment.[107]
The mainstay treatment of epilepsy is anticonvulsant medications, possibly for the person's entire life.[11][27] The choice of anticonvulsant is based on seizure type, epilepsy syndrome, other medications used, other health problems, and the person's age and lifestyle.[147] A single medication is recommended initially;[148] if this is not effective, switching to a single other medication is recommended.[12][107] Two medications at once is recommended only if a single medication does not work.[107] In about half, the first agent is effective; a second single agent helps in about 13% and a third or two agents at the same time may help an additional 4%.[149] About 30% of people continue to have seizures despite anticonvulsant treatment.[7]
There are a number of medications available including phenytoin, carbamazepine and valproate. Evidence suggests that phenytoin, carbamazepine, and valproate may be equally effective in both focal and generalized seizures.[150][151] Controlled release carbamazepine appears to work as well as immediate release carbamazepine, and may have fewer side effects.[152]  In the United Kingdom, carbamazepine or lamotrigine are recommended as first-line treatment for focal seizures, with levetiracetam and valproate as second-line due to issues of cost and side effects.[107][153] Valproate is recommended first-line for generalized seizures with lamotrigine being second-line.[107] In those with absence seizures, ethosuximide or valproate are recommended; valproate is particularly effective in myoclonic seizures and tonic or atonic seizures.[107] If seizures are well-controlled on a particular treatment, it is not usually necessary to routinely check the medication levels in the blood.[107]
The least expensive anticonvulsant is phenobarbital at around US$5 a year.[20] The World Health Organization gives it a first-line recommendation in the developing world and it is commonly used there.[154][155] Access however may be difficult as some countries label it as a controlled drug.[20]
Adverse effects from medications are reported in 10 to 90% of people, depending on how and from whom the data is collected.[156] Most adverse effects are dose-related and mild.[156] Some examples include mood changes, sleepiness, or an unsteadiness in gait.[156] Certain medications have side effects that are not related to dose such as rashes, liver toxicity, or suppression of the bone marrow.[156] Up to a quarter of people stop treatment due to adverse effects.[156] Some medications are associated with birth defects when used in pregnancy.[107] Many of the common used medications, such as valproate, phenytoin, carbamazepine, phenobarbital, and gabapentin have been reported to cause increased risk of birth defects,[157] especially when used during the first trimester.[158] Despite this, treatment is often continued once effective, because the risk of untreated epilepsy is believed to be greater than the risk of the medications.[158] Among the antiepileptic medications, levetiracetam and lamotrigine seem to carry the lowest risk of causing birth defects.[157]
Slowly stopping medications may be reasonable in some people who do not have a seizure for two to four years; however, around a third of people have a recurrence, most often during the first six months.[107][159] Stopping is possible in about 70% of children and 60% of adults.[32] Measuring medication levels is not generally needed in those whose seizures are well controlled.[127]
Epilepsy surgery should be considered for any person with epilepsy who is medically refractory.[17] Patients are evaluated on a 'Case by case' epilepsy in centres that are familiar and have expertise in epilepsy surgery.[17] Epilepsy surgery may be an option for people with focal seizures that remain a problem despite other treatments.[160][161] These other treatments include at least a trial of two or three medications.[162] The goal of surgery has been total control of seizures.[163] However most physicians believe that even palliative surgery where the burden of seizures is reduced significantly can help in achieving developmental progress or reversal of developmental stagnation in children with drug resistant epilepsy.and this may be achieved in 60–70% of cases.[162] Common procedures include cutting out the hippocampus via an anterior temporal lobe resection, removal of tumors, and removing parts of the neocortex.[162] Some procedures such as a corpus callosotomy are attempted in an effort to decrease the number of seizures rather than cure the condition.[162] Following surgery, medications may be slowly withdrawn in many cases.[162][160]
Neurostimulation via neuro-cybernetic prosthesis implantation, may be another option in those who are not candidates for surgery, providing chronic, pulsatile electrical stimulation of specific nerve or brain regions, alongside standard care.[107] Three types have been used in those who do not respond to medications: vagus nerve stimulation (VNS), anterior thalamic stimulation, and closed-loop responsive stimulation (RNS).[5][164][165]
Non-pharmacological modulation of neurotransmitters via high-level VNS (h-VNS) may reduce seizure frequency in children and adults who do not respond to medical and/or surgical therapy,[12] when compared with low-level VNS (l-VNS).[165] In a 2022 Cochrane review of 4 randomized controlled trials, with moderate certainty of evidence, people receiving h-VNS treatment were 73% more likely (13% more likely to 164% more likely) to experience a reduction in seizure frequency by at least 50% (the minimum threshold defined for individual clinical response).[165] Potentially 249 (163 to 380) per 1000 people with drug-resistant epilepsy may achieve a 50% reduction in seizures following h-VNS, benefiting an additional 105 per 1000 people compared with l-VNS.[165]
This outcome was limited by the number of studies available, and the quality of one trial in particular, wherein 3 people received l-VNS in error. A sensitivity analysis suggested that the best case scenario was that the likelihood of clinical response to h-VNS may be 91% (27% to 189%) higher than those receiving l-VNS. In the worst-case scenario, the likelihood of clinical response to h-VNS was still 61% higher (7% higher to 143% higher) than l-VNS.[165]
Despite the potential benefit for h-VNS treatment, the Cochrane review also found that the risk of several adverse-effects was greater than those receiving l-VNS. There was moderate certainty of evidence that voice alteration or hoarseness risk may be 2.17(1.49 to 3.17) fold higher than people receiving l-VNS. Dyspnoea risk was also 2.45 (1.07 to 5.60) times that of l-VNS recipients, although the low number of events and studies meant that the certainty of evidence was low. The risk of rebound-withdrawal symptoms, coughing, pain and paraesthesia was unclear.[165]
There is promising evidence that a ketogenic diet[12](high-fat, low-carbohydrate, adequate-protein) decreases the number of seizures and eliminates seizures in some; however, further research is necessary.[6] A 2022 systematic review of the literature has found some evidence to support that a ketogenic diet or Modified Atkins Diet can be helpful in the treatment of epilepsy in some infants.[166] It is a reasonable option in those who have epilepsy that is not improved with medications and for whom surgery is not an option.[6] About 10% stay on the diet for a few years due to issues of effectiveness and tolerability.[6] Side effects include stomach and intestinal problems in 30%, and there are long-term concerns about heart disease.[6] Less radical diets are easier to tolerate and may be effective.[6] It is unclear why this diet works.[167] In people with coeliac disease or non-celiac gluten sensitivity and occipital calcifications, a gluten-free diet may decrease the frequency of seizures.[91]
Avoidance therapy consists of minimizing or eliminating triggers. For example, those who are sensitive to light may have success with using a small television, avoiding video games, or wearing dark glasses.[168] Operant-based biofeedback based on the EEG waves has some support in those who do not respond to medications.[169] Psychological methods should not, however, be used to replace medications.[107]
Exercise has been proposed as possibly useful for preventing seizures,[170] with some data to support this claim.[171] Some dogs, commonly referred to as seizure dogs, may help during or after a seizure.[172][173] It is not clear if dogs have the ability to predict seizures before they occur.[174]
There is moderate-quality evidence supporting the use of psychological interventions along with other treatments in epilepsy.[175] This can improve quality of life, enhance emotional wellbeing, and reduce fatigue in adults and adolescents.[175] Psychological interventions may also improve seizure control for some individuals by promoting self-management and adherence.[175]
As an add-on therapy in those who are not well controlled with other medications, cannabidiol appears to be useful in some children.[176][177] In 2018 the FDA approved this product for Lennox–Gastaut syndrome and Dravet syndrome.[178]
There are a few studies on the use of dexamethasone for the successful treatment of drug-resistant seizures in both adults and children.[179]
Alternative medicine, including acupuncture,[180] routine vitamins,[181] and yoga,[182] have no reliable evidence to support their use in epilepsy. Melatonin, as of 2016[update], is insufficiently supported by evidence.[183] The trials were of poor methodological quality and it was not possible to draw any definitive conclusions.[183]
Several supplements (with varied reliabilities of evidence) have been reported to be helpful for drug-resistant epilepsy. These include high-dose Omega-3, berberine, Manuka honey, Reishi and Lion's Mane mushrooms, curcumin,[184] vitamin E, coenzyme Q-10, and resveratrol. The reason these can work (in theory) is that they reduce inflammation or oxidative stress, two of the major mechanism contributing to epilepsy.[185]
Women of child-bearing age, including those with epilepsy, are at risk of unintended pregnancies if they are not using an effective form of contraception.[186]  Women with epilepsy may experience a temporary increase in seizure frequency when they begin hormonal contraception.[186]
Some anti-seizure medications interact with enzymes in the liver and cause the drugs in hormonal contraception to be broken down more quickly. These enzyme inducing drugs make hormonal contraception less effective, and this is particularly hazardous if the anti-seizure medication is associated with birth defects.[187] Potent enzyme inducing anti-seizure medications include carbamazepine, eslicarbazepine acetate, oxcarbazepine, phenobarbital, phenytoin, primidone, and rufinamide. The drugs perampanel and topiramate can be enzyme inducing at higher doses.[188] Conversely, hormonal contraception can lower the amount of the anti-seizure medication lamotrigine circulating in the body, making it less effective.[186] The failure rate of oral contraceptives, when used correctly, is 1%, but this increases to between 3–6% in women with epilepsy.[187] Overall, intrauterine devices (IUDs) are preferred for women with epilepsy who are not intending to become pregnant.[186]
Women with epilepsy, especially if they have other medical conditions, may have a slightly lower, but still high, chance of becoming pregnant.[186]  Women with infertility have about the same chance of success with in vitro fertilisation or other forms of assisted reproductive technology as women without epilepsy.[186]  There may be a higher risk of pregnancy loss.[186]
Once pregnant, there are two main concerns related to pregnancy.  The first concern is about the risk of seizures during pregnancy, and the second concern is that the anti-seizure medications may result in birth defects.[157]  Most women with epilepsy must continue treatment with anti-seizure drugs, and the treatment goal is to balance the need to prevent seizures with the need to prevent drug-induced birth defects.[186][189]
Pregnancy does not seem to change seizure frequency very much.[186]  When seizures happen, however, they can cause some pregnancy complications, such as pre-term births or the babies being smaller than usual when they are born.[186]
All pregnancies have a risk of birth defects, e.g., due to smoking during pregnancy.[186]  In addition to this typical level of risk, some anti-seizure drugs significantly increase the risk of birth defects and intrauterine growth restriction, as well as developmental, neurocognitive, and behavioral disorders.[189]  Most women with epilepsy receive safe and effective treatment and have typical, healthy children.[189]  The highest risks are associated with specific anti-seizure drugs, such as valproic acid and carbamazepine, and with higher doses.[157][186] Folic acid supplementation, such as through prenatal vitamins, reduced the risk.[186]  Planning pregnancies in advance gives women with epilepsy an opportunity to switch to a lower-risk treatment program and reduced drug doses.[186]
Although anti-seizure drugs can be found in breast milk, women with epilepsy can breastfeed their babies, and the benefits usually outweigh the risks.[186]
Epilepsy cannot usually be cured, but medication can control seizures effectively in about 70% of cases.[7] Of those with generalized seizures, more than 80% can be well controlled with medications while this is true in only 50% of people with focal seizures.[5] One predictor of long-term outcome is the number of seizures that occur in the first six months.[27] Other factors increasing the risk of a poor outcome include little response to the initial treatment, generalized seizures, a family history of epilepsy, psychiatric problems, and waves on the EEG representing generalized epileptiform activity.[190] In the developing world, 75% of people are either untreated or not appropriately treated.[32] In Africa, 90% do not get treatment.[32] This is partly related to appropriate medications not being available or being too expensive.[32]
People with epilepsy are at an increased risk of death.[191] This increase is between 1.6 and 4.1 fold greater than that of the general population.[192] The greatest increase in mortality from epilepsy is among the elderly.[192] Those with epilepsy due to an unknown cause have little increased risk.[192]
Mortality is often related to: the underlying cause of the seizures, status epilepticus, suicide, trauma, and sudden unexpected death in epilepsy (SUDEP).[191] Death from status epilepticus is primarily due to an underlying problem rather than missing doses of medications.[191] The risk of suicide is between 2 and 6 times higher in those with epilepsy;[193][194] the cause of this is unclear.[193] SUDEP appears to be partly related to the frequency of generalized tonic-clonic seizures[195] and accounts for about 15% of epilepsy-related deaths;[190] it is unclear how to decrease its risk.[195]
Risk factors for SUDEP include nocturnal generalized tonic-clonic seizures, seizures, sleeping alone and medically intractable epilepsy.[196]
In the United Kingdom, it is estimated that 40–60% of deaths are possibly preventable.[27] In the developing world, many deaths are due to untreated epilepsy leading to falls or status epilepticus.[20]
Epilepsy is one of the most common serious neurological disorders[197] affecting about 39 million people as of 2015[update].[8] It affects 1% of the population by age 20 and 3% of the population by age 75.[18] It is more common in males than females with the overall difference being small.[20][70] Most of those with the disorder (80%) are in low income populations[198] or the developing world.[32]
The estimated prevalence of active epilepsy (as of 2012[update]) is in the range 3–10 per 1,000, with active epilepsy defined as someone with epilepsy who has had a least one unprovoked seizure in the last five years.[70][199] Epilepsy begins each year in 40–70 per 100,000 in developed countries and 80–140 per 100,000 in developing countries.[32] Poverty is a risk and includes both being from a poor country and being poor relative to others within one's country.[20] In the developed world epilepsy most commonly starts either in the young or in the old.[20] In the developing world its onset is more common in older children and young adults due to the higher rates of trauma and infectious diseases.[20] In developed countries the number of cases a year has decreased in children and increased among the elderly between the 1970s and 2003.[199] This has been attributed partly to better survival following strokes in the elderly.[70]
The oldest medical records show that epilepsy has been affecting people at least since the beginning of recorded history.[200] Throughout ancient history, the disease was thought to be a spiritual condition.[200] The world's oldest description of an epileptic seizure comes from a text in Akkadian (a language used in ancient Mesopotamia) and was written around 2000 BC.[25] The person described in the text was diagnosed as being under the influence of a moon god, and underwent an exorcism.[25] Epileptic seizures are listed in the Code of Hammurabi (c. 1790 BC) as reason for which a purchased slave may be returned for a refund,[25] and the Edwin Smith Papyrus (c. 1700 BC) describes cases of individuals with epileptic convulsions.[25]
The oldest known detailed record of the disease itself is in the Sakikku, a Babylonian cuneiform medical text from 1067–1046 BC.[200] This text gives signs and symptoms, details treatment and likely outcomes,[25] and describes many features of the different seizure types.[200] As the Babylonians had no biomedical understanding of the nature of disease, they attributed the seizures to possession by evil spirits and called for treating the condition through spiritual means.[200] Around 900 BC, Punarvasu Atreya described epilepsy as loss of consciousness;[201] this definition was carried forward into the Ayurvedic text of Charaka Samhita (c. 400 BC).[202]
The ancient Greeks had contradictory views of the disease. They thought of epilepsy as a form of spiritual possession, but also associated the condition with genius and the divine. One of the names they gave to it was the sacred disease (Greek: ἠ ἱερὰ νόσος).[25][203] Epilepsy appears within Greek mythology: it is associated with the Moon goddesses Selene and Artemis, who afflicted those who upset them. The Greeks thought that important figures such as Julius Caesar and Hercules had the disease.[25] The notable exception to this divine and spiritual view was that of the school of Hippocrates. In the fifth century BC, Hippocrates rejected the idea that the disease was caused by spirits. In his landmark work On the Sacred Disease, he proposed that epilepsy was not divine in origin and instead was a medically treatable problem originating in the brain.[25][200] He accused those of attributing a sacred cause to the disease of spreading ignorance through a belief in superstitious magic.[25] Hippocrates proposed that heredity was important as a cause, described worse outcomes if the disease presents at an early age, and made note of the physical characteristics as well as the social shame associated with it.[25] Instead of referring to it as the sacred disease, he used the term great disease, giving rise to the modern term grand mal, used for tonic–clonic seizures.[25] Despite his work detailing the physical origins of the disease, his view was not accepted at the time.[200] Evil spirits continued to be blamed until at least the 17th century.[200]
In Ancient Rome people did not eat or drink with the same pottery as that used by someone who was affected.[204] People of the time would spit on their chest believing that this would keep the problem from affecting them.[204] According to Apuleius and other ancient physicians, to detect epilepsy, it was common to light a piece of gagates, whose smoke would trigger the seizure.[205] Occasionally a spinning potter's wheel was used, perhaps a reference to photosensitive epilepsy.[206]
In most cultures, persons with epilepsy have been stigmatized, shunned, or even imprisoned. As late as in the second half of the 20th century, in Tanzania and other parts of Africa epilepsy was associated with possession by evil spirits, witchcraft, or poisoning and was believed by many to be contagious.[207] In the Salpêtrière, the birthplace of modern neurology, Jean-Martin Charcot found people with epilepsy side by side with the mentally ill, those with chronic syphilis, and the criminally insane.[208] In Ancient Rome, epilepsy was known as the morbus comitialis or "disease of the assembly hall" and was seen as a curse from the gods. In northern Italy, epilepsy was traditionally known as Saint Valentine's malady.[209] In at least the 1840s in the United States of America, epilepsy was known as the falling sickness or the falling fits, and was considered a form of medical insanity.[210] Around the same time period, epilepsy was known in France as the French: haut-mal, lit. 'high evil', French: mal-de terre, lit. 'earthen sickness', French: mal de Saint Jean, lit. 'Saint John's sickness', French: mal des enfans, lit. 'child sickness', and French: mal-caduc, lit. 'falling sickness'.[210] Patients of epilepsy in France were also known as French: tombeurs, lit. 'people who fall', due to the seizures and loss of consciousness in an epileptic episode.[210]
In the mid-19th century, the first effective anti-seizure medication, bromide, was introduced.[156] The first modern treatment, phenobarbital, was developed in 1912, with phenytoin coming into use in 1938.[211]
Social stigma is commonly experienced, around the world, by those with epilepsy.[13][212] It can affect people economically, socially and culturally.[212] In India and China, epilepsy may be used as justification to deny marriage.[32] People in some areas still believe those with epilepsy to be cursed.[20] In parts of Africa, such as Tanzania and Uganda, epilepsy is claimed to be associated with possession by evil spirits, witchcraft, or poisoning and is incorrectly believed by many to be contagious.[207][20] Before 1971 in the United Kingdom, epilepsy was considered grounds for the annulment of marriage.[32] The stigma may result in some people with epilepsy denying that they have ever had seizures.[70]
Seizures result in direct economic costs of about one billion dollars in the United States.[21] Epilepsy resulted in economic costs in Europe of around 15.5 billion euros in 2004.[27] In India epilepsy is estimated to result in costs of US$1.7 billion or 0.5% of the GDP.[32] It is the cause of about 1% of emergency department visits (2% for emergency departments for children) in the United States.[213]
Those with epilepsy are at about twice the risk of being involved in a motor vehicular collision and thus in many areas of the world are not allowed to drive or only able to drive if certain conditions are met.[24] Diagnostic delay has been suggested to be a cause of some potentially avoidable motor vehicle collisions since at least one study showed that most motor vehicle accidents occurred in those with undiagnosed non-motor seizures as opposed to those with motor seizures at epilepsy onset.[214] In some places physicians are required by law to report if a person has had a seizure to the licensing body while in others the requirement is only that they encourage the person in question to report it himself.[24] Countries that require physician reporting include Sweden, Austria, Denmark and Spain.[24] Countries that require the individual to report include the UK and New Zealand, and physicians may report if they believe the individual has not already.[24] In Canada, the United States and Australia the requirements around reporting vary by province or state.[24] If seizures are well controlled most feel allowing driving is reasonable.[215] The amount of time a person must be free from seizures before he can drive varies by country.[215] Many countries require one to three years without seizures.[215] In the United States the time needed without a seizure is determined by each state and is between three months and one year.[215]
Those with epilepsy or seizures are typically denied a pilot license.[216]
There are organizations that provide support for people and families affected by epilepsy. The Out of the Shadows campaign, a joint effort by the World Health Organization, the ILAE and the International Bureau for Epilepsy, provides help internationally.[32] In the United States, the Epilepsy Foundation is a national organization that works to increase the acceptance of those with the disorder, their ability to function in society and to promote research for a cure.[222] The Epilepsy Foundation, some hospitals, and some individuals also run support groups in the United States.[223] In Australia, the Epilepsy Foundation provides support, delivers education and training and funds research for people living with epilepsy.
International Epilepsy Day (World Epilepsy Day) began in 2015 and occurs on the second Monday in February.[224][225]
Purple Day, a different world-wide epilepsy awareness day for epilepsy, was initiated by a nine-year-old Canadian named Cassidy Megan in 2008, and is every year on 26 March.[226]
Seizure prediction refers to attempts to forecast epileptic seizures based on the EEG before they occur.[227] As of 2011[update], no effective mechanism to predict seizures has been developed.[227] Although no effective device that can predict seizures is available, the science behind seizure prediction and ability to deliver such a tool has made progress.
Kindling, where repeated exposures to events that could cause seizures eventually causes seizures more easily, has been used to create animal models of epilepsy.[228]
Different animal models of epilepsy have been characterized in rodents that recapitulate the EEG and behavioral concomitants of different forms of epilepsy, in particular the occurrence of recurrent spontaneous seizures.[229] Because epileptic seizures of different kinds are observed naturally in some of these animals, strains of mice and rats have been selected to be used as genetic models of epilepsy. In particular, several lines of mice and rats display spike-and-wave discharges when EEG recorded and have been studied to understand Absence Epilepsy.[230] Among these models, the strain of GAERS (Genetic Absence Epilepsy Rats from Strasbourg) was characterized in the 80' and has helped to understand the mechanisms underlying Childhood Absence Epilepsy.[231]
One of the hypotheses present in the literature is based on inflammatory pathways. Studies supporting this mechanism revealed that inflammatory, glycolipid, and oxidative factors are higher in epilepsy patients, especially those with generalized epilepsy.[232]
Gene therapy is being studied in some types of epilepsy.[233] Medications that alter immune function, such as intravenous immunoglobulins, may reduce the frequency of seizures when including in normal care as an "add-on" therapy, however further research is required to determine whether these medications are very well tolerated in children and in adults with epilepsy.[234] Noninvasive stereotactic radiosurgery is, as of 2012[update], being compared to standard surgery for certain types of epilepsy.[235]
Epilepsy occurs in a number of other animals including dogs and cats; it is in fact the most common brain disorder in dogs.[236] It is typically treated with anticonvulsants such as phenobarbital or bromide in dogs and phenobarbital in cats.[236] Imepitoin is also used in dogs.[237] While generalized seizures in horses are fairly easy to diagnose, it may be more difficult in non-generalized seizures and EEGs may be useful.[238]
Creutzfeldt–Jakob disease


Crohn's disease is a type of inflammatory bowel disease (IBD) that may affect any segment of the gastrointestinal tract.[3] Symptoms often include abdominal pain, diarrhea (which may be bloody if inflammation is severe), fever, abdominal distension, and weight loss.[1][3] Complications outside of the gastrointestinal tract may include anemia, skin rashes, arthritis, inflammation of the eye, and fatigue.[1] The skin rashes may be due to infections as well as pyoderma gangrenosum or erythema nodosum.[1] Bowel obstruction may occur as a complication of chronic inflammation, and those with the disease are at greater risk of colon cancer and small bowel cancer.[1]
Although the precise causes of Crohn's disease (CD) are unknown, it is believed to be caused by a combination of environmental, immune, and bacterial factors in genetically susceptible individuals.[3][8][9][10] It results in a chronic inflammatory disorder, in which the body's immune system defends the gastrointestinal tract, possibly targeting microbial antigens.[9][11] While Crohn's is an immune-related disease, it does not appear to be an autoimmune disease (the immune system is not triggered by the body itself).[12] The exact underlying immune problem is not clear; however, it may be an immunodeficiency state.[11][13][14]
About half of the overall risk is related to genetics, with more than 70 genes involved.[1][15] Tobacco smokers are twice as likely to develop Crohn's disease as nonsmokers.[4] It often begins after gastroenteritis.[1] Diagnosis is based on biopsy and appearance of the bowel wall, medical imaging, and description of the disease.[1] Other conditions with similar symptoms include irritable bowel syndrome and Behçet's disease.[1]
There is no known cure for Crohn's disease.[1][3] Treatment options are intended to help with symptoms, maintain remission, and prevent relapse.[1] In those newly diagnosed, a corticosteroid may be used for a brief period of time to rapidly improve symptoms, alongside another medication such as either methotrexate or a thiopurine used to prevent recurrence.[1] Cessation of smoking is recommended for people with Crohn's disease.[1] One in five people with the disease is admitted to the hospital each year, and half of those with the disease will require surgery at some point over a ten-year period.[1] While surgery should be used as little as possible, it is necessary to address some abscesses, certain bowel obstructions, and cancers.[1] Checking for bowel cancer via colonoscopy is recommended every few years, starting eight years after the disease has begun.[1]
Crohn's disease affects about 3.2 per 1,000 people in Europe and North America;[7] it is less common in Asia and Africa.[16][17] It has historically been more common in the developed world.[18] Rates have, however, been increasing, particularly in the developing world, since the 1970s.[17][18] Inflammatory bowel disease resulted in 47,400 deaths in 2015,[19] and those with Crohn's disease have a slightly reduced life expectancy.[1] It tends to start in adolescence and young adulthood, though it can occur at any age.[20][1][3][21] Males and females are equally affected.[3]
The disease was named after gastroenterologist Burrill Bernard Crohn, who in 1932, together with Leon Ginzburg (1898–1988) and Gordon D. Oppenheimer (1900–1974) at Mount Sinai Hospital in New York, described a series of patients with inflammation of the terminal ileum of the small intestine, the area most commonly affected by the illness.[22] Why the disease was named after Crohn has controversy around it.[23][24] While Crohn in his memoir describes his original investigation of the disease, Ginzburg provided strong evidence how he and Oppenheimer were the first to study the disease.[25]
Many people with Crohn's disease have symptoms for years before the diagnosis.[28] The usual onset is in the teens and twenties, but can occur at any age.[21][1] Because of the 'patchy' nature of the gastrointestinal disease and the depth of tissue involvement, initial symptoms can be more subtle than those of ulcerative colitis. People with Crohn's disease experience chronic recurring periods of flare-ups and remission.[29] The symptoms experienced can change over time as inflammation increases and spreads. Symptoms can also be different depending on which organs are involved. It is generally thought that the presentation of Crohn's disease is different for each patient due to the high variability of symptoms, organ involvement, and initial presentation.
Perianal discomfort may also be prominent in Crohn's disease. Itchiness or pain around the anus may be suggestive of inflammation of the anus, or perianal complications such as anal fissures, fistulae, or abscesses around the anal area.[1] Perianal skin tags are also common in Crohn's disease, and may appear with or without the presence of colorectal polyps.[30] Fecal incontinence may accompany perianal Crohn's disease.
The intestines, especially the colon and terminal ileum, are the most commonly affected areas of the body. Abdominal pain is a common initial symptom of Crohn's disease,[3] especially in the lower right abdomen.[31] Flatulence, bloating, and abdominal distension are additional symptoms and may also add to the intestinal discomfort. Pain is often accompanied by diarrhea, which may or may not be bloody. Inflammation in different areas of the intestinal tract can affect the quality of the feces. Ileitis typically results in large-volume, watery feces, while colitis may result in a smaller volume of feces of higher frequency. Fecal consistency may range from solid to watery. In severe cases, an individual may have more than 20 bowel movements per day, and may need to awaken at night to defecate.[1][32][33][34] Visible bleeding in the feces is less common in Crohn's disease than in ulcerative colitis, but is not unusual.[1] Bloody bowel movements are usually intermittent, and may be bright red, dark maroon, or even black in color. The color of bloody stool depends on the location of the bleed. In severe Crohn's colitis, bleeding may be copious.[32]
The stomach is rarely the sole or predominant site of CD. To date there are only a few documented case reports of adults with isolated gastric CD and no reports in the pediatric population. Isolated stomach involvement is very unusual presentation accounting for less than 0.07% of all gastrointestinal CD.[35] Rarely, the esophagus and stomach may be involved in Crohn's disease. These can cause symptoms including difficulty swallowing (dysphagia), upper abdominal pain, and vomiting.[36]
The mouth may be affected by recurrent sores (aphthous ulcers). Recurrent aphthous ulcers are common; however, it is not clear whether this is due to Crohn's disease or simply that they are common in the general population. Other findings may include diffuse or nodular swelling of the mouth, a cobblestone appearance inside the mouth, granulomatous ulcers, or pyostomatitis vegetans. Medications that are commonly prescribed to treat CD, such as anti-inflammatory and sulfa-containing drugs, may cause lichenoid drug reactions in the mouth. Fungal infection such as candidiasis is also common due to the immunosuppression required in the treatment of the disease. Signs of anemia such as pallor and angular cheilitis or glossitis are also common due to nutritional malabsorption.[37]
People with Crohn's disease are also susceptible to angular stomatitis, an inflammation of the corners of the mouth, and pyostomatitis vegetans.[38]
Like many other chronic, inflammatory diseases, Crohn's disease can cause a variety of systemic symptoms.[1] Among children, growth failure is common. Many children are first diagnosed with Crohn's disease based on inability to maintain growth.[39] As it may manifest at the time of the growth spurt in puberty, up to 30% of children with Crohn's disease may have retardation of growth.[40] Fever may also be present, though fevers greater than 38.5 °C (101.3 °F) are uncommon unless there is a complication such as an abscess.[1] Among older individuals, Crohn's disease may manifest as weight loss, usually related to decreased food intake, since individuals with intestinal symptoms from Crohn's disease often feel better when they do not eat and might lose their appetite.[39] People with extensive small intestine disease may also have malabsorption of carbohydrates or lipids, which can further exacerbate weight loss.[41]
Crohn's disease can affect many organ systems beyond the gastrointestinal tract.[42]
Inflammation of the interior portion of the eye, known as uveitis, can cause blurred vision and eye pain, especially when exposed to light (photophobia).[46] Uveitis can lead to loss of vision if untreated.[42]
Inflammation may also involve the white part of the eye (sclera) or the overlying connective tissue (episclera), which causes conditions called scleritis and episcleritis, respectively.[46]
Other very rare ophthalmological manifestations include: conjunctivitis, glaucoma, and retinal vascular disease.[47]
Crohn's disease that affects the ileum may result in an increased risk of gallstones. This is due to a decrease in bile acid resorption in the ileum, and the bile gets excreted in the stool. As a result, the cholesterol/bile ratio increases in the gallbladder, resulting in an increased risk for gallstones.[46] Although the association is greater in the context of ulcerative colitis, Crohn's disease may also be associated with primary sclerosing cholangitis, a type of inflammation of the bile ducts.[48]
Liver involvement of Crohn's disease can include cirrhosis and steatosis. Nonalcoholic fatty liver disease (nonalcoholic steatohepatitis, NAFLD) are relatively common and can slowly progress to end-stage liver disease. NAFLD sensitizes the liver to injury and increases the risk of developing acute or chronic liver failure following another liver injury.[47]
Other rare hepatobiliary manifestations of Crohn's disease include: cholangiocarcinoma, granulomatous hepatitis, cholelithiasis, autoimmune hepatitis, hepatic abscess, and pericholangitis.[47]
Nephrolithiasis, obstructive uropathy, and fistulization of the urinary tract directly result from the underlying disease process. Nephrolithiasis is due to calcium oxalate or uric acid stones. Calcium oxalate is due to hyperoxaluria typically associated with either distal ileal CD or ileal resection. Oxalate absorption increases in the presence of unabsorbed fatty acids in the colon. The fatty acids compete with oxalate to bind calcium, displacing the oxalate, which can then be absorbed as unbound sodium oxalate across colonocytes and excreted into the urine. Because sodium oxalate only is absorbed in the colon, calcium-oxalate stones form only in patients with an intact colon. Patients with an ileostomy are prone to formation of uric-acid stones because of frequent dehydration. The sudden onset of severe abdominal, back, or flank pain in patients with IBD, particularly if different from the usual discomfort, should lead to inclusion of a renal stone in the differential diagnosis.[47]
Urological manifestations in patients with IBD may include ureteral calculi, enterovesical fistula, perivesical infection, perinephric abscess, and obstructive uropathy with hydronephrosis. Ureteral compression is associated with retroperitoneal extension of the phlegmonous inflammatory process involving the terminal ileum and cecum, and may result in hydronephrosis severe enough to cause hypertension.[47]
Immune complex glomerulonephritis presenting with proteinuria and hematuria has been described in children and adults with CD or UC. Diagnosis is by renal biopsy, and treatment parallels the underlying IBD.[47]
Amyloidosis (see endocrinological involvement) secondary to Crohn's disease has been described and is known to affect the kidneys.[47]
Pancreatitis may be associated with both UC and CD. The most common cause is iatrogenic and involves sensitivity to medications used to treat IBD (3% of patients), including sulfasalazine, mesalamine, 6-mercaptopurine, and azathioprine. Pancreatitis may present as symptomatic (in 2%) or more commonly asymptomatic (8–21%) disease in adults with IBD.[47]
Children and adults with IBD have been rarely (<1%) reported developing pleuropericarditis either at initial presentation or during active or quiescent disease. The pathogenesis of pleuropericarditis is unknown, although certain medications (e.g., sulfasalazine and mesalamine derivatives) have been implicated in some cases. The clinical presentation may include chest pain, dyspnea, or in severe cases pericardial tamponade requiring rapid drainage. Nonsteroidal anti-inflammatory drugs have been used as therapy, although this should be weighed against the hypothetical risk of exacerbating the underlying IBD.[47]
In rare cases, cardiomyopathy, endocarditis, and myocarditis have been described.[47]
Crohn's disease also increases the risk of blood clots;[46] painful swelling of the lower legs can be a sign of deep venous thrombosis, while difficulty breathing may be a result of pulmonary embolism.
Laryngeal involvement in inflammatory bowel disease is extremely rare. Only 12 cases of laryngeal involvement in Crohn's disease have been reported as of 2019[update]. Moreover, only one case of laryngeal manifestations in ulcerative colitis has been reported as of that date.[49] Nine patients complained of difficulty in breathing due to edema and ulceration from the larynx to the hypopharynx.[50] Hoarseness, sore throat, and odynophagia are other symptoms of laryngeal involvement of Crohn's disease.[51]
Considering extraintestinal manifestations of CD, those involving the lung are relatively rare. However, there is a wide array of lung manifestations, ranging from subclinical alterations, airway diseases and lung parenchymal diseases to pleural diseases and drug-related diseases. The most frequent manifestation is bronchial inflammation and suppuration with or without bronchiectasis. There are a number of mechanisms by which the lungs may become involved in CD. These include the same embryological origin of the lung and gastrointestinal tract by ancestral intestine, similar immune systems in the pulmonary and intestinal mucosa, the presence of circulating immune complexes and auto-antibodies, and the adverse pulmonary effects of some drugs.[52] A complete list of known pulmonary manifestations include: fibrosing alveolitis, pulmonary vasculitis, apical fibrosis, bronchiectasis, bronchitis, bronchiolitis, tracheal stenosis, granulomatous lung disease, and abnormal pulmonary function.[47]
Crohn's disease is associated with a type of rheumatologic disease known as seronegative spondyloarthropathy.[46] This group of diseases is characterized by inflammation of one or more joints (arthritis) or muscle insertions (enthesitis).[46] The arthritis in Crohn's disease can be divided into two types. The first type affects larger weight-bearing joints such as the knee (most common), hips, shoulders, wrists, or elbows.[46] The second type symmetrically involves five or more of the small joints of the hands and feet.[46] The arthritis may also involve the spine, leading to ankylosing spondylitis if the entire spine is involved, or simply sacroiliitis if only the sacroiliac joint is involved.[46]
Crohn's disease increases the risk of osteoporosis or thinning of the bones.[46] Individuals with osteoporosis are at increased risk of bone fractures.[53]
Crohn's disease may also involve the skin, blood, and endocrine system. Erythema nodosum is the most common type of skin problem, occurring in around 8% of people with Crohn's disease, producing raised, tender red nodules usually appearing on the shins.[46][54][55] Erythema nodosum is due to inflammation of the underlying subcutaneous tissue, and is characterized by septal panniculitis.[54]
Pyoderma gangrenosum is a less common skin problem, occurring in under 2%,[55] and is typically a painful ulcerating nodule.[54][42]
Clubbing, a deformity of the ends of the fingers, may also be a result of Crohn's disease.
Other very rare dermatological manifestations include: pyostomatitis vegetans, psoriasis, erythema multiforme, epidermolysis bullosa acquista (described in a case report), and metastatic CD (the spread of Crohn's inflammation to the skin[38]).[47] It is unknown if Sweet's syndrome is connected to Crohn's disease.[47]
Crohn's disease can also cause neurological complications (reportedly in up to 15%).[56] The most common of these are seizures, stroke, myopathy, peripheral neuropathy, headache, and depression.[56]
Central and peripheral neurological disorders are described in patients with IBD and include peripheral neuropathies, myopathies, focal central nervous system defects, convulsions, confusional episodes, meningitis, syncope, optic neuritis, and sensorineural loss. Autoimmune mechanisms are proposed for involvement with IBD. Nutritional deficiencies associated with neurological manifestations, such as vitamin B12 deficiency, should be investigated. Spinal abscess has been reported in both a child and an adult with initial complaints of severe back pain due to extension of a psoas abscess from the epidural space to the subarachnoid space.[47]
Crohn's disease is linked to many psychological disorders, including depression and anxiety, denial of one's disease, the need for dependence or dependent behaviors, feeling overwhelmed, and having a poor self-image.[57]
Many studies have found that patients with IBD report a higher frequency of depressive and anxiety disorders than the general population; most studies confirm that women with IBD are more likely than men to develop affective disorders and show that up to 65% of them may have depression and anxiety disorder.[58][59]
Autoimmune hemolytic anemia, a condition in which the immune system attacks the red blood cells, is also more common in Crohn's disease and may cause fatigue, a pale appearance, and other symptoms common in anemia.
Secondary amyloidosis (AA) is another rare but serious complication of inflammatory bowel disease (IBD), generally seen in Crohn's disease. At least 1% of patients with Crohn's disease develop amyloidosis. In the literature, the time lapse between the onset of Crohn's disease and the diagnosis of amyloidosis has been reported to range from 1 to 21 years.
Leukocytosis and thrombocytopenia are usually due to immunosuppressant treatments or sulfasalazine. Plasma erythropoietin levels often are lower in patients with IBD than expected, in conjunction with severe anemia.[47]
Thrombocytosis and thromboembolic events resulting from a hypercoagulable state in patients with IBD can lead to pulmonary embolism or thrombosis elsewhere in the body. Thrombosis has been reported in 1.8% of patients with UC and 3.1% of patients with CD. Thromboembolism and thrombosis are less frequently reported among pediatric patients, with three patients with UC and one with CD described in case reports.[47]
In rare cases, hypercoagulation disorders and portal vein thrombosis have been described.[47]
People with Crohn's disease may develop anemia due to vitamin B12, folate, iron deficiency, or due to anemia of chronic disease.[60][61] The most common is iron deficiency anemia[60] from chronic blood loss, reduced dietary intake, and persistent inflammation leading to increased hepcidin levels, restricting iron absorption in the duodenum.[61] As Crohn's disease most commonly affects the terminal ileum where the vitamin B12/intrinsic factor complex is absorbed, B12 deficiency may be seen.[61] This is particularly common after surgery to remove the ileum.[60] Involvement of the duodenum and jejunum can impair the absorption of many other nutrients including folate. People with Crohn's often also have issues with small bowel bacterial overgrowth syndrome, which can produce micronutrient deficiencies.[62][63]
Crohn's disease can lead to several mechanical complications within the intestines, including obstruction,[64] fistulae,[65] and abscesses.[66] Obstruction typically occurs from strictures or adhesions that narrow the lumen, blocking the passage of the intestinal contents. A fistula can develop between two loops of bowel, between the bowel and bladder, between the bowel and vagina, and between the bowel and skin. Abscesses are walled-off concentrations of infection, which can occur in the abdomen or in the perianal area. Crohn's is responsible for 10% of vesicoenteric fistulae, and is the most common cause of ileovesical fistulae.[67]
Symptoms caused by intestinal stenosis, or the tightening and narrowing of the bowel, are also common in Crohn's disease. Abdominal pain is often most severe in areas of the bowel with stenosis. Persistent vomiting and nausea may indicate stenosis from small bowel obstruction or disease involving the stomach, pylorus, or duodenum.[32]
Intestinal granulomas are a walled-off portions of the intestine by macrophages in order to isolate infections. Granuloma formation is more often seen in younger patients, and mainly in the severe, active penetrating disease.[68] Granuloma is considered the hallmark of microscopic diagnosis in Crohn's disease (CD), but granulomas can be detected in only 21–60% of CD patients.[68]
Crohn's disease also increases the risk of cancer in the area of inflammation. For example, individuals with Crohn's disease involving the small bowel are at higher risk for small intestinal cancer.[69] Similarly, people with Crohn's colitis have a relative risk of 5.6 for developing colon cancer.[70] Screening for colon cancer with colonoscopy is recommended for anyone who has had Crohn's colitis for at least eight years.[71]
Some studies suggest there is a role for chemoprotection in the prevention of colorectal cancer in Crohn's involving the colon; two agents have been suggested, folate and mesalamine preparations.[72] Also, immunomodulators and biologic agents used to treat this disease may promote developing extra-intestinal cancers.[73]
Some cancers, such as acute myelocytic leukaemia have been described in cases of Crohn's disease.[47] Hepatosplenic T-cell lymphoma (HSTCL) is a rare, lethal disease generally seen in young male patients with inflammatory bowel disease. TNF-α Inhibitor treatments (infliximab, adalimumab, certolizumab, natalizumab, and etanercept) are thought to be the cause of this rare disease.[74]
Major complications of Crohn's disease include bowel obstruction, abscesses, free perforation, and hemorrhage, which in rare cases may be fatal.[75][76]
Individuals with Crohn's disease are at risk of malnutrition for many reasons, including decreased food intake and malabsorption. The risk increases following resection of the small bowel. Such individuals may require oral supplements to increase their caloric intake, or in severe cases, total parenteral nutrition (TPN). Most people with moderate or severe Crohn's disease are referred to a dietitian for assistance in nutrition.[77]
Small intestinal bacterial overgrowth (SIBO) is characterized by excessive proliferation of colonic bacterial species in the small bowel. Potential causes of SIBO include fistulae, strictures, or motility disturbances. Hence, patients with Crohn's disease are especially predisposed to develop SIBO. As result, CD patients may experience malabsorption and report symptoms such as weight loss, watery diarrhea, meteorism, flatulence, and abdominal pain, mimicking acute flare in these patients.[63]
Crohn's disease can be problematic during pregnancy, and some medications can cause adverse outcomes for the fetus or mother. Consultation with an obstetrician and gastroenterologist about Crohn's disease and all medications facilitates preventive measures. In some cases, remission occurs during pregnancy. Certain medications can also lower sperm count or otherwise adversely affect a man's fertility.[78]
Common complications of an ostomy (a common surgery in Crohn's disease) are: mucosal edema, peristomal dermatitis, retraction, ostomy prolapse, mucosal/skin detachment, hematoma, necrosis, parastomal hernia, and stenosis.[79]
The etiology of Crohn's disease is unknown. Many theories have been disputed, with four main theories hypothesized to be the primary mechanism of Crohn's disease. In autoimmune diseases, antibodies and T lymphocytes are the primary mode of inflammation. These cells and bodies are part of the adaptive immune system, or the part of the immune system that learns to fight foreign bodies when first identified.[80] Autoinflammatory diseases are diseases where the innate immune system, or the immune system we are genetically coded with, is designed to attack our own cells.[81] Crohn's disease likely has involvement of both the adaptive and innate immune systems.[82]
Crohn's disease can be described as a multifactorial autoinflammatory disease. The etiopathogenesis of Crohn's disease is still unknown. In any event, a loss of the regulatory capacity of the immune apparatus would be implicated in the onset of the disease. In this respect interestingly enough, as for Blau's disease (a monogenic autoinflammatory disease), the NOD2 gene mutations have been linked to Crohn's disease. However, in Crohn's disease, NOD2 mutations act as a risk factor, being more common among Crohn's disease patients than the background population, while in Blau's disease NOD2 mutations are linked directly to this syndrome, as it is an autosomal-dominant disease. All this new knowledge in the pathogenesis of Crohn's disease allows us to put this multifactorial disease in the group of autoinflammatory syndromes.[81]
Some examples of how the innate immune system affects bowel inflammation have been described.[82] A meta-analysis of CD genome-wide association studies revealed 71 distinct CD-susceptibility loci. Interestingly, three very important CD-susceptibility genes (the intracellular pathogen-recognition receptor, NOD2; the autophagy-related 16-like 1, ATG16L1 and the immunity-related GTPase M, IRGM) are involved in innate immune responses against gut microbiota, while one (the X-box binding protein 1) is involved in regulation of the [adaptive] immune pathway via MHC class II,[83] resulting in autoinflammatory inflammation. Studies have also found that increased ILC3 can overexpress major histocompatibility complex (MHC) II. MHC class II can induce CD4+ T cell apoptosis, thus avoiding the T cell response to normal bowel micro bacteria. Further studies of IBD patients compared with non-IBD patients found that the expression of MHC II by ILC3 was significantly reduced in IBD patients, thus causing an immune reaction against intestinal cells or normal bowel bacteria and damaging the intestines. This can also make the intestines more susceptible to environmental factors, such as food or bacteria.[82]
The thinking is that because Crohn's disease has strong innate immune system involvement and has NOD2 mutations as a predisposition, Crohn's disease is more likely an autoinflammatory disease than an autoimmune disease.[82]
A substantial body of data has emerged in recent years to suggest that the primary defect in Crohn's disease is actually one of relative immunodeficiency.[84] This view has been bolstered recently by novel immunological and clinical studies that have confirmed gross aberrations in this early response, consistent with subsequent genetic studies that highlighted molecules important for innate immune function. The suggestion therefore is that Crohn's pathogenesis actually results from partial immunodeficiency, a theory that coincides with the frequent recognition of a virtually identical, non-infectious inflammatory bowel disease arising in patients with congenital monogenic disorders impairing phagocyte function.[84]
While the exact cause or causes are unknown, Crohn's disease seems to be due to a combination of environmental factors and genetic predisposition.[87] Crohn's is the first genetically complex disease in which the relationship between genetic risk factors and the immune system is understood in considerable detail.[88] Each individual risk mutation makes a small contribution to the overall risk of Crohn's (approximately 1:200). The genetic data, and direct assessment of immunity, indicates a malfunction in the innate immune system.[89] In this view, the chronic inflammation of Crohn's is caused when the adaptive immune system tries to compensate for a deficient innate immune system.[90]
Crohn's has a genetic component.[92] Because of this, siblings of known people with Crohn's are 30 times more likely to develop Crohn's than the general population.[93]
The first mutation found to be associated with Crohn's was a frameshift in the NOD2 gene (also known as the CARD15 gene),[94] followed by the discovery of point mutations.[95] Over 30 genes have been associated with Crohn's; a biological function is known for most of them. For example, one association is with mutations in the XBP1 gene, which is involved in the unfolded protein response pathway of the endoplasmic reticulum.[96][97] The gene variants of NOD2/CARD15 seem to be related with small-bowel involvement.[98] Other well documented genes which increase the risk of developing Crohn's disease are ATG16L1,[99] IL23R,[100] IRGM,[101] and SLC11A1.[102]
There is considerable overlap between susceptibility loci for IBD and mycobacterial infections.[103] Genome-wide association studies have shown that Crohn's disease is genetically linked to coeliac disease.[104]
Crohn's has been linked to the gene LRRK2 with one variant potentially increasing the risk of developing the disease by 70%, while another lowers it by 25%. The gene is responsible for making a protein, which collects and eliminates waste product in cells, and is also associated with Parkinson's disease.[105]
There was a prevailing view that Crohn's disease is a primary T cell autoimmune disorder; however, a newer theory hypothesizes that Crohn's results from an impaired innate immunity.[106] The later hypothesis describes impaired cytokine secretion by macrophages, which contributes to impaired innate immunity and leads to a sustained microbial-induced inflammatory response in the colon, where the bacterial load is high.[9][89] Another theory is that the inflammation of Crohn's was caused by an overactive Th1 and Th17 cytokine response.[107][108]
In 2007, the ATG16L1 gene was implicated in Crohn's disease, which may induce autophagy and hinder the body's ability to attack invasive bacteria.[99] Another study theorized that the human immune system traditionally evolved with the presence of parasites inside the body and that the lack thereof due to modern hygiene standards has weakened the immune system. Test subjects were reintroduced to harmless parasites, with positive responses.[109]
It is hypothesized that maintenance of commensal microorganism growth in the GI tract is dysregulated, either as a result or cause of immune dysregulation.[110][111]
There is an apparent connection between Crohn's disease, Mycobacterium, other pathogenic bacteria, and genetic markers.[112][113] A number of studies have suggested a causal role for Mycobacterium avium subspecies paratuberculosis (MAP), which causes a similar disease, Johne's disease, in cattle.[114][115] In many individuals, genetic factors predispose individuals to Mycobacterium avium subsp. paratuberculosis infection. This bacterium may produce certain compounds containing mannose, which may protect both itself and various other bacteria from phagocytosis, thereby possibly causing a variety of secondary infections.[116]
NOD2 is a gene involved in Crohn's genetic susceptibility. It is associated with macrophages' diminished ability to phagocytize MAP. This same gene may reduce innate and adaptive immunity in gastrointestinal tissue and impair the ability to resist infection by the MAP bacterium.[117] Macrophages that ingest the MAP bacterium are associated with high production of TNF-α.[118][119]
Other studies have linked specific strains of enteroadherent E. coli to the disease.[120] Adherent-invasive Escherichia coli (AIEC), more common in people with CD,[121][122][120] have the ability to make strong biofilms compared to non-AIEC strains correlating with high adhesion and invasion indices[123][124] of neutrophils and the ability to block autophagy at the autolysosomal step, which allows for intracellular survival of the bacteria and induction of inflammation.[125] Inflammation drives the proliferation of AIEC and dysbiosis in the ileum, irrespective of genotype.[126] AIEC strains replicate extensively inside macrophages inducing the secretion of very large amounts of TNF-α.[127]
Mouse studies have suggested some symptoms of Crohn's disease, ulcerative colitis, and irritable bowel syndrome have the same underlying cause. Biopsy samples taken from the colons of all three patient groups were found to produce elevated levels of a serine protease.[128] Experimental introduction of the serine protease into mice has been found to produce widespread pain associated with irritable bowel syndrome, as well as colitis, which is associated with all three diseases.[129] Regional and temporal variations in those illnesses follow those associated with infection with the protozoan Blastocystis.[130]
The "cold-chain" hypothesis is that psychrotrophic bacteria such as Yersinia and Listeria species contribute to the disease. A statistical correlation was found between the advent of the use of refrigeration in the United States and various parts of Europe and the rise of the disease.[131][132][133]
There is also a tentative association between Candida colonization and Crohn's disease.[134]
Still, these relationships between specific pathogens and Crohn's disease remain unclear.[135][136]
The increased incidence of Crohn's in the industrialized world indicates an environmental component. Crohn's is associated with an increased intake of animal protein, milk protein, and an increased ratio of omega-6 to omega-3 polyunsaturated fatty acids.[137]
Those who consume vegetable proteins appear to have a lower incidence of Crohn's disease. Consumption of fish protein has no association.[137]
Smoking increases the risk of the return of active disease (flares).[4] The introduction of hormonal contraception in the United States in the 1960s is associated with a dramatic increase in incidence, and one hypothesis is that these drugs work on the digestive system in ways similar to smoking.[138] Isotretinoin is associated with Crohn's.[139][140][141]
Although stress is sometimes claimed to exacerbate Crohn's disease, there is no concrete evidence to support such claim.[3] Still, it is well known that immune function is related to stress.[142] Dietary microparticles, such as those found in toothpaste, have been studied as they produce effects on immunity, but they were not consumed in greater amounts in patients with Crohn's.[143][144] The use of doxycycline has also been associated with increased risk of developing inflammatory bowel diseases.[145][146][147] In one large retrospective study, patients who were prescribed doxycycline for their acne had a 2.25-fold greater risk of developing Crohn's disease.[146]
During a colonoscopy, biopsies of the colon are often taken to confirm the diagnosis. Certain characteristic features of the pathology seen point toward Crohn's disease; it shows a transmural pattern of inflammation, meaning the inflammation may span the entire depth of the intestinal wall.[1]
Granulomas, aggregates of macrophage derivatives known as giant cells, are found in 50% of cases and are most specific for Crohn's disease. The granulomas of Crohn's disease do not show "caseation", a cheese-like appearance on microscopic examination characteristic of granulomas associated with infections, such as tuberculosis. Biopsies may also show chronic mucosal damage, as evidenced by blunting of the intestinal villi, atypical branching of the crypts, and a change in the tissue type (metaplasia). One example of such metaplasia, Paneth cell metaplasia, involves the development of Paneth cells (typically found in the small intestine and a key regulator of intestinal microbiota) in other parts of the gastrointestinal system.[149][150]
The diagnosis of Crohn's disease can sometimes be challenging,[28] and many tests are often required to assist the physician in making the diagnosis.[32] Even with a full battery of tests, it may not be possible to diagnose Crohn's with complete certainty; a colonoscopy is approximately 70% effective in diagnosing the disease, with further tests being less effective. Disease in the small bowel is particularly difficult to diagnose, as a traditional colonoscopy allows access to only the colon and lower portions of the small intestines; introduction of the capsule endoscopy[151] aids in endoscopic diagnosis. Giant (multinucleate) cells, a common finding in the lesions of Crohn's disease, are less common in the lesions of lichen nitidus.[152]
Endoscopic image of Crohn's colitis showing deep ulceration
CT scan showing Crohn's disease in the fundus of the stomach
Section of colectomy showing transmural inflammation
Resected ileum from a person with Crohn's disease
Crohn's disease is one type of inflammatory bowel disease (IBD). It typically manifests in the gastrointestinal tract and can be categorized by the specific tract region affected.
Gastroduodenal Crohn's disease causes inflammation in the stomach and the first part of the small intestine called the duodenum. Jejunoileitis causes spotty patches of inflammation in the top half of the small intestine, called the jejunum.[153] The disease can attack any part of the digestive tract, from mouth to anus. However, individuals affected by the disease rarely fall outside these three classifications, with presentations in other areas.[1]
Crohn's disease may also be categorized by the behavior of disease as it progresses. These categorizations formalized in the Vienna classification of the disease.[154] There are three categories of disease presentation in Crohn's disease: stricturing, penetrating, and inflammatory. Stricturing disease causes narrowing of the bowel that may lead to bowel obstruction or changes in the caliber of the feces. Penetrating disease creates abnormal passageways (fistulae) between the bowel and other structures, such as the skin. Inflammatory disease (or nonstricturing, nonpenetrating disease) causes inflammation without causing strictures or fistulae.[154][155]
A colonoscopy is the best test for making the diagnosis of Crohn's disease, as it allows direct visualization of the colon and the terminal ileum, identifying the pattern of disease involvement. On occasion, the colonoscope can travel past the terminal ileum, but it varies from person to person. During the procedure, the gastroenterologist can also perform a biopsy, taking small samples of tissue for laboratory analysis, which may help confirm a diagnosis. As 30% of Crohn's disease involves only the ileum,[1] cannulation of the terminal ileum is required in making the diagnosis. Finding a patchy distribution of disease, with involvement of the colon or ileum, but not the rectum, is suggestive of Crohn's disease, as are other endoscopic stigmata.[156]
The utility of capsule endoscopy for this, however, is still uncertain.[157]
A small bowel follow-through may suggest the diagnosis of Crohn's disease and is useful when the disease involves only the small intestine. Because colonoscopy and gastroscopy allow direct visualization of only the terminal ileum and beginning of the duodenum, they cannot be used to evaluate the remainder of the small intestine. As a result, a barium follow-through X-ray, wherein barium sulfate suspension is ingested and fluoroscopic images of the bowel are taken over time, is useful for looking for inflammation and narrowing of the small bowel.[156][158] Barium enemas, in which barium is inserted into the rectum and fluoroscopy is used to image the bowel, are rarely used in the work-up of Crohn's disease due to the advent of colonoscopy. They remain useful for identifying anatomical abnormalities when strictures of the colon are too small for a colonoscope to pass through, or in the detection of colonic fistulae (in this case contrast should be performed with iodate substances).[159]
CT and MRI scans are useful for evaluating the small bowel with enteroclysis protocols.[160] They are also useful for looking for intra-abdominal complications of Crohn's disease, such as abscesses, small bowel obstructions, or fistulae.[161] Magnetic resonance imaging (MRI) is another option for imaging the small bowel as well as looking for complications, though it is more expensive and less readily available.[162] MRI techniques such as diffusion-weighted imaging and high-resolution imaging are more sensitive in detecting ulceration and inflammation compared to CT.[163][164]
A complete blood count may reveal anemia, which commonly is caused by blood loss leading to iron deficiency or by vitamin B12 deficiency, usually caused by ileal disease impairing vitamin B12 absorption. Rarely autoimmune hemolysis may occur.[165] Ferritin levels help assess if iron deficiency is contributing to the anemia. Erythrocyte sedimentation rate (ESR) and C-reactive protein help assess the degree of inflammation, which is important as ferritin can also be raised in inflammation.[166]
Other causes of anemia include medication used in treatment of inflammatory bowel disease, like azathioprine, which can lead to cytopenia, and sulfasalazine, which can also result in folate deficiency. Testing for Saccharomyces cerevisiae antibodies (ASCA) and antineutrophil cytoplasmic antibodies (ANCA) has been evaluated to identify inflammatory diseases of the intestine[167] and to differentiate Crohn's disease from ulcerative colitis.[168] Furthermore, increasing amounts and levels of serological antibodies such as ASCA, antilaminaribioside [Glc(β1,3)Glb(β); ALCA], antichitobioside [GlcNAc(β1,4)GlcNAc(β); ACCA], antimannobioside [Man(α1,3)Man(α)AMCA], antiLaminarin [(Glc(β1,3))3n(Glc(β1,6))n; anti-L] and antichitin [GlcNAc(β1,4)n; anti-C] associate with disease behavior and surgery, and may aid in the prognosis of Crohn's disease.[169][170][171][172]
Low serum levels of vitamin D are associated with Crohn's disease.[173] Further studies are required to determine the significance of this association.[173]
The most common disease that mimics the symptoms of Crohn's disease is ulcerative colitis, as both are inflammatory bowel diseases that can affect the colon with similar symptoms. It is important to differentiate these diseases, since the course of the diseases and treatments may be different. In some cases, however, it may not be possible to tell the difference, in which case the disease is classified as indeterminate colitis.[1][32][33]
Other conditions with similar symptoms as Crohn's disease includes intestinal tuberculosis, Behçet's disease, ulcerative colitis, nonsteroidal anti-inflammatory drug enteropathy, irritable bowel syndrome and celiac disease.[5] Irritable bowel syndrome is excluded when there are inflammatory changes.[5] Celiac disease cannot be excluded if specific antibodies (anti-transglutaminase antibodies) are negative,[180][181] nor in absence of intestinal villi atrophy.[182][183]
There is no cure for Crohn's disease and remission may not be possible or prolonged if achieved. In cases where remission is possible, relapse can be prevented and symptoms controlled with medication, lifestyle and dietary changes, changes to eating habits (eating smaller amounts more often), reduction of stress, moderate activity, and exercise. Surgery is generally contraindicated and has not been shown to prevent relapse. Adequately controlled, Crohn's disease may not significantly restrict daily living.[187] Treatment for Crohn's disease involves first treating the acute problem and its symptoms, then maintaining remission of the disease.
Certain lifestyle changes can reduce symptoms, including dietary adjustments, elemental diet, proper hydration, and smoking cessation. 
Patients with Crohn's disease are very interested in diet. Recent reviews underlined the importance to adopt diets that are best supported by evidence, even if little is known about the impact of diets on these patients.[188][189]
Diets that include higher levels of fiber and fruit are associated with reduced risk, while diets rich in total fats, polyunsaturated fatty acids, meat, and omega-6 fatty acids may increase the risk of Crohn's.[190] Maintaining a balanced diet with proper portion control can help manage symptoms of the disease. Eating small meals frequently instead of big meals may also help with a low appetite. A food diary may help with identifying foods that trigger symptoms. Despite the recognized importance of dietary fiber for intestinal health, some people should follow a low residue diet to control acute symptoms especially if foods high in insoluble fiber cause symptoms, e.g., due to obstruction or irritation of the bowel.[187] Some find relief in eliminating casein (a protein found in cow's milk) and gluten (a protein found in wheat, rye and barley) from their diets. They may have specific dietary intolerances (not allergies), for example, lactose.[191] Fatigue can be helped with regular exercise, a healthy diet, and enough sleep, and for those with malabsorption of vitamin B12 due to disease or surgical resection of the terminal ileum, cobalamin injections. Smoking may worsen symptoms and the course of the disease, and stopping is recommended. Alcohol consumption can also worsen symptoms, and moderation or cessation is advised.[187]
Acute treatment uses medications to treat any infection (normally antibiotics) and to reduce inflammation (normally aminosalicylate anti-inflammatory drugs and corticosteroids). When symptoms are in remission, treatment enters maintenance, with a goal of avoiding the recurrence of symptoms. Prolonged use of corticosteroids has significant side-effects; as a result, they are, in general, not used for long-term treatment. Alternatives include aminosalicylates alone, though only a minority are able to maintain the treatment, and many require immunosuppressive drugs.[174] It has been also suggested that antibiotics change the enteric flora, and their continuous use may pose the risk of overgrowth with pathogens such as Clostridium difficile.[192]
Medications used to treat the symptoms of Crohn's disease include 5-aminosalicylic acid (5-ASA) formulations, prednisone, immunomodulators such as azathioprine (given as the prodrug for 6-mercaptopurine), methotrexate,[193] and anti-TNF therapies and monoclonal antibodies, such as infliximab, adalimumab,[33] certolizumab,[194] vedolizumab, ustekinumab,[195] natalizumab,[196][197]risankizumab-rzaa, and upadacitinib[198] Hydrocortisone should be used in severe attacks of Crohn's disease.[199] Biological therapies are medications used to avoid long-term steroid use, decrease inflammation, and treat people who have fistulas with abscesses.[31] The monoclonal antibody ustekinumab appears to be a safe treatment option, and may help people with moderate to severe active Crohn's disease.[200] The long term safety and effectiveness of monoclonal antibody treatment is not known.[200] The monoclonal antibody briakinumab is not effective for people with active Crohn's disease and it is no longer being manufactured.[200]
The gradual loss of blood from the gastrointestinal tract, as well as chronic inflammation, often leads to anemia, and professional guidelines suggest routinely monitoring for this.[201][202][203]
Many patients affected by Crohn’s disease need immunosuppressant therapies, which are known to be associated with a higher risk of contracting opportunistic infectious diseases and of pre-neoplastic or neoplastic lesions such as cervical high-grade dysplasia and cancer.[204][205]
Many of these potentially harmful diseases, such as Hepatitis B, Influenza, chickenpox, herpes zoster virus, pneumococcal pneumonia, or human papilloma virus, can be prevented by vaccines. Each drug used in the treatment of IBD should be classified according to the degree of immunosuppression induced in the patient. Several guidelines suggest investigating patients’ vaccination status before starting any treatment and performing vaccinations against Vaccine-preventable disease when required.[206][207]
Compared to the rest of the population, patients affected by IBD are known to be at higher risk of contracting some vaccine-preventable diseases such as flu and pneumonia.[208]
Nevertheless, despite the increased risk of infections, vaccination rates in IBD patients are known to be suboptimal and may also be lower than vaccination rates in the general population.[209][210]
Crohn's cannot be cured by surgery, as the disease eventually recurs, though it is used in the case of partial or full blockage of the intestine.[211] Surgery may also be required for complications such as obstructions, fistulas, or abscesses, or if the disease does not respond to drugs. After the first surgery, Crohn's usually comes back at the site where the diseased intestine was removed and the healthy ends were rejoined; it can also come back in other locations. After a resection, scar tissue builds up, which can cause strictures, which form when the intestines become too small to allow excrement to pass through easily, which can lead to a blockage. After the first resection, another resection may be necessary within five years.[212] For patients with an obstruction due to a stricture, two options for treatment are strictureplasty and resection of that portion of bowel. There is no statistical significance between strictureplasty alone versus strictureplasty and resection in cases of duodenal involvement. In these cases, re-operation rates were 31% and 27%, respectively, indicating that strictureplasty is a safe and effective treatment for selected people with duodenal involvement.[213]
Postsurgical recurrence of Crohn's disease is relatively common. Crohn's lesions are nearly always found at the site of the resected bowel. The join (or anastomosis) after surgery may be inspected, usually during a colonoscopy, and disease activity graded. The "Rutgeert's score" is an endoscopic scoring system for postoperative disease recurrence in Crohn's disease. Mild postsurgical recurrences of Crohn's disease are graded i1 and i2, moderate to severe recurrences are graded i3 and i4.[214] Fewer lesions result in a lower grade. Based on the score, treatment plans can be designed to give the patient the best chance of managing the recurrence of the disease.[215]
Short bowel syndrome (SBS, also short gut syndrome or simply short gut) is caused by the surgical removal of part of the small intestine. It usually develops in those patients who have had half or more of their small intestines removed.[216] Diarrhea is the main symptom, but others may include weight loss, cramping, bloating, and heartburn. Short bowel syndrome is treated with changes in diet, intravenous feeding, vitamin and mineral supplements, and treatment with medications. In some cases of SBS, intestinal transplant surgery may be considered; though the number of transplant centres offering this procedure is quite small and it comes with a high risk due to the chance of infection and rejection of the transplanted intestine.[217]
Bile acid diarrhea is another complication following surgery for Crohn's disease in which the terminal ileum has been removed. This leads to the development of excessive watery diarrhea. It is usually thought to be due to an inability of the ileum to reabsorb bile acids after resection of the terminal ileum and was the first type of bile acid malabsorption recognized.[218]
The use of oral probiotic supplements to modify the composition and behaviour of the gastrointestinal microbiome has been researched to understand whether it may help to improve remission rate in people with Crohn's disease. However, only two controlled trials were available in 2020, with no clear overall evidence of higher remission nor lower adverse effects, in people with Crohn's disease receiving probiotic supplementation.[219]
Crohn's may result in anxiety or mood disorders, especially in young people who may have stunted growth or embarrassment from fecal incontinence.[220] Counselling as well as antidepressant or anxiolytic medication may help some people manage.[220]
As of 2017[update] there is a small amount of research looking at mindfulness-based therapies, hypnotherapy, and cognitive behavioural therapy.[221]
It is common for people with Crohn's disease to try complementary or alternative therapy.[222] These include diets, probiotics, fish oil, and other herbal and nutritional supplements.
Crohn's disease is a chronic condition for which there is no known cure. It is characterised by periods of improvement followed by episodes when symptoms flare up. With treatment, most people achieve a healthy weight, and the mortality rate for the disease is relatively low. It can vary from being benign to very severe, and people with CD could experience just one episode or have continuous symptoms. It usually reoccurs, although some people can remain disease-free for years or decades. Up to 80% of people with Crohn's disease are hospitalized at some point during the course of their disease, with the highest rate occurring in the first year after diagnosis.[6] Most people with Crohn's live a normal lifespan.[233] However, Crohn's disease is associated with a small increase in risk of small bowel and colorectal carcinoma (bowel cancer).[234]
The percentage of people with Crohn's disease has been determined in Norway and the United States and is similar at 6 to 7.1:100,000. The Crohn's & Colitis Foundation of America cites this number as approx 149:100,000; NIH cites 28 to 199 per 100,000.[235][236] Crohn's disease is more common in northern countries, and with higher rates still in the northern areas of these countries.[237] The incidence of Crohn's disease is thought to be similar in Europe but lower in Asia and Africa.[235] It also has a higher incidence in Ashkenazi Jews[1][238] and smokers.[239]
Crohn's disease begins most commonly in people in their teens and 20s, and people in their 50s through to their 70s.[1][32][21] It is rarely diagnosed in early childhood. It usually affects female children more severely than males.[240] However, only slightly more women than men have Crohn's disease.[241] Parents, siblings or children of people with Crohn's disease are 3 to 20 times more likely to develop the disease.[242] Twin studies find that if one has the disease there is a 55% chance the other will too.[243]
The incidence of Crohn's disease is increasing in Europe[244] and in newly industrialised countries.[245] For example, in Brazil, there has been an annual increase of 11% in the incidence of Crohn's disease since 1990.[245]
Inflammatory bowel diseases were described by Giovanni Battista Morgagni (1682–1771) and by Scottish physician Thomas Kennedy Dalziel in 1913.[246]
Ileitis terminalis was first described by Polish surgeon Antoni Leśniowski in 1904, although it was not conclusively distinguished from intestinal tuberculosis.[247] In Poland, it is still called Leśniowski-Crohn's disease (Polish: choroba Leśniowskiego-Crohna). Burrill Bernard Crohn, an American gastroenterologist at New York City's Mount Sinai Hospital, described fourteen cases in 1932, and submitted them to the American Medical Association under the rubric of "Terminal ileitis: A new clinical entity". Later that year, he, along with colleagues Leon Ginzburg and Gordon Oppenheimer, published the case series "Regional ileitis: a pathologic and clinical entity". However, due to the precedence of Crohn's name in the alphabet, it later became known in the worldwide literature as Crohn's disease.[22]

Postural orthostatic tachycardia syndrome (POTS) is a condition characterized by an abnormally large increase in heart rate upon sitting up or standing.[1] POTS is a disorder of the autonomic nervous system that can lead the individual to experience a variety of symptoms.[10] Symptoms may include lightheadedness, brain fog, blurred vision, weakness, fatigue, headaches, heart palpitations, exercise intolerance, nausea, diminished concentration, tremulousness (shaking), syncope (fainting), coldness or pain in the extremities, chest pain and shortness of breath.[1][11][12] Other conditions associated with POTS include Ehlers–Danlos syndrome, mast cell activation syndrome, irritable bowel syndrome, insomnia, chronic headaches, chronic fatigue syndrome, fibromyalgia,[1] and amplified musculoskeletal pain syndrome. POTS symptoms may be treated with lifestyle changes such as increasing fluid and salt intake, wearing compression stockings, gentler and slow postural changes, avoiding prolonged bedrest, medication and physical therapy.
The causes of POTS are varied.[13] POTS may develop after a viral infection, surgery, trauma, or pregnancy.[7] It has been shown to emerge in previously healthy patients after COVID-19,[14][15] or in rare cases after COVID-19 vaccination.[16] POTS is more common among people who got infected with SARS-CoV-2 than among those who got vaccinated against COVID-19.[17] Risk factors include a family history of the condition.[1] A POTS diagnosis in adults is characterized by an increased heart rate of 30 beats per minute within ten minutes of standing up, while accompanied by symptoms.[1] This increased heart rate should occur in the absence of orthostatic hypotension (>20 mm Hg drop in systolic blood pressure)[18] to be considered POTS. A spinal fluid leak (called spontaneous intracranial hypotension) may have the same signs and symptoms as POTS and should be excluded.[19] Prolonged bedrest may lead to multiple symptoms, including blood volume loss and postural tachycardia.[20] Other conditions which can cause similar symptoms, such as dehydration, orthostatic hypotension, heart problems, adrenal insufficiency, epilepsy, and Parkinson's disease, must not be present.[6]
Treatment may include avoiding factors that bring on symptoms, increasing dietary salt and water, small and frequent meals,[21] avoidance of immobilization,[21] wearing compression stockings, and taking medications.[22][23][1][24] Medications used may include beta blockers,[25] pyridostigmine,[26] midodrine[27] or fludrocortisone.[1] More than 50% of patients whose condition was triggered by a viral infection get better within five years.[7] About 80% of patients have symptomatic improvement with treatment, while 25 percent of patients are so disabled they are unable to work.[8][7] A retrospective study on patients with adolescent-onset has shown that five years after diagnosis, 19% of patients had a full resolution of symptoms.[28]
It is estimated that 1 million - 3 million people in the United States have POTS.[29] The average age for POTS onset is 20 years old, and it occurs about five times more frequently in females than in males.[1]
In adults, the primary manifestation is an increase in heart rate of more than 30 beats per minute within ten minutes of standing up.[1][30] The resulting heart rate is typically more than 120 beats per minute.[1] For people between ages 12 and 19, the minimum increase for a POTS diagnosis is 40 beats per minute.[31] POTS is often accompanied by common features of orthostatic intolerance—in which symptoms that develop while upright are relieved by reclining.[30] These orthostatic symptoms include palpitations, light-headedness, chest discomfort, shortness of breath,[30] nausea, weakness or "heaviness" in the lower legs, blurred vision, and cognitive difficulties.[1] Symptoms may be exacerbated with prolonged sitting, prolonged standing, alcohol, heat, exercise, or eating a large meal.[citation needed]
Up to one-third of POTS patients experience fainting for many reasons, including but not limited to standing, physical exertion, or heat exposure.[1] POTS patients may also experience orthostatic headaches.[32] Some POTS patients may develop blood pooling in the extremities, characterized by a reddish-purple color of the legs and/or hands upon standing.[33][34][35][36] 48% of people with POTS report chronic fatigue and 32% report sleep disturbances.[37][38][39][40] Other POTS patients only exhibit the cardinal symptom of orthostatic tachycardia.[35] Additional signs and symptoms are varied, and may include excessive sweating, lack of sweating, heat intolerance, digestive issues such as nausea, indigestion, constipation or diarrhea, post-exertional malaise, coat-hanger pain, brain fog, and syncope or presyncope.[41]
One of the most disabling and prevalent symptoms in POTS is "brain fog",[11] a term used by patients to describe the cognitive difficulties they experience. In one survey of 138 POTS patients, brain fog was defined as "forgetful" (91%), "difficulty thinking" (89%), and "difficulty focusing" (88%). Other common description was "Difficulty processing what others say" (80%), Confusion (71%), Lost (64%), and "Thoughts moving too quickly" (40%).[12] The same survey described the most common triggers of brain fog to be fatigue (91%), lack of sleep (90%), prolonged standing (87%), and dehydration (86%).[citation needed]
Neuropsychological testing has shown that a POTS patient has reduced attention (Ruff 2&7 speed and WAIS-III digits forward), short-term memory (WAIS-III digits back), cognitive processing speed (Symbol digits modalities test), and executive function (Stroop word color and trails B).[42][43][44]
A potential cause for brain fog is a decrease in cerebral blood flow (CBF), especially in upright position.[45][46][47]
There may be a loss of neurovascular coupling and reduced functional hyperemia in response to cognitive challenge under orthostatic stress – perhaps related to a loss of autoregulatory buffering of beat-by-beat fluctuations in arterial blood flow.[48]
The symptoms of POTS can be caused by several distinct pathophysiological mechanisms.[30] These mechanisms are poorly understood,[31] and can overlap, with many patients showing features of multiple POTS types.[30] Many people with POTS exhibit low blood volume (hypovolemia), which can decrease the rate of blood flow to the heart.[30] To compensate for low blood volume, the heart increases its cardiac output by beating faster (reflex tachycardia),[49] leading to the symptoms of presyncope.
In the 30% to 60% of cases classified as hyperadrenergic POTS, norepinephrine levels are elevated on standing,[1] often due to hypovolemia or partial autonomic neuropathy.[30] A smaller minority of people with POTS have (typically very high) standing norepinephrine levels that are elevated even in the absence of hypovolemia and autonomic neuropathy; this is classified as central hyperadrenergic POTS.[30][36] The high norepinephrine levels contribute to symptoms of tachycardia.[30] Another subtype, neuropathic POTS, is associated with denervation of sympathetic nerves in the lower limbs.[30] In this subtype, it is thought that impaired constriction of the blood vessels causes blood to pool in the veins of the lower limbs.[1] Heart rate increases to compensate for this blood pooling.[50]
In up to 50% of cases, there was an onset of symptoms following a viral illness.[51] It may also be linked to physical trauma, concussion, pregnancy, or surgery.[52][21][35] It is believed that these events could act as a trigger for an autoimmune response that result in POTS.[53] It has been shown to emerge in previously healthy patients after COVID-19,[54][14][15] or, probably, after COVID-19 vaccination.[16] A 2023 review found that the chances of being diagnosed with POTS within 90 days after mRNA vaccination were 1.33 times higher compared to 90 days before vaccination, still, the results are inconclusive due to a small sample size; only 12 cases of newly diagnosed POTS after mRNA vaccination were reported, all these 12 patients having autoimmune antibodies. However, the risk of POTS-related diagnoses was 5.35 times higher after getting infected with SARS-CoV-2 compared to after mRNA vaccination.[55] Possible mechanisms for COVID-induced POTS are hypovolemia, autoimmunity/inflammation from antibody production against autonomic nerve fibers, and direct toxic effects of COVID-19, or secondary sympathetic nervous system stimulation.[54]
POTS is more common in females than males. It has also been shown to be linked in patients with acute stressors such as pregnancy, recent surgery, or recent trauma. POTS also has been linked to patients with a history of autoimmune diseases,[52] Long Covid,[56] irritable bowel syndrome, anemia, hyperthyroidism, fibromyalgia, diabetes, amyloidosis, sarcoidosis, systemic lupus erythematosus, and cancer. Genetics likely plays a role, with one study finding that one in eight POTS patients reported a history of orthostatic intolerance in their family.[49]
There is an increasing number of studies indicating that POTS is an autoimmune disease.[52][5][57][3][58][59] A high number of patients has elevated levels of autoantibodies against the adrenergic alpha 1 receptor and against the muscarinic acetylcholine M4 receptor.[60][4][61]
Elevations of autoantibodies targeting adrenergic α1 receptor has been associated with symptoms severity in patients with POTS.[60]
More recently, autoantibodies against other targets have been identified in small cohorts of POTS patients.[62] Signs of innate immune system activation with elaboration of pro-inflammatory cytokines has also been reported in a cohort of POTS patients.[63]
If POTS is caused by another condition, it may be classified as secondary POTS.[7] Chronic diabetes mellitus is one common cause.[7] POTS can also be secondary to gastrointestinal disorders that are associated with low fluid intake due to nausea or fluid loss through diarrhea, leading to hypovolemia.[1] Systemic lupus erythematosus and other autoimmune diseases have also been linked to POTS.[52]
There is a subset of patients who present with both POTS and mast cell activation syndrome (MCAS), and it is not yet clear whether MCAS is a secondary cause of POTS or simply comorbid, however, treating MCAS for these patients can significantly improve POTS symptoms.[22]
POTS can also co-occur in all types of Ehlers–Danlos syndrome (EDS),[35] a hereditary connective tissue disorder marked by loose hypermobile joints prone to subluxations and dislocations, skin that exhibits moderate or greater laxity, easy bruising, and many other symptoms. A trifecta of POTS, EDS, and mast cell activation syndrome (MCAS) is becoming increasingly more common, with a genetic marker common among all three conditions.[64][65][66][67] POTS is also often accompanied by vasovagal syncope, with a 25% overlap being reported.[68] There are some overlaps between POTS and chronic fatigue syndrome, with evidence of POTS in 10–20% of CFS cases.[69][68] Fatigue and reduced exercise tolerance are prominent symptoms of both conditions, and dysautonomia may underlie both conditions.[68]
POTS can sometimes be a paraneoplastic syndrome associated with cancer.[70]
There are case reports of people developing POTS and other forms of dysautonomia post-COVID.[15][71][72][73] There is no good large-scale empirical evidence yet to prove a connection, so for now the evidence is preliminary.[74]
POTS is most commonly diagnosed by a cardiologist (41%), cardiac electrophysiologist (15%), or neurologist (19%).[2] The average number of physicians seen before receiving diagnosis is seven, and the average delay before diagnosis is 4.7 years.[2]
A POTS diagnosis requires the following characteristics:[75]
Alternative tests to the tilt table test are also used, such as the NASA Lean Test[76] and the adapted Autonomic Profile (aAP)[77] which require less equipment to complete.
An increase in heart rate upon moving to an upright posture is known as orthostatic (upright) tachycardia (fast heart rate). It occurs without any coinciding drop in blood pressure, as that would indicate orthostatic hypotension.[30] Certain medications to treat POTS may cause orthostatic hypotension. It is accompanied by other features of orthostatic intolerance—symptoms that develop in an upright position and are relieved by reclining.[30] These orthostatic symptoms include palpitations, light-headedness, chest discomfort, shortness of breath,[30] nausea, weakness or "heaviness" in the lower legs, blurred vision, and cognitive difficulties.[1]
A variety of autonomic tests are employed to exclude autonomic disorders that could underlie symptoms, while endocrine testing is used to exclude hyperthyroidism and rarer endocrine conditions.[35] Electrocardiography is normally performed on all patients to exclude other possible causes of tachycardia.[1][35] In cases where a particular associated condition or complicating factor are suspected, other non-autonomic tests may be used: echocardiography to exclude mitral valve prolapse, and thermal threshold tests for small-fiber neuropathy.[35]
Testing the cardiovascular response to prolonged head-up tilting, exercise, eating, and heat stress may help determine the best strategy for managing symptoms.[35] POTS has also been divided into several types (see § Causes), which may benefit from distinct treatments.[78] People with neuropathic POTS show a loss of sweating in the feet during sweat tests, as well as impaired norepinephrine release in the leg,[79] but not arm.[1][78][80] This is believed to reflect peripheral sympathetic denervation in the lower limbs.[79][81][1] People with hyperadrenergic POTS show a marked increase of blood pressure and norepinephrine levels when standing, and are more likely to have from prominent palpitations, anxiety, and tachycardia.[82][83][51][78]
People with POTS can be misdiagnosed with inappropriate sinus tachycardia (IST) as they present similarly. One distinguishing feature is those with POTS rarely exhibit >100 bpm while in a supine position, while patients with IST often have a resting heart rate >100 bpm. Additionally patients with POTS display a more pronounced change in heart rate in response to postural change.[7]
POTS treatment involves using multiple methods in combination to counteract cardiovascular dysfunction, address symptoms, and simultaneously address any associated disorders.[35] For most patients, water intake should be increased, especially after waking, in order to expand blood volume (reducing hypovolemia).[35] Eight to ten cups of water daily are recommended.[22] Increasing salt intake, by adding salt to food, taking salt tablets, or drinking sports drinks and other electrolyte solutions is an effective way to raise blood pressure by helping the body retain water. Different physicians recommend different amounts of sodium to their patients.[84] Combining these techniques with gradual physical training enhances their effect.[35] In some cases, when increasing oral fluids and salt intake is not enough, intravenous saline or the drug desmopressin is used to help increase fluid retention.[35][36]
Large meals worsen symptoms for some people. These people may benefit from eating small meals frequently throughout the day instead.[35] Alcohol and food high in carbohydrates can also exacerbate symptoms of orthostatic hypotension.[31] Excessive consumption of caffeine beverages should be avoided, because they can promote urine production (leading to fluid loss) and consequently hypovolemia.[35] Exposure to extreme heat may also aggravate symptoms.[22]
Prolonged physical inactivity can worsen the symptoms of POTS.[35] Techniques that increase a person's capacity for exercise, such as endurance training or graded exercise therapy, can relieve symptoms for some patients.[35] Aerobic exercise performed for 20 minutes a day, three times a week, is sometimes recommended for patients who can tolerate it.[84] Exercise may have the immediate effect of worsening tachycardia, especially after a meal or on a hot day.[35] In these cases, it may be easier to exercise in a semi-reclined position, such as riding a recumbent bicycle, rowing, or swimming.[35]
When changing to an upright posture, finishing a meal, or concluding exercise, a sustained hand grip can briefly raise the blood pressure, possibly reducing symptoms.[35] Compression garments can also be of benefit by constricting blood pressures with external body pressure.[35]
Aggravating factors include exertion (81%), continued standing (80%), heat (79%), and after meals (42%).[85]
If nonpharmacological methods are ineffective, medication may be necessary.[35] Medications used may include beta blockers, pyridostigmine, midodrine,[86] or fludrocortisone.[87][1] As of 2013, no medication has been approved by the U.S. Food and Drug Administration to treat POTS, but a variety are used off-label.[22] Their efficacy has not yet been examined in long-term randomized controlled trials.[22]
Fludrocortisone may be used to enhance sodium retention and blood volume, which may be beneficial not only by augmenting sympathetically mediated vasoconstriction, but also because a large subset of POTS patients appear to have low absolute blood volume.[88] However, fludrocortisone may cause hypokalemia.[89]
While people with POTS typically have normal or even elevated arterial blood pressure, the neuropathic form of POTS is presumed to constitute a selective sympathetic venous denervation.[88] In these patients the selective Alpha-1 adrenergic receptor agonist midodrine may increase venous return, enhance stroke volume, and improve symptoms.[88] Midodrine should only be taken during the daylight hours as it may promote supine hypertension.[88]
Sinus node blocker Ivabradine can successfully restrain heart rate in POTS without affecting blood pressure, demonstrated in approximately 60% of people with POTS treated in an open-label trial of ivabradine experienced symptom improvement.[90][91][88]
Pyridostigmine has been reported to restrain heart rate and improve chronic symptoms in approximately half of people. However, it may cause GI side effects that limit its use in around 20% of its patient population. [92] [22]
The selective alpha-1 agonist phenylephrine has been used successfully to enhance venous return and stroke volume in some people with POTS.[93] However, this medication may be hampered by poor oral bioavailability.[94]
POTS has a favorable prognosis when managed appropriately.[35] Symptoms improve within five years of diagnosis for many patients, and 60% return to their original level of functioning.[35] Approximately 90% of people with POTS respond to a combination of pharmacological and physical treatments.[7] Those who develop POTS in their early to mid teens will likely respond well to a combination of physical methods as well as pharmacotherapy.[119] Outcomes are more guarded for adults newly diagnosed with POTS.[49] Some people do not recover, and a few even worsen with time.[7] The hyperadrenergic type of POTS typically requires continuous therapy.[7] If POTS is caused by another condition, outcomes depend on the prognosis of the underlying disorder.[7]
The prevalence of POTS is unknown.[35] One study estimated a minimal rate of 170 POTS cases per 100,000 individuals, but the true prevalence is likely higher due to underdiagnosis.[35] Another study estimated that there are at least 500,000 cases in the United States.[6] POTS is more common in women than men, with a female-to-male ratio of 4:1.[78][120] Most people with POTS are aged between 20 and 40, with an average onset of 21.[2][78] Diagnoses of POTS beyond age 40 are rare, perhaps because symptoms improve with age.[35]
As recently stated,[121] up to one-third of POTS patients also present with Vasovagal Syncope (VVS).  This ratio is probably higher if pre-Syncope patients, patients that report the symptoms of Syncope without overt fainting, were included.  Given the difficulty with current autonomic measurements in quantitatively isolating and differentiating Parasympathetic (Vagal) activity from Sympathetic activity without assumption or approximation, the current direction of research and clinical assessment is understandable:  perpetuating uncertainty regarding underlying cause, prescribing beta-blockers and proper daily hydration as the only therapy, not addressing the orthostatic dysfunction as the underlying cause, and recommending acceptance and associated lifestyle changes to cope. 
Direct measures of Parasympathetic (Vagal) activity obviates the uncertainty and lack of true relief of POTS as well as VVS.  For example, the hypothesis that POTS is an auto-immune disorder may be an indication that a significant number of POTS cases are indeed co-morbid with VVS.  Remember the Parasympathetic Nervous System is the memory for, and controls and coordinates, the immune system.  If Parasympathetic (Vagal) over-, or prolonged-, activation is chronic then portions of the immune system may remain active beyond the limits of the infection.  Given that portions of the immune system are not of self, these portions remain active and continue to “feed.”  Once the only source of “feed” is self, the immune system begins to attack the host.  This is the definition of autoimmune.  This is a counter-hypothesis that may provide a simpler explanation with a more immediate plan for therapy and relief.  For it may be that relieving the Vagal over-activation, will retires the self-attacking portion of the immune system, thereby relieving the autoimmunity.
Another example may be “Hyperadrenergic POTS.”  A counter hypothesis and perhaps a simpler explanation that leads to more direct therapy and improved outcomes is again the fact that POTS and VVS may be co-morbid.  It is well known that Parasympathetic (Vagal) over-activation may cause secondary Sympathetic over-activation.  Without direct Parasympathetic (Vagal) measures, the resulting assumption is that the secondary Sympathetic over-activation (the definition of “hyperadrenergic”) is actually the primary autonomic dysfunction.  Simply treating the (secondary) Sympathetic over-activation may be just treating a symptom in these cases, which may work for a while but then the body compensates and more medication is needed or the patient become unresponsive and the permanent degraded lifestyles are considered the only option.  Again, this is unfortunate.  Given that cases of POTS with VVS involves different portions of the nervous system (Parasympathetic and Sympathetic), and that both branches may be treated simultaneously, albeit differently, true relief of both conditions, as needed, is quite possible, and the cases of these newer hypothesized causes may be relieved with current, less expensive, and shorter-term therapy modalities.
Conditions that are commonly reported with POTS include:[2][122]
In 1871, physician Jacob Mendes Da Costa described a condition that resembled the modern concept of POTS. He named it irritable heart syndrome.[35] Cardiologist Thomas Lewis expanded on the description, coining the term soldier's heart because it was often found among military personnel.[35] The condition came to be known as Da Costa's syndrome,[35] which is now recognized as several distinct disorders, including POTS.[citation needed]
Postural tachycardia syndrome was coined in 1982 in a description of a patient who had postural tachycardia, but not orthostatic hypotension.[35] Ronald Schondorf and Phillip A. Low of the Mayo Clinic first used the name postural orthostatic tachycardia syndrome, POTS, in 1993.[35][123]
British politician Nicola Blackwood revealed in March 2015 that she had been diagnosed with Ehlers–Danlos syndrome in 2013 and that she had later been diagnosed with POTS.[124] She was appointed Parliamentary Under-Secretary of State for Life Science by Prime Minister Theresa May in 2019 and given a life peerage that enabled her to take a seat in Parliament. As a junior minister, it is her responsibility to answer questions in parliament on the subjects of Health and departmental business. When answering these questions, it is customary for ministers to sit when listening to the question and then to rise to give an answer from the despatch box, thus standing up and sitting down numerous times in quick succession throughout a series of questions. On 17 June 2019, she fainted during one of these questioning sessions after standing up from a sitting position four times in the space of twelve minutes,[125] and it was suggested that her POTS was a factor in her fainting. Asked about the incident, she stated: "I was frustrated and embarrassed my body gave up on me at work ... But I am grateful it gives me a chance to shine a light on a condition many others are also living with."[126]
Insomnia, also known as sleeplessness, is a sleep disorder where people have trouble sleeping.[1] They may have difficulty falling asleep, or staying asleep for as long as desired.[1][9][11] Insomnia is typically followed by daytime sleepiness, low energy, irritability, and a depressed mood.[1] It may result in an increased risk of accidents of all kinds as well as problems focusing and learning.[9] Insomnia can be short term, lasting for days or weeks, or long term, lasting more than a month.[1] The concept of the word insomnia has two possibilities: insomnia disorder (ID) and insomnia symptoms, and many abstracts of randomized controlled trials and systematic reviews often underreport on which of these two possibilities the word insomnia refers to.[12]
Insomnia can occur independently or as a result of another problem.[2] Conditions that can result in insomnia include psychological stress, chronic pain, heart failure, hyperthyroidism, heartburn, restless leg syndrome, menopause, certain medications, and drugs such as caffeine, nicotine, and alcohol.[2][8] Other risk factors include working night shifts and sleep apnea.[9] Diagnosis is based on sleep habits and an examination to look for underlying causes.[3] A sleep study may be done to look for underlying sleep disorders.[3] Screening may be done with two questions: "do you experience difficulty sleeping?" and "do you have difficulty falling or staying asleep?"[9]
Although their efficacy as first line treatments is not unequivocally established,[13] sleep hygiene and lifestyle changes are typically the first treatment for insomnia.[5][7] Sleep hygiene includes a consistent bedtime, a quiet and dark room, exposure to sunlight during the day and regular exercise.[7] Cognitive behavioral therapy may be added to this.[6][14] While sleeping pills may help, they are sometimes associated with injuries, dementia, and addiction.[5][6] These medications are not recommended for more than four or five weeks.[6] The effectiveness and safety of alternative medicine is unclear.[5][6]
Between 10% and 30% of adults have insomnia at any given point in time and up to half of people have insomnia in a given year.[8][9][10] About 6% of people have insomnia that is not due to another problem and lasts for more than a month.[9] People over the age of 65 are affected more often than younger people.[7] Women are more often affected than men.[8] Descriptions of insomnia occur at least as far back as ancient Greece.[15]
Symptoms of insomnia:[17]
Sleep onset insomnia is difficulty falling asleep at the beginning of the night, often a symptom of anxiety disorders. Delayed sleep phase disorder can be misdiagnosed as insomnia, as sleep onset is delayed to much later than normal while awakening spills over into daylight hours.[19]
It is common for patients who have difficulty falling asleep to also have nocturnal awakenings with difficulty returning to sleep. Two-thirds of these patients wake up in the middle of the night, with more than half having trouble falling back to sleep after a middle-of-the-night awakening.[20]
Early morning awakening is an awakening occurring earlier (more than 30 minutes) than desired with an inability to go back to sleep, and before total sleep time reaches 6.5 hours. Early morning awakening is often a characteristic of depression.[21] Anxiety symptoms may well lead to insomnia. Some of these symptoms include tension, compulsive worrying about the future, feeling overstimulated, and overanalyzing past events.[22]
Poor sleep quality can occur as a result of, for example, restless legs, sleep apnea or major depression. Poor sleep quality is defined as the individual not reaching stage 3 or delta sleep which has restorative properties.[23]
Major depression leads to alterations in the function of the hypothalamic–pituitary–adrenal axis, causing excessive release of cortisol which can lead to poor sleep quality.
Nocturnal polyuria, excessive night-time urination, can  also result in a poor quality of sleep.[24]
Some cases of insomnia are not really insomnia in the traditional sense, because people experiencing sleep state misperception often sleep for a normal amount of time.[25]  The problem is that, despite sleeping for multiple hours each night and typically not experiencing significant daytime sleepiness or other symptoms of sleep loss, they do not feel like they have slept very much, if at all.[25]  Because their perception of their sleep is incomplete, they incorrectly believe it takes them an abnormally long time to fall asleep, and they underestimate how long they stay asleep.[25]
While insomnia can be caused by a number of conditions, it can also occur without any identifiable cause. This is known as Primary Insomnia.[26] Primary Insomnia may also have an initial identifiable cause, but continues after the cause is no longer present. For example, a bout of insomnia may be triggered by a stressful work or life event. However the condition may continue after the stressful event has been resolved. In such cases, the insomnia is usually perpetuated by the anxiety or fear caused by the sleeplessness itself, rather than any external factors.[27]
Symptoms of insomnia can be caused by or be associated with:
Sleep studies using polysomnography have suggested that people who have sleep disruption have elevated night-time levels of circulating cortisol and adrenocorticotropic hormone.[44] They also have an elevated metabolic rate, which does not occur in people who do not have insomnia but whose sleep is intentionally disrupted during a sleep study. Studies of brain metabolism using positron emission tomography (PET) scans indicate that people with insomnia have higher metabolic rates by night and by day. The question remains whether these changes are the causes or consequences of long-term insomnia.[45]
Heritability estimates of insomnia vary between 38% in males to 59% in females.[46] A genome-wide association study (GWAS) identified 3 genomic loci and 7 genes that influence the risk of insomnia, and showed that insomnia is highly polygenic.[47] In particular, a strong positive association was observed for the MEIS1 gene in both males and females. This study showed that the genetic architecture of insomnia strongly overlaps with psychiatric disorders and metabolic traits.
It has been hypothesized that epigenetics might also influence insomnia through a controlling process of both sleep regulation and brain-stress response having an impact as well on the brain plasticity.[48]
Alcohol is often used as a form of self-treatment of insomnia to induce sleep. However, alcohol use to induce sleep can be a cause of insomnia. Long-term use of alcohol is associated with a decrease in NREM stage 3 and 4 sleep as well as suppression of REM sleep and REM sleep fragmentation. Frequent moving between sleep stages occurs with; awakenings due to headaches, the need to urinate, dehydration, and excessive sweating. Glutamine rebound also plays a role as when someone is drinking; alcohol inhibits glutamine, one of the body's natural stimulants. When the person stops drinking, the body tries to make up for lost time by producing more glutamine than it needs.
The increase in glutamine levels stimulates the brain while the drinker is trying to sleep, keeping him/her from reaching the deepest levels of sleep.[49] Stopping chronic alcohol use can also lead to severe insomnia with vivid dreams. During withdrawal, REM sleep is typically exaggerated as part of a rebound effect.[50]
Like alcohol, benzodiazepines, such as alprazolam, clonazepam, lorazepam, and diazepam, are commonly used to treat insomnia in the short-term (both prescribed and self-medicated), but worsen sleep in the long-term. While benzodiazepines can put people to sleep (i.e., inhibit NREM stage 1 and 2 sleep), while asleep, the drugs disrupt sleep architecture: decreasing sleep time, delaying time to REM sleep, and decreasing deep slow-wave sleep (the most restorative part of sleep for both energy and mood).[51][52][53]
Opioid medications such as hydrocodone, oxycodone, and morphine are used for insomnia that is associated with pain due to their analgesic properties and hypnotic effects. Opioids can fragment sleep and decrease REM and stage 2 sleep. By producing analgesia and sedation, opioids may be appropriate in carefully selected patients with pain-associated insomnia.[33] However, dependence on opioids can lead to long-term sleep disturbances.[54]
Insomnia affects people of all age groups but people in the following groups have a higher chance of acquiring insomnia:[55]
Two main models exists as to the mechanism of insomnia, cognitive and physiological. The cognitive model suggests rumination and hyperarousal contribute to preventing a person from falling asleep and might lead to an episode of insomnia.
The physiological model is based upon three major findings in people with insomnia; firstly, increased urinary cortisol and catecholamines have been found suggesting increased activity of the HPA axis and arousal; second, increased global cerebral glucose utilization during wakefulness and NREM sleep in people with insomnia; and lastly, increased full body metabolism and heart rate in those with insomnia. All these findings taken together suggest a deregulation of the arousal system, cognitive system, and HPA axis all contributing to insomnia.[9][58] However, it is unknown if the hyperarousal is a result of, or cause of insomnia. Altered levels of the inhibitory neurotransmitter GABA have been found, but the results have been inconsistent, and the implications of altered levels of such a ubiquitous neurotransmitter are unknown. Studies on whether insomnia is driven by circadian control over sleep or a wake dependent process have shown inconsistent results, but some literature suggests a deregulation of the circadian rhythm based on core temperature.[59] Increased beta activity and decreased delta wave activity has been observed on electroencephalograms; however, the implication of this is unknown.[60]
Around half of post-menopausal women experience sleep disturbances, and generally sleep disturbance is about twice as common in women as men; this appears to be due in part, but not completely, to changes in hormone levels, especially in and post-menopause.[34][61]
Changes in sex hormones in both men and women as they age may account in part for increased prevalence of sleep disorders in older people.[62]
In medicine, insomnia is widely measured using the Athens insomnia scale.[63] It is measured using eight different parameters related to sleep, finally represented as an overall scale which assesses an individual's sleep pattern.
A qualified sleep specialist should be consulted for the diagnosis of any sleep disorder so the appropriate measures can be taken. Past medical history and a physical examination need to be done to eliminate other conditions that could be the cause of insomnia. After all other conditions are ruled out a comprehensive sleep history should be taken. The sleep history should include sleep habits, medications (prescription and non-prescription), alcohol consumption, nicotine and caffeine intake, co-morbid illnesses, and sleep environment.[64] A sleep diary can be used to keep track of the individual's sleep patterns. The diary should include time to bed, total sleep time, time to sleep onset, number of awakenings, use of medications, time of awakening, and subjective feelings in the morning.[64] The sleep diary can be replaced or validated by the use of out-patient actigraphy for a week or more, using a non-invasive device that measures movement.[65]
Workers who complain of insomnia should not routinely have polysomnography to screen for sleep disorders.[66] This test may be indicated for patients with symptoms in addition to insomnia, including sleep apnea, obesity, a thick neck diameter, or high-risk fullness of the flesh in the oropharynx.[66] Usually, the test is not needed to make a diagnosis, and insomnia especially for working people can often be treated by changing a job schedule to make time for sufficient sleep and by improving sleep hygiene.[66]
Some patients may need to do an overnight sleep study to determine if insomnia is present. Such a study will commonly involve assessment tools including a polysomnogram and the multiple sleep latency test. Specialists in sleep medicine are qualified to diagnose disorders within the, according to the ICSD, 81 major sleep disorder diagnostic categories.[67] Patients with some disorders, including delayed sleep phase disorder, are often mis-diagnosed with primary insomnia; when a person has trouble getting to sleep and awakening at desired times, but has a normal sleep pattern once asleep, a circadian rhythm disorder is a likely cause.
In many cases, insomnia is co-morbid with another disease, side-effects from medications, or a psychological problem. Approximately half of all diagnosed insomnia is related to psychiatric disorders.[68] For those who have depression, "insomnia should be regarded as a co-morbid condition, rather than as a secondary one;" insomnia typically predates psychiatric symptoms.[68] "In fact, it is possible that insomnia represents a significant risk for the development of a subsequent psychiatric disorder."[9] Insomnia occurs in between 60% and 80% of people with depression.[69] This may partly be due to treatment used for depression.[69]
Determination of causation is not necessary for a diagnosis.[68]
The DSM-5 criteria for insomnia include the following:[70]
Predominant complaint of dissatisfaction with sleep quantity or quality, associated with one (or more) of the following symptoms:
In addition:
Insomnia can be classified as transient, acute, or chronic.
Prevention and treatment of insomnia may require a combination of cognitive behavioral therapy,[14] medications,[76] and lifestyle changes.[77]
Among lifestyle practices, going to sleep and waking up at the same time each day can create a steady pattern which may help to prevent insomnia.[11] Avoidance of vigorous exercise and caffeinated drinks a few hours before going to sleep is recommended, while exercise earlier in the day may be beneficial.[77] Other practices to improve sleep hygiene may include:[77][78]
It is recommended to rule out medical and psychological causes before deciding on the treatment for insomnia.[79] Cognitive behavioral therapy is generally the first line treatment once this has been done.[80] It has been found to be effective for chronic insomnia.[14] The beneficial effects, in contrast to those produced by medications, may last well beyond the stopping of therapy.[81]
Medications have been used mainly to reduce symptoms in insomnia of short duration; their role in the management of chronic insomnia remains unclear.[8] Several different types of medications may be used.[82][83][76] Many doctors do not recommend relying on prescription sleeping pills for long-term use.[77] It is also important to identify and treat other medical conditions that may be contributing to insomnia, such as depression, breathing problems, and chronic pain.[77][84] As of 2022, many people with insomnia were reported as not receiving overall sufficient sleep or treatment for insomnia.[85][86]
Non-medication based strategies have comparable efficacy to hypnotic medication for insomnia and they may have longer lasting effects. Hypnotic medication is only recommended for short-term use because dependence with rebound withdrawal effects upon discontinuation or tolerance can develop.[87]
Non medication based strategies provide long lasting improvements to insomnia and are recommended as a first line and long-term strategy of management. Behavioral sleep medicine (BSM) tries to address insomnia with non-pharmacological treatments. The BSM strategies used to address chronic insomnia include attention to sleep hygiene, stimulus control, behavioral interventions, sleep-restriction therapy, paradoxical intention, patient education, and relaxation therapy.[88] Some examples are keeping a journal, restricting the time spent awake in bed, practicing relaxation techniques, and maintaining a regular sleep schedule and a wake-up time.[84] Behavioral therapy can assist a patient in developing new sleep behaviors to improve sleep quality and consolidation. Behavioral therapy may include, learning healthy sleep habits to promote sleep relaxation, undergoing light therapy to help with worry-reduction strategies and regulating the circadian clock.[84]
Music may improve insomnia in adults (see music and sleep).[89] EEG biofeedback has demonstrated effectiveness in the treatment of insomnia with improvements in duration as well as quality of sleep.[90] Self-help therapy (defined as a psychological therapy that can be worked through on one's own) may improve sleep quality for adults with insomnia to a small or moderate degree.[91]
Stimulus control therapy is a treatment for patients who have conditioned themselves to associate the bed, or sleep in general, with a negative response. As stimulus control therapy involves taking steps to control the sleep environment, it is sometimes referred interchangeably with the concept of sleep hygiene. Examples of such environmental modifications include using the bed for sleep and sex only, not for activities such as reading or watching television; waking up at the same time every morning, including on weekends; going to bed only when sleepy and when there is a high likelihood that sleep will occur; leaving the bed and beginning an activity in another location if sleep does not occur in a reasonably brief period of time after getting into bed (commonly ~20 min); reducing the subjective effort and energy expended trying to fall asleep; avoiding exposure to bright light during night-time hours, and eliminating daytime naps.[92]
A component of stimulus control therapy is sleep restriction, a technique that aims to match the time spent in bed with actual time spent asleep. This technique involves maintaining a strict sleep-wake schedule, sleeping only at certain times of the day and for specific amounts of time to induce mild sleep deprivation. Complete treatment usually lasts up to 3 weeks and involves making oneself sleep for only a minimum amount of time that they are actually capable of on average, and then, if capable (i.e. when sleep efficiency improves), slowly increasing this amount (~15 min) by going to bed earlier as the body attempts to reset its internal sleep clock. Bright light therapy may be effective for insomnia.[93]
Paradoxical intention is a cognitive reframing technique where the insomniac, instead of attempting to fall asleep at night, makes every effort to stay awake (i.e. essentially stops trying to fall asleep). One theory that may explain the effectiveness of this method is that by not voluntarily making oneself go to sleep, it relieves the performance anxiety that arises from the need or requirement to fall asleep, which is meant to be a passive act. This technique has been shown to reduce sleep effort and performance anxiety and also lower subjective assessment of sleep-onset latency and overestimation of the sleep deficit (a quality found in many insomniacs).[94]
Sleep hygiene is a common term for all of the behaviors which relate to the promotion of good sleep. They include habits which provide a good foundation for sleep and help to prevent insomnia. However, sleep hygiene alone may not be adequate to address chronic insomnia.[65] Sleep hygiene recommendations are typically included as one component of cognitive behavioral therapy for insomnia (CBT-I).[65][6] Recommendations include reducing caffeine, nicotine, and alcohol consumption, maximizing the regularity and efficiency of sleep episodes, minimizing medication usage and daytime napping, the promotion of regular exercise, and the facilitation of a positive sleep environment.[95] The creation of a positive sleep environment may also be helpful in reducing the symptoms of insomnia.[96]
On the other hand, a systematic review by the AASM concluded that clinicians should not prescribe sleep hygiene for insomnia due to the evidence of absence of its efficacy and potential delaying of adequate treatment, recommending instead that effective therapies such as CBT-i should be preferred.[13]
There is some evidence that cognitive behavioral therapy for insomnia (CBT-I) is superior in the long-term to benzodiazepines and the nonbenzodiazepines in the treatment and management of insomnia.[97] In this therapy, patients are taught improved sleep habits and relieved of counter-productive assumptions about sleep. Common misconceptions and expectations that can be modified include:
Numerous studies have reported positive outcomes of combining cognitive behavioral therapy for insomnia treatment with treatments such as stimulus control and the relaxation therapies. Hypnotic medications are equally effective in the short-term treatment of insomnia, but their effects wear off over time due to tolerance. The effects of CBT-I have sustained and lasting effects on treating insomnia long after therapy has been discontinued.[98][99] The addition of hypnotic medications with CBT-I adds no benefit in insomnia. The long lasting benefits of a course of CBT-I shows superiority over pharmacological hypnotic drugs. Even in the short term when compared to short-term hypnotic medication such as zolpidem, CBT-I still shows significant superiority. Thus CBT-I is recommended as a first line treatment for insomnia.[100]
Common forms of CBT-I treatments include stimulus control therapy, sleep restriction, sleep hygiene, improved sleeping environments, relaxation training, paradoxical intention, and biofeedback.[101]
CBT is the well-accepted form of therapy for insomnia since it has no known adverse effects, whereas taking medications to alleviate insomnia symptoms have been shown to have adverse side effects.[102] Nevertheless, the downside of CBT is that it may take a lot of time and motivation.[103]
Treatments based on the principles of acceptance and commitment therapy (ACT) and metacognition have emerged as alternative approaches to treating insomnia.[104] ACT rejects the idea that behavioral changes can help insomniacs achieve better sleep, since they require "sleep efforts" - actions which create more "struggle" and arouse the nervous system, leading to hyperarousal.[105] The ACT approach posits that acceptance of the negative feelings associated with insomnia can, in time, create the right conditions for sleep. Mindfulness practice is a key feature of this approach, although mindfulness is not practised to induce sleep (this in itself is a sleep effort to be avoided) but rather as a longer-term activity to help calm the nervous system and create the internal conditions from which sleep can emerge.
A key distinction between CBT-i and ACT lies in the divergent approaches to time spent awake in bed. Proponents of CBT-i advocate minimizing time spent awake in bed, on the basis that this creates cognitive association between being in bed and wakefulness. The ACT approach proposes that avoiding time in bed may increase the pressure to sleep and arouse the nervous system further.[105]
Research has shown that "ACT has a significant effect on primary and comorbid insomnia and sleep quality, and ... can be used as an appropriate treatment method to control and improve insomnia".[106]
Despite the therapeutic effectiveness and proven success of CBT, treatment availability is significantly limited by a lack of trained clinicians, poor geographical distribution of knowledgeable professionals, and expense.[107] One way to potentially overcome these barriers is to use the Internet to deliver treatment, making this effective intervention more accessible and less costly. The Internet has already become a critical source of health-care and medical information.[108] Although the vast majority of health websites provide general information,[108][109] there is growing research literature on the development and evaluation of Internet interventions.[110][111]
These online programs are typically behaviorally-based treatments that have been operationalized and transformed for delivery via the Internet. They are usually highly structured; automated or human supported; based on effective face-to-face treatment; personalized to the user; interactive; enhanced by graphics, animations, audio, and possibly video; and tailored to provide follow-up and feedback.[111]
There is good evidence for the use of computer based CBT for insomnia.[112]
Many people with insomnia use sleeping tablets and other sedatives. In some places medications are prescribed in over 95% of cases.[113] They, however, are a second line treatment.[114] In 2019, the US Food and Drug Administration stated it is going to require warnings for eszopiclone, zaleplon, and zolpidem, due to concerns about serious injuries resulting from abnormal sleep behaviors, including sleepwalking or driving a vehicle while asleep.[115]
The percentage of adults using a prescription sleep aid increases with age. During 2005–2010, about 4% of U.S. adults aged 20 and over reported that they took prescription sleep aids in the past 30 days. Rates of use were lowest among the youngest age group (those aged 20–39) at about 2%, increased to 6% among those aged 50–59, and reached 7% among those aged 80 and over. More adult women (5%) reported using prescription sleep aids than adult men (3%). Non-Hispanic white adults reported higher use of sleep aids (5%) than non-Hispanic black (3%) and Mexican-American (2%) adults. No difference was shown between non-Hispanic black adults and Mexican-American adults in use of prescription sleep aids.[116]
As an alternative to taking prescription drugs, some evidence shows that an average person seeking short-term help may find relief by taking over-the-counter antihistamines such as diphenhydramine or doxylamine.[117] Diphenhydramine and doxylamine are widely used in nonprescription sleep aids. They are the most effective over-the-counter sedatives currently available, at least in much of Europe, Canada, Australia, and the United States, and are more sedating than some prescription hypnotics.[118] Antihistamine effectiveness for sleep may decrease over time, and anticholinergic side-effects (such as dry mouth) may also be a drawback with these particular drugs. While addiction does not seem to be an issue with this class of drugs, they can induce dependence and rebound effects upon abrupt cessation of use.[119] However, people whose insomnia is caused by restless legs syndrome may have worsened symptoms with antihistamines.[120]
While insomnia is a common symptom of depression, antidepressants are effective for treating sleep problems whether or not they are associated with depression. While all antidepressants help regulate sleep, some antidepressants, such as amitriptyline, doxepin, mirtazapine, trazodone, and trimipramine, can have an immediate sedative effect, and are prescribed to treat insomnia.[121] Trazodone was at the beginning of the 2020s the biggest prescribed drug for sleep in the United States despite not being indicated for sleep.[122]
Amitriptyline, doxepin, and trimipramine all have antihistaminergic, anticholinergic, antiadrenergic, and antiserotonergic properties, which contribute to both their therapeutic effects and side effect profiles, while mirtazapine's actions are primarily antihistaminergic and antiserotonergic and trazodone's effects are primarily antiadrenergic and antiserotonergic. Mirtazapine is known to decrease sleep latency (i.e., the time it takes to fall asleep), promoting sleep efficiency and increasing the total amount of sleeping time in people with both depression and insomnia.[123][124]
Agomelatine, a melatonergic antidepressant with claimed sleep-improving qualities that does not cause daytime drowsiness,[125] is approved for the treatment of depression though not sleep conditions in the European Union[126] and Australia.[127] After trials in the United States, its development for use there was discontinued in October 2011[128] by Novartis, who had bought the rights to market it there from the European pharmaceutical company Servier.[129]
A 2018 Cochrane review found the safety of taking antidepressants for insomnia to be uncertain with no evidence supporting long term use.[130]
Melatonin receptor agonists such as melatonin and ramelteon are used in the treatment of insomnia. The evidence for melatonin in treating insomnia is generally poor.[131] There is low-quality evidence that it may speed the onset of sleep by 6 minutes.[131] Ramelteon does not appear to speed the onset of sleep or the amount of sleep a person gets.[131]
Usage of melatonin as a treatment for insomnia in adults has increased from 0.4% between 1999 and 2000 to nearly 2.1% between 2017 and 2018.[132]
Most melatonin agonists have not been tested for longitudinal side effects.[133] Prolonged-release melatonin may improve quality of sleep in older people with minimal side effects.[134][135]
Studies have also shown that children who are on the autism spectrum or have learning disabilities, attention-deficit hyperactivity disorder (ADHD) or related neurological diseases can benefit from the use of melatonin. This is because they often have trouble sleeping due to their disorders. For example, children with ADHD tend to have trouble falling asleep because of their hyperactivity and, as a result, tend to be tired during most of the day. Another cause of insomnia in children with ADHD is the use of stimulants used to treat their disorder. Children who have ADHD then, as well as the other disorders mentioned, may be given melatonin before bedtime in order to help them sleep.[136]
The most commonly used class of hypnotics for insomnia are the benzodiazepines.[36]: 363  Benzodiazepines are not significantly better for insomnia than antidepressants.[138] Chronic users of hypnotic medications for insomnia do not have better sleep than chronic insomniacs not taking medications. In fact, chronic users of hypnotic medications have more regular night-time awakenings than insomniacs not taking hypnotic medications.[139] Many have concluded that these drugs cause an unjustifiable risk to the individual and to public health and lack evidence of long-term effectiveness. It is preferred that hypnotics be prescribed for only a few days at the lowest effective dose and avoided altogether wherever possible, especially in the elderly.[140] Between 1993 and 2010, the prescribing of benzodiazepines to individuals with sleep disorders has decreased from 24% to 11% in the US, coinciding with the first release of nonbenzodiazepines.[141]
The benzodiazepine and nonbenzodiazepine hypnotic medications also have a number of side-effects such as day time fatigue, motor vehicle crashes and other accidents, cognitive impairments, and falls and fractures. Elderly people are more sensitive to these side-effects.[142] Some benzodiazepines have demonstrated effectiveness in sleep maintenance in the short term but in the longer term benzodiazepines can lead to tolerance, physical dependence, benzodiazepine withdrawal syndrome upon discontinuation, and long-term worsening of sleep, especially after consistent usage over long periods of time. Benzodiazepines, while inducing unconsciousness, actually worsen sleep as – like alcohol – they promote light sleep while decreasing time spent in deep sleep.[143] A further problem is, with regular use of short-acting sleep aids for insomnia, daytime rebound anxiety can emerge.[144] Although there is little evidence for benefit of benzodiazepines in insomnia compared to other treatments and evidence of major harm, prescriptions have continued to increase.[145] This is likely due to their addictive nature, both due to misuse and because – through their rapid action, tolerance and withdrawal they can "trick" insomniacs into thinking they are helping with sleep. There is a general awareness that long-term use of benzodiazepines for insomnia in most people is inappropriate and that a gradual withdrawal is usually beneficial due to the adverse effects associated with the long-term use of benzodiazepines and is recommended whenever possible.[146][147]
Benzodiazepines all bind unselectively to the GABAA receptor.[138] Some theorize that certain benzodiazepines (hypnotic benzodiazepines) have significantly higher activity at the α1 subunit of the GABAA receptor compared to other benzodiazepines (for example, triazolam and temazepam have significantly higher activity at the α1 subunit compared to alprazolam and diazepam, making them superior sedative-hypnotics – alprazolam and diazepam, in turn, have higher activity at the α2 subunit compared to triazolam and temazepam, making them superior anxiolytic agents). Modulation of the α1 subunit is associated with sedation, motor impairment, respiratory depression, amnesia, ataxia, and reinforcing behavior (drug-seeking behavior). Modulation of the α2 subunit is associated with anxiolytic activity and disinhibition. For this reason, certain benzodiazepines may be better suited to treat insomnia than others.[96]
Nonbenzodiazepine or Z-drug sedative–hypnotic drugs, such as zolpidem, zaleplon, zopiclone, and eszopiclone, are a class of hypnotic medications that are similar to benzodiazepines in their mechanism of action, and indicated for mild to moderate insomnia. Their effectiveness at improving time to sleeping is slight, and they have similar—though potentially less severe—side effect profiles compared to benzodiazepines.[148] Prescribing of nonbenzodiazepines has seen a general increase since their initial release on the US market in 1992, from 2.3% in 1993 among individuals with sleep disorders to 13.7% in 2010.[141]
Orexin receptor antagonists are a more recently introduced class of sleep medications and include suvorexant, lemborexant, and daridorexant, all of which are FDA-approved for treatment of insomnia characterized by difficulties with sleep onset and/or sleep maintenance.[149][150] They are oriented towards blocking signals in the brain that stimulate wakefulness, therefore claiming to address insomnia without creating dependence. There are three dual orexin receptor (DORA) drugs on the market: Belsomra (Merck), Dayvigo (Eisai) and Quviviq (Idorsia).[151]
Certain atypical antipsychotics, particularly quetiapine, olanzapine, and risperidone, are used in the treatment of insomnia.[152][153] However, while common, use of antipsychotics for this indication is not recommended as the evidence does not demonstrate a benefit, and the risk of adverse effects are significant.[152][154][155][156] A major 2022 systematic review and network meta-analysis of medications for insomnia in adults found that quetiapine did not demonstrate any short-term benefits for insomnia.[157] Some of the more serious adverse effects may also occur at the low doses used, such as dyslipidemia and neutropenia.[158][159] Such concerns of risks at low doses are supported by Danish observational studies that showed an association of use of low-dose quetiapine (excluding prescriptions filled for tablet strengths >50 mg) with an increased risk of major cardiovascular events as compared to use of Z-drugs, with most of the risk being driven by cardiovascular death.[160] Laboratory data from an unpublished analysis of the same cohort also support the lack of dose-dependency of metabolic side effects, as new use of low-dose quetiapine was associated with a risk of increased fasting triglycerides at 1-year follow-up.[161] Concerns regarding side effects are greater in the elderly.[133]
Gabapentinoids like gabapentin and pregabalin have sleep-promoting effects but are not commonly used for treatment of insomnia.[162] Gabapentin is not effective in helping alcohol related insomnia.[163][164]
Barbiturates, while once used, are no longer recommended for insomnia due to the risk of addiction and other side effects.[165]
Medications for the treatment of insomnia have a wide range of effect sizes.[157] When comparing drugs such as benzodiazepines, Z-drugs, sedative antidepressants and antihistamines, quetiapine, orexin receptor antagonists, and melatonin receptor agonists, the orexin antagonist lemborexant and the Z-drug eszopiclone had the best profiles overall in terms of efficacy, tolerability, and acceptability.[157]
Herbal products, such as valerian, kava, chamomile, and lavender, have been used to treat insomnia.[166][167][168][169] However, there is no quality evidence that they are effective and safe.[166][167][168][169] The same is true for cannabis and cannabinoids.[170][171][172] It is likewise unclear whether acupuncture is useful in the treatment of insomnia.[173]
A survey of 1.1 million residents in the United States found that those that reported sleeping about 7 hours per night had the lowest rates of mortality, whereas those that slept for fewer than 6 hours or more than 8 hours had higher mortality rates.  Severe insomnia – sleeping less than 3.5 hours in women and 4.5 hours in men – is associated with a 15% increase in mortality, while getting 8.5 or more hours of sleep per night was associated with a 15% higher mortality rate.[174]
With this technique, it is difficult to distinguish lack of sleep caused by a disorder which is also a cause of premature death, versus a disorder which causes a lack of sleep, and the lack of sleep causing premature death. Most of the increase in mortality from severe insomnia was discounted after controlling for associated disorders. After controlling for sleep duration and insomnia, use of sleeping pills was also found to be associated with an increased mortality rate.[174]
The lowest mortality was seen in individuals who slept between six and a half and seven and a half hours per night. Even sleeping only 4.5 hours per night is associated with very little increase in mortality. Thus, mild to moderate insomnia for most people is associated with increased longevity and severe insomnia is associated only with a very small effect on mortality.[174] It is unclear why sleeping longer than 7.5 hours is associated with excess mortality.[174]
Between 10% and 30% of adults have insomnia at any given point in time and up to half of people have insomnia in a given year, making it the most common sleep disorder.[9][8][10][175] About 6% of people have insomnia that is not due to another problem and lasts for more than a month.[9] People over the age of 65 are affected more often than younger people.[7] Females are more often affected than males.[8] Insomnia is 40% more common in women than in men.[176]
There are higher rates of insomnia reported among university students compared to the general population.[177]
The word insomnia is from Latin: in + somnus "without sleep" and -ia as a nominalizing suffix.
The popular press have published stories about people who supposedly never sleep, such as that of Thái Ngọc and Al Herpin.[178] Horne writes "everybody sleeps and needs to do so", and generally this appears true. However, he also relates from contemporary accounts the case of Paul Kern, who was shot in wartime and then "never slept again" until his death in 1955.[179] Kern appears to be a completely isolated, unique case.

Fibromyalgia is a medical condition defined by the presence of chronic widespread pain, fatigue, waking unrefreshed, cognitive symptoms, lower abdominal pain or cramps, and depression.[9] Other symptoms include insomnia[10] and a general hypersensitivity.[11][12]
The cause of fibromyalgia is unknown, but is believed to involve a combination of genetic and environmental factors.[4] Environmental factors may include psychological stress, trauma, and certain infections.[4] The pain appears to result from processes in the central nervous system and the condition is referred to as a "central sensitization syndrome".[4][13]
The treatment of fibromyalgia is symptomatic[14] and multidisciplinary.[15] The European Alliance of Associations for Rheumatology strongly recommends aerobic and strengthening exercise.[15] Weak recommendations are given to mindfulness, psychotherapy, acupuncture, hydrotherapy, and meditative exercise such as qigong, yoga, and tai chi.[15] The use of medication in the treatment of fibromyalgia is debated,[15][16] although antidepressants can improve quality of life.[17] The medications duloxetine, milnacipran, or pregabalin have been approved by the US Food and Drug Administration (FDA) for the management of fibromyalgia. Other common helpful medications include serotonin-noradrenaline reuptake inhibitors, nonsteroidal anti-inflammatory drugs, and muscle relaxants.[18] Q10 coenzyme and vitamin D supplements may reduce pain and improve quality of life.[19] While fibromyalgia is persistent in nearly all patients, it does not result in death or tissue damage.[16]
Fibromyalgia is estimated to affect 2–4% of the population.[20] Women are affected about twice as often as men.[4][20] Rates appear similar in different areas of the world and among different cultures.[4] Fibromyalgia was first defined in 1990, with updated criteria in 2011,[4] 2016,[9] and 2019.[12] The term "fibromyalgia" is from Neo-Latin fibro-, meaning "fibrous tissues", Greek μυο- myo-, "muscle", and Greek άλγος algos, "pain"; thus, the term literally means "muscle and fibrous connective tissue pain".[21]
Fibromyalgia is classed as a disorder of pain processing due to abnormalities in how pain signals are processed in the central nervous system.[22] The International Classification of Diseases (ICD-11) includes fibromyalgia in the category of "Chronic widespread pain", code MG30.01.[23]
The defining symptoms of fibromyalgia are chronic widespread pain, fatigue, and sleep disturbance.[12] Other symptoms may include heightened pain in response to tactile pressure (allodynia),[12] cognitive problems,[12] musculoskeletal stiffness,[12] environmental sensitivity,[12] hypervigilance,[12] sexual dysfunction,[24] and visual symptoms.[25]
Fibromyalgia is predominantly a chronic pain disorder.[12] According to the NHS, widespread pain is one major symptom, which could feel like an ache, a burning sensation, or a sharp, stabbing pain.[26]
Fatigue is one of the defining symptoms of fibromyalgia.[12] Patients may experience physical or mental fatigue. Physical fatigue can be demonstrated by a feeling of exhaustion after exercise or by a limitation in daily activities.[12]
Sleep problems are a core symptom in fibromyalgia.[12] These include difficulty falling asleep or staying asleep, awakening while sleeping and waking up feeling unrefreshed.[12] A meta-analysis compared objective and subjective sleep metrics in people with fibromyalgia and healthy people. Individuals with fibromyalgia had lower sleep quality and efficiency, as well as longer wake time after sleep start, shorter sleep duration, lighter sleep, and greater trouble initiating sleep when objectively assessed, and more difficulty initiating sleep when subjectively assessed.[10] Sleep problems may contribute to pain by decreased release of IGF-1 and human growth hormone, leading to decreased tissue repair.[27] Improving sleep quality can help people with fibromyalgia minimize pain.[28][29]
Many people with fibromyalgia experience cognitive problems (known as fibrofog or brainfog). One study found that approximately 50% of fibromyalgia patients had subjective cognitive dysfunction and that it was associated with higher levels of pain and other fibromyalgia symptoms.[30] The American Pain Society recognizes these problems as a major feature of fibromyalgia, characterized by trouble concentrating, forgetfulness and disorganized or slow thinking.[12] About 75% of fibromyalgia patients report significant problems with concentration, memory, and multitasking.[31] A 2018 meta-analysis found that the largest differences between fibromyalgia patients and healthy subjects were for inhibitory control, memory, and processing speed.[31] It is hypothesized that the increased pain compromises attention systems, resulting in cognitive problems.[31]
In addition to a hypersensitivity to pain, patients with fibromyalgia show hypersensitivity to other stimuli,[11] such as bright lights, loud noises, perfumes and cold.[12] A review article found that they have a lower cold pain threshold.[32] Other studies documented an acoustic hypersensitivity.[33]
Fibromyalgia as a stand-alone diagnosis is uncommon, as most fibromyalgia patients often have other chronic overlapping pain problems or mental disorders.[11] Fibromyalgia is associated with mental health issues like anxiety,[34] posttraumatic stress disorder,[4][34] bipolar disorder,[34] alexithymia,[35] and depression.[34][36][37] Patients with fibromyalgia are five times more likely to have major depression than the general population.[38]
Fibromyalgia and numerous chronic pain conditions frequently coexist.[36] These include chronic tension headaches,[34] myofascial pain syndrome,[34] and temporomandibular disorders.[34] Multiple sclerosis, post-polio syndrome, neuropathic pain, and Parkinson's disease are four neurological disorders that have been linked to pain or fibromyalgia.[36] Fibromyalgia largely overlaps with chronic fatigue syndrome[39][40] and may share the same pathogenetic mechanisms.[40]
Comorbid fibromyalgia has been reported to occur in 20–30% of individuals with rheumatic diseases.[36] It has been reported in people with noninflammatory musculoskeletal diseases.[36]
The prevalence of fibromyalgia in gastrointestinal disease has been described mostly for celiac disease[36] and irritable bowel syndrome (IBS).[36][34] IBS and fibromyalgia share similar pathogenic mechanisms, involving immune system mast cells, inflammatory biomarkers, hormones, and neurotransmitters such as serotonin. Changes in the gut biome alter serotonin levels, leading to autonomic nervous system hyperstimulation.[41]
Fibromyalgia has also been linked with obesity.[42] Other conditions that are associated with fibromyalgia include connective tissue disorders,[43] cardiovascular autonomic abnormalities,[44] restless leg syndrome[45] and an overactive bladder.[46]
The cause of fibromyalgia is unknown. However, several risk factors, genetic and environmental, have been identified.
Genetics play a major role in fibromyalgia, and may explain up to 50% of the disease susceptibility.[47] Fibromyalgia is potentially associated with polymorphisms of genes in the serotoninergic,[48] dopaminergic[48] and catecholaminergic systems.[48] Several genes have been suggested as candidates for susceptibility to fibromyalgia. These include SLC6A4,[47] TRPV2,[47] MYT1L,[47] NRXN3,[47] and the 5-HT2A receptor 102T/C polymorphism.[49] The heritability of fibromyalgia is estimated to be higher in patients younger than 50.[50]
Nearly all the genes suggested as potential risk factors for fibromyalgia are associated with neurotransmitters and their receptors.[51] Neuropathic pain and major depressive disorder often co-occur with fibromyalgia – the reason for this comorbidity appears to be due to shared genetic abnormalities, which leads to impairments in monoaminergic, glutamatergic, neurotrophic, opioid and proinflammatory cytokine signaling. In these vulnerable individuals, psychological stress or illness can cause abnormalities in inflammatory and stress pathways that regulate mood and pain. Eventually, a sensitization and kindling effect occurs in certain neurons leading to the establishment of fibromyalgia and sometimes a mood disorder.[52]
Stress may be an important precipitating factor in the development of fibromyalgia.[53] A 2021 meta-analysis found psychological trauma to be strongly associated with fibromyalgia.[54][55] People who suffered abuse in their lifetime were three times more likely to have fibromyalgia, people who suffered medical trauma or other stressors in their lifetime were about twice as likely.[54]
Some authors have proposed that, because exposure to stressful conditions can alter the function of the hypothalamic-pituitary-adrenal (HPA) axis, the development of fibromyalgia may stem from stress-induced disruption of the HPA axis.[56][57]
Although some have suggested that fibromyalgia patients are more likely to have specific personality traits, when depression is statistically controlled for, it appears that their personality is no different than that of people in the general population.[58]
Other risk markers for fibromyalgia include premature birth, female sex, cognitive influences, primary pain disorders, multiregional pain, infectious illness, hypermobility of joints, iron deficiency and small-fiber polyneuropathy.[59] Metal-induced allergic inflammation has also been linked with fibromyalgia, especially in response to nickel but also inorganic mercury, cadmium, and lead.[60] Following the COVID-19 pandemic, some have suggested that the SARS-CoV-2 virus may trigger fibromyalgia.[61]
As of 2022, the pathophysiology of fibromyalgia has not yet been elucidated[62] and several theories have been suggested. The prevailing perspective considers fibromyalgia as a condition resulting from an amplification of pain by the central nervous system.[51] Substantial biological evidence backs up this notion, leading to the term of nociplastic pain.[51]
Chronic pain can be divided into three categories. Nociceptive pain is pain caused by inflammation or damage to tissues. Neuropathic pain is pain caused by nerve damage. Nociplastic pain (or central sensitization) is less understood and is the common explanation of the pain experienced in fibromyalgia.[13][20][63] Because the three forms of pain can overlap, fibromyalgia patients may experience nociceptive (e.g., rheumatic illnesses) and neuropathic (e.g., small fiber neuropathy) pain, in addition to nociplastic pain.[20]
Fibromyalgia can be viewed as a condition of nociplastic pain.[64] Nociplastic pain is caused by an altered function of pain-related sensory pathways in the periphery and the central nervous system, resulting in hypersensitivity.[65]
Nociplastic pain is commonly referred to as "Nociplastic pain syndrome" because it is coupled with other symptoms.[20] These include fatigue, sleep disturbance, cognitive disturbance, hypersensitivity to environmental stimuli, anxiety, and depression.[20] Nociplastic pain is caused by either (1) increased processing of pain stimuli or (2) decreased suppression of pain stimuli at several levels in the nervous system, or both.[20]
An alternative hypothesis to nociplastic pain views fibromyalgia as a stress-related dysautonomia with neuropathic pain features.[66] This view highlights the role of autonomic and peripheral nociceptive nervous systems in the generation of widespread pain, fatigue, and insomnia.[67] The description of small fiber neuropathy in a subgroup of fibromyalgia patients supports the disease neuropathic-autonomic underpinning.[66] However, others claim that small fiber neuropathy occurs only in small groups of those with fibromyalgia.[16]
Some suggest that fibromyalgia is caused or maintained by a decreased vagal tone, which is indicated by low levels of heart rate variability,[53] signaling a heightened sympathetic response.[68] Accordingly, several studies show that clinical improvement is associated with an increase in heart rate variability.[69][68][70] Some examples of interventions that increase the heart rate variability and vagal tone are meditation, yoga, mindfulness and exercise.[53] In 2023 the Fibromyalgia: Imbalance of Threat and Soothing Systems (FITSS) model was suggested as a working hypothesis.[71] According to the FITSS model, the salience network (also known as the midcingulo-insular network) may remain continuously hyperactive due to an imbalance in emotion regulation, which is reflected by an overactive "threat" system and an underactive "soothing" system. This hyperactivation, along with other mechanisms, may contribute to fibromyalgia.[71]
Some neurochemical abnormalities that occur in fibromyalgia also regulate mood, sleep, and energy, thus explaining why mood, sleep, and fatigue problems are commonly co-morbid with fibromyalgia.[22] Serotonin is the most widely studied neurotransmitter in fibromyalgia. It is hypothesized that an imbalance in the serotoninergic system may lead to the development of fibromyalgia.[72] There is also some data that suggests altered dopaminergic and noradrenergic signaling in fibromyalgia.[73] Supporting the monoamine related theories is the efficacy of monoaminergic antidepressants in fibromyalgia.[17] Glutamate/creatine ratios within the bilateral ventrolateral prefrontal cortex were found to be significantly higher in fibromyalgia patients than in controls, and may disrupt glutamate neurotransmission.[55][74]
Neuroimaging studies have observed that fibromyalgia patients have increased grey matter in the right postcentral gyrus and left angular gyrus, and decreased grey matter in the right cingulate gyrus, right paracingulate gyrus, left cerebellum, and left gyrus rectus.[75] These regions are associated with affective and cognitive functions and with motor adaptations to pain processing.[75] Other studies have documented decreased grey matter of the default mode network in people with fibromyalgia.[76] These deficits are associated with pain processing.[76]
Studies on the neuroendocrine system and HPA axis in fibromyalgia have been inconsistent. Depressed function of the HPA axis results in adrenal insufficiency and potentially chronic fatigue.[77]
One study found fibromyalgia patients exhibited higher plasma cortisol, more extreme peaks and troughs, and higher rates of dexamethasone non-suppression. However, other studies have only found correlations between a higher cortisol awakening response and pain, and not any other abnormalities in cortisol.[29] Increased baseline ACTH and increase in response to stress have been observed, hypothesized to be a result of decreased negative feedback.[73]
Pro-oxidative processes correlate with pain in fibromyalgia patients.[77] Decreased mitochondrial membrane potential, increased superoxide activity, and increased lipid peroxidation production are observed.[77] The high proportion of lipids in the central nervous system (CNS) makes the CNS especially vulnerable to free radical damage. Levels of lipid peroxidation products correlate with fibromyalgia symptoms.[77]
Inflammation has been suggested to have a role in the pathogenesis of fibromyalgia.[78] People with fibromyalgia tend to have higher levels of inflammatory cytokines IL-6,[72][79][80] and IL-8.[72][79][80] There are also increased levels of the pro-inflammatory cytokines IL-1 receptor antagonist.[79][80] Increased levels of pro-inflammatory cytokines may increase sensitivity to pain, and contribute to mood problems.[81] Anti-inflammatory interleukins such as IL-10 have also been associated with fibromyalgia.[72]
A repeated observation shows that autoimmunity triggers such as traumas and infections are among the most frequent events preceding the onset of fibromyalgia.[82] Neurogenic inflammation has been proposed as a contributing factor to fibromyalgia.[83]
Though there is a lack of evidence in this area, it is hypothesized that gut bacteria may play a role in fibromyalgia.[84] People with fibromyalgia are more likely to show dysbiosis, a decrease in microbiota diversity.[85] There is a bidirectional interplay between the gut and the nervous system. Therefore, the gut can affect the nervous system, but the nervous system can also affect the gut. Neurological effects mediated via the autonomic nervous system as well as the hypothalamic pituitary adrenal axis are directed to intestinal functional effector cells, which in turn are under the influence of the gut microbiota.[86]
The gut-brain axis, which connects the gut microbiome to the brain via the enteric nervous system, is another area of research. Fibromyalgia patients have less varied gut flora and altered serum metabolome levels of glutamate and serine,[87] implying abnormalities in neurotransmitter metabolism.[82]
Patients with fibromyalgia experience exercise intolerance. Primary fibromyalgia is idiopathic (cause unknown), whereas secondary fibromyalgia is in association with a known underlying disorder (such as Ankylosing spondylitis).[88][non-primary source needed] In patients with primary fibromyalgia, studies have found disruptions in energy metabolism within skeletal muscle, including: decreased levels of ATP, ADP, and phosphocreatine, and increased levels of AMP and creatine (use of creatine kinase and myokinase in the phosphagen system due to low ATP);[89][non-primary source needed] increased pyruvate;[90][non-primary source needed] as well as reduced capillary density impairing oxygen delivery to the muscle cells for oxidative phosphorylation.[91][92][non-primary source needed]
Despite being a small percentage of the body's total mass, the brain consumes approximately 20% of the energy produced by the body.[55][non-primary source needed] Parts of the brain—the anterior cingulate cortex (ACC), thalamus, and insula—were studied using proton magnetic resonance spectroscopy (MRS) in patients with fibromyalgia and compared to healthy controls. The fibromyalgia patients were found to have lower phosphocreatine (PCr) and lower creatine (Cr) than the control group. Phosphocreatine is used in the phosphagen system to produce ATP. The study found that low creatine and low phosphocreatine were associated with high pain, and that high stress, including PTSD, may contribute to these low levels.[55][non-primary source needed]
Low phosphocreatine levels may disrupt glutamate neurotransmission within the brains of those with fibromyalgia. Glutamate/creatine ratios within the bilateral ventrolateral prefrontal cortex were found to be significantly higher than in controls.[55][74][non-primary source needed]
There is no single pathological feature, laboratory finding, or biomarker that can diagnose fibromyalgia and there is debate over what should be considered diagnostic criteria and whether an objective diagnosis is possible.[59] In most cases, people with fibromyalgia symptoms may have laboratory test results that appear normal and many of their symptoms may mimic those of other rheumatic conditions such as arthritis or osteoporosis. The specific diagnostic criteria for fibromyalgia have evolved over time.[93]
The first widely accepted set of classification criteria for research purposes was elaborated in 1990 by the Multicenter Criteria Committee of the American College of Rheumatology. These criteria, which are known informally as "the ACR 1990", defined fibromyalgia according to the presence of the following criteria:
The ACR criteria for the classification of patients were originally established as inclusion criteria for research purposes and were not intended for clinical diagnosis but have later become the de facto diagnostic criteria in the clinical setting. A controversial study was done by a legal team looking to prove their client's disability based primarily on tender points and their widespread presence in non-litigious communities prompted the lead author of the ACR criteria to question now the useful validity of tender points in diagnosis.[94] Use of control points has been used to cast doubt on whether a person has fibromyalgia, and to claim the person is malingering.[95]
In 2010, the American College of Rheumatology approved provisional revised diagnostic criteria for fibromyalgia that eliminated the 1990 criteria's reliance on tender point testing.[96] The revised criteria used a widespread pain index (WPI) and symptom severity scale (SSS) in place of tender point testing under the 1990 criteria. The WPI counts up to 19 general body areas[a] in which the person has experienced pain in the preceding week.[9] The SSS rates the severity of the person's fatigue, unrefreshed waking, cognitive symptoms, and general somatic symptoms,[b] each on a scale from 0 to 3, for a composite score ranging from 0 to 12.[9] The revised criteria for diagnosis were:
In 2016, the provisional criteria of the American College of Rheumatology from 2010 were revised.[9] The new diagnosis required all of the following criteria:
In 2019, the American Pain Society in collaboration with the U.S. Food and Drug Administration developed a new diagnostic system using two dimensions.[12] The first dimension included core diagnostic criteria and the second included common features. In accordance to the 2016 diagnosis guidelines, the presence of another medical condition or pain disorder does not rule out the diagnosis of fibromyalgia. Nonetheless, other conditions should be ruled out as the main explaining reason for the patient's symptoms. The core diagnostic criteria are:[97]
Common features found in fibromyalgia patients can assist the diagnosis process. These are tenderness (sensitivity to light pressure), dyscognition (difficulty to think), musculoskeletal stiffness, and environmental sensitivity or hypervigilance.[12]
Some research has suggested using a multidimensional approach taking into consideration somatic symptoms, psychological factors, psychosocial stressors and subjective belief regarding fibromyalgia.[98] These symptoms can be assessed by several self-report questionnaires.[9]
The Widespread Pain Index (WPI) measures the number of painful body regions.[96]
The Symptom Severity Scale (SSS) assesses the severity of the fibromyalgia symptoms.
The Fibromyalgia Impact Questionnaire (FIQ)[99] and the Revised Fibromyalgia Impact Questionnaire (FIQR)[100] assess three domains: function, overall impact and symptoms.[100] It is considered a useful measure of disease impact.[101]
Other measures include the Hospital Anxiety and Depression Scale, Multiple Ability Self-Report Questionnaire,[102] Multidimensional Fatigue Inventory, and Medical Outcomes Study Sleep Scale.
As of 2009, as many as two out of every three people who are told that they have fibromyalgia by a rheumatologist may have some other medical condition instead.[103] Fibromyalgia could be misdiagnosed in cases of early undiagnosed rheumatic diseases such as preclinical rheumatoid arthritis, early stages of inflammatory spondyloarthritis, polymyalgia rheumatica, myofascial pain syndromes and hypermobility syndrome.[11][104] Neurological diseases with an important pain component include multiple sclerosis, Parkinson's disease and peripheral neuropathy.[11][104] Other medical illnesses that should be ruled out are endocrine disease or metabolic disorder (hypothyroidism, hyperparathyroidism, acromegaly, vitamin D deficiency), gastro-intestinal disease (celiac and non-celiac gluten sensitivity), infectious diseases (Lyme disease, hepatitis C and immunodeficiency disease) and the early stages of a malignancy such as multiple myeloma, metastatic cancer and leukemia/lymphoma.[11][104] Other systemic, inflammatory, endocrine, rheumatic, infectious, and neurologic disorders may cause fibromyalgia-like symptoms, such as systemic lupus erythematosus, Sjögren syndrome, ankylosing spondylitis, Ehlers-Danlos syndromes, psoriatic-related polyenthesitis, a nerve compression syndrome (such as carpal tunnel syndrome), and myasthenia gravis.[105][103][106][107] In addition, several medications can also evoke pain (statins, aromatose inhibitors, biophosphonates, and opioids).[12]
The differential diagnosis is made during the evaluation on the basis of the person's medical history, physical examination, and laboratory investigations.[105][103][106][107] The patient's history can provide some hints to a fibromyalgia diagnosis. A family history of early chronic pain, a childhood history of pain, an emergence of broad pain following physical and/or psychosocial stress, a general hypersensitivity to touch, smell, noise, taste, hypervigilance, and various somatic symptoms (gastrointestinal, urology, gynecology, neurology), are all examples of these signals [11]
Extensive laboratory tests are usually unnecessary in the differential diagnosis of fibromyalgia.[12] Common tests that are conducted include complete blood count, comprehensive metabolic panel, erythrocyte sedimentation rate, C-reactive protein, and thyroid function test.[12]
As with many other medically unexplained syndromes, there is no universally accepted treatment or cure for fibromyalgia, and treatment typically consists of symptom management and improving patient quality of life.[14] A personalized, multidisciplinary approach to treatment that includes both non-pharmacologic and pharmacologic therapy and begins with effective patient education is most beneficial.[14] Developments in the understanding of the pathophysiology of the disorder have led to improvements in treatment, which include prescription medication, behavioral intervention, and exercise.
A number of associations have published guidelines for the diagnosis and management of fibromyalgia. The European League Against Rheumatism (EULAR; 2017)[15] recommends a multidisciplinary approach, allowing a quick diagnosis and patient education. The recommended initial management should be non-pharmacological, later pharmacological treatment can be added. The European League Against Rheumatism gave the strongest recommendation for aerobic and strengthening exercise. Weak recommendations were given to a number of treatments, based on their outcomes. Qigong, yoga, and tai chi were weakly recommended for improving sleep and quality of life. Mindfulness was weakly recommended for improving pain and quality of life. Acupuncture and hydrotherapy were weakly recommended for improving pain. A weak recommendation was also given to psychotherapy. It was more suitable for patients with mood disorders or unhelpful coping strategies. Chiropractic was strongly recommended against, due to safety concerns. Some medications were weakly recommended for severe pain (duloxetine, pregabalin, tramadol) or sleep disturbance (amitriptyline, cyclobenzaprine, pregabalin). Others were not recommended due to a lack of efficacy (nonsteroidal anti-inflammatory drugs, monoamine oxidase inhibitors and selective serotonin reuptake inhibitors). Growth hormone, sodium oxybate, opioids and steroids were strongly recommended against due to lack of efficacy and side effects.
The guidelines published by the Association of the Scientific Medical Societies in Germany[108] inform patients that self-management strategies are an important component in managing the disease.[109] The Canadian Pain Society[110] also published guidelines for the diagnosis and management of fibromyalgia.
Exercise is the only fibromyalgia treatment which has been given a strong recommendation by the European Alliance of Associations for Rheumatology (EULAR). There is strong evidence indicating that exercise improves fitness, sleep and quality of life and may reduce pain and fatigue for people with fibromyalgia.[111][19][112] Exercise has an added benefit in that it does not cause any serious adverse effects.[112]
Exercise may diminish fibromyalgia symptoms through a number of hypothesized biological mechanisms.[113] Exercise may improve pain modulation[114][115] through serotoninergic pathways.[115] It may reduce pain by altering the hypothalamic-pituitary-adrenal axis and reducing cortisol levels.[116] It also has anti-inflammatory effects that may improve fibromyalgia symptoms.[117][118] Aerobic exercise can improve muscle metabolism and pain through mitochondrial pathways.[117]
When comparing different exercise programs, aerobic exercise is capable of modulating the autonomic nervous function of fibromyalgia patients, whereas resistance exercise does not show such effects.[119] A 2022 meta-analysis found that aerobic training showed a high effect size while strength interventions showed moderate effects.[120] Meditative exercise seems preferable for improving sleep,[121][122] with no differences between resistance, flexibility and aquatic exercise in their favorable effects on fatigue.[121]
Despite its benefits, exercise is a challenge for patients with fibromyalgia, due to the chronic fatigue and pain they experience.[123] They perceive it as more effortful than healthy adults.[124] Exercise may intimidate them, in fear that they will be asked to do more than they are capable of.[125] They may also feel that those who recommend or deliver exercise interventions do not fully understand the possible negative impact of exercise on fatigue and pain.[125] This is especially true for non-personalized exercise programs.[125] Adherence is higher when the exercise program is recommended by doctors or supervised by nurses.[126] Depression and higher pain intensity serve as  barriers to physical activity.[127] A recommended approach to a graded exercise program begins with small, frequent exercise periods and builds up from there.[120][128] In order to reduce pain, it is recommended to use an exercise program of 13 to 24 weeks, with each session lasting 30 to 60 minutes.[120]
Aerobic exercise for fibromyalgia patients is the most investigated type of exercise.[112] It includes activities such as walking, jogging, spinning, cycling, dancing and exercising in water,[117][119] with walking being named as one of the best methods.[129] A 2017 cochrane summary concluded that aerobic exercise probably improves quality of life, slightly decreases pain and improves physical function and makes no difference in fatigue and stiffness.[130] A 2019 meta-analysis showed that exercising aerobically can reduce autonomic dysfunction and increase heart rate variability.[119] This happens when patients exercise at least twice a week, for 45–60 minutes at about 60%-80% of the maximum heart rate.[119] Aerobic exercise also decreases anxiety and depression and improves the quality of life.[119]
Combinations of different exercises such as flexibility and aerobic training may improve stiffness.[131] However, the evidence is of low-quality.[131] It is not clear if flexibility training alone compared to aerobic training is effective at reducing symptoms or has any adverse effects.[132]
In resistance exercise, participants apply a load to their body using weights, elastic band, body weight or other measures.
Two meta-analyses on fibromyalgia have shown that resistance training can reduce anxiety and depression,[119][133] one found that it decreases pain and disease severity[134] and one found that it improves quality of life.[119] Resistance training may also improve sleep, with a greater effect than that of flexibility training and a similar effect to that of aerobic exercise.[135]
The dosage of resistance exercise for women with fibromyalgia was studied in a 2022 meta-analysis.[136] Effective dosages were found when exercising twice a week, for at least eight weeks. Symptom improvement was found for even low dosages such as 1–2 sets of 4–20 repetitions.[136] Most studies use moderate exercise intensity of 40% to 85% one-repetition maximum. This level of intensity was effective in reducing pain.[136] Some treatment regimes increase the intensity over time (from 40% to 80%), whereas others increase it when the participant is able to perform 12 repetitions.[136] High-intensity exercises may cause lower treatment adherence.
A 2021 meta-analysis found that meditative exercise programs (tai chi, yoga, qigong) were superior to other forms of exercise (aerobic, flexibility, resistance) in improving sleep quality.[121] Other meta-analyses also found positive effects of tai chi for sleep,[137] fibromyalgia symptoms,[138] and pain, fatigue, depression and quality of life.[139] These tai chi interventions frequently included 1-hour sessions practiced 1-3 times a week for 12 weeks. Meditative exercises, as a whole, may achieve desired outcomes through biological mechanisms such as antioxidation, anti-inflammation, reduction in sympathetic activity and modulation of glucocorticoid receptor sensitivity.[117]
Several reviews and meta-analyses suggest that aquatic training can improve symptoms and wellness in people with fibromyalgia.[140][141][142][143][144][145] It is recommended to practice aquatic therapy at least twice a week using a low to moderate intensity.[144] However, aquatic therapy does not appear to be superior to other types of exercise.[146]
Limited evidence suggests vibration training in combination with exercise may improve pain, fatigue, and stiffness.[147]
A few countries have published guidelines for the management and treatment of fibromyalgia. As of 2018, all of them emphasize that medications are not required. However, medications, though imperfect, continue to be a component of treatment strategy for fibromyalgia patients. The German guidelines outlined parameters for drug therapy termination and recommended considering drug holidays after six months.[16]
Health Canada and the US Food and Drug Administration (FDA) have approved pregabalin[148] (an anticonvulsant) and duloxetine (a serotonin–norepinephrine reuptake inhibitor) for the management of fibromyalgia. The FDA also approved milnacipran (another serotonin–norepinephrine reuptake inhibitor), but the European Medicines Agency refused marketing authority.[149]
Antidepressants are one of the common drugs for fibromyalgia. A 2021 meta-analysis concluded that antidepressants can improve the quality of life for fibromyalgia patients in the medium-term.[17] For most people with fibromyalgia, the potential benefits of treatment with the serotonin and norepinephrine reuptake inhibitors duloxetine and milnacipran and the tricyclic antidepressants, such as amitriptyline, are outweighed by significant adverse effects (more adverse effects than benefits), however, a small number of people may experience relief from symptoms with these medications.[150][151][152]
The length of time that antidepressant medications take to be effective at reducing symptoms can vary. Any potential benefits from the antidepressant amitriptyline may take up to three months to take effect and it may take between three and six months for duloxetine, milnacipran, and pregabalin to be effective at improving symptoms.[153] Some medications have the potential to cause withdrawal symptoms when stopping so gradual discontinuation may be warranted particularly for antidepressants and pregabalin.[95]
A 2023 meta analysis found that duloxetine improved fibromyalgia symptoms, regardless of the dosage.[154]  SSRIs may be also be used to treat depression in people diagnosed with fibromyalgia.[155]
While amitriptyline has been used as a first line treatment, the quality of evidence to support this use and comparison between different medications is poor.[156][152] Very weak evidence indicates that a very small number of people may benefit from treatment with the tetracyclic antidepressant mirtazapine, however, for most, the potential benefits are not great and the risk of adverse effects and potential harm outweighs any potential for benefit.[157] As of 2018, the only tricyclic antidepressant that has sufficient evidence is amitriptyline.[16][156]
Tentative evidence suggests that monoamine oxidase inhibitors (MAOIs) such as pirlindole and moclobemide are moderately effective for reducing pain.[158] Very low-quality evidence suggests pirlindole as more effective at treating pain than moclobemide.[158] Side effects of MAOIs may include nausea and vomiting.[158]
Central nervous system depressants include drug categories such as sedatives, tranquilizers, and hypnotics. A 2021 meta-analysis concluded that such drugs can improve the quality of life for fibromyalgia patients in the medium-term.[17]
The anti-convulsant medications gabapentin and pregabalin may be used to reduce pain.[8] There is tentative evidence that gabapentin may be of benefit for pain in about 18% of people with fibromyalgia.[8] It is not possible to predict who will benefit, and a short trial may be recommended to test the effectiveness of this type of medication. Approximately 6/10 people who take gabapentin to treat pain related to fibromyalgia experience unpleasant side effects such as dizziness, abnormal walking, or swelling from fluid accumulation.[159] Pregabalin demonstrates a benefit in about 9% of people.[160] Pregabalin reduced time off work by 0.2 days per week.[161]
Cannabinoids may have some benefits for people with fibromyalgia. However, as of 2022, the data on the topic is still limited.[162][163][164] Cannabinoids may also have adverse effects and may negatively interact with common rheumatological drugs.[165]
The use of opioids is controversial. As of 2015, no opioid is approved for use in this condition by the FDA.[166] A 2016 Cochrane review concluded that there is no good evidence to support or refute the suggestion that oxycodone, alone or in combination with naloxone, reduces pain in fibromyalgia.[167] The National Institute of Arthritis and Musculoskeletal and Skin Diseases (NIAMS) in 2014 stated that there was a lack of evidence for opioids for most people.[5] The Association of the Scientific Medical Societies in Germany in 2012 made no recommendation either for or against the use of weak opioids because of the limited amount of scientific research addressing their use in the treatment of fibromyalgia. They strongly advise against using strong opioids.[108] The Canadian Pain Society in 2012 said that opioids, starting with a weak opioid like tramadol, can be tried but only for people with moderate to severe pain that is not well-controlled by non-opioid painkillers. They discourage the use of strong opioids and only recommend using them while they continue to provide improved pain and functioning. Healthcare providers should monitor people on opioids for ongoing effectiveness, side effects, and possible unwanted drug behaviors.[110]
A 2015 review found fair evidence to support tramadol use if other medications do not work.[166] A 2018 review found little evidence to support the combination of paracetamol (acetaminophen) and tramadol over a single medication.[168] Goldenberg et al suggest that tramadol works via its serotonin and norepinephrine reuptake inhibition, rather than via its action as a weak opioid receptor agonist.[169]
A large study of US people with fibromyalgia found that between 2005 and 2007 37.4% were prescribed short-acting opioids and 8.3% were prescribed long-acting opioids,[3] with around 10% of those prescribed short-acting opioids using tramadol;[170] and a 2011 Canadian study of 457 people with fibromyalgia found 32% used opioids and two-thirds of those used strong opioids.[110]
Capsaicin has been suggested as a topical pain reliever. Preliminary results suggest that it may improve sleep quality and fatigue, but there are not enough studies to support this claim.[171]
Sodium oxybate increases growth hormone production levels through increased slow-wave sleep patterns. However, this medication was not approved by the FDA for the indication for use in people with fibromyalgia due to the concern for abuse.[172]
The muscle relaxants cyclobenzaprine, carisoprodol with acetaminophen and caffeine, and tizanidine are sometimes used to treat fibromyalgia; however, as of 2015 they are not approved for this use in the United States.[173][174] The use of nonsteroidal anti-inflammatory drugs is not recommended as first-line therapy.[175] Moreover, nonsteroidal anti-inflammatory drugs cannot be considered as useful in the management of fibromyalgia.[176]
Very low-quality evidence suggests quetiapine may be effective in fibromyalgia.[177]
No high-quality evidence exists that suggests synthetic THC (nabilone) helps with fibromyalgia.[178]
Nutrition is related to fibromyalgia in several ways. Some nutritional risk factors for fibromyalgia complications are obesity, nutritional deficiencies, food allergies and consuming food additives.[179] The consumption of fruits and vegetables, low-processed foods, high-quality proteins, and healthy fats may have some benefits.[179] Low-quality evidence found some benefits of a vegetarian or vegan diet.[180]
Although dietary supplements have been widely investigated in relation to fibromyalgia, most of the evidence, as of 2021, is of poor quality. It is therefore difficult to reach conclusive recommendations.[181] It appears that Q10 coenzyme and vitamin D supplements can reduce pain and improve quality of life for fibromyalgia patients.[19][182] Q10 coenzyme has beneficial effects on fatigue in fibromyalgia patients, with most studies using doses of 300 mg per day for three months.[183] Q10 coenzyme is hypothesized to improve mitochondrial activity and decrease inflammation.[184] Vitamin D has been shown to improve some fibromyalgia measures, but not others.[182][185]
Two review articles found that melatonin treatment has several positive effects on fibromyalgia patients, including the improvement of sleep quality, pain, and disease impact.[186][187] No major adverse events were reported.[186]
Due to the uncertainty about the pathogenesis of fibromyalgia, current treatment approaches focus on management of symptoms to improve quality of life,[188] using integrated pharmacological and non-pharmacological approaches.[4] There is no single intervention shown to be effective for all patients.[189] In a 2020 Cochrane review, cognitive behavioral therapy was found to have a small but beneficial effect for reducing pain and distress but adverse events were not well evaluated.[190] Cognitive behavioral therapy and related psychological and behavioural therapies have a small to moderate effect in reducing symptoms of fibromyalgia.[191][192] Effect sizes tend to be small when cognitive behavioral therapy is used as a stand-alone treatment for patients with fibromyalgia, but these improve significantly when it is part of a wider multidisciplinary treatment program.[192]
A 2010 systematic review of 14 studies reported that cognitive behavioral therapy improves self-efficacy or coping with pain and reduces the number of physician visits at post-treatment, but has no significant effect on pain, fatigue, sleep, or health-related quality of life at post-treatment or follow-up. Depressed mood was also improved but this could not be distinguished from some risks of bias.[193] A 2022 meta-analysis found that cognitive behavioral therapy reduces insomnia in people with chronic pain, including people with fibromyalgia.[194]
Patient education is recommended by the European League Against Rheumatism (EULAR) as an important treatment component.[15] As of 2022, there is only low-quality evidence showing that patient education can decrease pain and fibromyalgia impact.[195][196]
Sleep hygiene interventions show low effectiveness in improving insomnia in people with chronic pain.[194]
A 2021 meta-analysis concluded that massage and myofascial release diminish pain in the medium-term.[17] As of 2015, there was no good evidence for the benefit of other mind-body therapies.[197]
A 2013 review found moderate-level evidence on the usage of acupuncture with electrical stimulation for improvement of the overall well-being. Acupuncture alone will not have the same effects, but will enhance the influence of exercise and medication in pain and stiffness.[198]
Several forms of electrical neuromodulation, including transcutaneous electrical nerve stimulation (TENS) and transcranial direct current stimulation (tDCS), have been used to treat fibromyalgia. In general, they have been found to be helpful in reducing pain and depression and improving functioning.[199][200]
Transcutaneous electrical nerve stimulation (TENS) is the delivery of pulsed electrical currents to the skin to stimulate peripheral nerves. TENS is widely used to treat pain and is considered to be a low-cost, safe, and self-administered treatment.[201] As such, it is commonly recommended by clinicians to people suffering from pain.[202] On 2019, an overview of eight Cochrane reviews was conducted, covering 51 TENS-related randomized controlled trials.[202] The review concluded that the quality of the available evidence was insufficient to make any recommendations.[202] A later review concluded that transcutaneous electrical nerve stimulation may diminish pain in the short-term, but there was uncertainty about the relevance of the results.[17]
Preliminary findings suggest that electrically stimulating the vagus nerve through an implanted device can potentially reduce fibromyalgia symptoms.[203] However, there may be adverse reactions to the procedure.[203]
Noninvasive brain stimulation includes methods such as transcranial direct current stimulation and high-frequency repetitive transcranial magnetic stimulation (TMS). Both methods have been found to improve pain scores in neuropathic pain and fibromyalgia.[204]
A 2023 meta analysis of 16 RCTs found that transcranial direct current stimulation (tDCS) of over 4 weeks can decrease pain in patients with fibromyalgia.[205]
A 2021 meta-analysis of multiple intervention types concluded that magnetic field therapy and transcranial magnetic stimulation may diminish pain in the short-term, but conveyed an uncertainty about the relevance of the result.[17] Several 2022 meta-analyses focusing on transcranial magnetic stimulation found positive effects on fibromyalgia.[206][207][208] Repetitive transcranial magnetic stimulation improved pain in the short-term[207][208] and quality of life after 5–12 weeks.[207][208] Repetitive transcranial magnetic stimulation did not improve anxiety, depression, and fatigue.[208] Transcranial magnetic stimulation to the left dorsolateral prefrontal cortex was also ineffective.[207]
A systematic review of EEG neurofeedback for treatment of fibromyalgia found most treatments showed significant improvements of the main symptoms of the disease.[209] However, the protocols were so different, and the lack of controls or randomization impede drawing conclusive results.[209]
Hyperbaric oxygen therapy (HBOT) has shown beneficial effects in treating chronic pain by reducing inflammation and oxidative stress.[77]  However, treating fibromyalgia with hyperbaric oxygen therapy is still controversial, in light of the scarcity of large-scale clinical trials.[117] In addition, hyperbaric oxygen therapy raises safety concerns due to the oxidative damage that may follow it.[117] An evaluation of nine trials with 288 patients in total found that HBOT was more effective at relieving fibromyalgia patients' pain than the control intervention. In most of the trials HBOT improved sleep disturbance, multidimensional function, patient satisfaction, and tender spots. Unfortunately, 24% of the patients experienced negative outcomes.[210]
Although in itself fibromyalgia is neither degenerative nor fatal, the chronic pain of fibromyalgia is pervasive and persistent. Most people with fibromyalgia report that their symptoms do not improve over time. However, most patients learn to adapt to the symptoms over time. The German guidelines for patients explain that:
An 11-year follow-up study on 1,555 patients found that most remained with high levels of self-reported symptoms and distress.[non-primary source needed][211] However, there was a great deal of patient heterogeneity accounting for almost half of the variance. At the final observation, 10% of the patients showed substantial improvement with minimal symptoms. An additional 15% had moderate improvement. This state, though, may be transient, given the fluctuations in symptom severity.[non-primary source needed][211]
A study of 97 adolescents diagnosed with fibromyalgia followed them for eight years.[non-primary source needed] After eight years, the majority of youth still experienced pain and disability in physical, social, and psychological areas. At the last follow-up, all participants reported experiencing one or more fibromyalgia symptoms such as pain, fatigue, and/or sleep problems, with 58% matching the complete ACR 2010 criteria for fibromyalgia. Based on the WPI and SS score cut-points, the remaining 42% exhibited subclinical symptoms. Pain and emotional symptom trajectories, on the other hand, displayed a variety of longitudinal patterns. The study concluded that while most patient's fibromyalgia symptoms endure, the severity of their pain tends to reduce over time.[212]
Baseline depressive symptoms in adolescents appear to predict worse pain at follow-up periods.[213][214]
A meta-analysis based on close to 200,000 fibromyalgia patients found that they were at a higher risk for all-cause mortality. Specific mortality causes that were suggested were accidents, infections and suicide.[215]
Fibromyalgia is estimated to affect 1.8% of the population.[216]
Despite the fact that more than 90% of fibromyalgia patients are women, only 60% of people with fibromyalgia symptoms are female in the general population.[217]
Chronic widespread pain had already been described in the literature in the 19th century but the term fibromyalgia was not used until 1976 when Dr P.K. Hench used it to describe these symptoms.[95] Many names, including "muscular rheumatism", "fibrositis", "psychogenic rheumatism", and "neurasthenia" were applied historically to symptoms resembling those of fibromyalgia.[218] The term fibromyalgia was coined by researcher Mohammed Yunus as a synonym for fibrositis and was first used in a scientific publication in 1981.[219] Fibromyalgia is from the Latin fibra (fiber)[220] and the Greek words myo (muscle)[221] and algos (pain).[222]
Historical perspectives on the development of the fibromyalgia concept note the "central importance" of a 1977 paper by Smythe and Moldofsky on fibrositis.[223][224] The first clinical, controlled study of the characteristics of fibromyalgia syndrome was published in 1981,[225] providing support for symptom associations. In 1984, an interconnection between fibromyalgia syndrome and other similar conditions was proposed,[226] and in 1986, trials of the first proposed medications for fibromyalgia were published.[226]
A 1987 article in the Journal of the American Medical Association used the term "fibromyalgia syndrome" while saying it was a "controversial condition".[227] The American College of Rheumatology (ACR) published its first classification criteria for fibromyalgia in 1990.[228] Later revisions were made in 2010,[96] 2016,[9] and 2019.[12]
People with fibromyalgia generally have higher healthcare costs and utilization rates. A review of 36 studies found that fibromyalgia causes a significant economic burden on health care systems.[229] Annual costs per patient were estimated to be up to $35,920 in the US and $8,504 in Europe.[229]
Fibromyalgia was defined relatively recently. In the past, it was a disputed diagnosis. Frederick Wolfe, lead author of the 1990 paper that first defined the diagnostic guidelines for fibromyalgia, stated in 2008 that he believed it "clearly" not to be a disease but instead a physical response to depression and stress.[230] In 2013, Wolfe added that its causes "are controversial in a sense" and "there are many factors that produce these symptoms – some are psychological and some are physical and it does exist on a continuum".[231] Some members of the medical community do not consider fibromyalgia a disease because of a lack of abnormalities on physical examination and the absence of objective diagnostic tests.[223][232]
In the past, some psychiatrists have viewed fibromyalgia as a type of affective disorder, or a somatic symptom disorder. These controversies do not engage healthcare specialists alone; some patients object to fibromyalgia being described in purely somatic terms.[233]
As of 2022, neurologists and pain specialists tend to view fibromyalgia as a pathology due to dysfunction of muscles and connective tissue as well as functional abnormalities in the central nervous system. Rheumatologists define the syndrome in the context of "central sensitization" – heightened brain response to normal stimuli in the absence of disorders of the muscles, joints, or connective tissues. Because of this symptomatic overlap, some researchers have proposed that fibromyalgia and other analogous syndromes be classified together as central sensitivity syndromes.[234][13]
Chronic pain is classified as pain that lasts longer than three months.[1] In medicine, the distinction between acute and chronic pain is sometimes determined by the amount of time since onset. Two commonly used markers are pain that continues at three months and six months since onset,[2] but some theorists and researchers have placed the transition from acute to chronic pain at twelve months.[3] Others apply the term acute to pain that lasts less than 30 days, chronic to pain of more than six months duration, and subacute to pain that lasts from one to six months.[4] A popular alternative definition of chronic pain, involving no fixed duration, is "pain that extends beyond the expected period of healing".[2]
Chronic pain may originate in the body, or in the brain or spinal cord. It is often difficult to treat. Epidemiological studies have found that 8–11.2% of people in various countries have chronic widespread pain.[5] Various non-opioid medicines are initially recommended to treat chronic pain, depending on whether the pain is due to tissue damage or is neuropathic.[6][7] Psychological treatments including cognitive behavioral therapy and acceptance and commitment therapy may be effective for improving quality of life in those with chronic pain. Some people with chronic pain may benefit from opioid treatment while others can be harmed by it.[8][9] People with non-cancer pain who have not been helped by non-opioid medicines might be recommended to try opioids if there is no history of substance use disorder and no current mental illness.[10]
People with chronic pain tend to have higher rates of depression[11] and although the exact connection between the comorbidities is unclear, a 2017 study on neuroplasticity found that "injury sensory pathways of body pains have been shown to share the same brain regions involved in mood management."[12] Chronic pain can contribute to decreased physical activity due to fear of making the pain worse. Pain intensity, pain control, and resilience to pain can be influenced by different levels and types of social support that a person with chronic pain receives, and are also influenced by the person's socioeconomic status.[13]
One approach to predicting a person's experience of chronic pain is the biopsychosocial model, according to which an individual's experience of chronic pain may be affected by a complex mixture of their biology, psychology, and their social environment.[14]
The International Association for the Study of Pain defines chronic pain as pain with no biological value, that persists past normal tissue healing.  The DSM-5 recognizes one chronic pain disorder, somatic symptom disorders. The criteria include pain lasting longer than six months.[15]
The International Classification of Disease, Eleventh Revision (ICD-11) suggests seven categories for chronic pain.[1]
Chronic pain may be divided into "nociceptive" (caused by inflamed or damaged tissue activating specialized pain sensors called nociceptors), and "neuropathic" (caused by damage to or malfunction of the nervous system).[16]
Nociceptive pain can be divided into "superficial" and "deep", and deep pain into "deep somatic" and "visceral". Superficial pain is initiated by activation of nociceptors in the skin or superficial tissues. Deep somatic pain is initiated by stimulation of nociceptors in ligaments, tendons, bones, blood vessels, fasciae and muscles, and is dull, aching, poorly-localized pain. Visceral pain originates in the viscera (organs). Visceral pain may be well-localized, but often it is extremely difficult to locate, and several visceral regions produce "referred" pain when damaged or inflamed, where the sensation is located in an area distant from the site of pathology or injury.[17]
Neuropathic pain[18] is divided into "peripheral" (originating in the peripheral nervous system) and "central" (originating in the brain or spinal cord).[19] Peripheral neuropathic pain is often described as "burning", "tingling", "electrical", "stabbing", or "pins and needles".[20]
Under persistent activation, the transmission of pain signals to the dorsal horn may produce a pain wind-up phenomenon. This triggers changes that lower the threshold for pain signals to be transmitted. In addition, it may cause nonnociceptive nerve fibers to respond to, generate, and transmit pain signals. The type of nerve fibers that are believed to generate the pain signals are the C-fibers, since they have a slow conductivity and give rise to a painful sensation that persists over a long time.[21] In chronic pain, this process is difficult to reverse or stop once established.[22] In some cases, chronic pain can be caused by genetic factors which interfere with neuronal differentiation, leading to a permanently lowered threshold for pain.[23]
Chronic pain of different causes has been characterized as a disease that affects brain structure and function. MRI studies have shown abnormal anatomical[24] and functional connectivity, even during rest[25][26] involving areas related to the processing of pain. Also, persistent pain has been shown to cause grey matter loss, which is reversible once the pain has resolved.[27][28]
These structural changes can be explained by neuroplasticity. In the case of chronic pain, the somatotopic representation of the body is inappropriately reorganized following peripheral and central sensitization. This can cause allodynia or hyperalgesia. In individuals with chronic pain, EEGs showed altered brain activity, suggesting pain-induced neuroplastic changes. More specifically, the relative beta activity (compared to the rest of the brain) was increased, the relative alpha activity was decreased, and the theta activity was diminished.[29]
Dysfunctional dopamine management in the brain could potentially act as a shared mechanism between chronic pain, insomnia and major depressive disorder.[30] Astrocytes, microglia, and satellite glial cells have also been found to be dysfunctional in chronic pain.  Increased activity of microglia, alterations of microglial networks, and increased production of chemokines and cytokines by microglia might aggravate chronic pain.  Astrocytes have been observed to lose their ability to regulate the excitability of neurons, increasing spontaneous neural activity in pain circuits.[31]
Pain management is a branch of medicine that uses an interdisciplinary approach. The combined knowledge of various medical professions and allied health professions is used to ease pain and improve the quality of life of those living with pain.[32] The typical pain management team includes medical practitioners (particularly anesthesiologists), rehabilitation psychologists, physiotherapists, occupational therapists, physician assistants, and nurse practitioners.[33] Acute pain usually resolves with the efforts of one practitioner; however, the management of chronic pain frequently requires the coordinated efforts of a treatment team.[34][35][36] Complete, longterm remission of many types of chronic pain is rare.[37]
Initially recommended efforts are non-opioid based therapies.[10] Non-opioid treatment of chronic pain with pharmaceutical medicines might include acetaminophen (paracetamol)[38] or NSAIDs.[39]
Various other nonopioid medicines can be used, depending on whether the pain is a result of tissue damage or is neuropathic (pain caused by a damaged or dysfunctional nervous system). There is limited evidence that cancer pain or chronic pain from tissue damage as a result of a conditions (e.g. rheumatoid arthritis) is best treated with opioids. For neuropathic pain other drugs may be more effective than opioids,[6][7][40][41] such as tricyclic antidepressants,[42] serotonin-norepinephrine reuptake inhibitors,[43] and anticonvulsants.[43] Some atypical antipsychotics, such as olanzapine, may also be effective, but the evidence to support this is in very early stages.[44] In women with chronic pain, hormonal medications such as oral contraceptive pills ("the pill") might be helpful.[45] When there is no evidence of a single best fit, doctors may need to look for a treatment that works for the individual person.[42] It is difficult for doctors to predict who will use opioids just for pain management and who will go on to develop an addiction. It is also challenging for doctors to know which patients ask for opioids because they are living with an opioid addiction. Withholding, interrupting or withdrawing opioid treatment in people who benefit from it can cause harm.[8]
Interventional pain management may be appropriate, including techniques such as trigger point injections, neurolytic blocks, and radiotherapy. While there is no high quality evidence to support ultrasound, it has been found to have a small effect on improving function in non-specific chronic low back pain.[46]
Psychological treatments, including cognitive behavioral therapy[47][48] and acceptance and commitment therapy[49][50] can be helpful for improving quality of life and reducing pain interference. Brief mindfulness-based treatment approaches have been used, but they are not yet recommended as a first-line treatment.[51] The effectiveness of mindfulness-based pain management (MBPM) has been supported by a range of studies.[52][53][54]
Among older adults psychological interventions can help reduce pain and improve self-efficacy for pain management.[55] Psychological treatments have also been shown to be effective in children and teens with chronic headache or mixed chronic pain conditions.[56]
While exercise has been offered as a method to lessen chronic pain and there is some evidence of benefit, this evidence is tentative.[57] For people living with chronic pain, exercise results in few side effects.[57]
In those who have not benefited from other measures and have no history of either mental illness or substance use disorder treatment with opioids may be tried.[10] If significant benefit does not occur it is recommended that they be stopped.[10] In those on opioids, stopping or decreasing their use may improve outcomes including pain.[58]
Some people with chronic pain benefit from opioid treatment and others do not; some are harmed by the treatment.[8] Possible harms include reduced sex hormone production, hypogonadism, infertility, impaired immune system, falls and fractures in older adults, neonatal abstinence syndrome, heart problems, sleep-disordered breathing, opioid-induced hyperalgesia, physical dependence, addiction, abuse, and overdose.[59][60]
Alternative medicine refers to health practices or products that are used to treat pain or illness that are not necessarily considered a part of conventional medicine.[61] When dealing with chronic pain, these practices generally fall into the following four categories: biological, mind-body, manipulative body, and energy medicine.[61]
Implementing dietary changes, which is considered a biological-based alternative medicine practice, has been shown to help improve symptoms of chronic pain over time.[61] Adding supplements to one's diet is a common dietary change when trying to relieve chronic pain, with some of the most studied supplements being: acetyl-L-carnitine, alpha-lipoic acid, and vitamin E.[61][62][63][64] Vitamin E is perhaps the most studied out of the three, with strong evidence that it helps lower neurotoxicity in those with cancer, multiple sclerosis, and cardiovascular diseases.[64]
Hypnosis, including self-hypnosis, has tentative evidence.[65] Hypnosis, specifically, can offer pain relief for most people and may be a safe alternative to pharmaceutical medication.[66] Evidence does not support hypnosis for chronic pain due to a spinal cord injury.[67]
Preliminary studies have found medical marijuana to be beneficial in treating neuropathic pain, but not other kinds of long term pain.[68] As of 2018[update], the evidence for its efficacy in treating neuropathic pain or pain associated with rheumatic diseases is not strong for any benefit and further research is needed.[69][70][71] For chronic non-cancer pain, a recent study concluded that it is unlikely that cannabinoids are highly effective.[72] However, more rigorous research into cannabis or cannabis-based medicines is needed.[71]
Tai chi has been shown to improve pain, stiffness, and quality of life in chronic conditions such as osteoarthritis, low back pain, and osteoporosis.[73][74] Acupuncture has also been found to be an effective and safe treatment in reducing pain and improving quality of life in chronic pain including chronic pelvic pain syndrome.[75][76]
Transcranial magnetic stimulation for reduction of chronic pain is not supported by high quality evidence, and the demonstrated effects are small and short-term.[77]
Spa therapy could potentially improve pain in patients with chronic lower back pain, but more studies are needed to provide stronger evidence of this.[78]
While some studies have investigated the efficacy of St John's Wort or nutmeg for treating neuropathic (nerve) pain, their findings have raised serious concerns about the accuracy of their results.[79]
Kinesio tape has not been shown to be effective in managing chronic non-specific low-back pain.[80]
Myofascial release has been used in some cases of fibromyalgia, chronic low back pain, and tennis elbow but there is not enough evidence to support this as method of treatment.[81]
Chronic pain varies in different countries affecting anywhere from 8% to 55% of the population. It affects women at a higher rate than men, and chronic pain uses a large amount of healthcare resources around the globe.[82][5]
A large-scale telephone survey of 15 European countries and Israel found that 19% of respondents over 18 years of age had suffered pain for more than 6 months, including the last month, and more than twice in the last week, with pain intensity of 5 or more for the last episode, on a scale of 1 (no pain) to 10 (worst imaginable). 4839 of these respondents with chronic pain were interviewed in-depth. Sixty-six percent scored their pain intensity at moderate (5–7), and 34% at severe (8–10); 46% had constant pain, 56% intermittent; 49% had suffered pain for 2–15 years; and 21% had been diagnosed with depression due to the pain. Sixty-one percent were unable or less able to work outside the home, 19% had lost a job, and 13% had changed jobs due to their pain. Forty percent had inadequate pain management and less than 2% were seeing a pain management specialist.[83]
In the United States, chronic pain has been estimated to occur in approximately 35% of the population, with approximately 50 million Americans experiencing partial or total disability as a consequence.[84] According to the Institute of Medicine, there are about 116 million Americans living with chronic pain, which suggests that approximately half of American adults have some chronic pain condition.[85][86] The Mayday Fund estimate of 70 million Americans with chronic pain is slightly more conservative.[87] In an internet study, the prevalence of chronic pain in the United States was calculated to be 30.7% of the population: 34.3% for women and 26.7% for men.[88]
In Canada it is estimated that approximately 1 in 5 Canadians live with chronic pain and half of those people have lived with chronic pain for 10 years or longer.[89] Chronic pain in Canada also occurs more and is more severe in women and Canada's Indigenous communities.[89]
Sleep disturbance, and insomnia due to medication and illness symptoms are often experienced by those with chronic pain.[90] These conditions can be difficult to treat due to the high potential of medication interactions, especially when the conditions are treated by different doctors.[citation needed]
Severe chronic pain is associated with increased risk of death over a ten-year period, particularly from heart disease and respiratory disease.[91] Several mechanisms have been proposed for this increase, such as an abnormal stress response in the body's endocrine system.[92] Additionally, chronic stress seems to affect risks to heart and lung (cardiovascular) health by increasing how quickly plaque can build up on artery walls (arteriosclerosis). However, further research is needed to clarify the relationship between severe chronic pain, stress and cardiovascular health.[91]
Two of the most frequent personality profiles found in people with chronic pain by the Minnesota Multiphasic Personality Inventory (MMPI) are the conversion V and the neurotic triad. The conversion V personality expresses exaggerated concern over body feelings, develops bodily symptoms in response to stress, and often fails to recognize their own emotional state, including depression. The neurotic triad personality also expresses exaggerated concern over body feelings and develops bodily symptoms in response to stress, but is demanding and complaining.[93]
Some investigators have argued that it is this neuroticism that causes acute pain to turn chronic, but clinical evidence points the other way, to chronic pain causing neuroticism. When long term pain is relieved by therapeutic intervention, scores on the neurotic triad and anxiety fall, often to normal levels.[94][95][96][97] Self-esteem, often low in people with chronic pain, also shows improvement once pain has resolved.[97]
It has been suggested that catastrophizing might play a role in the experience of pain. Pain catastrophizing is the tendency to describe a pain experience in more exaggerated terms than the average person, to think a great deal more about the pain when it occurs, or to feel more helpless about the experience.[98] People who score highly on measures of catastrophization are likely to rate a pain experience as more intense than those who score low on such measures. It is often reasoned that the tendency to catastrophize causes the person to experience the pain as more intense. One suggestion is that catastrophizing influences pain perception through altering attention and anticipation, and heightening emotional responses to pain.[99] However, at least some aspects of catastrophization may be the product of an intense pain experience, rather than its cause. That is, the more intense the pain feels to the person, the more likely they are to have thoughts about it that fit the definition of catastrophization.[100]
Individuals with post-traumatic stress disorder (PTSD) have a high comorbidity with chronic pain.[101] Patients with both PTSD and chronic pain report higher severity of pain than those who do not have a PTSD comorbidity.[102][103]
People with chronic pain may also have symptoms of depression.[104][105] In 2017, the British Medical Association found that 49% of people with chronic pain had depression.[106]
Chronic pain's impact on cognition is an under-researched area, but several tentative conclusions have been published. Most people with chronic pain complain of cognitive impairment, such as forgetfulness, difficulty with attention, and difficulty completing tasks. Objective testing has found that people in chronic pain tend to experience impairment in attention, memory, mental flexibility, verbal ability, speed of response in a cognitive task, and speed in executing structured tasks.[107] A review of studies in 2018 reports a relationship between people in chronic pain and abnormal results in test of memory, attention, and processing speed.[108]
Social support has important consequences for individuals with chronic pain. In particular, pain intensity, pain control, and resiliency to pain have been implicated as outcomes influenced by different levels and types of social support. Much of this research has focused on emotional, instrumental, tangible and informational social support. People with persistent pain conditions tend to rely on their social support as a coping mechanism and therefore have better outcomes when they are a part of larger more supportive social networks. Across a majority of studies investigated, there was a direct significant association between social activities or social support and pain. Higher levels of pain were associated with a decrease in social activities, lower levels of social support, and reduced social functioning.[109][110]
Evidence exists for unconscious biases and negative stereotyping against racial minorities requesting pain treatment, although clinical decision making was not affected, according to one 2017 review.[111] Minorities may be denied diagnoses for pain and pain medications, and are more likely to go through substance abuse assessment, and are less likely to transfer for pain specialist referral.[112] A 2010 University of Michigan Health study found that black patients in pain clinics received 50% of the amount of drugs that patients who were white received.[113] Preliminary research showed that health providers might have less empathy for black patients and underestimated their pain levels, resulting in treatment delays.[111][112] Minorities may experience a language barrier, limiting the high level of engagement between the person with pain and health providers for treatment.[112]
Similar to the damaging effects seen with catastrophizing, perceived injustice is thought to contribute to the severity and duration of chronic pain.[114] Pain-related injustice perception has been conceptualized as a cognitive appraisal reflecting the severity and irreparability of pain- or injury-related loss (e.g., "I just want my life back"), and externalizing blame and unfairness ("I am suffering because of someone else's negligence.").[115] It has been suggested that understanding problems with top down processing/cognitive appraisals can be used to better understand and treat this problem.[116]
COVID-19 has disrupted the lives of many, leading to major physical, psychological and socioeconomic impacts in the general population.[117] Social distancing practices defining the response to the pandemic alter familiar patterns of social interaction, creating the conditions for what some psychologists are describing as a period of collective grief.[118] Individuals with chronic pain tend to embody an ambiguous status, at times expressing that their type of suffering places them between and outside of conventional medicine.[119] With a large proportion of the global population enduring prolonged periods of social isolation and distress, one study found that people with chronic pain from COVID-19 experienced more empathy towards their suffering during the pandemic.[117]
In the workplace, chronic pain conditions are a significant problem for both the person with the condition and the organization; a problem only expected to increase in many countries due to an aging workforce.[14] In light of this, it may be helpful for organizations to consider the social environment of their workplace, and how it may be working to ease or worsen chronic pain issues for employees.[14] As an example of how the social environment can affect chronic pain, some research has found that high levels of socially prescribed perfectionism (perfectionism induced by external pressure from others, such as a supervisor) can interact with the guilt felt by a person with chronic pain, thereby increasing job tension, and decreasing job satisfaction.[14]

Acne, also known as acne vulgaris, is a long-term skin condition that occurs when dead skin cells and oil from the skin clog hair follicles.[10] Typical features of the condition include blackheads or whiteheads, pimples, oily skin, and possible scarring.[1][2][11] It primarily affects skin with a relatively high number of oil glands, including the face, upper part of the chest, and back.[12] The resulting appearance can lead to lack of confidence, anxiety, reduced self-esteem, and, in extreme cases, depression or thoughts of suicide.[3][4]
Susceptibility to acne is primarily genetic in 80% of cases.[2] The roles of diet and cigarette smoking in the condition are unclear, and neither cleanliness nor exposure to sunlight appear to play a part.[2][13][14] In both sexes, hormones called androgens appear to be part of the underlying mechanism, by causing increased production of sebum.[5] Another common factor is the excessive growth of the bacterium Cutibacterium acnes, which is present on the skin.[15]
Treatments for acne are available, including lifestyle changes, medications, and medical procedures. Eating fewer simple carbohydrates such as sugar may minimize the condition.[7] Treatments applied directly to the affected skin, such as azelaic acid, benzoyl peroxide, and salicylic acid, are commonly used.[8] Antibiotics and retinoids are available in formulations that are applied to the skin and taken by mouth for the treatment of acne.[8] However, resistance to antibiotics may develop as a result of antibiotic therapy.[16] Several types of birth control pills help prevent acne in women.[8] Medical professionals typically reserve isotretinoin pills for severe acne, due to greater potential side effects.[8][17] Early and aggressive treatment of acne is advocated by some in the medical community to decrease the overall long-term impact on individuals.[4]
In 2015, acne affected approximately 633 million people globally, making it the eighth-most common disease worldwide.[9][18] Acne commonly occurs in adolescence and affects an estimated 80–90% of teenagers in the Western world.[19][20][21] Some rural societies report lower rates of acne than industrialized ones.[21][22] Children and adults may also be affected before and after puberty.[23] Although acne becomes less common in adulthood, it persists in nearly half of affected people into their twenties and thirties, and a smaller group continues to have difficulties in their forties.[2]
The severity of acne vulgaris (Gr. ἀκµή, "point" + L. vulgaris, "common")[24] can be classified as mild, moderate, or severe to determine an appropriate treatment regimen.[20] There is no universally accepted scale for grading acne severity.[15] The presence of clogged skin follicles (known as comedones) limited to the face with occasional inflammatory lesions defines mild acne.[20] Moderate severity acne is said to occur when a higher number of inflammatory papules and pustules occur on the face, compared to mild cases of acne, and appear on the trunk of the body.[20] Severe acne is said to occur when nodules (the painful 'bumps' lying under the skin) are the characteristic facial lesions, and involvement of the trunk is extensive.[20][25]
Large nodules were previously called cysts. The term nodulocystic has been used in the medical literature to describe severe cases of inflammatory acne.[25] True cysts are rare in those with acne, and the term severe nodular acne is now the preferred terminology.[25]
Acne inversa (L.  invertō, "upside-down") and acne rosacea (rosa, "rose-colored" + -āceus, "forming") are not forms of acne and are alternate names that respectively refer to the skin conditions hidradenitis suppurativa (HS) and rosacea.[26][27][28] Although HS shares certain overlapping features with acne vulgaris, such as a tendency to clog skin follicles with skin cell debris, the condition otherwise lacks the hallmark features of acne and is therefore considered a distinct skin disorder.[26]
Typical features of acne include increased secretion of oily sebum by the skin, microcomedones, comedones, papules, nodules (large papules), pustules, and often results in scarring.[29][30] The appearance of acne varies with skin color. It may result in psychological and social problems.[20]
Acne scars are caused by inflammation within the dermis and are estimated to affect 95% of people with acne vulgaris.[31] Abnormal healing and dermal inflammation create the scar.[32] Scarring is most likely to take place with severe acne but may occur with any form of acne vulgaris.[31] Acne scars are classified based on whether the abnormal healing response following dermal inflammation leads to excess collagen deposition or loss at the site of the acne lesion.[33]
Atrophic acne scars have lost collagen from the healing response and are the most common type of acne scar (accounting for approximately 75% of all acne scars).[32][33] Ice-pick scars, boxcar scars, and rolling scars are subtypes of atrophic acne scars.[31] Boxcar scars are round or ovoid indented scars with sharp borders and vary in size from 1.5–4 mm across.[32] Ice-pick scars are narrow (less than 2 mm across), deep scars that extend into the dermis.[32] Rolling scars are broader than ice-pick and boxcar scars (4–5 mm across) and have a wave-like pattern of depth in the skin.[32]
Hypertrophic scars are uncommon and are characterized by increased collagen content after the abnormal healing response.[32] They are described as firm and raised from the skin.[32][34] Hypertrophic scars remain within the original margins of the wound, whereas keloid scars can form scar tissue outside of these borders.[32] Keloid scars from acne occur more often in men and people with darker skin, and usually occur on the trunk of the body.[32]
In November 2021 a study[35] was published exposing the consensus of twenty-four renowned international plastic surgeons and dermatologists about the most effective energy-based devices for the treatment of acne scars.[36]
After an inflamed nodular acne lesion resolves, it is common for the skin to darken in that area, which is known as postinflammatory hyperpigmentation (PIH). The inflammation stimulates specialized pigment-producing skin cells (known as melanocytes) to produce more melanin pigment, which leads to the skin's darkened appearance.[37] PIH occurs more frequently in people with darker skin color.[38] Pigmented scar is a common term used for PIH, but is misleading as it suggests the color change is permanent. Often, PIH can be prevented by avoiding any aggravation of the nodule and can fade with time. However, untreated PIH can last for months, years, or even be permanent if deeper layers of skin are affected.[39] Even minimal skin exposure to the sun's ultraviolet rays can sustain hyperpigmentation.[37] Daily use of SPF 15 or higher sunscreen can minimize such a risk.[39] Whitening agents like azelaic acid, arbutin or else may be used to improve hyperpigmentation.[40]
Risk factors for the development of acne, other than genetics, have not been conclusively identified. Possible secondary contributors include hormones, infections, diet, and stress. Studies investigating the impact of smoking on the incidence and severity of acne have been inconclusive.[2][41][42] Sunlight and cleanliness are not associated with acne.[14]
Acne appears to be highly heritable; genetics explain 81% of the variation in the population.[15] Studies performed in affected twins and first-degree relatives further demonstrate the strongly inherited nature of acne.[2][15] Acne susceptibility is likely due to the influence of multiple genes, as the disease does not follow a classic (Mendelian) inheritance pattern. These gene candidates include certain variations in tumor necrosis factor-alpha (TNF-alpha), IL-1 alpha, and CYP1A1 genes, among others.[19] The 308 G/A single nucleotide polymorphism variation in the gene for TNF is associated with an increased risk for acne.[43] Acne can be a feature of rare genetic disorders such as Apert's syndrome.[15] Severe acne may be associated with XYY syndrome.[44]
Hormonal activity, such as occurs during menstrual cycles and puberty, may contribute to the formation of acne. During puberty, an increase in sex hormones called androgens causes the skin follicle glands to grow larger and make more oily sebum.[12] The androgen hormones testosterone, dihydrotestosterone (DHT), and dehydroepiandrosterone (DHEA) are all linked to acne. High levels of growth hormone (GH) and insulin-like growth factor 1 (IGF-1) are also associated with worsened acne.[45] Both androgens and IGF-1 seem to be essential for acne to occur, as acne does not develop in individuals with complete androgen insensitivity syndrome (CAIS) or Laron syndrome (insensitivity to GH, resulting in very low IGF-1 levels).[46][47]
Medical conditions that commonly cause a high-androgen state, such as polycystic ovary syndrome, congenital adrenal hyperplasia, and androgen-secreting tumors, can cause acne in affected individuals.[48][49] Conversely, people who lack androgenic hormones or are insensitive to the effects of androgens rarely have acne.[48] Pregnancy can increase androgen levels, and consequently, oily sebum synthesis.[49][50] Acne can be a side effect of testosterone replacement therapy or anabolic steroid use.[1][51] Over-the-counter bodybuilding and dietary supplements often contain illegally added anabolic steroids.[1][52]
The anaerobic bacterial species Cutibacterium acnes (formerly Propionibacterium acnes) contributes to the development of acne, but its exact role is not well understood.[2] There are specific sub-strains of C. acnes associated with normal skin and others with moderate or severe inflammatory acne.[53] It is unclear whether these undesirable strains evolve on-site or are acquired, or possibly both depending on the person. These strains have the capability of changing, perpetuating, or adapting to the abnormal cycle of inflammation, oil production, and inadequate sloughing of dead skin cells from acne pores. Infection with the parasitic mite Demodex is associated with the development of acne.[30][54] It is unclear whether eradication of the mite improves acne.[54]
High-glycemic-load diets have been found to have different degrees of effect on acne severity.[7][55][56] Multiple randomized controlled trials and nonrandomized studies have found a lower-glycemic-load diet to be effective in reducing acne.[55] There is weak observational evidence suggesting that dairy milk consumption is positively associated with a higher frequency and severity of acne.[54][55][57][58][59] Milk contains whey protein and hormones such as bovine IGF-1 and precursors of dihydrotestosterone.[55] Studies suggest these components promote the effects of insulin and IGF-1 and thereby increase the production of androgen hormones, sebum, and promote the formation of comedones.[55] Available evidence does not support a link between eating chocolate or salt and acne severity.[57][58] Few studies have examined the relationship between obesity and acne.[2] Vitamin B12 may trigger skin outbreaks similar to acne (acneiform eruptions), or worsen existing acne when taken in doses exceeding the recommended daily intake.[60]
There are few high-quality studies to demonstrate that stress causes or worsens acne.[61] Despite being controversial, some research indicates that increased acne severity is associated with high stress levels in certain contexts, such as hormonal changes seen in premenstrual syndrome.[62][63]
Some individuals experience severe intensification of their acne when they are exposed to hot humid climates; this is due to bacteria and fungus thriving in warm, moist environments. This climate-induced acne exacerbation has been termed tropical acne. 
Mechanical obstruction of skin follicles with helmets or chinstraps can worsen pre-existing acne.[64] However, when acne is caused by mechanical obstruction it is not considered a form of acne vulgaris when being very technical, and would be an other acneiform eruption known as Acne mechanica.
Several medications can also worsen pre-existing acne; this condition is the acne medicamentosa 
form of acne. Examples of such medications include lithium, hydantoin, isoniazid, glucocorticoids, iodides, bromides, and testosterone.[44] When acne medicamentosa is specifically caused by anabolic–androgenic steroids it can simply be referred to as steroid acne.  
Genetically susceptible individuals can get acne breakouts as a result of polymorphous light eruption; a condition triggered by sunlight and artificial UV light exposure. This form of acne is called Acne aestivalis and is specifically caused by intense UVA light exposure. Affected individuals usually experience seasonal acne breakouts on their upper arms, shoulder girdle, back, and chest. The breakouts typically occur one-to-three days after exposure to intese UVA radiation. Unlike other forms of acne, the condition spares the face; this could possibly be a result of the pathogenesis of polymorphous light eruption, in which areas of the skin that are newly exposed to intense ultraviolet radiation are affected. Since faces are typically left uncovered at all stages of life, there is little-to-no likelihood for an eruption to appear there. Studies show that both polymorphous light eruption outbreaks and the acne aestivalis breakout response can be prevented by topical antioxidants combined with the application of a broad spectrum sunscreen.[65]
Acne vulgaris is a chronic skin disease of the pilosebaceous unit and develops due to blockages in the skin's hair follicles. These blockages occur as a result of the following four abnormal processes: increased oily sebum production (influenced by androgens), excessive deposition of the protein keratin leading to comedo formation, colonization of the follicle by Cutibacterium acnes (C. acnes) bacteria, and the local release of pro-inflammatory chemicals in the skin.[53]
The earliest pathologic change is the formation of a plug (a microcomedone), which is driven primarily by excessive growth, reproduction, and accumulation of skin cells in the hair follicle.[1] In healthy skin, the skin cells that have died come up to the surface and exit the pore of the hair follicle.[10] In people with acne, the increased production of oily sebum causes the dead skin cells to stick together.[10] The accumulation of dead skin cell debris and oily sebum blocks the pore of the hair follicle, thus forming the microcomedone.[10] The C. acnes biofilm within the hair follicle worsens this process.[48] If the microcomedone is superficial within the hair follicle, the skin pigment melanin is exposed to air, resulting in its oxidation and dark appearance (known as a blackhead or open comedo).[1][10][20] In contrast, if the microcomedone occurs deep within the hair follicle, this causes the formation of a whitehead (known as a closed comedo).[1][10]
The main hormonal driver of oily sebum production in the skin is dihydrotestosterone.[1] Another androgenic hormone responsible for increased sebaceous gland activity is DHEA-S. The adrenal glands secrete higher amounts of DHEA-S during adrenarche (a stage of puberty), and this leads to an increase in sebum production. In a sebum-rich skin environment, the naturally occurring and largely commensal skin bacterium C. acnes readily grows and can cause inflammation within and around the follicle due to activation of the innate immune system.[10] C. acnes triggers skin inflammation in acne by increasing the production of several pro-inflammatory chemical signals (such as IL-1α, IL-8, TNF-α, and LTB4); IL-1α is essential to comedo formation.[48]
C. acnes' ability to bind and activate a class of immune system receptors known as toll-like receptors (TLRs), especially TLR2 and TLR4, is a core mechanism of acne-related skin inflammation.[48][66][67] Activation of TLR2 and TLR4 by C. acnes leads to increased secretion of IL-1α, IL-8, and TNF-α.[48] The release of these inflammatory signals attracts various immune cells to the hair follicle, including neutrophils, macrophages, and Th1 cells.[48] IL-1α stimulates increased skin cell activity and reproduction, which, in turn, fuels comedo development.[48] Furthermore, sebaceous gland cells produce more antimicrobial peptides, such as HBD1 and HBD2, in response to the binding of TLR2 and TLR4.[48]
C. acnes also provokes skin inflammation by altering the fatty composition of oily sebum.[48] Oxidation of the lipid squalene by C. acnes is of particular importance. Squalene oxidation activates NF-κB (a protein complex) and consequently increases IL-1α levels.[48] Additionally, squalene oxidation increases 5-lipoxygenase enzyme activity, which catalyzes the conversion of arachidonic acid to leukotriene B4 (LTB4).[48] LTB4 promotes skin inflammation by acting on the peroxisome proliferator-activated receptor alpha (PPARα) protein.[48] PPARα increases the activity of activator protein 1 (AP-1) and NF-κB, thereby leading to the recruitment of inflammatory T cells.[48] C. acnes' ability to convert sebum triglycerides to pro-inflammatory free fatty acids via secretion of the enzyme lipase further explains its inflammatory properties.[48] These free fatty acids spur increased production of cathelicidin, HBD1, and HBD2, thus leading to further inflammation.[48]
This inflammatory cascade typically leads to the formation of inflammatory acne lesions, including papules, infected pustules, or nodules.[1] If the inflammatory reaction is severe, the follicle can break into the deeper layers of the dermis and subcutaneous tissue and cause the formation of deep nodules.[1][68][69] The involvement of AP-1 in the aforementioned inflammatory cascade activates matrix metalloproteinases, which contribute to local tissue destruction and scar formation.[48]
Along with the bacteria C. acnes, the bacterial species Staphylococcus epidermidis (S. epidermidis) also takes a part in the physiopathology of acne vulgaris. The proliferation of S. epidermidis with C. acnes causes the formation of biofilms, which blocks the hair follicles and pores, creating an anaerobic environment under the skin. This enables for increased growth of both C. acnes and S. epidermidis under the skin. The proliferation of C. acnes causes the formation of biofilms and a biofilm matrix, making it even harder to treat the acne.[70]
Acne vulgaris is diagnosed based on a medical professional's clinical judgment.[15] The evaluation of a person with suspected acne should include taking a detailed medical history about a family history of acne, a review of medications taken, signs or symptoms of excessive production of androgen hormones, cortisol, and growth hormone.[15] Comedones (blackheads and whiteheads) must be present to diagnose acne. In their absence, an appearance similar to that of acne would suggest a different skin disorder.[28] Microcomedones (the precursor to blackheads and whiteheads) are not visible to the naked eye when inspecting the skin and require a microscope to be seen.[28] Many features may indicate that a person's acne vulgaris is sensitive to hormonal influences. Historical and physical clues that may suggest hormone-sensitive acne include onset between ages 20 and 30; worsening the week before a woman's period; acne lesions predominantly over the jawline and chin; and inflammatory/nodular acne lesions.[1]
Several scales exist to grade the severity of acne vulgaris, but disagreement persists about the ideal one for diagnostic use.[71][72] Cook's acne grading scale uses photographs to grade severity from 0 to 8, with higher numbers representing more severe acne. This scale was the first to use a standardized photographic protocol to assess acne severity; since its creation in 1979, the scale has undergone several revisions.[72] The Leeds acne grading technique counts acne lesions on the face, back, and chest and categorizes them as inflammatory or non-inflammatory. Leeds scores range from 0 (least severe) to 10 (most severe) though modified scales have a maximum score of 12.[72][73] The Pillsbury acne grading scale classifies the severity of the acne from grade 1 (least severe) to grade 4 (most severe).[71][74]
Many skin conditions can mimic acne vulgaris, and these are collectively known as acneiform eruptions.[28] Such conditions include angiofibromas, epidermal cysts, flat warts, folliculitis, keratosis pilaris, milia, perioral dermatitis, and rosacea, among others.[20][75] Age is one factor that may help distinguish between these disorders. Skin disorders such as perioral dermatitis and keratosis pilaris can appear similar to acne but tend to occur more frequently in childhood. Rosacea tends to occur more frequently in older adults.[20] Facial redness triggered by heat or the consumption of alcohol or spicy food is also more suggestive of rosacea.[76] The presence of comedones helps health professionals differentiate acne from skin disorders that are similar in appearance.[8] Chloracne and occupational acne due to exposure to certain chemicals & industrial compounds, may look very similar to acne vulgaris.[77]
Many different treatments exist for acne. These include alpha hydroxy acid, anti-androgen medications, antibiotics, antiseborrheic medications, azelaic acid, benzoyl peroxide, hormonal treatments, keratolytic soaps, nicotinamide, retinoids, and salicylic acid.[78] Acne treatments work in at least four different ways, including the following: reducing inflammation, hormonal manipulation, killing C. acnes, and normalizing skin cell shedding and sebum production in the pore to prevent blockage.[15] Typical treatments include topical therapies such as antibiotics, benzoyl peroxide, and retinoids, and systemic therapies, including antibiotics, hormonal agents, and oral retinoids.[20][79]
Recommended therapies for first-line use in acne vulgaris treatment include topical retinoids, benzoyl peroxide, and topical or oral antibiotics.[80] Procedures such as light therapy and laser therapy are not first-line treatments and typically have only an add on role due to their high cost and limited evidence.[79] Blue light therapy is of unclear benefit.[81] Medications for acne target the early stages of comedo formation and are generally ineffective for visible skin lesions; acne generally improves between eight and twelve weeks after starting therapy.[15]
People often view acne as a short-term condition, some expecting it to disappear after puberty. This misconception can lead to depending on self-management or problems with long-term adherence to treatment. Communicating the long-term nature of the condition and better access to reliable information about acne can help people know what to expect from treatments.[82][83]
In general, it is recommended that people with acne do not wash affected skin more than twice daily.[15] The application of a fragrance-free moisturizer to sensitive and acne-prone skin may reduce irritation. Skin irritation from acne medications typically peaks at two weeks after onset of use and tends to improve with continued use.[15] Dermatologists recommend using cosmetic products that specifically say non-comedogenic, oil-free, and will not clog pores.[15]
Acne vulgaris patients, even those with oily skin,[84] should moisturize in order to support the skin's moisture barrier since skin barrier dysfunction may contribute to acne.[84] Moisturizers, especially ceramide-containing moisturizers, as an adjunct therapy are particularly helpful for the dry skin and irritation that commonly results from topical acne treatment. Studies show that ceramide-containing moisturizers are important for optimal skin care; they enhance acne therapy adherence and complement existing acne therapies.[84] In a study where acne patients used 1.2% clindamycin phosphate / 2.5% benzoyl peroxide gel in the morning and applied a micronized 0.05% tretinoin gel in the evening the overwhelming majority of patients experienced no cutaneous adverse events throughout the study. It was concluded that using ceramide cleanser and ceramide moisturizing cream caused the favorable tolerability, did not interfere with the treatment efficacy, and improved adherence to the regimen.[85] The importance of preserving the acidic mantle and its barrier functions is widely accepted in the scientific community. Thus, maintaining a pH in the range 4.5 - 5.5 is essential in order to keep the skin surface in its optimal, healthy conditions.[86][87][88][89][90]
Causal relationship is rarely observed with diet/nutrition and dermatologic conditions. Rather, associations — some of them compelling — have been found between diet and outcomes including disease severity and the number of conditions experienced by a patient. Evidence is emerging in support of medical nutrition therapy as a way of reducing the severity and incidence of dermatologic diseases, including acne.  Researchers observed a link between high glycemic index diets and acne.[91] Dermatologists also recommend a diet low in simple sugars as a method of improving acne.[55] As of 2014, the available evidence is insufficient to use milk restriction for this purpose.[55]
Benzoyl peroxide (BPO) is a first-line treatment for mild and moderate acne due to its effectiveness and mild side-effects (mainly skin irritation). In the skin follicle, benzoyl peroxide kills C. acnes by oxidizing its proteins through the formation of oxygen free radicals and benzoic acid. These free radicals likely interfere with the bacterium's metabolism and ability to make proteins.[92][93] Additionally, benzoyl peroxide is mildly effective at breaking down comedones and inhibiting inflammation.[80][93] Combination products use benzoyl peroxide with a topical antibiotic or retinoid, such as benzoyl peroxide/clindamycin and benzoyl peroxide/adapalene, respectively.[38] Topical benzoyl peroxide is effective at treating acne.[94]
Side effects include increased skin photosensitivity, dryness, redness, and occasional peeling.[95] Sunscreen use is often advised during treatment, to prevent sunburn. Lower concentrations of benzoyl peroxide are just as effective as higher concentrations in treating acne but are associated with fewer side effects.[93][96] Unlike antibiotics, benzoyl peroxide does not appear to generate bacterial antibiotic resistance.[95]
Retinoids are medications that reduce inflammation, normalize the follicle cell life cycle, and reduce sebum production.[48][97] They are structurally related to vitamin A.[97] Studies show dermatologists and primary care doctors underprescribe them for acne.[15] The retinoids appear to influence the cell life cycle in the follicle lining. This helps prevent the accumulation of skin cells within the hair follicle that can create a blockage. They are a first-line acne treatment,[1] especially for people with dark-colored skin. Retinoids are known to lead to faster improvement of postinflammatory hyperpigmentation.[38]
Topical retinoids include adapalene, retinol, retinaldehyde, isotretinoin, tazarotene, trifarotene, and tretinoin.[50][98][99] They often cause an initial flare-up of acne and facial flushing and can cause significant skin irritation. Generally speaking, retinoids increase the skin's sensitivity to sunlight and are therefore recommended for use at night.[1] Tretinoin is the least expensive of the topical retinoids and is the most irritating to the skin, whereas adapalene is the least irritating but costs significantly more.[1][100] Most formulations of tretinoin are incompatible for use with benzoyl peroxide.[15] Tazarotene is the most effective and expensive topical retinoid but is usually not as well tolerated.[1][100] In 2019 a tazarotene lotion formulation, marketed to be a less irritating option, was approved by the FDA.[101] Retinol is a form of vitamin A that has similar but milder effects and is present in many over-the-counter moisturizers and other topical products.
Isotretinoin is an oral retinoid that is very effective for severe nodular acne, and moderate acne that is stubborn to other treatments.[1][20] One to two months of use is typically adequate to see improvement. Acne often resolves completely or is much milder after a 4–6 month course of oral isotretinoin.[1] After a single round of treatment, about 80% of people report an improvement, with more than 50% reporting complete remission.[20] About 20% of people require a second course, but 80% of those report improvement, resulting in a cumulative 96% efficacy rate.[20]
There are concerns that isotretinoin is linked to adverse effects, like depression, suicidality, and anemia. There is no clear evidence to support some of these claims.[1][20] Isotretinoin has been found in some studies to be superior to antibiotics or placebo in reducing acne lesions.[17] However, a 2018 review comparing inflammatory lesions after treatment with antibiotics or isotretinoin found no difference.[102] The frequency of adverse events was about twice as high with isotretinoin use, although these were mostly dryness-related events.[17] No increased risk of suicide or depression was conclusively found.[17]
Medical authorities strictly regulate isotretinoin use in women of childbearing age due to its known harmful effects in pregnancy.[20] For such a woman to be considered a candidate for isotretinoin, she must have a confirmed negative pregnancy test and use an effective form of birth control.[20] In 2008, the United States started the iPLEDGE program to prevent isotretinoin use during pregnancy.[103] iPledge requires the woman to have two negative pregnancy tests and to use two types of birth control for at least one month before isotretinoin therapy begins and one month afterward.[103] The effectiveness of the iPledge program is controversial due to continued instances of contraception nonadherence.[103][104]
People may apply antibiotics to the skin or take them orally to treat acne. They work by killing C. acnes and reducing inflammation.[20][95][105] Although multiple guidelines call for healthcare providers to reduce the rates of prescribed oral antibiotics, many providers do not follow this guidance.[106] Oral antibiotics remain the most commonly prescribed systemic therapy for acne.[106] Widespread broad-spectrum antibiotic overuse for acne has led to higher rates of antibiotic-resistant C. acnes strains worldwide, especially to the commonly used tetracycline (e.g., doxycycline) and macrolide antibiotics (e.g., topical erythromycin).[16][95][105][106] Therefore, dermatologists prefer antibiotics as part of combination therapy and not for use alone.[15]
Commonly used antibiotics, either applied to the skin or taken orally, include clindamycin, erythromycin, metronidazole, sulfacetamide, and tetracyclines (e.g., doxycycline or minocycline).[50] Doxycycline 40 milligrams daily (low-dose) appears to have similar efficacy to 100 milligrams daily and has fewer gastrointestinal side effects.[15] However, low-dose doxycycline is not FDA-approved for the treatment of acne.[107]  Antibiotics applied to the skin are typically used for mild to moderately severe acne.[20] Oral antibiotics are generally more effective than topical antibiotics and produce faster resolution of inflammatory acne lesions than topical applications.[1] The Global Alliance to Improve Outcomes in Acne recommends that topical and oral antibiotics are not used together.[105]
Oral antibiotics are recommended for no longer than three months as antibiotic courses exceeding this duration are associated with the development of antibiotic resistance and show no clear benefit over shorter durations.[105] If long-term oral antibiotics beyond three months are used, then it is recommended that benzoyl peroxide or a retinoid be used at the same time to limit the risk of C. acnes developing antibiotic resistance.[105]
The antibiotic dapsone is effective against inflammatory acne when applied to the skin. It is generally not a first-line choice due to its higher cost and a lack of clear superiority over other antibiotics.[1][15] Topical dapsone is sometimes a preferred therapy in women or for people with sensitive or darker-toned skin.[15] It is not recommended for use with benzoyl peroxide due to the risk of causing yellow-orange skin discoloration with this combination.[10] Minocycline is an effective acne treatment, but it is not a first-line antibiotic due to a lack of evidence that it is better than other treatments, and concerns about its safety compared to other tetracyclines.[108]
Sarecycline is the most recent oral antibiotic developed specifically for the treatment of acne, and is FDA-approved for the treatment of moderate to severe inflammatory acne in patients nine years of age and older.[109][110][111]  It is a narrow-spectrum tetracycline antibiotic that exhibits the necessary antibacterial activity against pathogens related to acne vulgaris and a low propensity for inducing antibiotic resistance.[112][113]  In clinical trials, sarecycline demonstrated clinical efficacy in reducing inflammatory acne lesions as early as three weeks and reduced truncal (back and chest) acne.[111][114]
In women, the use of combined birth control pills can improve acne.[115] These medications contain an estrogen and a progestin.[116] They work by decreasing the production of androgen hormones by the ovaries and by decreasing the free and hence biologically active fractions of androgens, resulting in lowered skin production of sebum and consequently reduce acne severity.[10][117] First-generation progestins such as norethindrone and norgestrel have androgenic properties and may worsen acne.[15] Although oral estrogens decrease IGF-1 levels in some situations, which could theoretically improve acne symptoms,[118][119] combined birth control pills do not appear to affect IGF-1 levels in fertile women.[116][120] Cyproterone acetate-containing birth control pills seem to decrease total and free IGF-1 levels.[121] Combinations containing third- or fourth-generation progestins, including desogestrel, dienogest, drospirenone, or norgestimate, as well as birth control pills containing cyproterone acetate or chlormadinone acetate, are preferred for women with acne due to their stronger antiandrogenic effects.[122][123][124] Studies have shown a 40 to 70% reduction in acne lesions with combined birth control pills.[117] A 2014 review found that oral antibiotics appear to be somewhat more effective than birth control pills at reducing the number of inflammatory acne lesions at three months.[125] However, the two therapies are approximately equal in efficacy at six months for decreasing the number of inflammatory, non-inflammatory, and total acne lesions.[125] The authors of the analysis suggested that birth control pills may be a preferred first-line acne treatment, over oral antibiotics, in certain women due to similar efficacy at six months and a lack of associated antibiotic resistance.[125] In contrast to combined birth control pills, progestogen-only birth control forms that contain androgenic progestins have been associated with worsened acne.[106]
Antiandrogens such as cyproterone acetate and spironolactone can successfully treat acne, especially in women with signs of excessive androgen production, such as increased hairiness or skin production of sebum, or scalp hair loss.[10][50] Spironolactone is an effective treatment for acne in adult women.[126][127] Unlike combined birth control pills, it is not approved by the United States Food and Drug Administration for this purpose.[1][38][126] Spironolactone is an aldosterone antagonist and is a useful acne treatment due to its ability to additionally block the androgen receptor at higher doses.[38][106] Alone or in combination with a birth control pill, spironolactone has shown a 33 to 85% reduction in acne lesions in women.[117] The effectiveness of spironolactone for acne appears to be dose-dependent.[117] High-dose cyproterone acetate alone reportedly decreases acne symptoms in women by 75 to 90% within three months.[128] It is usually combined with an estrogen to avoid menstrual irregularities and estrogen deficiency.[129] The medication appears to be effective in the treatment of acne in males, with one study finding that a high dosage reduced inflammatory acne lesions by 73%.[130][131] However, spironolactone and cyproterone acetate's side effects in males, such as gynecomastia, sexual dysfunction, and decreased bone mineral density, generally make their use for male acne impractical.[130][131][132]
Pregnant and lactating women should not receive antiandrogens for their acne due to a possibility of birth disorders such as hypospadias and feminization of male babies.[50] Women who are sexually active and who can or may become pregnant should use an effective method of contraception to prevent pregnancy while taking an antiandrogen.[133] Antiandrogens are often combined with birth control pills for this reason, which can result in additive efficacy.[38][134] The FDA added a black-box warning to spironolactone about possible tumor risks based on preclinical research with very high doses (>100-fold clinical doses) and cautioned that unnecessary use of the medication should be avoided.[80][106][135] However, several large epidemiological studies subsequently found no greater risk of tumors in association with spironolactone in humans.[106][136][137][138] Conversely, strong associations of cyproterone acetate with certain brain tumors have been discovered and its use has been restricted.[139][140][141] The brain tumor risk with cyproterone acetate is due to its strong progestogenic actions and is not related to antiandrogenic activity nor shared by other antiandrogens.[139][142][141]
Flutamide, a pure antagonist of the androgen receptor, is effective in treating acne in women.[128][143] It appears to reduce acne symptoms by 80 to 90% even at low doses, with several studies showing complete acne clearance.[128][144][145] In one study, flutamide decreased acne scores by 80% within three months, whereas spironolactone decreased symptoms by only 40% in the same period.[145][146][147] In a large long-term study, 97% of women reported satisfaction with the control of their acne with flutamide.[148] Although effective, flutamide has a risk of serious liver toxicity, and cases of death in women taking even low doses of the medication to treat androgen-dependent skin and hair conditions have occurred.[149] As such, the use of flutamide for acne has become increasingly limited,[148][150][151] and it has been argued that continued use of flutamide for such purposes is unethical.[149] Bicalutamide, a pure androgen receptor antagonist with the same mechanism as flutamide and with comparable or superior antiandrogenic efficacy but with a far lower risk of liver toxicity, is an alternative option to flutamide in the treatment of androgen-dependent skin and hair conditions in women.[133][152][153][154]
Clascoterone is a topical antiandrogen that has demonstrated effectiveness in the treatment of acne in both males and females and was approved for clinical use for this indication in August 2020.[155][156][157][158][159] It has shown no systemic absorption or associated antiandrogenic side effects.[158][159][160] In a small direct head-to-head comparison, clascoterone showed greater effectiveness than topical isotretinoin.[158][159][160] 5α-Reductase inhibitors such as finasteride and dutasteride may be useful for the treatment of acne in both males and females but have not been adequately evaluated for this purpose.[1][161][162][163] Moreover, 5α-reductase inhibitors have a strong potential for producing birth defects in male babies and this limits their use in women.[1][162] However, 5α-reductase inhibitors are frequently used to treat excessive facial/body hair in women and can be combined with birth control pills to prevent pregnancy.[161] There is no evidence as of 2010 to support the use of cimetidine or ketoconazole in the treatment of acne.[164]
Hormonal treatments for acne such as combined birth control pills and antiandrogens may be considered first-line therapy for acne under many circumstances, including desired contraception, known or suspected hyperandrogenism, acne during adulthood, acne that flares premenstrually, and when symptoms of significant sebum production (seborrhea) are co-present.[164] Hormone therapy is effective for acne both in women with hyperandrogenism and in women with normal androgen levels.[164]
Azelaic acid is effective for mild to moderate acne when applied topically at a 15–20% concentration.[68][165][166][167] Treatment twice daily for six months is necessary, and is as effective as topical benzoyl peroxide 5%, isotretinoin 0.05%, and erythromycin 2%.[168] Azelaic acid is an effective acne treatment due to its ability to reduce skin cell accumulation in the follicle and its antibacterial and anti-inflammatory properties.[68] It has a slight skin-lightening effect due to its ability to inhibit melanin synthesis. Therefore, it is useful in treating individuals with acne who are also affected by post-inflammatory hyperpigmentation.[1] Azelaic acid may cause skin irritation.[169] It is less effective and more expensive than retinoids.[1] Azelaic acid also led to worse treatment response when compared to benzoyl peroxide. When compared to tretinoin, azelaic acid makes little or no treatment response.[170]
Salicylic acid is a topically applied beta-hydroxy acid that stops bacteria from reproducing and has keratolytic properties.[171][172] It is less effective than retinoid therapy.[20] Salicylic acid opens obstructed skin pores and promotes the shedding of epithelial skin cells.[171] Dry skin is the most commonly seen side effect with topical application, though darkening of the skin can occur in individuals with darker skin types.[1]
Topical and oral preparations of nicotinamide (the amide form of vitamin B3) are alternative medical treatments.[173] Nicotinamide reportedly improves acne due to its anti-inflammatory properties, its ability to suppress sebum production, and its wound healing properties.[173] Topical and oral preparations of zinc are suggested treatments for acne; evidence to support their use for this purpose is limited.[174] Zinc's capacities to reduce inflammation and sebum production as well as inhibit C. acnes growth are its proposed mechanisms for improving acne.[174] Antihistamines may improve symptoms among those already taking isotretinoin due to their anti-inflammatory properties and their ability to suppress sebum production.[175]
Hydroquinone lightens the skin when applied topically by inhibiting tyrosinase, the enzyme responsible for converting the amino acid tyrosine to the skin pigment melanin, and is used to treat acne-associated post-inflammatory hyperpigmentation.[37] By interfering with the production of melanin in the epidermis, hydroquinone leads to less hyperpigmentation as darkened skin cells are naturally shed over time.[37] Improvement in skin hyperpigmentation is typically seen within six months when used twice daily. Hydroquinone is ineffective for hyperpigmentation affecting deeper layers of skin such as the dermis.[37] The use of a sunscreen with SPF 15 or higher in the morning with reapplication every two hours is recommended when using hydroquinone.[37] Its application only to affected areas lowers the risk of lightening the color of normal skin but can lead to a temporary ring of lightened skin around the hyperpigmented area.[37] Hydroquinone is generally well-tolerated; side effects are typically mild (e.g., skin irritation) and occur with the use of a higher than the recommended 4% concentration.[37] Most preparations contain the preservative sodium metabisulfite, which has been linked to rare cases of allergic reactions, including anaphylaxis and severe asthma exacerbations in susceptible people.[37] In extremely rare cases, the frequent and improper application of high-dose hydroquinone has been associated with a systemic condition known as exogenous ochronosis (skin discoloration and connective tissue damage from the accumulation of homogentisic acid).[37]
Combination therapy—using medications of different classes together, each with a different mechanism of action—has been demonstrated to be a more effective approach to acne treatment than monotherapy.[10][50] The use of topical benzoyl peroxide and antibiotics together is more effective than antibiotics alone.[10] Similarly, using a topical retinoid with an antibiotic clears acne lesions faster than the use of antibiotics alone.[10] Frequently used combinations include the following: antibiotic and benzoyl peroxide, antibiotic and topical retinoid, or topical retinoid and benzoyl peroxide.[50] Dermatologists generally prefer combining benzoyl peroxide with a retinoid over the combination of a topical antibiotic with a retinoid. Both regimens are effective, but benzoyl peroxide does not lead to antibiotic resistance.[10]
Although sebaceous gland activity in the skin increases during the late stages of pregnancy, pregnancy has not been reliably associated with worsened acne severity.[176] In general, topically applied medications are considered the first-line approach to acne treatment during pregnancy, as they have little systemic absorption and are therefore unlikely to harm a developing fetus.[176] Highly recommended therapies include topically applied benzoyl peroxide (pregnancy category C) and azelaic acid (category B).[176] Salicylic acid carries a category C safety rating due to higher systemic absorption (9–25%), and an association between the use of anti-inflammatory medications in the third trimester and adverse effects to the developing fetus including too little amniotic fluid in the uterus and early closure of the babies' ductus arteriosus blood vessel.[50][176] Prolonged use of salicylic acid over significant areas of the skin or under occlusive (sealed) dressings is not recommended as these methods increase systemic absorption and the potential for fetal harm.[176] Tretinoin (category C) and adapalene (category C) are very poorly absorbed, but certain studies have suggested teratogenic effects in the first trimester.[176] The data examining the association between maternal topical retinoid exposure in the first trimester of pregnancy and adverse pregnancy outcomes is limited.[177] A systematic review of observational studies concluded that such exposure does not appear to increase the risk of major birth defects, miscarriages, stillbirths, premature births, or low birth weight.[177] Similarly, in studies examining the effects of topical retinoids during pregnancy, fetal harm has not been seen in the second and third trimesters.[176]  Nevertheless, since rare harms from topical retinoids are not ruled out, they are not recommended for use during pregnancy due to persistent safety concerns.[177][178] Retinoids contraindicated for use during pregnancy include the topical retinoid tazarotene, and oral retinoids isotretinoin and acitretin (all category X).[176] Spironolactone is relatively contraindicated for use during pregnancy due to its antiandrogen effects.[1] Finasteride is not recommended as it is highly teratogenic.[1]
Topical antibiotics deemed safe during pregnancy include clindamycin, erythromycin, and metronidazole (all category B), due to negligible systemic absorption.[50][176] Nadifloxacin and dapsone (category C) are other topical antibiotics that may be used to treat acne in pregnant women but have received less study.[50][176] No adverse fetal events have been reported from the topical use of dapsone.[176] If retinoids are used there is a high risk of abnormalities occurring in the developing fetus; women of childbearing age are therefore required to use effective birth control if retinoids are used to treat acne.[20] Oral antibiotics deemed safe for pregnancy (all category B) include azithromycin, cephalosporins, and penicillins.[176] Tetracyclines (category D) are contraindicated during pregnancy as they are known to deposit in developing fetal teeth, resulting in yellow discoloration and thinned tooth enamel.[1][176] Their use during pregnancy has been associated with the development of acute fatty liver of pregnancy and is further avoided for this reason.[176]
Limited evidence supports comedo extraction, but it is an option for comedones that do not improve with standard treatment.[8][80] Another procedure for immediate relief is the injection of a corticosteroid into an inflamed acne comedo.[80] Electrocautery and electrofulguration are effective alternative treatments for comedones.[179]
Light therapy is a treatment method that involves delivering certain specific wavelengths of light to an area of skin affected by acne. Both regular and laser light have been used. The evidence for light therapy as a treatment for acne is weak and inconclusive.[8][180] Various light therapies appear to provide a short-term benefit, but data for long-term outcomes, and outcomes in those with severe acne, are sparse;[181] it may have a role for individuals whose acne has been resistant to topical medications.[10] A 2016 meta-analysis was unable to conclude whether light therapies were more beneficial than placebo or no treatment, nor the duration of benefit.[182]
When regular light is used immediately following the application of a sensitizing substance to the skin such as aminolevulinic acid or methyl aminolevulinate, the treatment is referred to as photodynamic therapy (PDT).[106][165] PDT has the most supporting evidence of all light therapy modalities.[80] PDT treats acne by using various forms of light (e.g., blue light or red light) that preferentially target the pilosebaceous unit.[106] Once the light activates the sensitizing substance, this generates free radicals and reactive oxygen species in the skin, which purposefully damage the sebaceous glands and kill C. acnes bacteria.[106]  Many different types of nonablative lasers (i.e., lasers that do not vaporize the top layer of the skin but rather induce a physiologic response in the skin from the light) have been used to treat acne, including those that use infrared wavelengths of light. Ablative lasers (such as CO2 and fractional types) have also been used to treat active acne and its scars. When ablative lasers are used, the treatment is often referred to as laser resurfacing because, as mentioned previously, the entire upper layers of the skin are vaporized.[183] Ablative lasers are associated with higher rates of adverse effects compared with non-ablative lasers, with examples being post-inflammatory hyperpigmentation, persistent facial redness, and persistent pain.[8][184][185] Physiologically, certain wavelengths of light, used with or without accompanying topical chemicals, are thought to kill bacteria and decrease the size and activity of the glands that produce sebum.[165] Disadvantages of light therapy can include its cost, the need for multiple visits, the time required to complete the procedure(s), and pain associated with some of the treatment modalities.[10] Typical side effects include skin peeling, temporary reddening of the skin, swelling, and post-inflammatory hyperpigmentation.[10]
Dermabrasion is an effective therapeutic procedure for reducing the appearance of superficial atrophic scars of the boxcar and rolling varieties.[32] Ice-pick scars do not respond well to treatment with dermabrasion due to their depth.[32] The procedure is painful and has many potential side effects such as skin sensitivity to sunlight, redness, and decreased pigmentation of the skin.[32] Dermabrasion has fallen out of favor with the introduction of laser resurfacing.[32] Unlike dermabrasion, there is no evidence that microdermabrasion is an effective treatment for acne.[8]
Dermal or subcutaneous fillers are substances injected into the skin to improve the appearance of acne scars. Fillers are used to increase natural collagen production in the skin and to increase skin volume and decrease the depth of acne scars.[186] Examples of fillers used for this purpose include hyaluronic acid; poly(methyl methacrylate) microspheres with collagen; human and bovine collagen derivatives, and fat harvested from the person's own body (autologous fat transfer).[186]
Microneedling is a procedure in which an instrument with multiple rows of tiny needles is rolled over the skin to elicit a wound healing response and stimulate collagen production to reduce the appearance of atrophic acne scars in people with darker skin color.[183] Notable adverse effects of microneedling include post-inflammatory hyperpigmentation and tram track scarring (described as discrete slightly raised scars in a linear distribution similar to a tram track). The latter is thought to be primarily attributable to improper technique by the practitioner, including the use of excessive pressure or inappropriately large needles.[183][187]
Subcision is useful for the treatment of superficial atrophic acne scars and involves the use of a small needle to loosen the fibrotic adhesions that result in the depressed appearance of the scar.[188][189][190]
Chemical peels can be used to reduce the appearance of acne scars.[32] Mild peels include those using glycolic acid, lactic acid, salicylic acid, Jessner's solution, or a lower concentration (20%) of trichloroacetic acid. These peels only affect the epidermal layer of the skin and can be useful in the treatment of superficial acne scars as well as skin pigmentation changes from inflammatory acne.[32] Higher concentrations of trichloroacetic acid (30–40%) are considered to be medium-strength peels and affect the skin as deep as the papillary dermis.[32] Formulations of trichloroacetic acid concentrated to 50% or more are considered to be deep chemical peels.[32] Medium-strength and deep-strength chemical peels are more effective for deeper atrophic scars but are more likely to cause side effects such as skin pigmentation changes, infection, and small white superficial cysts known as milia.[32]
Researchers are investigating complementary therapies as treatment for people with acne.[191] Low-quality evidence suggests topical application of tea tree oil or bee venom may reduce the total number of skin lesions in those with acne.[191] Tea tree oil appears to be approximately as effective as benzoyl peroxide or salicylic acid but is associated with allergic contact dermatitis.[1] Proposed mechanisms for tea tree oil's anti-acne effects include antibacterial action against C. acnes and anti-inflammatory properties.[67] Numerous other plant-derived therapies have demonstrated positive effects against acne (e.g., basil oil; oligosaccharides from seaweed; however, few well-done studies have examined their use for this purpose.[192] There is a lack of high-quality evidence for the use of acupuncture, herbal medicine, or cupping therapy for acne.[191]
Many over-the-counter treatments in many forms are available, which are often known as cosmeceuticals.[193] Certain types of makeup may be useful to mask acne.[194] In those with oily skin, a water-based product is often preferred.[194][195]
Acne usually improves around the age of 20 but may persist into adulthood.[78] Permanent physical scarring may occur.[20] Rare complications from acne or its treatment include the formation of pyogenic granulomas, osteoma cutis, and acne with facial edema.[196] Early and aggressive treatment of acne is advocated by some in the medical community to reduce the chances of these poor outcomes.[4]
There is good evidence to support the idea that acne and associated scarring negatively affect a person's psychological state, worsen mood, lower self-esteem, and are associated with a higher risk of anxiety disorders, depression, and suicidal thoughts.[3][31][54][82]
Misperceptions about acne's causative and aggravating factors are common, and people with acne often blame themselves, and others often blame those with acne for their condition.[197][82] Such blame can worsen the affected person's sense of self-esteem.[197] Until the 20th century, even among dermatologists, the list of causes was believed to include excessive sexual thoughts and masturbation.[198] Dermatology's association with sexually transmitted infections, especially syphilis, contributed to the stigma.[198]
Another psychological complication of acne vulgaris is acne excoriée, which occurs when a person persistently picks and scratches pimples, irrespective of the severity of their acne.[62][199] This can lead to significant scarring, changes in the affected person's skin pigmentation, and a cyclic worsening of the affected person's anxiety about their appearance.[62]
Globally, acne affects approximately 650 million people, or about 9.4% of the population, as of 2010.[200] It affects nearly 90% of people in Western societies during their teenage years, but can occur before adolescence and may persist into adulthood.[19][20][23] While acne that first develops between the ages of 21 and 25 is uncommon, it affects 54% of women and 40% of men older than 25 years of age[50][201] and has a lifetime prevalence of 85%.[50] About 20% of those affected have moderate or severe cases.[2] It is slightly more common in females than males (9.8% versus 9.0%).[200] In those over 40 years old, 1% of males and 5% of females still have problems.[20]
Rates appear to be lower in rural societies.[22] While some research has found it affects people of all ethnic groups,[202] acne may not occur in the non-Westernized peoples of Papua New Guinea and Paraguay.[203]
Acne affects 40–50 million people in the United States (16%) and approximately 3–5 million in Australia (23%).[125][204] Severe acne tends to be more common in people of Caucasian or Amerindian descent than in people of African descent.[21]
Historical records indicate Pharaohs had acne, which may be the earliest known reference to the disease. Sulfur's usefulness as a topical remedy for acne dates back to at least the reign of Cleopatra (69–30 BCE).[205] The sixth-century Greek physician Aëtius of Amida reportedly coined the term "ionthos" (ίονθωξ,) or "acnae", which seems to be a reference to facial skin lesions that occur during "the 'acme' of life" (puberty).[206]
In the 16th century, the French physician and botanist François Boissier de Sauvages de Lacroix provided one of the earlier descriptions of acne. He used the term "psydracia achne" to describe small, red, and hard tubercles that altered a person's facial appearance during adolescence and were neither itchy nor painful.[206]
The recognition and characterization of acne progressed in 1776 when Josef Plenck (an Austrian physician) published a book that proposed the novel concept of classifying skin diseases by their elementary (initial) lesions.[206] In 1808 the English dermatologist Robert Willan refined Plenck's work by providing the first detailed descriptions of several skin disorders using morphologic terminology that remains in use today.[206] Thomas Bateman continued and expanded on Robert Willan's work as his student and provided the first descriptions and illustrations of acne accepted as accurate by modern dermatologists.[206] Erasmus Wilson, in 1842, was the first to make the distinction between acne vulgaris and rosacea.[207] The first professional medical monograph dedicated entirely to acne was written by Lucius Duncan Bulkley and published in New York in 1885.[198][208]
Scientists initially hypothesized that acne represented a disease of the skin's hair follicle, and occurred due to blockage of the pore by sebum. During the 1880s, they observed bacteria by microscopy in skin samples from people with acne. Investigators believed the bacteria caused comedones, sebum production, and ultimately acne.[206] During the mid-twentieth century, dermatologists realized that no single hypothesized factor (sebum, bacteria, or excess keratin) fully accounted for the disease in its entirety.[206] This led to the current understanding that acne could be explained by a sequence of related events, beginning with blockage of the skin follicle by excessive dead skin cells, followed by bacterial invasion of the hair follicle pore, changes in sebum production, and inflammation.[206]
The approach to acne treatment underwent significant changes during the twentieth century. Retinoids became a medical treatment for acne in 1943.[97] Benzoyl peroxide was first proposed as a treatment in 1958 and remains a staple of acne treatment.[209] The introduction of oral tetracycline antibiotics (such as minocycline) modified acne treatment in the 1950s. These reinforced the idea amongst dermatologists that bacterial growth on the skin plays an important role in causing acne.[206] Subsequently, in the 1970s, tretinoin (original trade name Retin A) was found to be an effective treatment.[210] The development of oral isotretinoin (sold as Accutane and Roaccutane) followed in 1980.[211] After its introduction in the United States, scientists identified isotretinoin as a medication highly likely to cause birth defects if taken during pregnancy. In the United States, more than 2,000 women became pregnant while taking isotretinoin between 1982 and 2003, with most pregnancies ending in abortion or miscarriage. Approximately 160 babies were born with birth defects due to maternal use of isotretinoin during pregnancy.[212][213]
Treatment of acne with topical crushed dry ice, known as cryoslush, was first described in 1907 but is no longer performed commonly.[214] Before 1960, the use of X-rays was also a common treatment.[215][216]
The costs and social impact of acne are substantial. In the United States, acne vulgaris is responsible for more than 5 million doctor visits and costs over US$2.5 billion each year in direct costs.[13] Similarly, acne vulgaris is responsible for 3.5 million doctor visits each year in the United Kingdom.[20] Sales for the top ten leading acne treatment brands in the US in 2015 amounted to $352 million.[217]
Acne vulgaris and its resultant scars are associated with significant social and academic difficulties that can last into adulthood.[31][218] During the Great Depression, dermatologists discovered that young men with acne had difficulty obtaining jobs.[198] Until the 1930s, many people viewed acne as a trivial problem among middle-class girls because, unlike smallpox and tuberculosis, no one died from it, and a feminine problem, because boys were much less likely to seek medical assistance for it.[198] During World War II, some soldiers in tropical climates developed such severe and widespread tropical acne on their bodies that they were declared medically unfit for duty.[198]
Efforts to better understand the mechanisms of sebum production are underway. This research aims to develop medications that target and interfere with the hormones that are known to increase sebum production (e.g., IGF-1 and alpha-melanocyte-stimulating hormone).[10] Other sebum-lowering medications such as topical antiandrogens, peroxisome proliferator-activated receptor modulators, and inhibitors of the stearoyl-CoA desaturase-1 enzyme are also a focus of research efforts.[10][106] Particles that release nitric oxide into the skin to decrease skin inflammation caused by C. acnes and the immune system have shown promise for improving acne in early clinical trials.[106] Another avenue of early-stage research has focused on how to best use laser and light therapy to selectively destroy sebum-producing glands in the skin's hair follicles to reduce sebum production and improve acne appearance.[10]
The use of antimicrobial peptides against C. acnes is under investigation as a treatment for acne to overcoming antibiotic resistance.[10] In 2007, scientists reported the first genome sequencing of a C. acnes bacteriophage (PA6). The authors proposed applying this research toward the development of bacteriophage therapy as an acne treatment to overcome the problems associated with long-term antibiotic use, such as bacterial resistance.[219] Oral and topical probiotics are under evaluation as treatments for acne.[220] Probiotics may have therapeutic effects for those affected by acne due to their ability to decrease skin inflammation and improve skin moisture by increasing the skin's ceramide content.[220] As of 2014, knowledge of the effects of probiotics on acne in humans was limited.[220]
Decreased levels of retinoic acid in the skin may contribute to comedo formation. Researchers are investigating methods to increase the skin's production of retinoic acid to address this deficiency.[10] A vaccine against inflammatory acne has shown promising results in mice and humans.[53][221] Some have voiced concerns about creating a vaccine designed to neutralize a stable community of normal skin bacteria that is known to protect the skin from colonization by more harmful microorganisms.[222]
Acne can occur on cats,[223] dogs,[224] and horses.[225][226]

Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body.[2][7] These contrast with benign tumors, which do not spread.[7] Possible signs and symptoms include a lump, abnormal bleeding, prolonged cough, unexplained weight loss, and a change in bowel movements.[1] While these symptoms may indicate cancer, they can also have other causes.[1] Over 100 types of cancers affect humans.[7]
Tobacco use is the cause of about 22% of cancer deaths.[2] Another 10% are due to obesity, poor diet, lack of physical activity or excessive alcohol consumption.[2][8][9] Other factors include certain infections, exposure to ionizing radiation, and environmental pollutants.[3] In the developing world, 15% of cancers are due to infections such as Helicobacter pylori, hepatitis B, hepatitis C, human papillomavirus infection, Epstein–Barr virus and human immunodeficiency virus (HIV).[2] These factors act, at least partly, by changing the genes of a cell.[10] Typically, many genetic changes are required before cancer develops.[10] Approximately 5–10% of cancers are due to inherited genetic defects.[11] Cancer can be detected by certain signs and symptoms or screening tests.[2] It is then typically further investigated by medical imaging and confirmed by biopsy.[12]
The risk of developing certain cancers can be reduced by not smoking, maintaining a healthy weight, limiting alcohol intake, eating plenty of vegetables, fruits, and whole grains, vaccination against certain infectious diseases, limiting consumption of processed meat and red meat, and limiting exposure to direct sunlight.[13][14] Early detection through screening is useful for cervical and colorectal cancer.[15] The benefits of screening for breast cancer are controversial.[15][16] Cancer is often treated with some combination of radiation therapy, surgery, chemotherapy and targeted therapy.[2][4] Pain and symptom management are an important part of care.[2] Palliative care is particularly important in people with advanced disease.[2] The chance of survival depends on the type of cancer and extent of disease at the start of treatment.[10] In children under 15 at diagnosis, the five-year survival rate in the developed world is on average 80%.[17] For cancer in the United States, the average five-year survival rate is 66% for all ages.[5]
In 2015, about 90.5 million people worldwide had cancer.[18] In 2019, annual cancer cases grew by 23.6 million people and there were 10 million deaths worldwide, representing over the previous decade increases of 26% and 21%, respectively.[6][19]
The most common types of cancer in males are lung cancer, prostate cancer, colorectal cancer, and stomach cancer.[20][21] In females, the most common types are breast cancer, colorectal cancer, lung cancer, and cervical cancer.[10][22] If skin cancer other than melanoma were included in total new cancer cases each year, it would account for around 40% of cases.[23][24] In children, acute lymphoblastic leukemia and brain tumors are most common, except in Africa, where non-Hodgkin lymphoma occurs more often.[17] In 2012, about 165,000 children under 15 years of age were diagnosed with cancer.[20] The risk of cancer increases significantly with age, and many cancers occur more commonly in developed countries.[10] Rates are increasing as more people live to an old age and as lifestyle changes occur in the developing world.[25] The global total economic costs of cancer were estimated at US$1.16 trillion (equivalent to $1.56 trillion in 2022) per year as of 2010[update].[26]
The word comes from the ancient Greek καρκίνος, meaning 'crab' and 'tumor'. Greek physicians Hippocrates and Galen, among others, noted the similarity of crabs to some tumors with swollen veins. The word was introduced in English in the modern medical sense around 1600.[27]
Cancers comprise a large family of diseases that involve abnormal cell growth with the potential to invade or spread to other parts of the body.[2][7] They form a subset of neoplasms. A neoplasm or tumor is a group of cells that have undergone unregulated growth and will often form a mass or lump, but may be distributed diffusely.[28][29]
All tumor cells show the six hallmarks of cancer. These characteristics are required to produce a malignant tumor. They include:[30]
The progression from normal cells to cells that can form a detectable mass to cancer involves multiple steps known as malignant progression.[30][31]
When cancer begins, it produces no symptoms. Signs and symptoms appear as the mass grows or ulcerates. The findings that result depend on cancer's type and location. Few symptoms are specific. Many frequently occur in individuals who have other conditions. Cancer can be difficult to diagnose and can be considered a "great imitator."[32]
People may become anxious or depressed post-diagnosis. The risk of suicide in people with cancer is approximately double.[33]
Local symptoms may occur due to the mass of the tumor or its ulceration. For example, mass effects from lung cancer can block the bronchus resulting in cough or pneumonia; esophageal cancer can cause narrowing of the esophagus, making it difficult or painful to swallow; and colorectal cancer may lead to narrowing or blockages in the bowel, affecting bowel habits. Masses in breasts or testicles may produce observable lumps. Ulceration can cause bleeding that can lead to symptoms such as coughing up blood (lung cancer), anemia or rectal bleeding (colon cancer), blood in the urine (bladder cancer), or abnormal vaginal bleeding (endometrial or cervical cancer). Although localized pain may occur in advanced cancer, the initial tumor is usually painless. Some cancers can cause a buildup of fluid within the chest or abdomen.[32]
Systemic symptoms may occur due to the body's response to the cancer. This may include fatigue, unintentional weight loss, or skin changes.[34] Some cancers can cause a systemic inflammatory state that leads to ongoing muscle loss and weakness, known as cachexia.[35]
Some cancers, such as Hodgkin's disease, leukemias, and liver or kidney cancers, can cause a persistent fever.[32]
Shortness of breath, called dyspnea, is a common symptom of cancer and its treatment. The causes of cancer-related dyspnea can include tumors in or around the lung, blocked airways, fluid in the lungs, pneumonia, or treatment reactions including an allergic response.[36] Treatment for dyspnea in patients with advanced cancer can include fans, bilevel ventilation, acupressure/reflexology and multicomponent nonpharmacological interventions.[37]
Some systemic symptoms of cancer are caused by hormones or other molecules produced by the tumor, known as paraneoplastic syndromes. Common paraneoplastic syndromes include hypercalcemia, which can cause altered mental state, constipation and dehydration, or hyponatremia, which can also cause altered mental status, vomiting, headaches, or seizures.[38]
Metastasis is the spread of cancer to other locations in the body. The dispersed tumors are called metastatic tumors, while the original is called the primary tumor. Almost all cancers can metastasize.[39] Most cancer deaths are due to cancer that has metastasized.[40]
Metastasis is common in the late stages of cancer and it can occur via the blood or the lymphatic system or both. The typical steps in metastasis are local invasion, intravasation into the blood or lymph, circulation through the body, extravasation into the new tissue, proliferation and angiogenesis. Different types of cancers tend to metastasize to particular organs, but overall the most common places for metastases to occur are the lungs, liver, brain, and the bones.[39]
While some cancers can be cured if detected early, metastatic cancer is more difficult to treat and control. Nevertheless, some recent treatments are demonstrating encouraging results.[41]
The majority of cancers, some 90–95% of cases, are due to genetic mutations from environmental and lifestyle factors.[3] The remaining 5–10% are due to inherited genetics.[3] Environmental refers to any cause that is not inherited, such as lifestyle, economic, and behavioral factors and not merely pollution.[43] Common environmental factors that contribute to cancer death include tobacco use (25–30%), diet and obesity (30–35%), infections (15–20%), radiation (both ionizing and non-ionizing, up to 10%), lack of physical activity, and pollution.[3][44] Psychological stress does not appear to be a risk factor for the onset of cancer,[45][46] though it may worsen outcomes in those who already have cancer.[45]
It is not generally possible to prove what caused a particular cancer because the various causes do not have specific fingerprints. For example, if a person who uses tobacco heavily develops lung cancer, then it was probably caused by the tobacco use, but since everyone has a small chance of developing lung cancer as a result of air pollution or radiation, the cancer may have developed for one of those reasons. Excepting the rare transmissions that occur with pregnancies and occasional organ donors, cancer is generally not a transmissible disease,[47] however factors that may have contributed to the development of cancer can be transmissible; such as oncoviruses like hepatitis B, Epstein-Barr virus and HIV.
Exposure to particular substances have been linked to specific types of cancer. These substances are called carcinogens.
Tobacco smoke, for example, causes 90% of lung cancer.[48] Tobacco use can cause cancer throughout the body including in the mouth and throat, larynx, esophagus, stomach, bladder, kidney, cervix, colon/rectum, liver and pancreas.[49][50] Tobacco smoke contains over fifty known carcinogens, including nitrosamines and polycyclic aromatic hydrocarbons.[51]
Tobacco is responsible for about one in five cancer deaths worldwide[51] and about one in three in the developed world.[52] Lung cancer death rates in the United States have mirrored smoking patterns, with increases in smoking followed by dramatic increases in lung cancer death rates and, more recently, decreases in smoking rates since the 1950s followed by decreases in lung cancer death rates in men since 1990.[53][54]
In Western Europe, 10% of cancers in males and 3% of cancers in females are attributed to alcohol exposure, especially liver and digestive tract cancers.[55] Cancer from work-related substance exposures may cause between 2 and 20% of cases,[56] causing at least 200,000 deaths.[57] Cancers such as lung cancer and mesothelioma can come from inhaling tobacco smoke or asbestos fibers, or leukemia from exposure to benzene.[57]
Exposure to perfluorooctanoic acid (PFOA), which is predominantly used in the production of Teflon, is known to cause two kinds of cancer.[58][59]
Diet, physical inactivity, and obesity are related to up to 30–35% of cancer deaths.[3][60] In the United States, excess body weight is associated with the development of many types of cancer and is a factor in 14–20% of cancer deaths.[60] A UK study including data on over 5 million people showed higher body mass index to be related to at least 10 types of cancer and responsible for around 12,000 cases each year in that country.[61] Physical inactivity is believed to contribute to cancer risk, not only through its effect on body weight but also through negative effects on the immune system and endocrine system.[60] More than half of the effect from the diet is due to overnutrition (eating too much), rather than from eating too few vegetables or other healthful foods.
Some specific foods are linked to specific cancers. A high-salt diet is linked to gastric cancer.[62] Aflatoxin B1, a frequent food contaminant, causes liver cancer.[62] Betel nut chewing can cause oral cancer.[62] National differences in dietary practices may partly explain differences in cancer incidence. For example, gastric cancer is more common in Japan due to its high-salt diet[63] while colon cancer is more common in the United States. Immigrant cancer profiles mirror those of their new country, often within one generation.[64]
Worldwide, approximately 18% of cancer deaths are related to infectious diseases.[3] This proportion ranges from a high of 25% in Africa to less than 10% in the developed world.[3] Viruses are the usual infectious agents that cause cancer but cancer bacteria and parasites may also play a role. Oncoviruses (viruses that can cause cancer) include:
Bacterial infection may also increase the risk of cancer, as seen in Helicobacter pylori-induced gastric carcinoma.[65][66]
Parasitic infections associated with cancer include:
Radiation exposure such as ultraviolet radiation and radioactive material is a risk factor for cancer.[68][69][70] Many non-melanoma skin cancers are due to ultraviolet radiation, mostly from sunlight.[69] Sources of ionizing radiation include medical imaging and radon gas.[69]
Ionizing radiation is not a particularly strong mutagen.[71] Residential exposure to radon gas, for example, has similar cancer risks as passive smoking.[71] Radiation is a more potent source of cancer when combined with other cancer-causing agents, such as radon plus tobacco smoke.[71] Radiation can cause cancer in most parts of the body, in all animals and at any age. Children are twice as likely to develop radiation-induced leukemia as adults; radiation exposure before birth has ten times the effect.[71]
Medical use of ionizing radiation is a small but growing source of radiation-induced cancers. Ionizing radiation may be used to treat other cancers, but this may, in some cases, induce a second form of cancer.[71] It is also used in some kinds of medical imaging.[72]
Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies.[73] Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world.[73]
Non-ionizing radio frequency radiation from mobile phones, electric power transmission and other similar sources has been described as a possible carcinogen by the World Health Organization's International Agency for Research on Cancer.[74] Evidence, however, has not supported a concern.[75] [68] This includes that studies have not found a consistent link between mobile phone radiation and cancer risk.[76]
The vast majority of cancers are non-hereditary (sporadic). Hereditary cancers are primarily caused by an inherited genetic defect. Less than 0.3% of the population are carriers of a genetic mutation that has a large effect on cancer risk and these cause less than 3–10% of cancer.[77] Some of these syndromes include: certain inherited mutations in the genes BRCA1 and BRCA2 with a more than 75% risk of breast cancer and ovarian cancer,[77] and hereditary nonpolyposis colorectal cancer (HNPCC or Lynch syndrome), which is present in about 3% of people with colorectal cancer,[78] among others.
Statistically for cancers causing most mortality, the relative risk of developing colorectal cancer when a first-degree relative (parent, sibling or child) has been diagnosed with it is about 2.[79] The corresponding relative risk is 1.5 for lung cancer,[80] and 1.9 for prostate cancer.[81] For breast cancer, the relative risk is 1.8 with a first-degree relative having developed it at 50 years of age or older, and 3.3 when the relative developed it when being younger than 50 years of age.[82]
Taller people have an increased risk of cancer because they have more cells than shorter people. Since height is genetically determined to a large extent, taller people have a heritable increase of cancer risk.[83]
Some substances cause cancer primarily through their physical, rather than chemical, effects.[84] A prominent example of this is prolonged exposure to asbestos, naturally occurring mineral fibers that are a major cause of mesothelioma (cancer of the serous membrane) usually the serous membrane surrounding the lungs.[84] Other substances in this category, including both naturally occurring and synthetic asbestos-like fibers, such as wollastonite, attapulgite, glass wool and rock wool, are believed to have similar effects.[84] Non-fibrous particulate materials that cause cancer include powdered metallic cobalt and nickel and crystalline silica (quartz, cristobalite and tridymite).[84] Usually, physical carcinogens must get inside the body (such as through inhalation) and require years of exposure to produce cancer.[84]
Physical trauma resulting in cancer is relatively rare.[85] Claims that breaking bones resulted in bone cancer, for example, have not been proven.[85] Similarly, physical trauma is not accepted as a cause for cervical cancer, breast cancer or brain cancer.[85] One accepted source is frequent, long-term application of hot objects to the body. It is possible that repeated burns on the same part of the body, such as those produced by kanger and kairo heaters (charcoal hand warmers), may produce skin cancer, especially if carcinogenic chemicals are also present.[85] Frequent consumption of scalding hot tea may produce esophageal cancer.[85] Generally, it is believed that cancer arises, or a pre-existing cancer is encouraged, during the process of healing, rather than directly by the trauma.[85] However, repeated injuries to the same tissues might promote excessive cell proliferation, which could then increase the odds of a cancerous mutation.
Chronic inflammation has been hypothesized to directly cause mutation.[85][86] Inflammation can contribute to proliferation, survival, angiogenesis and migration of cancer cells by influencing the tumor microenvironment.[87][88] Oncogenes build up an inflammatory pro-tumorigenic microenvironment.[89]
Some hormones play a role in the development of cancer by promoting cell proliferation.[90] Insulin-like growth factors and their binding proteins play a key role in cancer cell proliferation, differentiation and apoptosis, suggesting possible involvement in carcinogenesis.[91]
Hormones are important agents in sex-related cancers, such as cancer of the breast, endometrium, prostate, ovary and testis and also of thyroid cancer and bone cancer.[90] For example, the daughters of women who have breast cancer have significantly higher levels of estrogen and progesterone than the daughters of women without breast cancer. These higher hormone levels may explain their higher risk of breast cancer, even in the absence of a breast-cancer gene.[90] Similarly, men of African ancestry have significantly higher levels of testosterone than men of European ancestry and have a correspondingly higher level of prostate cancer.[90] Men of Asian ancestry, with the lowest levels of testosterone-activating androstanediol glucuronide, have the lowest levels of prostate cancer.[90]
Other factors are relevant: obese people have higher levels of some hormones associated with cancer and a higher rate of those cancers.[90] Women who take hormone replacement therapy have a higher risk of developing cancers associated with those hormones.[90] On the other hand, people who exercise far more than average have lower levels of these hormones and lower risk of cancer.[90] Osteosarcoma may be promoted by growth hormones.[90] Some treatments and prevention approaches leverage this cause by artificially reducing hormone levels and thus discouraging hormone-sensitive cancers.[90]
There is an association between celiac disease and an increased risk of all cancers. People with untreated celiac disease have a higher risk, but this risk decreases with time after diagnosis and strict treatment, probably due to the adoption of a gluten-free diet, which seems to have a protective role against development of malignancy in people with celiac disease. However, the delay in diagnosis and initiation of a gluten-free diet seems to increase the risk of malignancies.[92] Rates of gastrointestinal cancers are increased in people with Crohn's disease and ulcerative colitis, due to chronic inflammation. Also, immunomodulators and biologic agents used to treat these diseases may promote developing extra-intestinal malignancies.[93]
Cancer is fundamentally a disease of tissue growth regulation. For a normal cell to transform into a cancer cell, the genes that regulate cell growth and differentiation must be altered.[94]
The affected genes are divided into two broad categories. Oncogenes are genes that promote cell growth and reproduction. Tumor suppressor genes are genes that inhibit cell division and survival. Malignant transformation can occur through the formation of novel oncogenes, the inappropriate over-expression of normal oncogenes, or by the under-expression or disabling of tumor suppressor genes. Typically, changes in multiple genes are required to transform a normal cell into a cancer cell.[95]
Genetic changes can occur at different levels and by different mechanisms. The gain or loss of an entire chromosome can occur through errors in mitosis. More common are mutations, which are changes in the nucleotide sequence of genomic DNA.
Large-scale mutations involve the deletion or gain of a portion of a chromosome. Genomic amplification occurs when a cell gains copies (often 20 or more) of a small chromosomal locus, usually containing one or more oncogenes and adjacent genetic material. Translocation occurs when two separate chromosomal regions become abnormally fused, often at a characteristic location. A well-known example of this is the Philadelphia chromosome, or translocation of chromosomes 9 and 22, which occurs in chronic myelogenous leukemia and results in production of the BCR-abl fusion protein, an oncogenic tyrosine kinase.
Small-scale mutations include point mutations, deletions, and insertions, which may occur in the promoter region of a gene and affect its expression, or may occur in the gene's coding sequence and alter the function or stability of its protein product. Disruption of a single gene may also result from integration of genomic material from a DNA virus or retrovirus, leading to the expression of viral oncogenes in the affected cell and its descendants.
Replication of the data contained within the DNA of living cells will probabilistically result in some errors (mutations). Complex error correction and prevention are built into the process and safeguard the cell against cancer. If a significant error occurs, the damaged cell can self-destruct through programmed cell death, termed apoptosis. If the error control processes fail, then the mutations will survive and be passed along to daughter cells.
Some environments make errors more likely to arise and propagate. Such environments can include the presence of disruptive substances called carcinogens, repeated physical injury, heat, ionising radiation, or hypoxia.[96]
The errors that cause cancer are self-amplifying and compounding, for example:
The transformation of a normal cell into cancer is akin to a chain reaction caused by initial errors, which compound into more severe errors, each progressively allowing the cell to escape more controls that limit normal tissue growth. This rebellion-like scenario is an undesirable survival of the fittest, where the driving forces of evolution work against the body's design and enforcement of order. Once cancer has begun to develop, this ongoing process, termed clonal evolution, drives progression towards more invasive stages.[97] Clonal evolution leads to intra-tumour heterogeneity (cancer cells with heterogeneous mutations) that complicates designing effective treatment strategies and requires an evolutionary approach to designing treatment.
Characteristic abilities developed by cancers are divided into categories, specifically evasion of apoptosis, self-sufficiency in growth signals, insensitivity to anti-growth signals, sustained angiogenesis, limitless replicative potential, metastasis, reprogramming of energy metabolism and evasion of immune destruction.[30][31]
The classical view of cancer is a set of diseases driven by progressive genetic abnormalities that include mutations in tumor-suppressor genes and oncogenes, and in chromosomal abnormalities. A role for epigenetic alterations was identified in the early 21st century.[98]
Epigenetic alterations are functionally relevant modifications to the genome that do not change the nucleotide sequence. Examples of such modifications are changes in DNA methylation (hypermethylation and hypomethylation), histone modification[99] and changes in chromosomal architecture (caused by inappropriate expression of proteins such as HMGA2 or HMGA1).[100] Each of these alterations regulates gene expression without altering the underlying DNA sequence. These changes may remain through cell divisions, endure for multiple generations, and can be considered as equivalent to mutations.
Epigenetic alterations occur frequently in cancers. As an example, one study listed protein coding genes that were frequently altered in their methylation in association with colon cancer. These included 147 hypermethylated and 27 hypomethylated genes. Of the hypermethylated genes, 10 were hypermethylated in 100% of colon cancers and many others were hypermethylated in more than 50% of colon cancers.[101]
While epigenetic alterations are found in cancers, the epigenetic alterations in DNA repair genes, causing reduced expression of DNA repair proteins, may be of particular importance. Such alterations may occur early in progression to cancer and are a possible cause of the genetic instability characteristic of cancers.[102][103][104]
Reduced expression of DNA repair genes disrupts DNA repair. This is shown in the figure at the 4th level from the top. (In the figure, red wording indicates the central role of DNA damage and defects in DNA repair in progression to cancer.) When DNA repair is deficient DNA damage remains in cells at a higher than usual level (5th level) and causes increased frequencies of mutation and/or epimutation (6th level). Mutation rates increase substantially in cells defective in DNA mismatch repair[105][106] or in homologous recombinational repair (HRR).[107] Chromosomal rearrangements and aneuploidy also increase in HRR defective cells.[108]
Higher levels of DNA damage cause increased mutation (right side of figure) and increased epimutation. During repair of DNA double strand breaks, or repair of other DNA damage, incompletely cleared repair sites can cause epigenetic gene silencing.[109][110]
Deficient expression of DNA repair proteins due to an inherited mutation can increase cancer risks. Individuals with an inherited impairment in any of 34 DNA repair genes (see article DNA repair-deficiency disorder) have increased cancer risk, with some defects ensuring a 100% lifetime chance of cancer (e.g. p53 mutations).[111] Germ line DNA repair mutations are noted on the figure's left side. However, such germline mutations (which cause highly penetrant cancer syndromes) are the cause of only about 1 percent of cancers.[112]
In sporadic cancers, deficiencies in DNA repair are occasionally caused by a mutation in a DNA repair gene but are much more frequently caused by epigenetic alterations that reduce or silence expression of DNA repair genes. This is indicated in the figure at the 3rd level. Many studies of heavy metal-induced carcinogenesis show that such heavy metals cause a reduction in expression of DNA repair enzymes, some through epigenetic mechanisms. DNA repair inhibition is proposed to be a predominant mechanism in heavy metal-induced carcinogenicity. In addition, frequent epigenetic alterations of the DNA sequences code for small RNAs called microRNAs (or miRNAs). miRNAs do not code for proteins, but can "target" protein-coding genes and reduce their expression.
Cancers usually arise from an assemblage of mutations and epimutations that confer a selective advantage leading to clonal expansion (see Field defects in progression to cancer). Mutations, however, may not be as frequent in cancers as epigenetic alterations. An average cancer of the breast or colon can have about 60 to 70 protein-altering mutations, of which about three or four may be "driver" mutations and the remaining ones may be "passenger" mutations.[113]
Metastasis is the spread of cancer to other locations in the body. The dispersed tumors are called metastatic tumors, while the original is called the primary tumor. Almost all cancers can metastasize.[39] Most cancer deaths are due to cancer that has metastasized.[40]
Metastasis is common in the late stages of cancer and it can occur via the blood or the lymphatic system or both. The typical steps in metastasis are local invasion, intravasation into the blood or lymph, circulation through the body, extravasation into the new tissue, proliferation and angiogenesis. Different types of cancers tend to metastasize to particular organs, but overall the most common places for metastases to occur are the lungs, liver, brain and the bones.[39]
Normal cells typically generate only about 30% of energy from glycolysis,[114] whereas most cancers rely on glycolysis for energy production (Warburg effect).[115][116] But a minority of cancer types rely on oxidative phosphorylation as the primary energy source, including lymphoma, leukemia, and endometrial cancer.[117] Even in these cases, however, the use of glycolysis as an energy source rarely exceeds 60%.[114] A few cancers use glutamine as the major energy source, partly because it provides nitrogen required for nucleotide (DNA, RNA) synthesis.[118] Cancer stem cells often use oxidative phosphorylation or glutamine as a primary energy source.[119]
Most cancers are initially recognized either because of the appearance of signs or symptoms or through screening.[120] Neither of these leads to a definitive diagnosis, which requires the examination of a tissue sample by a pathologist.[121] People with suspected cancer are investigated with medical tests. These commonly include blood tests, X-rays, (contrast) CT scans and endoscopy.
The tissue diagnosis from the biopsy indicates the type of cell that is proliferating, its histological grade, genetic abnormalities and other features. Together, this information is useful to evaluate the prognosis and to choose the best treatment.
Cytogenetics and immunohistochemistry are other types of tissue tests. These tests provide information about molecular changes (such as mutations, fusion genes and numerical chromosome changes) and may thus also indicate the prognosis and best treatment.
Cancer diagnosis can cause psychological distress and psychosocial interventions, such as talking therapy, may help people with this.[122]
Cancers are classified by the type of cell that the tumor cells resemble and is therefore presumed to be the origin of the tumor. These types include:
Cancers are usually named using -carcinoma, -sarcoma or -blastoma as a suffix, with the Latin or Greek word for the organ or tissue of origin as the root. For example, cancers of the liver parenchyma arising from malignant epithelial cells is called hepatocarcinoma, while a malignancy arising from primitive liver precursor cells is called a hepatoblastoma and a cancer arising from fat cells is called a liposarcoma. For some common cancers, the English organ name is used. For example, the most common type of breast cancer is called ductal carcinoma of the breast. Here, the adjective ductal refers to the appearance of cancer under the microscope, which suggests that it has originated in the milk ducts.
Benign tumors (which are not cancers) are named using -oma as a suffix with the organ name as the root. For example, a benign tumor of smooth muscle cells is called a leiomyoma (the common name of this frequently occurring benign tumor in the uterus is fibroid). Confusingly, some types of cancer use the -noma suffix, examples including melanoma and seminoma.
Some types of cancer are named for the size and shape of the cells under a microscope, such as giant cell carcinoma, spindle cell carcinoma and small-cell carcinoma.
An invasive ductal carcinoma of the breast (pale area at the center) surrounded by spikes of whitish scar tissue and yellow fatty tissue
An invasive colorectal carcinoma (top center) in a colectomy specimen
A squamous-cell carcinoma (the whitish tumor) near the bronchi in a lung specimen
A large invasive ductal carcinoma in a mastectomy specimen
Cancer prevention is defined as active measures to decrease cancer risk.[125] The vast majority of cancer cases are due to environmental risk factors. Many of these environmental factors are controllable lifestyle choices. Thus, cancer is generally preventable.[126] Between 70% and 90% of common cancers are due to environmental factors and therefore potentially preventable.[127]
Greater than 30% of cancer deaths could be prevented by avoiding risk factors including: tobacco, excess weight/obesity, poor diet, physical inactivity, alcohol, sexually transmitted infections and air pollution.[128] Further, poverty could be considered as an indirect risk factor in human cancers.[129] Not all environmental causes are controllable, such as naturally occurring background radiation and cancers caused through hereditary genetic disorders and thus are not preventable via personal behavior.
In 2019, ~44% of all cancer deaths – or ~4.5 M deaths or ~105 million lost disability-adjusted life years – were due to known clearly preventable risk factors, led by smoking, alcohol use and high BMI, according to a GBD systematic analysis.[124]
While many dietary recommendations have been proposed to reduce cancer risks, the evidence to support them is not definitive.[13][130] The primary dietary factors that increase risk are obesity and alcohol consumption. Diets low in fruits and vegetables and high in red meat have been implicated but reviews and meta-analyses do not come to a consistent conclusion.[131][132] A 2014 meta-analysis found no relationship between fruits and vegetables and cancer.[133] Coffee is associated with a reduced risk of liver cancer.[134] Studies have linked excessive consumption of red or processed meat to an increased risk of breast cancer, colon cancer and pancreatic cancer, a phenomenon that could be due to the presence of carcinogens in meats cooked at high temperatures.[135][136] In 2015 the IARC reported that eating processed meat (e.g., bacon, ham, hot dogs, sausages) and, to a lesser degree, red meat was linked to some cancers.[137][138]
Dietary recommendations for cancer prevention typically include an emphasis on vegetables, fruit, whole grains and fish and an avoidance of processed and red meat (beef, pork, lamb), animal fats, pickled foods and refined carbohydrates.[13][130]
Medications can be used to prevent cancer in a few circumstances.[139] In the general population, NSAIDs reduce the risk of colorectal cancer; however, due to cardiovascular and gastrointestinal side effects, they cause overall harm when used for prevention.[140] Aspirin has been found to reduce the risk of death from cancer by about 7%.[141] COX-2 inhibitors may decrease the rate of polyp formation in people with familial adenomatous polyposis; however, it is associated with the same adverse effects as NSAIDs.[142] Daily use of tamoxifen or raloxifene reduce the risk of breast cancer in high-risk women.[143] The benefit versus harm for 5-alpha-reductase inhibitor such as finasteride is not clear.[144]
Vitamin supplementation does not appear to be effective at preventing cancer.[145] While low blood levels of vitamin D are correlated with increased cancer risk,[146][147][148] whether this relationship is causal and vitamin D supplementation is protective is not determined.[149][150] One 2014 review found that supplements had no significant effect on cancer risk.[150] Another 2014 review concluded that vitamin D3 may decrease the risk of death from cancer (one fewer death in 150 people treated over 5 years), but concerns with the quality of the data were noted.[151]
Beta-Carotene supplementation increases lung cancer rates in those who are high risk.[152] Folic acid supplementation is not effective in preventing colon cancer and may increase colon polyps.[153] Selenium supplementation has not been shown to reduce the risk of cancer.[154]
Vaccines have been developed that prevent infection by some carcinogenic viruses.[155] Human papillomavirus vaccine (Gardasil and Cervarix) decrease the risk of developing cervical cancer.[155] The hepatitis B vaccine prevents infection with hepatitis B virus and thus decreases the risk of liver cancer.[155] The administration of human papillomavirus and hepatitis B vaccinations is recommended where resources allow.[156]
Unlike diagnostic efforts prompted by symptoms and medical signs, cancer screening involves efforts to detect cancer after it has formed, but before any noticeable symptoms appear.[157] This may involve physical examination, blood or urine tests or medical imaging.[157]
Cancer screening is not available for many types of cancers. Even when tests are available, they may not be recommended for everyone. Universal screening or mass screening involves screening everyone.[158] Selective screening identifies people who are at higher risk, such as people with a family history.[158] Several factors are considered to determine whether the benefits of screening outweigh the risks and the costs of screening.[157] These factors include:
The U.S. Preventive Services Task Force (USPSTF) issues recommendations for various cancers:
Screens for gastric cancer using photofluorography due to the high incidence there.[25]
Genetic testing for individuals at high-risk of certain cancers is recommended by unofficial groups.[156][172] Carriers of these mutations may then undergo enhanced surveillance, chemoprevention, or preventative surgery to reduce their subsequent risk.[172]
Many treatment options for cancer exist. The primary ones include surgery, chemotherapy, radiation therapy, hormonal therapy, targeted therapy and palliative care. Which treatments are used depends on the type, location and grade of the cancer as well as the patient's health and preferences. The treatment intent may or may not be curative.
Chemotherapy is the treatment of cancer with one or more cytotoxic anti-neoplastic drugs (chemotherapeutic agents) as part of a standardized regimen. The term encompasses a variety of drugs, which are divided into broad categories such as alkylating agents and antimetabolites.[173] Traditional chemotherapeutic agents act by killing cells that divide rapidly, a critical property of most cancer cells.
It was found that providing combined cytotoxic drugs is better than a single drug, a process called the combination therapy, which has an advantage in the statistics of survival and response to the tumor and in the progress of the disease.[174] A Cochrane review concluded that combined therapy was more effective to treat metastasized breast cancer. However, generally it is not certain whether combination chemotherapy leads to better health outcomes, when both survival and toxicity are considered.[175]
Targeted therapy is a form of chemotherapy that targets specific molecular differences between cancer and normal cells. The first targeted therapies blocked the estrogen receptor molecule, inhibiting the growth of breast cancer. Another common example is the class of Bcr-Abl inhibitors, which are used to treat chronic myelogenous leukemia (CML).[4] Currently, targeted therapies exist for many of the most common cancer types, including bladder cancer, breast cancer, colorectal cancer, kidney cancer, leukemia, liver cancer, lung cancer, lymphoma, pancreatic cancer, prostate cancer, skin cancer, and thyroid cancer as well as other cancer types.[176]
The efficacy of chemotherapy depends on the type of cancer and the stage. In combination with surgery, chemotherapy has proven useful in cancer types including breast cancer, colorectal cancer, pancreatic cancer, osteogenic sarcoma, testicular cancer, ovarian cancer and certain lung cancers.[177] Chemotherapy is curative for some cancers, such as some leukemias,[178][179] ineffective in some brain tumors,[180] and needless in others, such as most non-melanoma skin cancers.[181] The effectiveness of chemotherapy is often limited by its toxicity to other tissues in the body. Even when chemotherapy does not provide a permanent cure, it may be useful to reduce symptoms such as pain or to reduce the size of an inoperable tumor in the hope that surgery will become possible in the future.
Radiation therapy involves the use of ionizing radiation in an attempt to either cure or improve symptoms. It works by damaging the DNA of cancerous tissue, causing mitotic catastrophe resulting in the death of the cancer cells.[182] To spare normal tissues (such as skin or organs, which radiation must pass through to treat the tumor), shaped radiation beams are aimed from multiple exposure angles to intersect at the tumor, providing a much larger dose there than in the surrounding, healthy tissue. As with chemotherapy, cancers vary in their response to radiation therapy.[183][184][185]
Radiation therapy is used in about half of cases. The radiation can be either from internal sources (brachytherapy) or external sources. The radiation is most commonly low energy X-rays for treating skin cancers, while higher energy X-rays are used for cancers within the body.[186] Radiation is typically used in addition to surgery and or chemotherapy. For certain types of cancer, such as early head and neck cancer, it may be used alone.[187] Radiation therapy after surgery for brain metastases has been shown to not improve overall survival in patients compared to surgery alone.[188] For painful bone metastasis, radiation therapy has been found to be effective in about 70% of patients.[187]
Surgery is the primary method of treatment for most isolated, solid cancers and may play a role in palliation and prolongation of survival. It is typically an important part of definitive diagnosis and staging of tumors, as biopsies are usually required. In localized cancer, surgery typically attempts to remove the entire mass along with, in certain cases, the lymph nodes in the area. For some types of cancer this is sufficient to eliminate the cancer.[177]
Palliative care is treatment that attempts to help the patient feel better and may be combined with an attempt to treat the cancer. Palliative care includes action to reduce physical, emotional, spiritual and psycho-social distress. Unlike treatment that is aimed at directly killing cancer cells, the primary goal of palliative care is to improve quality of life.
People at all stages of cancer treatment typically receive some kind of palliative care. In some cases, medical specialty professional organizations recommend that patients and physicians respond to cancer only with palliative care.[189] This applies to patients who:[190]
Palliative care may be confused with hospice and therefore only indicated when people approach end of life. Like hospice care, palliative care attempts to help the patient cope with their immediate needs and to increase comfort. Unlike hospice care, palliative care does not require people to stop treatment aimed at the cancer.
Multiple national medical guidelines recommend early palliative care for patients whose cancer has produced distressing symptoms or who need help coping with their illness. In patients first diagnosed with metastatic disease, palliative care may be immediately indicated. Palliative care is indicated for patients with a prognosis of less than 12 months of life even given aggressive treatment.[191][192][193]
A variety of therapies using immunotherapy, stimulating or helping the immune system to fight cancer, have come into use since 1997. Approaches include antibodies, checkpoint therapy, and adoptive cell transfer.[194]
Laser therapy uses high-intensity light to treat cancer by shrinking or destroying tumors or precancerous growths. Lasers are most commonly used to treat superficial cancers that are on the surface of the body or the lining of internal organs. It is used to treat basal cell skin cancer and the very early stages of others like cervical, penile, vaginal, vulvar, and non-small cell lung cancer. It is often combined with other treatments, such as surgery, chemotherapy, or radiation therapy. Laser-induced interstitial thermotherapy (LITT), or interstitial laser photocoagulation, uses lasers to treat some cancers using hyperthermia, which uses heat to shrink tumors by damaging or killing cancer cells. Laser are more precise than surgery and cause less damage, pain, bleeding, swelling, and scarring. A disadvantage is surgeons must have specialized training. It may be more expensive than other treatments.[195]
Complementary and alternative cancer treatments are a diverse group of therapies, practices and products that are not part of conventional medicine.[196] "Complementary medicine" refers to methods and substances used along with conventional medicine, while "alternative medicine" refers to compounds used instead of conventional medicine.[197] Most complementary and alternative medicines for cancer have not been studied or tested using conventional techniques such as clinical trials. Some alternative treatments have been investigated and shown to be ineffective but still continue to be marketed and promoted. Cancer researcher Andrew J. Vickers stated, "The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'."[198]
Survival rates vary by cancer type and by the stage at which it is diagnosed, ranging from majority survival to complete mortality five years after diagnosis. Once a cancer has metastasized, prognosis normally becomes much worse. About half of patients receiving treatment for invasive cancer (excluding carcinoma in situ and non-melanoma skin cancers) die from that cancer or its treatment.[25] A majority of cancer deaths are due to metastases of the primary tumor.[200]
Survival is worse in the developing world,[25] partly because the types of cancer that are most common there are harder to treat than those associated with developed countries.[201]
Those who survive cancer develop a second primary cancer at about twice the rate of those never diagnosed.[202] The increased risk is believed to be due to the random chance of developing any cancer, the likelihood of surviving the first cancer, the same risk factors that produced the first cancer, unwanted side effects of treating the first cancer (particularly radiation therapy), and better compliance with screening.[202]
Predicting short- or long-term survival depends on many factors. The most important are the cancer type and the patient's age and overall health. Those who are frail with other health problems have lower survival rates than otherwise healthy people. Centenarians are unlikely to survive for five years even if treatment is successful. People who report a higher quality of life tend to survive longer.[203] People with lower quality of life may be affected by depression and other complications and/or disease progression that both impairs quality and quantity of life. Additionally, patients with worse prognoses may be depressed or report poorer quality of life because they perceive that their condition is likely to be fatal.
People with cancer have an increased risk of blood clots in their veins which can be life-threatening.[204] The use of blood thinners such as heparin decrease the risk of blood clots but have not been shown to increase survival in people with cancer.[204] People who take blood thinners also have an increased risk of bleeding.[204]
Although extremely rare, some forms of cancer, even from an advanced stage, can heal spontaneously. This phenomenon is known as the spontaneous remission.[205]
Estimates are that in 2018, 18.1 million new cases of cancer and 9.6 million deaths occur globally.[207] About 20% of males and 17% of females will get cancer at some point in time while 13% of males and 9% of females will die from it.[207]
In 2008, approximately 12.7 million cancers were diagnosed (excluding non-melanoma skin cancers and other non-invasive cancers)[25] and in 2010 nearly 7.98 million people died.[208] Cancers account for approximately 16% of deaths. The most common as of 2018[update] are lung cancer (1.76 million deaths), colorectal cancer (860,000) stomach cancer (780,000), liver cancer (780,000), and breast cancer (620,000).[2] This makes invasive cancer the leading cause of death in the developed world and the second leading in the developing world.[25] Over half of cases occur in the developing world.[25]
Deaths from cancer were 5.8 million in 1990.[208] Deaths have been increasing primarily due to longer lifespans and lifestyle changes in the developing world.[25] The most significant risk factor for developing cancer is age.[209] Although it is possible for cancer to strike at any age, most patients with invasive cancer are over 65.[209] According to cancer researcher Robert A. Weinberg, "If we lived long enough, sooner or later we all would get cancer."[210] Some of the association between aging and cancer is attributed to immunosenescence,[211] errors accumulated in DNA over a lifetime[212] and age-related changes in the endocrine system.[213] Aging's effect on cancer is complicated by factors such as DNA damage and inflammation promoting it and factors such as vascular aging and endocrine changes inhibiting it.[214]
Some slow-growing cancers are particularly common, but often are not fatal. Autopsy studies in Europe and Asia showed that up to 36% of people have undiagnosed and apparently harmless thyroid cancer at the time of their deaths and that 80% of men develop prostate cancer by age 80.[215][216] As these cancers do not cause the patient's death, identifying them would have represented overdiagnosis rather than useful medical care.
The three most common childhood cancers are leukemia (34%), brain tumors (23%) and lymphomas (12%).[217] In the United States cancer affects about 1 in 285 children.[218] Rates of childhood cancer increased by 0.6% per year between 1975 and 2002 in the United States[219] and by 1.1% per year between 1978 and 1997 in Europe.[217] Death from childhood cancer decreased by half between 1975 and 2010 in the United States.[218]
Cancer has existed for all of human history.[220] The earliest written record regarding cancer is from circa 1600 BC in the Egyptian Edwin Smith Papyrus and describes breast cancer.[220] Hippocrates (c. 460 BC – c. 370 BC) described several kinds of cancer, referring to them with the Greek word καρκίνος karkinos (crab or crayfish).[220] This name comes from the appearance of the cut surface of a solid malignant tumor, with "the veins stretched on all sides as the animal the crab has its feet, whence it derives its name".[221] Galen stated that "cancer of the breast is so called because of the fancied resemblance to a crab given by the lateral prolongations of the tumor and the adjacent distended veins".[222]: 738  Celsus (c. 25 BC – 50 AD) translated karkinos into the Latin cancer, also meaning crab and recommended surgery as treatment.[220] Galen (2nd century AD) disagreed with the use of surgery and recommended purgatives instead.[220] These recommendations largely stood for 1000 years.[220]
In the 15th, 16th and 17th centuries, it became acceptable for doctors to dissect bodies to discover the cause of death.[223] The German professor Wilhelm Fabry believed that breast cancer was caused by a milk clot in a mammary duct. The Dutch professor Francois de la Boe Sylvius, a follower of Descartes, believed that all disease was the outcome of chemical processes and that acidic lymph fluid was the cause of cancer. His contemporary Nicolaes Tulp believed that cancer was a poison that slowly spreads and concluded that it was contagious.[224]
The physician John Hill described tobacco sniffing as the cause of nose cancer in 1761.[223] This was followed by the report in 1775 by British surgeon Percivall Pott that chimney sweeps' carcinoma, a cancer of the scrotum, was a common disease among chimney sweeps.[225] With the widespread use of the microscope in the 18th century, it was discovered that the 'cancer poison' spread from the primary tumor through the lymph nodes to other sites ("metastasis"). This view of the disease was first formulated by the English surgeon Campbell De Morgan between 1871 and 1874.[226]
Although many diseases (such as heart failure) may have a worse prognosis than most cases of cancer, cancer is the subject of widespread fear and taboos. The euphemism of "a long illness" to describe cancers leading to death is still commonly used in obituaries, rather than naming the disease explicitly, reflecting an apparent stigma.[227] Cancer is also euphemised as "the C-word";[228][229][230] Macmillan Cancer Support uses the term to try to lessen the fear around the disease.[231] In Nigeria, one local name for cancer translates into English as "the disease that cannot be cured".[232] This deep belief that cancer is necessarily a difficult and usually deadly disease is reflected in the systems chosen by society to compile cancer statistics: the most common form of cancer—non-melanoma skin cancers, accounting for about one-third of cancer cases worldwide, but very few deaths[233][234]—are excluded from cancer statistics specifically because they are easily treated and almost always cured, often in a single, short, outpatient procedure.[235]
Western conceptions of patients' rights for people with cancer include a duty to fully disclose the medical situation to the person, and the right to engage in shared decision-making in a way that respects the person's own values. In other cultures, other rights and values are preferred. For example, most African cultures value whole families rather than individualism. In parts of Africa, a diagnosis is commonly made so late that cure is not possible, and treatment, if available at all, would quickly bankrupt the family. As a result of these factors, African healthcare providers tend to let family members decide whether, when and how to disclose the diagnosis, and they tend to do so slowly and circuitously, as the person shows interest and an ability to cope with the grim news.[232] People from Asian and South American countries also tend to prefer a slower, less candid approach to disclosure than is idealized in the United States and Western Europe, and they believe that sometimes it would be preferable not to be told about a cancer diagnosis.[232] In general, disclosure of the diagnosis is more common than it was in the 20th century, but full disclosure of the prognosis is not offered to many patients around the world.[232]
In the United States and some other cultures, cancer is regarded as a disease that must be "fought" to end the "civil insurrection"; a War on Cancer was declared in the US. Military metaphors are particularly common in descriptions of cancer's human effects, and they emphasize both the state of the patient's health and the need to take immediate, decisive actions himself rather than to delay, to ignore or to rely entirely on others. The military metaphors also help rationalize radical, destructive treatments.[236][237]
In the 1970s, a relatively popular alternative cancer treatment in the US was a specialized form of talk therapy, based on the idea that cancer was caused by a bad attitude.[238] People with a "cancer personality"—depressed, repressed, self-loathing and afraid to express their emotions—were believed to have manifested cancer through subconscious desire. Some psychotherapists said that treatment to change the patient's outlook on life would cure the cancer.[238] Among other effects, this belief allowed society to blame the victim for having caused the cancer (by "wanting" it) or having prevented its cure (by not becoming a sufficiently happy, fearless and loving person).[239] It also increased patients' anxiety, as they incorrectly believed that natural emotions of sadness, anger or fear shorten their lives.[239] The idea was ridiculed by Susan Sontag, who published Illness as Metaphor while recovering from treatment for breast cancer in 1978.[238] Although the original idea is now generally regarded as nonsense, the idea partly persists in a reduced form with a widespread, but incorrect, belief that deliberately cultivating a habit of positive thinking will increase survival.[239] This notion is particularly strong in breast cancer culture.[239]
One idea about why people with cancer are blamed or stigmatized, called the just-world hypothesis, is that blaming cancer on the patient's actions or attitudes allows the blamers to regain a sense of control. This is based upon the blamers' belief that the world is fundamentally just and so any dangerous illness, like cancer, must be a type of punishment for bad choices, because in a just world, bad things would not happen to good people.[240]
The total health care expenditure on cancer in the US was estimated to be $80.2 billion in 2015.[241] Even though cancer-related health care expenditure have increased in absolute terms during recent decades, the share of health expenditure devoted to cancer treatment has remained close to 5% between the 1960s and 2004.[242][243] A similar pattern has been observed in Europe where about 6% of all health care expenditure are spent on cancer treatment.[244][245] In addition to health care expenditure and financial toxicity, cancer causes indirect costs in the form of productivity losses due to sick days, permanent incapacity and disability as well as premature death during working age. Cancer causes also costs for informal care. Indirect costs and informal care costs are typically estimated to exceed or equal the health care costs of cancer.[246][245]
In the United States, cancer is included as a protected condition by the Equal Employment Opportunity Commission (EEOC), mainly due to the potential for cancer having discriminating effects on workers.[247] Discrimination in the workplace could occur if an employer holds a false belief that a person with cancer is not capable of doing a job properly, and may ask for more sick leave than other employees. Employers may also make hiring or firing decisions based on misconceptions about cancer disabilities, if present. The EEOC provides interview guidelines for employers, as well as lists of possible solutions for assessing and accommodating employees with cancer.[247]
Women are six times more likely to be separated or divorced soon after a diagnosis of cancer or multiple sclerosis than men. Doctors in neuro-oncology practices noticed that divorce occurred almost exclusively when the wife was the patient.[248][249]
Because cancer is a class of diseases,[250][251] it is unlikely that there will ever be a single "cure for cancer" any more than there will be a single treatment for all infectious diseases.[252] Angiogenesis inhibitors were once incorrectly thought to have potential as a "silver bullet" treatment applicable to many types of cancer.[253] Angiogenesis inhibitors and other cancer therapeutics are used in combination to reduce cancer morbidity and mortality.[254]
Experimental cancer treatments are studied in clinical trials to compare the proposed treatment to the best existing treatment. Treatments that succeeded in one cancer type can be tested against other types.[255] Diagnostic tests are under development to better target the right therapies to the right patients, based on their individual biology.[256]
Cancer research focuses on the following issues:
The improved understanding of molecular biology and cellular biology due to cancer research has led to new treatments for cancer since US President Richard Nixon declared the "War on Cancer" in 1971. Since then, the country has spent over $200 billion on cancer research, including resources from public and private sectors.[257] The cancer death rate (adjusting for size and age of the population) declined by five percent between 1950 and 2005.[258]
Competition for financial resources appears to have suppressed the creativity, cooperation, risk-taking and original thinking required to make fundamental discoveries, unduly favoring low-risk research into small incremental advancements over riskier, more innovative research. Other consequences of competition appear to be many studies with dramatic claims whose results cannot be replicated and perverse incentives that encourage grantee institutions to grow without making sufficient investments in their own faculty and facilities.[259][260][261][262]
Virotherapy, which uses convert viruses, is being studied.
In the wake of the COVID-19 pandemic, there has been a worry that cancer research and treatment are slowing down.[263][264]
Cancer affects approximately 1 in 1,000 pregnant women. The most common cancers found during pregnancy are the same as the most common cancers found in non-pregnant women during childbearing ages: breast cancer, cervical cancer, leukemia, lymphoma, melanoma, ovarian cancer and colorectal cancer.[265]
Diagnosing a new cancer in a pregnant woman is difficult, in part because any symptoms are commonly assumed to be a normal discomfort associated with pregnancy. As a result, cancer is typically discovered at a somewhat later stage than average. Some imaging procedures, such as MRIs (magnetic resonance imaging), CT scans, ultrasounds and mammograms with fetal shielding are considered safe during pregnancy; some others, such as PET scans, are not.[265]
Treatment is generally the same as for non-pregnant women. However, radiation and radioactive drugs are normally avoided during pregnancy, especially if the fetal dose might exceed 100 cGy. In some cases, some or all treatments are postponed until after birth if the cancer is diagnosed late in the pregnancy. Early deliveries are often used to advance the start of treatment. Surgery is generally safe, but pelvic surgeries during the first trimester may cause miscarriage. Some treatments, especially certain chemotherapy drugs given during the first trimester, increase the risk of birth defects and pregnancy loss (spontaneous abortions and stillbirths).[265]
Elective abortions are not required and, for the most common forms and stages of cancer, do not improve the mother's survival. In a few instances, such as advanced uterine cancer, the pregnancy cannot be continued and in others, the patient may end the pregnancy so that she can begin aggressive chemotherapy.[265]
Some treatments can interfere with the mother's ability to give birth vaginally or to breastfeed.[265] Cervical cancer may require birth by Caesarean section. Radiation to the breast reduces the ability of that breast to produce milk and increases the risk of mastitis. Also, when chemotherapy is given after birth, many of the drugs appear in breast milk, which could harm the baby.[265]
Veterinary oncology, concentrating mainly on cats and dogs, is a growing specialty in wealthy countries and the major forms of human treatment such as surgery and radiotherapy may be offered. The most common types of cancer differ, but the cancer burden seems at least as high in pets as in humans. Animals, typically rodents, are often used in cancer research and studies of natural cancers in larger animals may benefit research into human cancer.[266]
Across wild animals, there is still limited data on cancer. Nonetheless, a study published in 2022, explored cancer risk in (non-domesticated) zoo mammals, belonging to 191 species, 110,148 individual, demonstrated that cancer is a ubiquitous disease of mammals and it can emerge anywhere along the mammalian phylogeny.[267] This research also highlighted that cancer risk is not uniformly distributed along mammals. For instance, species in the order Carnivora are particularly prone to be affected by cancer (e.g. over 25% of clouded leopards, bat-eared foxes and red wolves die of cancer), while ungulates (especially even-toed ungulates) appear to face consistently low cancer risks.
In non-humans, a few types of transmissible cancer have also been described, wherein the cancer spreads between animals by transmission of the tumor cells themselves. This phenomenon is seen in dogs with Sticker's sarcoma (also known as canine transmissible venereal tumor), and in Tasmanian devils with devil facial tumour disease (DFTD).[268]

Sickle cell disease (SCD), one of the hemoglobinopathies, is a group of blood disorders typically inherited.[2] The most common type is known as sickle cell anaemia.[2] It results in an abnormality in the oxygen-carrying protein haemoglobin found in red blood cells.[2] This leads to a rigid, sickle-like shape under certain circumstances.[2] Problems in sickle cell disease typically begin around 5 to 6 months of age.[1] A number of health problems may develop, such as attacks of pain (known as a sickle cell crisis), anemia, swelling in the hands and feet, bacterial infections, and stroke.[1] Long-term pain may develop as people get older.[2] The average life expectancy in the developed world is 40 to 60 years.[2]
Sickle cell disease occurs when a person inherits two abnormal copies of the β-globin gene (HBB) that makes haemoglobin, one from each parent.[3] This gene occurs in chromosome 11.[9] Several subtypes exist, depending on the exact mutation in each haemoglobin gene.[2] An attack can be set off by temperature changes, stress, dehydration, and high altitude.[1] A person with a single abnormal copy does not usually have symptoms and is said to have sickle cell trait.[3] Such people are also referred to as carriers.[5] Diagnosis is by a blood test, and some countries test all babies at birth for the disease.[4] Diagnosis is also possible during pregnancy.[4]
The care of people with sickle cell disease may include infection prevention with vaccination and antibiotics, high fluid intake, folic acid supplementation, and pain medication.[5][6] Other measures may include blood transfusion and the medication hydroxycarbamide (hydroxyurea).[6] A small percentage of people can be cured by a transplant of bone marrow cells.[2]
As of 2015[update], about 4.4 million people have sickle cell disease, while an additional 43 million have sickle cell trait.[7][10] About 80% of sickle cell disease cases are believed to occur in Sub-Saharan Africa.[11] It also occurs to a lesser degree in parts of India, Southern Europe, West Asia, North Africa and among  people of African origin (sub-Saharan) living in other parts of the world.[12] In 2015, it resulted in about 114,800 deaths.[8] The condition was first described in the medical literature by American physician James B. Herrick in 1910.[13][14] In 1949, its genetic transmission was determined by E. A. Beet and J. V. Neel.[14] In 1954, the protective effect against malaria of sickle cell trait was described.[14]
Signs of sickle cell disease usually begin in early childhood. The severity of symptoms can vary from person to person.[15] Sickle cell disease may lead to various acute and chronic complications, several of which have a high mortality rate.[16]
The terms "sickle cell crisis" or "sickling crisis" may be used to describe several independent acute conditions occurring in patients with SCD, which results in anaemia and crises that could be of many types, including the vaso-occlusive crisis, aplastic crisis, splenic sequestration crisis, haemolytic crisis, and others. Most episodes of sickle cell crises last between five and seven days.[17] "Although infection, dehydration, and acidosis (all of which favor sickling) can act as triggers, in most instances, no predisposing cause is identified."[18]
The vaso-occlusive crisis is caused by sickle-shaped red blood cells that obstruct capillaries and restrict blood flow to an organ, resulting in ischaemia, pain, necrosis, and often organ damage. The frequency, severity, and duration of these crises vary considerably. Painful crises are treated with hydration, analgesics, and blood transfusion; pain management requires opioid drug administration at regular intervals until the crisis has settled. For milder crises, a subgroup of patients manages on nonsteroidal anti-inflammatory drugs  such as diclofenac or naproxen. For more severe crises, most patients require inpatient management for intravenous opioids; patient-controlled analgesia devices are commonly used in this setting. Vaso-occlusive crisis involving organs such as the penis[19] or lungs are considered an emergency and treated with red blood cell transfusions. Incentive spirometry, a technique to encourage deep breathing to minimise the development of atelectasis, is recommended.[20]
The spleen is frequently affected in sickle cell disease, as the sickle-shaped red blood cells cause narrowing of blood vessels and reduced function in clearing the defective cells.[21] It is usually infarcted before the end of childhood in individuals with sickle cell anaemia. This spleen damage increases the risk of infection from encapsulated organisms;[22][23] preventive antibiotics and vaccinations are recommended for those lacking proper spleen function.
Splenic sequestration crises are acute, painful enlargements of the spleen, caused by intrasplenic trapping of red cells and resulting in a precipitous fall in haemoglobin levels with the potential for hypovolemic shock. Sequestration crises are considered an emergency. If not treated, patients may die within 1–2 hours due to circulatory failure. Management is supportive, sometimes with blood transfusion. These crises are transient; they continue for 3–4 hours and may last for one day.[24]
Acute chest syndrome is defined by at least two of these signs or symptoms: chest pain, fever, pulmonary infiltrate or focal abnormality, respiratory symptoms, or hypoxemia.[20] It is the second-most common complication and it accounts for about 25% of deaths in patients with SCD. Most cases present with vaso-occlusive crises, and then develop acute chest syndrome.[25][26] Nevertheless, about 80% of people have vaso-occlusive crises during acute chest syndrome.[citation needed]
Aplastic crises are instances of an acute worsening of the patient's baseline anaemia, producing pale appearance, fast heart rate, and fatigue. This crisis is normally triggered by parvovirus B19, which directly affects production of red blood cells by invading the red cell precursors and multiplying in and destroying them.[27] Parvovirus infection almost completely prevents red blood cell production for two to three days. In normal individuals, this is of little consequence, but the shortened red cell life of SCD patients results in an abrupt, life-threatening situation. Reticulocyte counts drop dramatically during the disease (causing reticulocytopenia), and the rapid turnover of red cells leads to the drop in haemoglobin. This crisis takes 4  to 7 days to disappear. Most patients can be managed supportively; some need a blood transfusion.[28]
Haemolytic crises are acute accelerated drops in haemoglobin level. The red blood cells break down at a faster rate. This is particularly common in people with coexistent G6PD deficiency.[29] Another influence of hemolytic crises in sickle cell disease is oxidative stress on the erythrocytes, leukocytes, and platelets. When there is not enough red blood cell production in the bone marrow, the oxygen that the body receives, processes, and transports is unbalanced with the body's antioxidants. There is an imbalance in the oxygen reactive species in the cells, which leads to more production of red blood cells that are not properly oxygenated or formed. Oxidative stress may lead to anemia because of the imbalance of oxygen in the tissue.[30]
Management is supportive, sometimes with blood transfusions.[20]
One of the earliest clinical manifestations is dactylitis, presenting as early as six months of age, and may occur in children with sickle cell trait.[31] The crisis can last up to a month.[32] Given that pneumonia and sickling in the lung can both produce symptoms of acute chest syndrome, the patient is treated for both conditions.[33] It can be triggered by painful crisis, respiratory infection, bone-marrow embolisation, or possibly by atelectasis, opiate administration, or surgery.[34] Hematopoietic ulcers may also occur.[35]
Sickle cell anaemia can lead to various complications, including:
Normally, humans have haemoglobin A, which consists of two alpha and two beta chains, haemoglobin A2, which consists of two alpha and two delta chains, and haemoglobin F (HbF), consisting of two alpha and two gamma chains in their bodies. Of these three types, haemoglobin F dominates until about 6 weeks of age. Afterwards, haemoglobin A dominates throughout life.[53] In people diagnosed with sickle cell disease, at least one of the β-globin subunits in haemoglobin A is replaced with what is known as haemoglobin S. In sickle cell anaemia, a common form of sickle cell disease, haemoglobin S replaces both β-globin subunits in the haemoglobin.[15]
Sickle cell disease has an autosomal recessive pattern of inheritance from parents.[54] The types of haemoglobin a person makes in the red blood cells depend on what haemoglobin genes are inherited from her or his parents. If one parent has sickle cell anaemia and the other has sickle cell trait, then the child has a 50% chance of having sickle cell disease and a 50% chance of having sickle cell trait. When both parents have the sickle cell trait, a child has a 25% chance of sickle cell disease; a 25% chance of no sickle cell alleles, and a 50% chance of the heterozygous condition.[55]
Sickle cell gene mutation probably arose spontaneously in different geographic areas,[56] as suggested by restriction endonuclease analysis. These variants are known as Cameroon, Senegal, Benin, Bantu, and Saudi-Asian. Their clinical importance is because some are associated with higher HbF levels, e.g., Senegal and Saudi-Asian variants, and tend to have milder disease.[57]
The gene defect is a single nucleotide mutation (see single-nucleotide polymorphism – SNP) (GAG codon changing to GTG) of the β-globin gene, which results in glutamate (E/Glu) being substituted by valine (V/Val) at position 6 (E6V substitution).[58][note 1] Haemoglobin S with this mutation is referred to as HbS, as opposed to the normal adult HbA. This is normally a benign mutation, causing no apparent effects on the secondary, tertiary, or quaternary structures of haemoglobin in conditions of normal oxygen concentration. However, under low oxygen concentration, HbS polymerizes and forms fibrous precipitates because the deoxy form of haemoglobin exposes a hydrophobic patch on the protein between the E and F helices (Phe 85, Leu 88).[59]
In people heterozygous for HbS (carriers of sickling haemoglobin), the polymerisation problems are minor because the normal allele can produce half of the haemoglobin. In people homozygous for HbS, the presence of long-chain polymers of HbS distort the shape of the red blood cell from a smooth, doughnut-like shape to ragged and full of spikes, making it fragile and susceptible to breaking within capillaries. Carriers have symptoms only if they are deprived of oxygen (for example, while climbing a mountain) or while severely dehydrated.[citation needed]
The allele responsible for sickle cell anaemia can be found on the short arm of chromosome 11, more specifically 11p15.5. A person who receives the defective gene from both father and mother develops the disease; a person who receives one defective and one healthy allele remains healthy, but can pass on the disease and is known as a carrier or heterozygote. Heterozygotes are still able to contract malaria, but their symptoms are generally less severe.[60]
Due to the adaptive advantage of the heterozygote, the disease is still prevalent, especially among people with recent ancestry in malaria-stricken areas, such as Africa, the Mediterranean, India, and the Middle East.[61] Malaria was historically endemic to southern Europe, but it was declared eradicated in the mid-20th century, with the exception of rare sporadic cases.[62]
The malaria parasite has a complex lifecycle and spends part of it in red blood cells. In a carrier, the presence of the malaria parasite causes the red blood cells with defective haemoglobin to rupture prematurely, making the Plasmodium parasite unable to reproduce. Further, the polymerization of Hb affects the ability of the parasite to digest Hb in the first place. Therefore, in areas where malaria is a problem, people's chances of survival actually increase if they carry sickle cell traits (selection for the heterozygote).[citation needed]
In the United States, with no endemic malaria, the prevalence of sickle cell anaemia among people of African ancestry is lower (about 0.25%) than among people in West Africa (about 4.0%) and is falling. Without endemic malaria, the sickle cell mutation is purely disadvantageous and tends to decline in the affected population by natural selection, and now artificially through prenatal genetic screening. However, the African American community is descended from several African and non-African ethnic groups including American slaves. Thus, a degree of genetic dilution via crossbreeding with non-African people and high health-selective pressure through slavery (especially the slave trade and the frequently deadly Middle Passage) may be the most plausible explanations for the lower prevalence of sickle cell anaemia (and, possibly, other genetic diseases) among African Americans compared to West Africans. Another factor that limits the spread of sickle cell genes in North America is the relative absence of polygamy. In polygamous societies, affected males may father many children with multiple partners.[63]
The loss of red blood cell elasticity is central to the pathophysiology of sickle cell disease. Normal red blood cells are quite elastic and have a biconcave disc shape, which allows the cells to deform to pass through capillaries.[64] In sickle cell disease, low oxygen tension promotes red blood cell sickling and repeated episodes of sickling damage the cell membrane and decrease the cell's elasticity. These cells fail to return to normal shape when normal oxygen tension is restored. As a consequence, these rigid blood cells are unable to deform as they pass through narrow capillaries, leading to vessel occlusion and ischaemia.[citation needed]
The actual anaemia of the illness is caused by haemolysis, the destruction of the red cells, because of their shape. Although the bone marrow attempts to compensate by creating new red cells, it does not match the rate of destruction.[65] Healthy red blood cells typically function for 90–120 days, but sickled cells only last 10–20 days.[66]
In HbS, the complete blood count reveals haemoglobin levels in the range of 6–8 g/dl with a high reticulocyte count (as the bone marrow compensates for the destruction of sickled cells by producing more red blood cells). In other forms of sickle cell disease, Hb levels tend to be higher. A blood film may show features of hyposplenism (target cells and Howell-Jolly bodies).[67]
Sickling of the red blood cells, on a blood film, can be induced by the addition of sodium metabisulfite. The presence of sickle haemoglobin can also be demonstrated with the "sickle solubility test" (also called "sickledex").[68] A mixture of haemoglobin S (HbS) in a reducing solution (such as sodium dithionite) gives a turbid appearance, whereas normal Hb gives a clear solution.[69]
Abnormal haemoglobin forms can be detected on haemoglobin electrophoresis, a form of gel electrophoresis on which the various types of haemoglobin move at varying speeds. Sickle cell haemoglobin (HgbS) and haemoglobin C with sickling (HgbSC)—the two most common forms—can be identified from there. The diagnosis can be confirmed with high-performance liquid chromatography. Genetic testing is rarely performed, as other investigations are highly specific for HbS and HbC.[70]
An acute sickle cell crisis is often precipitated by infection. Therefore, a urinalysis to detect an occult urinary tract infection, and chest X-ray to look for occult pneumonia should be routinely performed.[71]
People who are known carriers of the disease or at risk of having a child with sickle cell anemia may undergo genetic counseling. Genetic counselors work with families to discuss the benefits, limitations, and logistics of genetic testing options as well as the potential impact of testing and test results on the individual.[72][73] During pregnancy, genetic testing can be done on either a blood sample from the fetus or a sample of amniotic fluid. During the first trimester of pregnancy, chorionic villus sampling (CVS) is also a technique used for SCD prenatal diagnosis.[74] Since taking a blood sample from a fetus has greater risks, the latter test is usually used. Neonatal screening sometimes referred to as newborn screening, provides not only a method of early detection for individuals with sickle cell disease but also allows for the identification of the groups of people who carry the sickle cell trait.[75] Genetic counselors can help individuals of colour and their families tackle the racial and ethnic disparities that exist in healthcare.[76]
In 2010, there was significant consideration and debate in the US surrounding comprehensive screening of athletes for SCD.[77][78][79][80] The American Society of Hematology concluded in a statement in 2012 that they do not support testing or disclosure of sickle cell trait status as a prerequisite for participation in athletic activities due to lack of scientific evidence, inconsistency with good medical practice, and inconsistency with public health ethics. They recommended universal interventions to reduce exertion-related injuries and deaths effective for all athletes irrespective of their sickle cell status.[81]
Treatment involves a number of measures. While it has been historically recommended that people with sickle cell disease avoid exercise, regular exercise may benefit people.[82] Dehydration should be avoided.[83] A diet high in calcium is recommended[84] but the effectiveness of vitamin D supplementation remains uncertain.[85] L-glutamine use was supported by the FDA starting at the age of five, as it decreases complications.[86]
From birth to five years of age, penicillin daily, due to the immature immune system that makes them more prone to early childhood illnesses, is recommended.[87] Dietary supplementation of folic acid had been previously recommended by the WHO.[5] A 2016 Cochrane review of its use found "the effect of supplementation on anaemia and any symptoms of anaemia remains unclear" due to a lack of medical evidence.[88]
The protective effect of sickle cell trait does not apply to people with sickle cell disease; in fact, they are more vulnerable to malaria, since the most common cause of painful crises in malarial countries is infection with malaria. People with sickle cell disease living in malarial countries should receive lifelong medication for prevention.[89]
Most people with sickle cell disease have intensely painful episodes called vaso-occlusive crises. However, the frequency, severity, and duration of these crises vary tremendously. Painful crises are treated symptomatically with pain medications; pain management requires opioid drug administration at regular intervals until the crisis has settled. For milder crises, a subgroup of patients manages on NSAIDs (such as diclofenac or naproxen). For more severe crises, most patients require inpatient management for intravenous opioids.[90]
Extra fluids, administered either orally or intravenously, are a routine part of treatment of vaso-occlusive crises but the evidence about the most effective route, amount and type of fluid replacement remains uncertain.[91]
Crizanlizumab, a monoclonal antibody target towards p-selectin was approved in 2019 in the United States to reduce the frequency of vaso-occlusive crisis in those 16 years and older.[92]
Transcranial Doppler ultrasound (TCD) can detect children with sickle cell that have a high risk for stroke.  The ultrasound test detects blood vessels partially obstructed by sickle cells by measuring the rate of blood into the brain, as blood flow velocity is inversely related to arterial diameter, and consequently, high blood-flow velocity is correlated with narrowing of the arteries.[93]  In 2002 the National Institute of Health (NIH) issued a statement recommending that children with sickle cell get the Transcranial Doppler ultrasound screen annually, and in 2014 a panel of experts convened by the NIH issued guidelines reiterating the same recommendation.  One review of medical records, by hematologist Dr. Julie Kanter at the University of Alabama at Birmingham, showed that on average only 48.4 per cent of children with sickle cell get the recommended ultrasound test.[94]
A 1994 NIH study showed that children at risk for strokes who received blood transfusions had an annual stroke rate of less than 1 per cent, whereas those children who did not receive blood transfusions had a 10 per cent stroke rate per year.  (Also see 1998 study in the New England Journal of Medicine.[93])  In addition to ultrasounds and blood transfusions, the inexpensive generic drug hydroxyurea can reduce the risk of irreversible organ and brain damage.  Guidelines from NIH published in 2014 state that all children and adolescents should take hydroxyurea, as should adults with serious complications or three or more pain crises in a year.[95]
Management is similar to vaso-occlusive crisis, with the addition of antibiotics (usually a quinolone or macrolide, since cell wall-deficient ["atypical"] bacteria are thought to contribute to the syndrome),[96] oxygen supplementation for hypoxia, and close observation. In the absence of high quality evidence regarding the effectiveness of antibiotics for acute chest syndrome in people with sickle cell disease, there is no standard antibiotic treatment as of 2019.[97] It is recommended that people with suspected acute chest syndrome should be admitted to the hospital with worsening A-a gradient an indication for ICU admission.[20]
Should the pulmonary infiltrate worsen or the oxygen requirements increase, simple blood transfusion or exchange transfusion is indicated. The latter involves the exchange of a significant portion of the person's red cell mass for normal red cells, which decreases the level of haemoglobin S in the patient's blood. However, there is currently uncertain evidence about the possible benefits or harms of blood transfusion for acute chest syndrome in people with sickle cell disease.[98]
Hydroxyurea, also known as hydroxycarbamide, probably reduces the frequency of painful episodes and the risk of life-threatening illness or death but there is currently insufficient evidence regarding the risk of adverse effects.[99] Hydroxyurea and phlebotomy combined may be more effective than transfusion and chelation combined in terms of pain, life-threatening illness and risk of death.[99]
It was the first approved drug for the treatment of sickle cell anaemia, and was shown to decrease the number and severity of attacks in 1995[100] and shown to possibly increase survival time in a study in 2003.[101] This is achieved, in part, by reactivating fetal haemoglobin production in place of the haemoglobin S that causes sickle cell anaemia. Hydroxyurea had previously been used as a chemotherapy agent, and some concern exists that long-term use may be harmful, but this risk is either absent or very small and  the benefits  likely  outweigh the risks.[16][102]
Voxelotor was approved in the United States in 2019 to increase hemoglobin in people with SS disease.[103]
Blood transfusions are often used in the management of sickle cell disease in acute cases and to prevent complications by decreasing the number of red blood cells (RBCs) that can sickle by adding normal red blood cells.[104] In children, preventive RBC transfusion therapy has been shown to reduce the risk of first stroke or silent stroke when transcranial Doppler ultrasonography shows abnormal cerebral blood flow.[6] In those who have sustained a prior stroke event, it also reduces the risk of recurrent stroke and additional silent strokes.[105][106]
Bone marrow transplants have proven effective in children; they are the only known cure for SCD.[107] However, bone marrow transplants are difficult to obtain because of the specific HLA typing necessary. Ideally, a close relative (allogeneic) would donate the bone marrow necessary for transplantation. Some gene therapies are under development that would alter the patient's own bone marrow stem cells ex vivo, which can then be transplanted back into the patient after chemotherapy eliminates the original unmodified cells.[108]
When treating avascular necrosis of the bone in people with sickle cell disease, the aim of treatment is to reduce or stop the pain and maintain joint mobility.[40] Current treatment options include resting the joint, physical therapy, pain-relief medicine, joint-replacement surgery, or bone grafting.[40] High quality, randomized, controlled trials are needed to assess the most effective treatment option and determine if a combination of physical therapy and surgery is more effective than physical therapy alone.[40]
Psychological therapies such as patient education, cognitive therapy, behavioural therapy, and psychodynamic psychotherapy, that aim to complement current medical treatments, require further research to determine their effectiveness.[21]
About 90% of people survive to age 20, and close to 50% survive beyond age 50.[109] In 2001, according to one study performed in Jamaica, the estimated mean survival for people was 53 years  for men and 58 years for women with homozygous SCD.[110] The specific life expectancy in much of the developing world is unknown.[111] In 1975 about 7.3% of people with SCD died before their 23rd birthday; while in 1989 2.6% of people with SCD died by the age of 20.[112]: 348 
The highest frequency of sickle cell disease is found in tropical regions, particularly sub-Saharan Africa, tribal regions of India, and the Middle East.[113] Migration of substantial populations from these high-prevalence areas to low-prevalence countries in Europe has dramatically increased in recent decades and in some European countries, sickle cell disease has now overtaken more familiar genetic conditions such as haemophilia and cystic fibrosis.[114] In 2015, it resulted in about 114,800 deaths.[8]
Sickle cell disease occurs more commonly among people whose ancestors lived in tropical and subtropical sub-Saharan regions where malaria is or was common. Where malaria is common, carrying a single sickle cell allele (trait) confers a heterozygote advantage; humans with one of the two alleles of sickle cell disease show less severe symptoms when infected with malaria.[115]
This condition is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents each carry one copy of the mutated gene, but they typically do not show signs and symptoms of the condition.[116]
Three-quarters of sickle cell cases occur in Africa. A recent WHO report estimated that around 2% of newborns in Nigeria were affected by sickle cell anaemia, giving a total of 150,000 affected children born every year in Nigeria alone. The carrier frequency ranges between 10 and 40% across equatorial Africa, decreasing to 1–2% on the North African coast and <1% in South Africa.[117]
Studies in Africa show a significant decrease in infant mortality rate, ages 2–16 months, because of the sickle cell trait. This happened in areas of  predominant malarial cases.[118]
Uganda has the fifth-highest sickle cell disease burden in Africa.[119] One study indicates that 20 000 babies per year are born with sickle cell disease with the sickle cell trait at 13·3% and with disease 0·7%.[120]
The number of people with the disease in the United States is about 100,000 (one in 3,300), mostly affecting Americans of sub-Saharan African descent.[121] In the United States, about one out of 365 African-American children and one in every 16,300 Hispanic-American children have sickle cell anaemia.[122] The life expectancy for men with SCD is approximately 42 years of age while women live approximately six years longer.[123]  An additional 2 million are carriers of the sickle cell trait.[124] Most infants with SCD born in the United States are identified by routine neonatal screening. As of 2016 all 50 states include screening for sickle cell disease as part of their newborn screen.[125] The newborn's blood is sampled through a heel-prick and is sent to a lab for testing. The baby must have been eating for a minimum of 24 hours before the heel-prick test can be done. Some states also require a second blood test to be done when the baby is two weeks old to ensure the results.[126] Sickle cell anemia is the most common genetic disorder among African Americans. Approximately 8% are carriers and 1 in 375 are born with the disease.[127] Patient advocates for sickle cell disease have complained that it gets less government and private research funding than similar rare diseases such as cystic fibrosis, with researcher Elliott Vichinsky saying this shows racial discrimination or the role of wealth in health care advocacy.[128]
As a result of population growth in African-Caribbean regions of overseas France and immigration from North and sub-Saharan Africa to mainland France, sickle cell disease has become a major health problem in France.[129] SCD has become the most common genetic disease in the country, with an overall birth prevalence of one in 2,415 in metropolitan France, ahead of phenylketonuria (one in 10,862), congenital hypothyroidism (one in 3,132), congenital adrenal hyperplasia (one in 19,008) and cystic fibrosis (one in 5,014) for the same reference period.[citation needed]
Since 2000, neonatal screening of SCD has been performed at the national level for all newborns defined as being "at-risk" for SCD based on ethnic origin (defined as those born to parents originating from sub-Saharan Africa, North Africa, the Mediterranean area (South Italy, Greece, and Turkey), the Arabic peninsula, the French overseas islands, and the Indian subcontinent).[130]
In the United Kingdom,  between 12,000 and 15,000 people are thought to  have sickle cell disease [131] with an estimated 250,000 carriers of the condition in England alone. As the number of carriers is only estimated, all newborn babies in the UK receive a routine blood test to screen for the condition.[132] Due to many adults in high-risk groups not knowing if they are carriers, pregnant women and both partners in a couple are offered screening so they can get counselling if they have the sickle cell trait.[133] In addition, blood donors from those in high-risk groups are also screened to confirm whether they are carriers and whether their blood filters properly.[134] Donors who are found to be carriers are then informed and their blood, while often used for those of the same ethnic group, is not used for those with sickle cell disease who require a blood transfusion.[135]
In Saudi Arabia, about 4.2% of the population carry the sickle cell trait and 0.26% have sickle cell disease. The highest prevalence is in the Eastern province, where approximately 17% of the population carry the gene and 1.2% have sickle cell disease.[136]
In 2005, Saudi Arabia introduced a mandatory premarital test including HB electrophoresis, which aimed to decrease the incidence of SCD and thalassemia.[137]
In Bahrain, a study published in 1998 that covered about 56,000 people in hospitals in Bahrain found that 2% of newborns have sickle cell disease, 18% of the surveyed people have the sickle cell trait, and 24% were carriers of the gene mutation causing the disease.[138] The country began screening of all pregnant women in 1992 and newborns started being tested if the mother was a carrier. In 2004, a law was passed requiring couples planning to marry to undergo free premarital counseling. These programs were accompanied by public education campaigns.[139]
Sickle cell disease is common in some ethnic groups of central India,[140] where the prevalence has ranged from 9.4 to 22.2% in endemic areas of Madhya Pradesh, Rajasthan, and Chhattisgarh.[141] It is also endemic among Tharu people of Nepal and India; however, they have a sevenfold lower rate of malaria despite living in a malaria infested zone.[142]
In Jamaica, 10% of the population carry the sickle cell gene, making it the most prevalent genetic disorder in the country.[143]
The first modern report of sickle cell disease may have been in 1846, where the autopsy of an executed runaway slave was discussed; the key finding was the absence of the spleen.[144][145] Reportedly, African slaves in the United States exhibited resistance to malaria, but were prone to leg ulcers.[145] The abnormal characteristics of the red blood cells, which later lent their name to the condition, was first described by Ernest E. Irons (1877–1959), intern to Chicago cardiologist and professor of medicine James B. Herrick (1861–1954), in 1910. Irons saw "peculiar elongated and sickle-shaped" cells in the blood of a man named Walter Clement Noel, a 20-year-old first-year dental student from Grenada. Noel had been admitted to the Chicago Presbyterian Hospital in December 1904 with anaemia.[13][146] Noel was readmitted several times over the next three years for "muscular rheumatism" and "bilious attacks" but completed his studies and returned to the capital of Grenada (St. George's) to practice dentistry. He died of pneumonia in 1916 and is buried in the Catholic cemetery at Sauteurs in the north of Grenada.[13][14] Shortly after the report by Herrick, another case appeared in the Virginia Medical Semi-Monthly with the same title, "Peculiar Elongated and Sickle-Shaped Red Blood Corpuscles in a Case of Severe Anemia."[147] This article is based on a patient admitted to the University of Virginia Hospital on 15 November 1910.[148] In the later description by Verne Mason in 1922, the name "sickle cell anemia" is first used.[14][149] Childhood problems related to sickle cells disease were not reported until the 1930s, despite the fact that this cannot have been uncommon in African-American populations.[145]
Memphis physician Lemuel Diggs, a prolific researcher into sickle cell disease, first introduced the distinction between sickle cell disease and trait in 1933, although until 1949, the genetic characteristics had not been elucidated by James V. Neel and E.A. Beet.[14] 1949 was the year when Linus Pauling described the unusual chemical behaviour of haemoglobin S, and attributed this to an abnormality in the molecule itself.[14][150] The molecular change in HbS was described in 1956 by Vernon Ingram.[151] The late 1940s and early 1950s saw further understanding in the link between malaria and sickle cell disease. In 1954, the introduction of haemoglobin electrophoresis allowed the discovery of particular subtypes, such as HbSC disease.[14]
Large-scale natural history studies and further intervention studies were introduced in the 1970s and 1980s, leading to widespread use of prophylaxis against pneumococcal infections amongst other interventions. Bill Cosby's Emmy-winning 1972 TV movie, To All My Friends on Shore, depicted the story of the parents of a child with sickle cell disease.[152] The 1990s had the development of hydroxycarbamide, and reports of cure through bone marrow transplantation appeared in 2007.[14]
Some old texts refer to it as drepanocytosis.[153]
Sickle cell disease is frequently contested as a disability. [154] Effective 15 September 2017, the U.S. Social Security Administration issued a Policy Interpretation Ruling providing background information on sickle cell disease and a description of how Social Security evaluates the disease during its adjudication process for disability claims.[155][156]
In the U.S., there are stigmas surrounding SCD that discourage people with SCD from receiving necessary care. These stigmas mainly affect people of African American and Latin American ancestries, according to the National Heart, Lung, and Blood Institute.[157] People with SCD experience the impact of stigmas of the disease on multiple aspects of life including social and psychological well-being. Studies have shown that those with SCD frequently feel as though they must keep their diagnosis a secret to avoid discrimination in the workplace and also among peers in relationships.[158] In the 1960s, the US government supported initiatives for workplace screening for genetic diseases in an attempt to be protective towards people with SCD. By having this screening, it was intended that employees would not be placed in environments that could potentially be harmful and trigger SCD.[159]
Uganda has the 5th highest sickle cell disease (SCD) burden in the world.[160] In Uganda, social stigma exists for those with sickle cell disease because of the lack of general knowledge of the disease. The general gap in knowledge surrounding sickle cell disease is noted among adolescents and young adults due to the culturally sanctioned secrecy about the disease.[160] While most people have heard generally about the disease, a large portion of the population is relatively misinformed about how SCD is diagnosed or inherited. Those who are informed about the disease learned about it from family or friends and not from health professionals. Failure to provide the public with information about sickle cell disease results in a population with a poor understanding of the causes of the disease, symptoms, and prevention techniques.[161] The differences, physically and socially, that arise in those with sickle cell disease, such as jaundice, stunted physical growth, and delayed sexual maturity, can also lead them to become targets of bullying, rejection, and stigma.[160]
The data compiled on sickle cell disease in Uganda has not been updated since the early 1970s. The deficiency of data is due to a lack of government research funds, even though Ugandans die daily from SCD.[162] Data shows that the trait frequency of sickle cell disease is 20% of the population in Uganda.[162] This means that 66 million people are at risk of having a child who has sickle cell disease.[162] It is also estimated that about 25,000 Ugandans are born each year with SCD and 80% of those people don't live past five years old.[162] SCD also contributes 25% to the child mortality rate in Uganda.[162] The Bamba people of Uganda, located in the southwest of the country, carry 45% of the gene which is the highest trait frequency recorded in the world.[162] The Sickle Cell Clinic in Mulago is only one sickle cell disease clinic in the country and on average sees 200 patients a day.[162]
The stigma around the disease is particularly bad in regions of the country that are not as affected. For example, Eastern Ugandans tend to be more knowledgeable of the disease than Western Ugandans, who are more likely to believe that sickle cell disease resulted as a punishment from God or witchcraft.[163] Other misconceptions about SCD include the belief that it is caused by environmental factors but, in reality, SCD is a genetic disease.[164] There have been efforts throughout Uganda to address the social misconceptions about the disease. In 2013, the Uganda Sickle Cell Rescue Foundation was established to spread awareness of sickle cell disease and combat the social stigma attached to the disease.[165] In addition to this organization's efforts, there is a need for the inclusion of sickle cell disease education in preexisting community health education programs in order to reduce the stigmatization of sickle cell disease in Uganda.[161]
The deeply rooted stigma of SCD from society causes families to often hide their family members' sick status for fear of being labeled, cursed, or left out of social events.[166] Sometimes in Uganda, when it is confirmed that a family member has sickle cell disease, intimate relationships with all members of the family are avoided.[166] The stigmatization and social isolation people with sickle cell disease tend to experience is often the consequence of popular misconceptions that people with SCD should not socialize with those free from the disease. This mentality robs people with SCD of the right to freely participate in community activities like everyone else[160] SCD-related stigma and social isolation in schools, especially, can make a life for young people living with sickle cell disease extremely difficult.[160]  For school-aged children living with SCD, the stigma they face can lead to peer rejection.[160]  Peer rejection involves the exclusion from social groups or gatherings. It often leads the excluded individual to experience emotional distress and may result in their academic underperformance, avoidance of school, and occupational failure later in life.[160]  This social isolation is also likely to negatively impact people with SCD's self-esteem and overall quality of life.[160]
Mothers of children with sickle cell disease tend to receive disproportionate amounts of stigma from their peers and family members. These women will often be blamed for their child's diagnosis of SCD, especially if SCD is not present in earlier generations, due to the suspicion that the child's poor health may have been caused by the mother's failure to implement preventative health measures or promote a healthy environment for her child to thrive.[164] The reliance on theories related to environmental factors to place blame on the mother reflects many Ugandan's poor knowledge of how the disease is acquired as it is determined by genetics, not environment.[164] Mothers of children with sickle cell disease are also often left with very limited resources to safeguard their futures against the stigma of having SCD.[164] This lack of access to resources results from their subordinating roles within familial structures as well as the class disparities that hinder many mothers' ability to satisfy additional childcare costs and responsibilities.[164]
Women living with SCD who become pregnant often face extreme discrimination and discouragement in Uganda. These women are frequently branded by their peers as irresponsible for having a baby while living with sickle cell disease or even engaging in sex while living with SCD.[citation needed] The criticism and judgement these women receive, not only from healthcare professionals but also from their families, often leaves them feeling alone, depressed, anxious, ashamed, and with very little social support.[citation needed] Most pregnant women with SCD also go on to be single mothers as it is common for them to be left by their male partners who claim they were unaware of their partner's SCD status.[citation needed] Not only does the abandonment experienced by these women cause emotional distress for them, but this low level of parental support can be linked to depressive symptoms and overall lower quality of life for the child once they are born.[167]
In 2021 many patients were found to be afraid to visit hospitals so purchasing pain relief to treat themselves outside the NHS, such was the level of ignorance among staff. They were often waiting a long time for pain relief, and sometimes suspected of "drugs-seeking" behaviour.  Delays to treatment, failure to inform the hospital haematology team and poor pain management had caused deaths.  Specialist haematology staff prefer to work in bigger, teaching hospitals, leading to shortages of expertise elsewhere.[168] In 2021 the NHS initiated its first new treatment in 20 years for Sickle Cell. This involved the use of Crizanlizumab, a drug given via transfusion drips, which reduces the number of visits to A and E by sufferers. The treatment can be accessed, via consultants, at any of ten new hubs set up around the country.[169] In the same year, however, an All-Party Parliamentary Group produced a report on Sickle Cell and Thalassaemia entitled 'No-one is listening'.[170] Partly in response to this, on 19 June 2022, World Sickle Cell Day, the NHS launched a campaign called " Can you tell it's sickle cell?". The campaign had twin aims. One was to increase awareness of the key signs and symptoms of the blood disorder so that people are as alert to signs of a sickle cell crisis as they are to an imminent heart attack or stroke. The second aim was to set up a new training programme to help paramedics, Accident and Emergency staff, carers and the general public to care effectively for sufferers in crisis.[171]
While umbilical cord blood transplant can potentially cure the condition, a suitable donor is available in only 10% of people.[172] About 7% of people also die as a result of the procedure and graft versus host disease may occur.[172]
Diseases such as sickle cell disease for which a person's normal phenotype or cell function may be restored in cells that have the disease by a normal copy of the gene that is mutated, may be a good candidate for gene therapy treatment. The risks and benefits related to gene therapy for sickle cell disease are not known.[173]
In 2001,  sickle cell disease reportedly had been successfully treated in mice using gene therapy.[174][175] The researchers used a viral vector to make the mice—which have essentially the same defect that causes human sickle cell disease—express production of fetal haemoglobin (HbF), which an individual normally ceases to produce shortly after birth. In humans, using hydroxyurea to stimulate the production of HbF has been known to temporarily alleviate sickle cell disease symptoms. The researchers demonstrated that this gene therapy method is a more permanent way to increase therapeutic HbF production.[176]
Phase 1 clinical trials of gene therapy for sickle cell disease in humans were started in 2014. The clinical trials will assess the safety of lentiviral vector-modified bone marrow for adults with severe sickle cell disease.[177][178] As of 2020, however, no randomized controlled trials have been reported.[173] A case report for the first person treated was published in March 2017, with a few more people being treated since then.[179][180]
Gene editing platforms like CRISPR/Cas9 have been used to correct the disease-causing mutation in hematopoietic stem cells taken from a person with the condition.[181] In July 2019 the gene-editing tool CRISPR was used to edit bone marrow cells from a person with SCD to boost fetal haemoglobin by inhibiting the BCL11A gene.[182][183] A number of researchers have considered the ethical implications of SCD being one of the first potential applications of CRISPR technology, given the historical abuses  and neglect of the African American community by the medical field.[184]
In 2017 twelve clinical trials were focusing on gene therapy to treat sickle cell anemia. Of those 12 trials, four of them replaced the mutated HBB gene with a healthy one. Three trials used Mozobil, a medication used to treat types of cancer, to determine whether the increase of stem cells can be used for gene therapy. One trial focused on analyzing bone marrow samples from patients with sickle cell anemia. Another trial experimented with using umbilical cord blood from babies both with and without sickle cell anemia to develop gene therapy.[185]
In November 2023, the British Medical Journal reported that a gene treatment using the CRISPR gene editing tool had been approved by UK regulators for the treatment of  sickle cell disease and also for the blood disorder β thalassaemia.[186]
There is no strong medical evidence to determine the risks and potential benefits related to treating people with sickle cell disease with hematopoietic stem cell transplantations.[187]

Colorectal cancer (CRC), also known as bowel cancer, colon cancer, or rectal cancer, is the development of cancer from the colon or rectum (parts of the large intestine).[5] Signs and symptoms may include blood in the stool, a change in bowel movements, weight loss, and fatigue.[9] Most colorectal cancers are due to old age and lifestyle factors, with only a small number of cases due to underlying genetic disorders.[2][3] Risk factors include diet, obesity, smoking, and lack of physical activity.[2] Dietary factors that increase the risk include red meat, processed meat, and alcohol.[2][4] Another risk factor is inflammatory bowel disease, which includes Crohn's disease and ulcerative colitis.[2] Some of the inherited genetic disorders that can cause colorectal cancer include familial adenomatous polyposis and hereditary non-polyposis colon cancer; however, these represent less than 5% of cases.[2][3] It typically starts as a benign tumor, often in the form of a polyp, which over time becomes cancerous.[2]
Colorectal cancer may be diagnosed by obtaining a sample of the colon during a sigmoidoscopy or colonoscopy.[1] This is then followed by medical imaging to determine whether the disease has spread.[5] Screening is effective for preventing and decreasing deaths from colorectal cancer.[10] Screening, by one of a number of methods, is recommended starting from the age of 45 to 75. It was recommended starting at age 50 but it was changed to 45 due to increasing amount of colon cancers.[10][11] During colonoscopy, small polyps may be removed if found.[2] If a large polyp or tumor is found, a biopsy may be performed to check if it is cancerous. Aspirin and other non-steroidal anti-inflammatory drugs decrease the risk of pain during polyp excision.[2][12] Their general use is not recommended for this purpose, however, due to side effects.[13]
Treatments used for colorectal cancer may include some combination of surgery, radiation therapy, chemotherapy, and targeted therapy.[5] Cancers that are confined within the wall of the colon may be curable with surgery, while cancer that has spread widely is usually not curable, with management being directed towards improving quality of life and symptoms.[5] The five-year survival rate in the United States was around 65% in 2014.[6] The individual likelihood of survival depends on how advanced the cancer is, whether or not all the cancer can be removed with surgery, and the person's overall health.[1] Globally, colorectal cancer is the third most common type of cancer, making up about 10% of all cases.[14] In 2018, there were 1.09 million new cases and 551,000 deaths from the disease.[8] It is more common in developed countries, where more than 65% of cases are found.[2] It is less common in women than men.[2]
The signs and symptoms of colorectal cancer depend on the location of the tumor in the bowel, and whether it has spread elsewhere in the body (metastasis). The classic warning signs include: worsening constipation, blood in the stool, decrease in stool caliber (thickness), loss of appetite, loss of weight, and nausea or vomiting in someone over 50 years old.[15] Around 50% of people who have colorectal cancer do not report any symptoms.[16]
Rectal bleeding or anemia are high-risk symptoms in people over the age of 50.[17] Weight loss and changes in a person's bowel habit are typically only concerning if they are associated with rectal bleeding.[17][18]
75–95% of colorectal cancer cases occur in people with little or no genetic risk.[19][20] Risk factors include older age, male sex,[20] high intake of fat, sugar, alcohol, red meat, processed meats, obesity, smoking, and a lack of physical exercise.[19][21] The Rectal Cancer Survival Calculator developed by the MD Anderson Cancer Center additionally considers race to be a risk factor; however, there are equity issues concerning whether this might lead to inequity in clinical decision making.[22][23] Approximately 10% of cases are linked to insufficient activity.[24] The risk from alcohol appears to increase at greater than one drink per day.[25] Drinking five glasses of water a day is linked to a decrease in the risk of colorectal cancer and adenomatous polyps.[26] Streptococcus gallolyticus is associated with colorectal cancer.[27] Some strains of Streptococcus bovis/Streptococcus equinus complex are consumed by millions of people daily and thus may be safe.[28] 25 to 80% of people with Streptococcus bovis/gallolyticus bacteremia have concomitant colorectal tumors.[29] Seroprevalence of Streptococcus bovis/gallolyticus is considered as a candidate practical marker for the early prediction of an underlying bowel lesion at high risk population.[29] It has been suggested that the presence of antibodies to Streptococcus bovis/gallolyticus antigens or the antigens themselves in the bloodstream may act as markers for the carcinogenesis in the colon.[29]
Pathogenic Escherichia coli may increase the risk of colorectal cancer by producing the genotoxic metabolite, colibactin.[30]
People with inflammatory bowel disease (ulcerative colitis and Crohn's disease) are at increased risk of colon cancer.[31][32] The risk increases the longer a person has the disease, and the worse the severity of inflammation.[33] In these high risk groups, both prevention with aspirin and regular colonoscopies are recommended.[34] Endoscopic surveillance in this high-risk population may reduce the development of colorectal cancer through early diagnosis and may also reduce the chances of dying from colon cancer.[34] People with inflammatory bowel disease account for less than 2% of colon cancer cases yearly.[33] In those with Crohn's disease, 2% get colorectal cancer after 10 years, 8% after 20 years, and 18% after 30 years.[33] In people who have ulcerative colitis, approximately 16% develop either a cancer precursor or cancer of the colon over 30 years.[33]
Those with a family history in two or more first-degree relatives (such as a parent or sibling) have a two to threefold greater risk of disease, and this group accounts for about 20% of all cases. A number of genetic syndromes are also associated with higher rates of colorectal cancer. The most common of these is hereditary nonpolyposis colorectal cancer (HNPCC, or Lynch syndrome) which is present in about 3% of people with colorectal cancer.[20] Other syndromes that are strongly associated with colorectal cancer include Gardner syndrome and familial adenomatous polyposis (FAP).[35] For people with these syndromes, cancer almost always occurs and makes up 1% of the cancer cases.[36] A total proctocolectomy may be recommended for people with FAP as a preventive measure due to the high risk of malignancy. Colectomy, removal of the colon, may not suffice as a preventive measure because of the high risk of rectal cancer if the rectum remains.[37] The most common polyposis syndrome affecting the colon is serrated polyposis syndrome,[38] which is associated with a 25-40% risk of CRC.[39]
Mutations in the pair of genes (POLE and POLD1) have been associated with familial colon cancer.[40]
Most deaths due to colon cancer are associated with metastatic disease. A gene that appears to contribute to the potential for metastatic disease, metastasis associated in colon cancer 1 (MACC1), has been isolated.[41] It is a transcriptional factor that influences the expression of hepatocyte growth factor. This gene is associated with the proliferation, invasion, and scattering of colon cancer cells in cell culture, and tumor growth and metastasis in mice. MACC1 may be a potential target for cancer intervention, but this possibility needs to be confirmed with clinical studies.[42]
Epigenetic factors, such as abnormal DNA methylation of tumor suppressor promoters, play a role in the development of colorectal cancer.[43]
Ashkenazi Jews have a 6% higher risk rate of getting adenomas and then colon cancer due to mutations in the APC gene being more common.[44]
Colorectal cancer is a disease originating from the epithelial cells lining the colon or rectum of the gastrointestinal tract, most frequently as a result of genetic mutations in the Wnt signaling pathway that increases signaling activity.[45] The Wnt signaling pathway normally plays an important role for normal function of these cells including maintaining this lining. Mutations can be inherited or acquired, and most probably occur in the intestinal crypt stem cell.[46][47][48] The most commonly mutated gene in all colorectal cancer is the APC gene, which produces the APC protein.[45] The APC protein prevents the accumulation of β-catenin protein. Without APC, β-catenin accumulates to high levels and translocates (moves) into the nucleus, binds to DNA, and activates the transcription of proto-oncogenes. These genes are normally important for stem cell renewal and differentiation, but when inappropriately expressed at high levels, they can cause cancer.[45] While APC is mutated in most colon cancers, some cancers have increased β-catenin because of mutations in β-catenin (CTNNB1) that block its own breakdown, or have mutations in other genes with function similar to APC such as AXIN1, AXIN2, TCF7L2, or NKD1.[49]
Beyond the defects in the Wnt signaling pathway, other mutations must occur for the cell to become cancerous. The p53 protein, produced by the TP53 gene, normally monitors cell division and induces their programmed death if they have Wnt pathway defects. Eventually, a cell line acquires a mutation in the TP53 gene and transforms the tissue from a benign epithelial tumor into an invasive epithelial cell cancer. Sometimes the gene encoding p53 is not mutated, but another protective protein named BAX is mutated instead.[49]
Other proteins responsible for programmed cell death that are commonly deactivated in colorectal cancers are TGF-β and DCC (Deleted in Colorectal Cancer). TGF-β has a deactivating mutation in at least half of colorectal cancers. Sometimes TGF-β is not deactivated, but a downstream protein named SMAD is deactivated.[49] DCC commonly has a deleted segment of a chromosome in colorectal cancer.[50]
Approximately 70% of all human genes are expressed in colorectal cancer, with just over 1% of having increased expression in colorectal cancer compared to other forms of cancer.[51] Some genes are oncogenes: they are overexpressed in colorectal cancer. For example, genes encoding the proteins KRAS, RAF, and PI3K, which normally stimulate the cell to divide in response to growth factors, can acquire mutations that result in over-activation of cell proliferation. The chronological order of mutations is sometimes important. If a previous APC mutation occurred, a primary KRAS mutation often progresses to cancer rather than a self-limiting hyperplastic or borderline lesion.[52] PTEN, a tumor suppressor, normally inhibits PI3K, but can sometimes become mutated and deactivated.[49]
Comprehensive, genome-scale analysis has revealed that colorectal carcinomas can be categorized into hypermutated and non-hypermutated tumor types.[53] In addition to the oncogenic and inactivating mutations described for the genes above, non-hypermutated samples also contain mutated CTNNB1, FAM123B, SOX9, ATM, and ARID1A. Progressing through a distinct set of genetic events, hypermutated tumors display mutated forms of ACVR2A, TGFBR2, MSH3, MSH6, SLC9A9, TCF7L2, and BRAF. The common theme among these genes, across both tumor types, is their involvement in Wnt and TGF-β signaling pathways, which results in increased activity of MYC, a central player in colorectal cancer.[53]
Mismatch repair (MMR) deficient tumours are characterized by a relatively high amount of poly-nucleotide tandem repeats.[54] This is caused by a deficiency in MMR proteins – which are typically caused by epigenetic silencing and or inherited mutations (e.g., Lynch syndrome).[55] 15 to 18 percent of colorectal cancer tumours have MMR deficiencies, with 3 percent developing due to Lynch syndrome.[56] The role of the mismatch repair system is to protect the integrity of the genetic material within cells (i.e., error detecting and correcting).[55] Consequently, a deficiency in MMR proteins may lead to an inability to detect and repair genetic damage, allowing for further cancer-causing mutations to occur and colorectal cancer to progress.[55]
The polyp to cancer progression sequence is the classical model of colorectal cancer pathogenesis.[57] In this adenoma-carcinoma sequence, [58] normal epithelial cells proress to dysplastic cells such as adenomas, and then to carcinoma, by a process of progressive genetic mutation.[59] Central to the polyp to CRC sequence are gene mutations, epigenetic alterations, and local inflammatory changes.[57] The polyp to CRC sequence can be used as an underlying framework to illustrate how specific molecular changes lead to various cancer subtypes.[57]
The term "field cancerization" was first used in 1953 to describe an area or "field" of epithelium that has been preconditioned (by what were largely unknown processes at the time) to predispose it towards development of cancer.[60] Since then, the terms "field cancerization", "field carcinogenesis", "field defect", and "field effect" have been used to describe pre-malignant or pre-neoplastic tissue in which new cancers are likely to arise.[61]
Field defects are important in progression to colon cancer.[62][63]
However, as pointed out by Rubin, "The vast majority of studies in cancer research has been done on well-defined tumors in vivo, or on discrete neoplastic foci in vitro. Yet there is evidence that more than 80% of the somatic mutations found in mutator phenotype human colorectal tumors occur before the onset of terminal clonal expansion."[64][65] Similarly, Vogelstein et al.[66] pointed out that more than half of somatic mutations identified in tumors occurred in a pre-neoplastic phase (in a field defect), during growth of apparently normal cells. Likewise, epigenetic alterations present in tumors may have occurred in pre-neoplastic field defects.[67]
An expanded view of field effect has been termed "etiologic field effect", which encompasses not only molecular and pathologic changes in pre-neoplastic cells but also influences of exogenous environmental factors and molecular changes in the local microenvironment on neoplastic evolution from tumor initiation to death.[68]
Epigenetic alterations are much more frequent in colon cancer than genetic (mutational) alterations. As described by Vogelstein et al.,[66] an average cancer of the colon has only 1 or 2 oncogene mutations and 1 to 5 tumor suppressor mutations (together designated "driver mutations"), with about 60 further "passenger" mutations. The oncogenes and tumor suppressor genes are well studied and are described above under Pathogenesis.[69][70]
In addition to epigenetic alteration of expression of miRNAs, other common types of epigenetic alterations in cancers that change gene expression levels include direct hypermethylation or hypomethylation of CpG islands of protein-encoding genes and alterations in histones and chromosomal architecture that influence gene expression.[71] As an example, 147 hypermethylations and 27 hypomethylations of protein coding genes were frequently associated with colorectal cancers. Of the hypermethylated genes, 10 were hypermethylated in 100% of colon cancers, and many others were hypermethylated in more than 50% of colon cancers.[72] In addition, 11 hypermethylations and 96 hypomethylations of miRNAs were also associated with colorectal cancers.[72] Abnormal (aberrant) methylation occurs as a normal consequence of normal aging and the risk of colorectal cancer increases as a person gets older.[73] The source and trigger of this age-related methylation is unknown.[73][74] Approximately half of the genes that show age-related methylation changes are the same genes that have been identified to be involved in the development of colorectal cancer.[73] These findings may suggest a reason for age being associated with the increased risk of developing colorectal cancer.[73]
Epigenetic reductions of DNA repair enzyme expression may likely lead to the genomic and epigenomic instability characteristic of cancer.[75][76][67] As summarized in the articles Carcinogenesis and Neoplasm, for sporadic cancers in general, a deficiency in DNA repair is occasionally due to a mutation in a DNA repair gene, but is much more frequently due to epigenetic alterations that reduce or silence expression of DNA repair genes.[77]
Epigenetic alterations involved in the development of colorectal cancer may affect a person's response to chemotherapy.[78]
Consensus molecular subtypes (CMS) classification of colorectal cancer was first introduced in 2015. CMS classification so far has been considered the most robust classification system available for CRC that has a clear biological interpretability and the basis for future clinical stratification and subtype-based targeted interventions.[79]
A novel Epigenome-based Classification (EpiC) of colorectal cancer was proposed in 2021 introducing 4 enhancer subtypes in people with CRC. Chromatin states using 6 histone marks are characterized to identify EpiC subtypes. A combinatorial therapeutic approach based on the previously introduced consensus molecular subtypes (CMSs) and EpiCs could significantly enhance current treatment strategies.[80]
Colorectal cancer diagnosis is performed by sampling of areas of the colon suspicious for possible tumor development, typically during colonoscopy or sigmoidoscopy, depending on the location of the lesion.[20] It is confirmed by microscopical examination of a tissue sample.[citation needed]
A colorectal cancer is sometimes initially discovered on CT scan.[81]
Presence of metastases is determined by a CT scan of the chest, abdomen and pelvis.[20] Other potential imaging tests such as PET and MRI may be used in certain cases.[20] MRI is particularly useful to determine local stage of the tumor and to plan the optimal surgical approach.[81]
MRI is also performed after completion of neoadjuvant chemoradiotherapy to identify patients who achieve complete response. Patients with complete response on both MRI and endoscopy may not require surgical resection and can avoid unnecessary surgical morbidity and complications.[82] Patients selected for non-surgical treatment of rectal cancer should have periodic MRI scans, receive physical examinations, and undergo endoscopy procedures to detect any tumor re-growth which can occur in a minority of these patients. When local recurrence occurs, periodic follow up can detect it when it is still small and curable with salvage surgery. In addition, MRI tumor regression grades can be assigned after chemoradiotherapy which correlate with patients' long-term survival outcomes.[83]
The histopathologic characteristics of the tumor are reported from the analysis of tissue taken from a biopsy or surgery. A pathology report contains a description of the microscopical characteristics of the tumor tissue, including both tumor cells and how the tumor invades into healthy tissues and finally if the tumor appears to be completely removed. The most common form of colon cancer is adenocarcinoma, constituting between 95%[85] and 98%[86] of all cases of colorectal cancer. Other, rarer types include lymphoma, adenosquamous and squamous cell carcinoma. Some subtypes are more aggressive.[87] Immunohistochemistry may be used in uncertain cases.[88]
Staging of the cancer is based on both radiological and pathological findings. As with most other forms of cancer, tumor staging is based on the TNM system which considers how much the initial tumor has spread and the presence of metastases in lymph nodes and more distant organs.[20] The AJCC 8th edition was published in 2018.[89]
It has been estimated that about half of colorectal cancer cases are due to lifestyle factors, and about a quarter of all cases are preventable.[90] Increasing surveillance, engaging in physical activity, consuming a diet high in fiber, and reducing smoking and alcohol consumption decrease the risk.[91][92]
Lifestyle risk factors with strong evidence include lack of exercise, cigarette smoking, alcohol, and obesity.[93][94][95] The risk of colon cancer can be reduced by maintaining a normal body weight through a combination of sufficient exercise and eating a healthy diet.[96]
Current research consistently links eating more red meat and processed meat to a higher risk of the disease.[97] Starting in the 1970s, dietary recommendations to prevent colorectal cancer often included increasing the consumption of whole grains, fruits and vegetables, and reducing the intake of red meat and processed meats. This was based on animal studies and retrospective observational studies. However, large scale prospective studies have failed to demonstrate a significant protective effect, and due to the multiple causes of cancer and the complexity of studying correlations between diet and health, it is uncertain whether any specific dietary interventions will have significant protective effects.[98]: 432–433 [99]: 125–126  In 2018 the National Cancer Institute stated that "There is no reliable evidence that a diet started in adulthood that is low in fat and meat and high in fiber, fruits, and vegetables reduces the risk of CRC by a clinically important degree."[93][100]
According to the World Cancer Research Fund, consuming alcohol drinks and consuming processed meat both increase the risk of colorectal cancer.[101]
The 2014 World Health Organization cancer report noted that it has been hypothesized that dietary fiber might help prevent colorectal cancer, but that most studies at the time had not yet studied the correlation.[99] A 2019 review, however, found evidence of benefit from dietary fiber and whole grains.[102] The World Cancer Research Fund listed the benefit of fiber for prevention of colorectal cancer as "probable" as of 2017.[103] A 2022 umbrella review says there is "convincing evidence" for that association.[104]
Higher physical activity is recommended.[21][105] Physical exercise is associated with a modest reduction in colon but not rectal cancer risk.[106][107] High levels of physical activity reduce the risk of colon cancer by about 21%.[108] Sitting regularly for prolonged periods is associated with higher mortality from colon cancer. Regular exercise does not negate the risk but does lower it.[109]
Aspirin and celecoxib appear to decrease the risk of colorectal cancer in those at high risk.[110][111] Aspirin is recommended in those who are 50 to 60 years old, do not have an increased risk of bleeding, and are at risk for cardiovascular disease to prevent colorectal cancer.[112] It is not recommended in those at average risk.[113]
There is tentative evidence for calcium supplementation, but it is not sufficient to make a recommendation.[114] Vitamin D intake and blood levels are associated with a lower risk of colon cancer.[115][116]
As more than 80% of colorectal cancers arise from adenomatous polyps, screening for this cancer is effective for both early detection and for prevention.[20][117] Diagnosis of cases of colorectal cancer through screening tends to occur 2–3 years before diagnosis of cases with symptoms.[20] Any polyps that are detected can be removed, usually by colonoscopy or sigmoidoscopy, and thus prevent them from turning into cancer. Screening has the potential to reduce colorectal cancer deaths by 60%.[118]
The three main screening tests are colonoscopy, fecal occult blood testing, and flexible sigmoidoscopy. Of the three, only sigmoidoscopy cannot screen the right side of the colon where 42% of cancers are found.[119] Flexible sigmoidoscopy, however, has the best evidence for decreasing the risk of death from any cause.[120]
Fecal occult blood testing (FOBT) of the stool is typically recommended every two years and can be either guaiac-based or immunochemical.[20] If abnormal FOBT results are found, participants are typically referred for a follow-up colonoscopy examination. When done once every 1–2 years, FOBT screening reduces colorectal cancer deaths by 16% and among those participating in screening, colorectal cancer deaths can be reduced up to 23%, although it has not been proven to reduce all-cause mortality.[121] Immunochemical tests are accurate and do not require dietary or medication changes before testing.[122] However, research in the UK has found that for these immunochemical tests, the threshold for further investigation is set at a point that may miss more than half of bowel cancer cases. The research suggests that the NHS England's Bowel Cancer Screening Programme could make better use of the test's ability to provide the exact concentration of blood in faeces (rather than only whether it is above or below a cutoff level).[123][124]
Other options include virtual colonoscopy and stool DNA screening testing (FIT-DNA). Virtual colonoscopy via a CT scan appears as good as standard colonoscopy for detecting cancers and large adenomas but is expensive, associated with radiation exposure, and cannot remove any detected abnormal growths as standard colonoscopy can.[20] Stool DNA screening test looks for biomarkers associated with colorectal cancer and precancerous lesions, including altered DNA and blood hemoglobin. A positive result should be followed by colonoscopy. FIT-DNA has more false positives than FIT and thus results in more adverse effects.[10] Further study is required as of 2016 to determine whether a three-year screening interval is correct.[10]
In the United States, screening is typically recommended between ages 50 and 75 years.[10][125] The American Cancer Society recommends starting at the age of 45.[126] For those between 76 and 85 years old, the decision to screen should be individualized.[10] For those at high risk, screenings usually begin at around 40.[20][127]
Several screening methods are recommended including stool-based tests every 2 years, sigmoidoscopy every 10 years with fecal immunochemical testing every two years, and colonoscopy every 10 years.[125] It is unclear which of these two methods is better.[128] Colonoscopy may find more cancers in the first part of the colon, but is associated with greater cost and more complications.[128] For people with average risk who have had a high-quality colonoscopy with normal results, the American Gastroenterological Association does not recommend any type of screening in the 10 years following the colonoscopy.[129][130] For people over 75 or those with a life expectancy of less than 10 years, screening is not recommended.[131] It takes about 10 years after screening for one out of a 1000 people to benefit.[132] The USPSTF list seven potential strategies for screening, with the most important thing being that at least one of these strategies is appropriately used.[10]
In Canada, among those 50 to 75 years old at normal risk, fecal immunochemical testing or FOBT is recommended every two years or sigmoidoscopy every 10 years.[133] Colonoscopy is less preferred.[133]
Some countries have national colorectal screening programs which offer FOBT screening for all adults within a certain age group, typically starting between ages 50 and 60. Examples of countries with organised screening include the United Kingdom,[134] Australia,[135] the Netherlands,[136] Hong Kong, and Taiwan.[137]
The UK Bowel Cancer Screening Programme aims to find warning signs in people aged 60 to 74, by recommending a faecal immunochemical test (FIT) every two years. FIT measures blood in faeces, and people with levels above a certain threshold may have bowel tissue examined for signs of cancer. Growths having cancerous potential are removed.[138][124]
The treatment of colorectal cancer can be aimed at cure or palliation. The decision on which aim to adopt depends on various factors, including the person's health and preferences, as well as the stage of the tumor.[139] Assessment in multidisciplinary teams is a critical part of determining whether the patient is suitable for surgery or not.[140] When colorectal cancer is caught early, surgery can be curative. However, when it is detected at later stages (for which metastases are present), this is less likely and treatment is often directed at palliation, to relieve symptoms caused by the tumour and keep the person as comfortable as possible.[20]
At an early stage, colorectal cancer may be removed during a colonoscopy using one of several techniques, including endoscopic mucosal resection or endoscopic submucosal dissection.[5] Endoscopic resection is possible if there is low possibility of lymph node metastasis and the size and location of the tumor make en bloc resection possible.[141] For people with localized cancer, the preferred treatment is complete surgical removal with adequate margins, with the attempt of achieving a cure. The procedure of choice is a partial colectomy (or proctocolectomy for rectal lesions) where the affected part of the colon or rectum is removed along with parts of its mesocolon and blood supply to facilitate removal of draining lymph nodes. This can be done either by an open laparotomy or laparoscopically, depending on factors related to the individual person and lesion factors.[20] The colon may then be reconnected or a person may have a colostomy.[5]
If there are only a few metastases in the liver or lungs, these may also be removed. Chemotherapy may be used before surgery to shrink the cancer before attempting to remove it. The two most common sites of recurrence of colorectal cancer are the liver and lungs.[20] For peritoneal carcinomatosis cytoreductive surgery, sometimes in combination with HIPEC can be used in an attempt to remove the cancer.[142]
In both cancer of the colon and rectum, chemotherapy may be used in addition to surgery in certain cases. The decision to add chemotherapy in management of colon and rectal cancer depends on the stage of the disease.[143]
In Stage I colon cancer, no chemotherapy is offered, and surgery is the definitive treatment. The role of chemotherapy in Stage II colon cancer is debatable, and is usually not offered unless risk factors such as T4 tumor, undifferentiated tumor, vascular and perineural invasion or inadequate lymph node sampling is identified.[144] It is also known that the people who carry abnormalities of the mismatch repair genes do not benefit from chemotherapy. For stage III and Stage IV colon cancer, chemotherapy is an integral part of treatment.[20]
If cancer has spread to the lymph nodes or distant organs, which is the case with stage III and stage IV colon cancer respectively, adding chemotherapy agents fluorouracil, capecitabine or oxaliplatin increases life expectancy. If the lymph nodes do not contain cancer, the benefits of chemotherapy are controversial. If the cancer is widely metastatic or unresectable, treatment is then palliative. Typically in this setting, a number of different chemotherapy medications may be used.[20] Chemotherapy drugs for this condition may include capecitabine, fluorouracil, irinotecan, oxaliplatin and UFT.[145] The drugs capecitabine and fluorouracil are interchangeable, with capecitabine being an oral medication and fluorouracil being an intravenous medicine. Some specific regimens used for CRC are CAPOX, FOLFOX, FOLFOXIRI, and FOLFIRI.[146] Antiangiogenic drugs such as bevacizumab are often added in first line therapy.[citation needed] Another class of drugs used in the second line setting are epidermal growth factor receptor inhibitors, of which the three FDA approved ones are aflibercept, cetuximab and panitumumab.[147][148]
The primary difference in the approach to low stage rectal cancer is the incorporation of radiation therapy. Often, it is used in conjunction with chemotherapy in a neoadjuvant fashion to enable surgical resection, so that ultimately a colostomy is not required. However, it may not be possible in low lying tumors, in which case, a permanent colostomy may be required. Stage IV rectal cancer is treated similar to stage IV colon cancer.
Stage IV colorectal cancer due to peritoneal carcinomatosis can be treated using HIPEC combined with cytoreductive surgery, in some people.[149][150][151] Also, T4 colorectal cancer can be treated with HIPEC to avoid future relapses.[152]
While a combination of radiation and chemotherapy may be useful for rectal cancer,[20] for some people requiring treatment, chemoradiotherapy can increase acute treatment-related toxicity, and has not been shown to improve survival rates compared to radiotherapy alone, although it is associated with less local recurrence.[142]  The use of radiotherapy in colon cancer is not routine due to the sensitivity of the bowels to radiation.[153] As with chemotherapy, radiotherapy can be used as a neoadjuvant for clinical stages T3 and T4 for rectal cancer.[154] This results in downsizing or downstaging of the tumour, preparing it for surgical resection, and also decreases local recurrence rates.[154] For locally advanced rectal cancer, neoadjuvant chemoradiotherapy has become the standard treatment.[155] Additionally, when surgery is not possible radiation therapy has been suggested to be an effective treatment against CRC pulmonary metastases, which are developed by 10-15% of people with CRC.[156]
Immunotherapy with immune checkpoint inhibitors has been found to be useful for a type of colorectal cancer with mismatch repair deficiency and microsatellite instability.[157][158][159] Pembrolizumab is approved for advanced CRC tumours that are MMR deficient and have failed usual treatments.[160] Most people who do improve, however, still worsen after months or years.[158]
On the other hand, in a prospective phase 2 study published in June 2022 in The New England Journal of Medicine, 12 patients with Deficient Mismatch Repair (dMMR) stage II or III rectal adenocarcinoma were administered single-agent dostarlimab, an anti–PD-1 monoclonal antibody, every three weeks for six months. After a median follow-up of 12 months (range, 6 to 25 months), all 12 patients had a complete clinical response with no evidence of tumor on MRI, 18F-fluorodeoxyglucose–positron-emission tomography, endoscopic evaluation, digital rectal examination, or biopsy. Moreover, no patient in the trial needed chemoradiotherapy or surgery, and no patient reported adverse events of grade 3 or higher. However, although the results of this study are promising, the study is small and has uncertainties about long-term outcomes.[161]
Palliative care is recommended for any person who has advanced colon cancer or who has significant symptoms.[162][163]
Involvement of palliative care may be beneficial to improve the quality of life for both the person and his or her family, by improving symptoms, anxiety and preventing admissions to the hospital.[164]
In people with incurable colorectal cancer, palliative care can consist of procedures that relieve symptoms or complications from the cancer but do not attempt to cure the underlying cancer, thereby improving quality of life. Surgical options may include non-curative surgical removal of some of the cancer tissue, bypassing part of the intestines, or stent placement. These procedures can be considered to improve symptoms and reduce complications such as bleeding from the tumor, abdominal pain and intestinal obstruction.[165] Non-operative methods of symptomatic treatment include radiation therapy to decrease tumor size as well as pain medications.[166]
The U.S. National Comprehensive Cancer Network and American Society of Clinical Oncology provide guidelines for the follow-up of colon cancer.[167][168] A medical history and physical examination are recommended every 3 to 6 months for 2 years, then every 6 months for 5 years. Carcinoembryonic antigen blood level measurements follow the same timing, but are only advised for people with T2 or greater lesions who are candidates for intervention. A CT-scan of the chest, abdomen and pelvis can be considered annually for the first 3 years for people who are at high risk of recurrence (for example, those who had poorly differentiated tumors or venous or lymphatic invasion) and are candidates for curative surgery (with the aim to cure). A colonoscopy can be done after 1 year, except if it could not be done during the initial staging because of an obstructing mass, in which case it should be performed after 3 to 6 months. If a villous polyp, a polyp >1 centimeter or high-grade dysplasia is found, it can be repeated after 3 years, then every 5 years. For other abnormalities, the colonoscopy can be repeated after 1 year.[143]
Routine PET or ultrasound scanning, chest X-rays, complete blood count or liver function tests are not recommended.[167][168]
For people who have undergone curative surgery or adjuvant therapy (or both) to treat non-metastatic colorectal cancer, intense surveillance and close follow-up have not been shown to provide additional survival benefits.[169]
Exercise may be recommended in the future as secondary therapy to cancer survivors. In epidemiological studies, exercise may decrease colorectal cancer-specific mortality and all-cause mortality. Results for the specific amounts of exercise needed to observe a benefit were conflicting. These differences may reflect differences in tumour biology and the expression of biomarkers. People with tumors that lacked CTNNB1 expression (β-catenin), involved in Wnt signalling pathway, required more than 18 Metabolic equivalent (MET) hours per week, a measure of exercise, to observe a reduction in colorectal cancer mortality. The mechanism of how exercise benefits survival may be involved in immune surveillance and inflammation pathways. In clinical studies, a pro-inflammatory response was found in people with stage II-III colorectal cancer who underwent 2 weeks of moderate exercise after completing their primary therapy. Oxidative balance may be another possible mechanism for benefits observed. A significant decrease in 8-oxo-dG was found in the urine of people who underwent 2 weeks of moderate exercise after primary therapy. Other possible mechanisms may involve metabolic hormone and sex-steroid hormones, although these pathways may be involved in other types of cancers.[170][171]
Another potential biomarker may be p27. Survivors with tumors that expressed p27 and performed greater and equal to 18 MET hours per week were found to have reduced colorectal cancer mortality survival compared to those with less than 18 MET hours per week. Survivors without p27 expression who exercised were shown to have worse outcomes. The constitutive activation of PI3K/AKT/mTOR pathway may explain the loss of p27 and excess energy balance may up-regulate p27 to stop cancer cells from dividing.[171]
Physical activity provides benefits to people with non-advanced colorectal cancer. Improvements in aerobic fitness, cancer-related fatigue and health-related quality of life have been reported in the short term.[172] However, these improvements were not observed at the level of disease-related mental health, such as anxiety and depression.[172]
Fewer than 600 genes are linked to outcomes in colorectal cancer.[51] These include both unfavorable genes, where high expression is related to poor outcome, for example the heat shock 70 kDa protein 1 (HSPA1A), and favorable genes where high expression is associated with better survival, for example the putative RNA-binding protein 3 (RBM3).[51] The prognosis is also correlated with a poor fidelity of the pre-mRNA splicing apparatus, and thus a high number of deviating alternative splicing.[173]
The average five-year recurrence rate in people where surgery is successful is 5% for stage I cancers, 12% in stage II and 33% in stage III. However, depending on the number of risk factors it ranges from 9–22% in stage II and 17–44% in stage III.[174]
In Europe the five-year survival rate for colorectal cancer is less than 60%. In the developed world about a third of people who get the disease die from it.[20]
Survival is directly related to detection and the type of cancer involved, but overall is poor for symptomatic cancers, as they are typically quite advanced. Survival rates for early stage detection are about five times that of late stage cancers. People with a tumor that has not breached the muscularis mucosa (TNM stage Tis, N0, M0) have a five-year survival rate of 100%, while those with invasive cancer of T1 (within the submucosal layer) or T2 (within the muscular layer) have an average five-year survival rate of approximately 90%. Those with a more invasive tumor yet without node involvement (T3-4, N0, M0) have an average five-year survival rate of approximately 70%. People with positive regional lymph nodes (any T, N1-3, M0) have an average five-year survival rate of approximately 40%, while those with distant metastases (any T, any N, M1) have a poor prognosis and the five year survival ranges from <5 percent to 31 percent.[175][176][177][178][179] The prognosis depends on a multitude of factors which include the physical fitness level of the person, extent of metastases, and tumor grade.[citation needed]
Whilst the impact of colorectal cancer on those who survive varies greatly there will often be a need to adapt to both physical and psychological outcomes of the illness and its treatment.[180] For example, it is common for people to experience incontinence,[181] sexual dysfunction,[182] problems with stoma care[183] and fear of cancer recurrence[184] after primary treatment has concluded.
A qualitative systematic review published in 2021 highlighted that there are three main factors influencing adaptation to living with and beyond colorectal cancer: support mechanisms, severity of late effects of treatment and psychosocial adjustment. Therefore, it is essential that people are offered appropriate support to help them better adapt to life following treatment.[185]
Globally more than 1 million people get colorectal cancer every year[20] resulting in about 715,000 deaths as of 2010 up from 490,000 in 1990.[186]
As of 2012[update], it is the second most common cause of cancer in women (9.2% of diagnoses) and the third most common in men (10.0%)[14]: 16  with it being the fourth most common cause of cancer death after lung, stomach, and liver cancer.[187] It is more common in developed than developing countries.[188] Global incidence varies 10-fold, with highest rates in Australia, New Zealand, Europe and the US and lowest rates in Africa and South-Central Asia.[189]
In 2022, the incidence of colorectal cancer in the United States was anticipated to be about 151,000 adults, including over 106,000 new cases of colon cancer (some 54,000 men and 52,000 women) and about 45,000 new cases of rectal cancer.[190] Since the 1980s, the incidence of colorectal cancer decreased, dropping by about 2% annually from 2014 to 2018 in adults aged 50 and older, due mainly to improved screening.[190] However, incidence of colorectal cancer has increased in individuals aged 25 to 50. In early 2023, the American Cancer Society (ACS) reported that 20% of diagnoses (of colon cancer) in 2019 were in patients under age 55, which is about double the rate in 1995, and rates of advanced disease increased by about 3% annually in people younger than 50. It predicted that, in 2023, an estimated 19,550 diagnoses and 3,750 deaths would be in people younger than 50.[191] Colorectal cancer also disproportionately affects the Black community, where the rates are the highest of any racial/ethnic group in the US. African Americans are about 20% more likely to get colorectal cancer and about 40% more likely to die from it than most other groups. Black Americans often experience greater obstacles to cancer prevention, detection, treatment, and survival, including systemic racial disparities that are complex and go beyond the obvious connection to cancer.
In the UK about 41,000 people a year get colon cancer making it the fourth most common type.[192]
One in 19 men and one in 28 women in Australia will develop colorectal cancer before the age of 75; one in 10 men and one in 15 women will develop it by 85 years of age.[193]
In the developing countries like Papua New Guinea and other Pacific Island States including the Solomon Islands, colorectal cancer is a very rare cancer amongst the people, which is least common compared to lung, stomach, liver or breast cancer. It is estimated that at least 8 in 100,000 of the people are most likely to developed colorectal cancer every year, which is unlike lung or breast cancer, where for the latter alone is 24 in 100,000 women.[194]
Rectal cancer has been diagnosed in an Ancient Egyptian mummy who had lived in the Dakhleh Oasis during the Ptolemaic period.[195]
In the United States, March is colorectal cancer awareness month.[118]
Preliminary in-vitro evidence suggests lactic acid bacteria (e.g., lactobacilli, streptococci or lactococci) may be protective against the development and progression of colorectal cancer through several mechanisms such as antioxidant activity, immunomodulation, promoting programmed cell death, antiproliferative effects, and epigenetic modification of cancer cells.[196]
Mouse models of colorectal and intestinal cancer have been developed and are used in research.[197][198][199]
Cronkhite–Canada
A chronic condition (also known as chronic disease or chronic illness) is a health condition or disease that is persistent or otherwise long-lasting in its effects or a disease that comes with time. The term chronic is often applied when the course of the disease lasts for more than three months. Common chronic diseases include diabetes, functional gastrointestinal disorder, eczema, arthritis, asthma, chronic obstructive pulmonary disease, autoimmune diseases, genetic disorders and some viral diseases such as hepatitis C and acquired immunodeficiency syndrome. An illness which is lifelong because it ends in death is a terminal illness.  It is possible and not unexpected for an illness to change in definition from terminal to chronic. Diabetes and HIV for example were once terminal yet are now considered chronic due to the availability of insulin for diabetics and daily drug treatment for individuals with HIV which allow these individuals to live while managing symptoms.[1]
In medicine, chronic conditions are distinguished from those that are acute. An acute condition typically affects one portion of the body and responds to treatment. A chronic condition, on the other hand, usually affects multiple areas of the body, is not fully responsive to treatment, and persists for an extended period of time.[2]
Chronic conditions may have periods of remission or relapse where the disease temporarily goes away, or subsequently reappears. Periods of remission and relapse are commonly discussed when referring to substance abuse disorders which some consider to fall under the category of chronic condition.[3]
Chronic conditions are often associated with non-communicable diseases which are distinguished by their non-infectious causes. Some chronic conditions though, are caused by transmissible infections such as HIV/AIDS.[citation needed]
63% of all deaths worldwide are from chronic conditions.[4] Chronic diseases constitute a major cause of mortality, and the World Health Organization (WHO) attributes 38 million deaths a year to non-communicable diseases.[5] In the United States approximately 40% of adults have at least two chronic conditions.[6][7] Living with two or more chronic conditions is referred to as multimorbidity.[8]
Chronic conditions have often been used to describe the various health related states of the human body such as syndromes, physical impairments, disabilities as well as diseases. Epidemiologists have found interest in chronic conditions due to the fact they contribute to disease, disability, and diminished physical and/or mental capacity.[9]
For example, high blood pressure or hypertension is considered to be not only a chronic condition itself but also correlated with diseases such as heart attack or stroke. Additionally, some socioeconomic factors may be considered as a chronic condition as they lead to disability in daily life. An important one that public health officials in the social science setting have begun highlighting is chronic poverty.[10][11][12]
Researchers, particularly those studying the United States, utilize the Chronic Condition Indicator (CCI) which maps ICD codes as "chronic" or "non-chronic".[13]
The list below includes these chronic conditions and diseases:
In 2015 the World Health Organization produced a report on non-communicable diseases, citing the four major types as:[14]
Other examples of chronic diseases and health conditions include:
While risk factors vary with age and gender, many of the common chronic diseases in the US are caused by dietary, lifestyle and metabolic risk factors.[15] Therefore, these conditions might be prevented by behavioral changes, such as quitting smoking, adopting a healthy diet, and increasing physical activity. Social determinants are important risk factors for chronic diseases.[16] Social factors, e.g., socioeconomic status, education level, and race/ethnicity, are a major cause for the disparities observed in the care of chronic disease.[16] Lack of access and delay in receiving care result in worse outcomes for patients from minorities and underserved populations.[17] Those barriers to medical care complicate patients monitoring and continuity in treatment.[citation needed]
In the US, minorities and low-income populations are less likely to seek, access and receive preventive services necessary to detect conditions at an early stage.[18]
The majority of US health care and economic costs associated with medical conditions are incurred by chronic diseases and conditions and associated health risk behaviors. Eighty-four percent of all health care spending in 2006 was for the 50% of the population who have one or more common chronic medical conditions (CDC, 2014).
There are several psychosocial risk and resistance factors among children with chronic illness and their family members. Adults with chronic illness were significantly more likely to report life dissatisfaction than those without chronic illness.[19] Compared to their healthy peers, children with chronic illness have about a twofold increase in psychiatric disorders.[20] Higher parental depression and other family stressors predicted more problems among patients.[21] In addition, sibling problems along with the burden of illness on the family as a whole led to more psychological strain on the patients and their families.[21]
A growing body of evidence supports that prevention is effective in reducing the effect of chronic conditions; in particular, early detection results in less severe outcomes. Clinical preventive services include screening for the existence of the disease or predisposition to its development, counseling and immunizations against infectious agents. Despite their effectiveness, the utilization of preventive services is typically lower than for regular medical services.  In contrast to their apparent cost in time and money, the benefits of preventive services are not directly perceived by patient because their effects are on the long term or might be greater for society as a whole than at the individual level.[22]
Therefore, public health programs are important in educating the public, and promoting healthy lifestyles and awareness about chronic diseases. While those programs can benefit from funding at different levels (state, federal, private) their implementation is mostly in charge of local agencies and community-based organizations.[23]
Studies have shown that public health programs are effective in reducing mortality rates associated to cardiovascular disease, diabetes and cancer, but the results are somewhat heterogeneous depending on the type of condition and the type of programs involved.[24] For example, results from different approaches in cancer prevention and screening depended highly on the type of cancer.[25]
The rising number of patient with chronic diseases has renewed the interest in prevention and its potential role in helping control costs.  In 2008, the Trust for America's Health produced a report that estimated investing $10 per person annually in community-based programs of proven effectiveness and promoting healthy lifestyle (increase in physical activity, healthier diet and preventing tobacco use) could save more than $16 billion annually within a period of just five years.[26]
A 2017 review (updated in 2022) found that it is uncertain whether school-based policies on targeting risk factors on chronic diseases such as healthy eating policies, physical activity policies, and tobacco policies can improve student health behaviours or knowledge of staffs and students.[27] The updated review in 2022 did determine a slight improvement in measures of obesity and physical activity as the use of improved strategies lead to increased implementation interventions but continued to call for additional research to address questions related to alcohol use and risk.[27]  Encouraging those with chronic conditions to continue with their outpatient (ambulatory) medical care and attend scheduled medical appointments may help improve outcomes and reduce medical costs due to missed appointments.[28] Finding patient-centered alternatives to doctors or consultants scheduling medical appointments has been suggested as a means of improving the number of people with chronic conditions that miss medical appointments, however there is no strong evidence that these approaches make a difference.[28]
Nursing can play an important role in assisting patients with chronic diseases achieve longevity and experience wellness.[29] Scholars point out that the current neoliberal era emphasizes self-care, in both affluent and low-income communities.[30] This self-care focus extends to the nursing of patients with chronic diseases, replacing a more holistic role for nursing with an emphasis on patients managing their own health conditions. Critics note that this is challenging if not impossible for patients with chronic disease in low-income communities where health care systems, and economic and social structures do not fully support this practice.[30]
A study in Ethiopia showcases a nursing-heavy approach to the management of chronic disease. Foregrounding the problem of distance from healthcare facility, the study recommends patients increase their request for care. It uses nurses and health officers to fill, in a cost-efficient way, the large unmet need for chronic disease treatment.[31] They led their health centers manned by nurses and health officers; so, there are specific training required for involvement in the programmed must be carried out regularly, to ensure that new staff is educated in administering chronic disease care.[31] The program shows that community-based care and education, primarily driven by nurses and health officers, works.[31] It highlights the importance of nurses following up with individuals in the community, and allowing nurses flexibility in meeting their patients' needs and educating them for self-care in their homes.[citation needed]
The epidemiology of chronic disease is diverse and the epidemiology of some chronic diseases can change in response to new treatments.  In the treatment of HIV, the success of anti-retroviral therapies means that many patients will experience this infection as a chronic disease that for many will span several decades of their chronic life.[32]
Some epidemiology of chronic disease can apply to multiple diagnosis.  Obesity and body fat distribution for example contribute and are risk factors for many chronic diseases such as diabetes, heart, and kidney disease.[33]  Other epidemiological factors, such as social, socioeconomic, and environment do not have a straightforward cause and effect relationship with chronic disease diagnosis.  While typically higher socioeconomic status is correlated with lower occurrence of chronic disease, it is not known is there is a direct cause and effect relationship between these two variables.[34]
The epidemiology of communicable chronic diseases such as AIDS is also different from that of noncommunicable chronic disease.  While Social factors do play a role in AIDS prevalence, only exposure is truly needed to contract this chronic disease. Communicable chronic diseases are also typically only treatable with medication intervention, rather than lifestyle change as some non-communicable chronic diseases can be treated.[35]
As of 2003, there are a few programs which aim to gain more knowledge on the epidemiology of chronic disease using data collection.  The hope of these programs is to gather epidemiological data on various chronic diseases across the United States and demonstrate how this knowledge can be valuable in addressing chronic disease.[36]
In the United States, as of 2004 nearly one in two Americans (133 million) has at least one chronic medical condition, with most subjects (58%) between the ages of 18 and 64.[13] The number is projected to increase by more than one percent per year by 2030, resulting in an estimated chronically ill population of 171 million.[13] The most common chronic conditions are high blood pressure, arthritis, respiratory diseases like emphysema, and high cholesterol.[citation needed]
Based on data from 2014 Medical Expenditure Panel Survey (MEPS), about 60% of adult Americans were estimated to have one chronic illness, with about 40% having more than one; this rate appears to be mostly unchanged from 2008.[37] MEPS data from 1998 showed 45% of adult Americans had at least one chronic illness, and 21% had more than one.[38]
According to research by the CDC, chronic disease is also especially a concern in the elderly population in America. Chronic diseases like stroke, heart disease, and cancer were among the leading causes of death among Americans aged 65 or older in 2002, accounting for 61% of all deaths among this subset of the population.[39] It is estimated that at least 80% of older Americans are currently living with some form of a chronic condition, with 50% of this population having two or more chronic conditions.[39] The two most common chronic conditions in the elderly are high blood pressure and arthritis, with diabetes, coronary heart disease, and cancer also being reported among the elder population.[40]
In examining the statistics of chronic disease among the living elderly, it is also important to make note of the statistics pertaining to fatalities as a result of chronic disease. Heart disease is the leading cause of death from chronic disease for adults older than 65, followed by cancer, stroke, diabetes, chronic lower respiratory diseases, influenza and pneumonia, and, finally, Alzheimer's disease.[39] Though the rates of chronic disease differ by race for those living with chronic illness, the statistics for leading causes of death among elderly are nearly identical across racial/ethnic groups.[39]
Chronic illnesses cause about 70% of deaths in the US and in 2002 chronic conditions (heart disease, cancers, stroke, chronic respiratory diseases, diabetes, Alzheimer's disease, mental illness and kidney diseases) were six of the top ten causes of mortality in the general US population.[41]
Chronic diseases are a major factor in the continuous growth of medical care spending.[42] In 2002, the U.S. Department of Health and Human Services stated that the health care for chronic diseases cost the most among all health problems in the U.S.[43] Healthy People 2010 reported that more than 75% of the $2 trillion spent annually in U.S. medical care are due to chronic conditions; spending are even higher in proportion for Medicare beneficiaries (aged 65 years and older).[18] Furthermore, in 2017 it was estimated that 90% of the $3.3 billion spent on healthcare in the United States was due to the treatment of chronic diseases and conditions.[44][45] Spending growth is driven in part by the greater prevalence of chronic illnesses and the longer life expectancy of the population. Also, improvement in treatments has significantly extended the lifespans of patients with chronic diseases but results in additional costs over long period of time. A striking success is the development of combined antiviral therapies that led to remarkable improvement in survival rates and quality of life of HIV-infected patients.[citation needed]
In addition to direct costs in health care, chronic diseases are a significant burden to the economy, through limitations in daily activities, loss in productivity and loss of days of work. A particular concern is the rising rates of overweight and obesity in all segments of the U.S. population.[18] Obesity itself is a medical condition and not a disease, but it constitutes a major risk factor for developing chronic illnesses, such as diabetes, stroke, cardiovascular disease and cancers. Obesity results in significant health care spending and indirect costs, as illustrated by a recent study from the Texas comptroller reporting that obesity alone cost Texas businesses an extra $9.5 billion in 2009, including more than $4 billion for health care, $5 billion for lost productivity and absenteeism, and $321 million for disability.[46]
There have been recent links between social factors and prevalence as well as outcome of chronic conditions.
The connection between loneliness, overall health, and chronic conditions has recently been highlighted.  Some studies have shown that loneliness has detrimental health effects similar to that of smoking and obesity.[47]  One study found that feelings of isolation are associated with higher self reporting of health as poor, and feelings of loneliness increased the likelihood of mental health disorders in individuals.[48]  The connection between chronic illness and loneliness is established, yet oftentimes ignored in treatment.  One study for example found that a greater number of chronic illnesses per individual were associated with feelings of loneliness.[49]  Some of the possible reasons for this listed are an inability to maintain independence as well as the chronic illness being a source of stress for the individual.  A study of loneliness in adults over age 65 found that low levels of loneliness as well as high levels of familial support were associated with better outcomes of multiple chronic conditions such as hypertension and diabetes.[49]  There are some recent movements in the medical sphere to address these connections when treating patients with chronic illness.  The biopsychosocial approach for example, developed in 2006 focuses on patients "patient's personality, family, culture, and health dynamics."[50]  Physicians are leaning more towards a psychosocial approach to chronic illness to aid the increasing number of individuals diagnosed with these conditions.  Despite this movement, there is still criticism that chronic conditions are not being treated appropriately, and there is not enough emphasis on the behavioral aspects of chronic conditions[51] or psychological types of support for patients.[52] 
The mental health intersectionality on those with chronic conditions is a large aspect often over looked by doctors. And chronic illness therapists are available for support to help with the mental toll of chronic illness a it is often underestimated in society. Adults with chronic illness that restrict their daily life present with more depression and lower self-esteem than healthy adults and adults with non-restricting chronic illness.[53] The emotional influence of chronic illness also has an effect on the intellectual and educational development of the individual.[54] For example, people living with type 1 diabetes endure a lifetime of monotonous and rigorous health care management usually involving daily blood glucose monitoring, insulin injections, and constant self-care. This type of constant attention that is required by type 1 diabetes and other chronic illness can result in psychological maladjustment. There have been several theories, namely one called diabetes resilience theory, that posit that protective processes buffer the impact of risk factors on the individual's development and functioning.[55]
People with chronic conditions pay more out-of-pocket; a study found that Americans spent $2,243 more on average.[56] The financial burden can increase medication non-adherence.[57][58]
In some countries, laws protect patients with chronic conditions from excessive financial responsibility; for example, as of 2008 France limited copayments for those with chronic conditions, and Germany limits cost sharing to 1% of income versus 2% for the general public.[59]
Within the medical-industrial complex, chronic illnesses can impact the relationship between pharmaceutical companies and people with chronic conditions. Life-saving drugs, or life-extending drugs, can be inflated for a profit.[60] There is little regulation on the cost of chronic illness drugs, which suggests that abusing the lack of a drug cap can create a large market for drug revenue.[61] Likewise, certain chronic conditions can last throughout one's lifetime and create pathways for pharmaceutical companies to take advantage of this.[62]
Gender influences how chronic disease is viewed and treated in society.  Women's chronic health issues are often considered to be most worthy of treatment or most severe when the chronic condition interferes with a woman's fertility.  Historically, there is less of a focus on a woman's chronic conditions when it interferes with other aspects of her life or well-being.  Many women report feeling less than or even "half of a woman" due to the pressures that society puts on the importance of fertility and health when it comes to typically feminine ideals.  These kinds of social barriers interfere with women's ability to perform various other activities in life and fully work toward their aspirations.[63]
Race is also allegedly implicated in chronic illness, although there may be many other factors involved.  Racial minorities are 1.5-2 times more likely to have most chronic diseases than white individuals.  Non-Hispanic blacks are 40% more likely to have high blood pressure that non-Hispanic whites, diagnosed diabetes is 77% higher among non-Hispanic blacks, and American Indians and Alaska Natives are 60% more likely to be obese than non-Hispanic whites.[64]  Some of this prevalence has been suggested to be in part from environmental racism.  Flint, Michigan, for example, had high levels of lead poisoning in their drinkable water after waste was dumped into low-value housing areas.[65]  There are also higher rates of asthma in children who live in lower income areas due to an abundance of pollutants being released on a much larger scale in these areas.[66][67]
In Europe, the European Chronic Disease Alliance was formed in 2011, which represents over 100,000 healthcare workers.[68]
In the United States, there are a number of nonprofits focused on chronic conditions, including entities focused on specific diseases such as the American Diabetes Association, Alzheimer's Association, or Crohn's and Colitis Foundation. There are also broader groups focused on advocacy or research into chronic illness in general, such as the National Association of Chronic Disease Directors, Partnership to Fight Chronic Disease, the Chronic Disease Coalition which arose in Oregon in 2015,[69] and the Chronic Policy Care Alliance.[70]
Signs and symptomsSyndromeDisease
Medical diagnosisDifferential diagnosisPrognosis
AcuteChronicCure
Eponymous diseaseAcronym or abbreviationRemission

Infertility is the inability of a person, animal or plant to reproduce by natural means. It is usually not the natural state of a healthy adult, except notably among certain eusocial species (mostly haplodiploid insects). It is the normal state of a human child or other young offspring, because they have not undergone puberty, which is the body's start of reproductive capacity.
In humans, infertility is the inability to become pregnant after one year of unprotected and regular sexual intercourse  involving a male and female partner.[2] There are many causes of infertility, including some that medical intervention can treat.[3] Estimates from 1997 suggest that worldwide about five percent of all heterosexual couples have an unresolved problem with infertility. Many more couples, however, experience involuntary childlessness for at least one year: estimates range from 12% to 28%.[4]
The main cause of infertility in humans is age, and an advanced maternal age can raise the probability of suffering a spontaneous abortion during pregnancy.
Male infertility is responsible for 20–30% of infertility cases, while 20–35% are due to female infertility, and 25–40% are due to combined problems in both parts.[2][5] In 10–20% of cases, no cause is found.[5] The most common cause of female infertility is age, which generally manifests in sparse or absent menstrual periods.[6]  Male infertility is most commonly due to deficiencies in the semen, and semen quality is used as a surrogate measure of male fecundity.[7]
Women who are fertile experience a period of fertility before and during ovulation, and are infertile for the rest of the menstrual cycle. Fertility awareness methods are used to discern when these changes occur by tracking changes in cervical mucus or basal body temperature.
"Demographers tend to define infertility as childlessness in a population of women of reproductive age," whereas the epidemiological definition refers to "trying for" or "time to" a pregnancy, generally in a population of women exposed to a probability of conception.[8] Currently, female fertility normally peaks in young adulthood and diminishes after 35 with pregnancy occurring rarely after age 50. A female is most fertile within 24 hours of ovulation. Male fertility peaks usually in young adulthood and declines after age 40.[9]
The time needed to pass (during which the couple tries to conceive) for that couple to be diagnosed with infertility differs between different jurisdictions. Existing definitions of infertility lack uniformity, rendering comparisons in prevalence between countries or over time problematic. Therefore, data estimating the prevalence of infertility cited by various sources differ significantly.[8] A couple that tries unsuccessfully to have a child after a certain period of time (often a short period, but definitions vary) is sometimes said to be subfertile, meaning less fertile than a typical couple. Both infertility and subfertility are defined similarly and often used interchangeably, but subfertility is the delay in conceiving within six to twelve months, whereas infertility is the inability to conceive naturally within a full year.[10]
The World Health Organization defines infertility as follows:[11]
Infertility is "a disease of the reproductive system defined by the failure to achieve a clinical pregnancy after 12 months or more of regular unprotected sexual intercourse (and there is no other reason, such as breastfeeding or postpartum amenorrhoea). Primary infertility is infertility in a couple who have never had a child. Secondary infertility is failure to conceive following a previous pregnancy. Infertility may be caused by infection in the man or woman, but often there is no obvious underlying cause"One definition of infertility that is frequently used in the United States by reproductive endocrinologists, doctors who specialize in infertility, to consider a couple eligible for treatment is:
These time intervals would seem to be reversed; this is an area where public policy trumps science. The idea is that for women beyond age 35, every month counts and if made to wait another six months to prove the necessity of medical intervention, the problem could become worse. The corollary to this is that, by definition, failure to conceive in women under 35 is not regarded with the same urgency as it is in those over 35.[citation needed]
In the UK, previous NICE guidelines defined infertility as failure to conceive after regular unprotected sexual intercourse for two years in the absence of known reproductive pathology.[12] Updated NICE guidelines do not include a specific definition, but recommend that "A woman of reproductive age who has not conceived after 1 year of unprotected vaginal sexual intercourse, in the absence of any known cause of infertility, should be offered further clinical assessment and investigation along with her partner, with earlier referral to a specialist if the woman is over 36 years of age."[13]
Researchers commonly base demographic studies on infertility prevalence over a five-year period.[14] Practical measurement problems, however, exist for any definition, because it is difficult to measure continuous exposure to the risk of pregnancy over a period of years.
Primary infertility is defined as the absence of a live birth for women who desire a child and have been in a union for at least 12 months, during which they have not used any contraceptives.[15] The World Health Organisation also adds that 'women whose pregnancy spontaneously miscarries, or whose pregnancy results in a still born child, without ever having had a live birth would present with primarily infertility'.[15]
Secondary infertility is defined as the absence of a live birth for women who desire a child and have been in a union for at least 12 months since their last live birth, during which they did not use any contraceptives.[15]
Thus the distinguishing feature is whether or not the couple have ever had a pregnancy that led to a live birth.
The consequences of infertility are manifold and can include societal repercussions and personal suffering. Advances in assisted reproductive technologies, such as IVF, can offer hope to many couples where treatment is available, although barriers exist in terms of medical coverage and affordability. The medicalization of infertility has unwittingly led to a disregard for the emotional responses that couples experience, which include distress, loss of control, stigmatization, and a disruption in the developmental trajectory of adulthood.[16] One of the main challenges in assessing the distress levels in women with infertility is the accuracy of self-report measures. It is possible that women "fake good" in order to appear mentally healthier than they are. It is also possible that women feel a sense of hopefulness/increased optimism prior to initiating infertility treatment, which is when most assessments of distress are collected. Some early studies concluded that infertile women did not report any significant differences in symptoms of anxiety and depression than fertile women. The further into treatment a patient goes, the more often they display symptoms of depression and anxiety. Patients with one treatment failure had significantly higher levels of anxiety, and patients with two failures experienced more depression when compared with those without a history of treatment. However, it has also been shown that the more depressed the infertile woman, the less likely she is to start infertility treatment and the more likely she is to drop out after only one cycle. Researchers have also shown that despite a good prognosis and having the finances available to pay for treatment, discontinuation is most often due to psychological reasons.[17] Fertility does not seem to increase when the women takes antioxidants to reduce the oxidative stress brought by the situation.[18]
Infertility may have psychological effects. Parenthood is one of the major transitions in adult life for both men and women. The stress of the non-fulfilment of a wish for a child has been associated with emotional consequences such as anger, depression, anxiety, marital problems and feelings of worthlessness.[19]
Partners may become more anxious to conceive, increasing sexual dysfunction.[20] Marital discord often develops, especially when they are under pressure to make medical decisions. Women trying to conceive often have depression rates similar to women who have heart disease or cancer.[21] Emotional stress and marital difficulties are greater in couples where the infertility lies with the man.[22]
Male and female partner respond differently to infertility problems. In general, women show higher depression levels than their male partners when dealing with infertility. A possible explanation may be that women feel more responsible and guilty than men during the process of trying to conceive. On the other hand, infertile men experience a psychosomatic distress.[19]
Having a child is considered to be important in most societies. Infertile couples may experience social and family pressure leading to a feeling of social isolation. Factors of gender, age, religion, and socioeconomic status are important influences.[23] Societal pressures may affect a couple's decision to approach, avoid, or experience an infertility treatment.[24]
Moreover, the socioeconomic status influences the psychology of the infertile couples: low socioeconomic status is associated with increased chances of developing depression.[19]
In many cultures, inability to conceive bears a stigma. In closed social groups, a degree of rejection (or a sense of being rejected by the couple) may cause considerable anxiety and disappointment. Some respond by actively avoiding the issue altogether.[25]
In the United States some treatments for infertility, including diagnostic tests, surgery and therapy for depression, can qualify one for Family and Medical Leave Act leave. It has been suggested that infertility be classified as a form of disability.[26]
Couples that suffer from infertility have a higher risk than other couples to develop sexual dysfunctions. The most common sexual issue facing the couples is a decline of sexual desire and erectile dysfunction.[27]
Male infertility is responsible for 20–30% of infertility cases, while 20–35% are due to female infertility, and 25–40% are due to combined problems in both parts.[2][5] In 10–20% of cases, no cause is found.[5] The most common cause of female infertility are ovulation problems, usually manifested by scanty or absent menstrual periods.[6]  Male infertility is most commonly due to deficiencies in the semen, and semen quality is used as a surrogate measure of male fecundity.[7]
Iodine deficiency may lead to infertility.[28]
Before puberty, humans are naturally infertile; their gonads have not yet developed the gametes required to reproduce: boys' testicles have not developed the sperm cells required to impregnate a female; girls have not begun the process of ovulation which activates the fertility of their egg cells (ovulation is confirmed by the first menstrual cycle, known as menarche, which signals the biological possibility of pregnancy). Infertility in children is commonly referred to as prepubescence (or being prepubescent, an adjective used to also refer to humans without secondary sex characteristics)[citation needed].
The absence of fertility in children is considered a natural part of human growth and child development, as the hypothalamus in their brain is still underdeveloped and cannot release the hormones required to activate the gonads' gametes. Fertility in children before the ages of eight or nine is considered a disease known as precocious puberty. This disease is usually triggered by a brain tumor or other related injury.[29]
Delayed puberty, puberty absent past or occurring later than the average onset (between the ages of ten and fourteen), may be a cause of infertility. In the United States, girls are considered to have delayed puberty if they have not started menstruating by age 16 (alongside lacking breast development by age 13).[30] Boys are considered to have delayed puberty if they lack enlargement of the testicles by age 14.[30] Delayed puberty affects about 2% of adolescents.[31][32]
Most commonly, puberty may be delayed for several years and still occur normally, in which case it is considered constitutional delay of growth and puberty, a common variation of healthy physical development.[30] Delay of puberty may also occur due to various causes such as malnutrition, various systemic diseases, or defects of the reproductive system (hypogonadism) or the body's responsiveness to sex hormones.[30]
Antisperm antibodies (ASA) have been considered as infertility cause in around 10–30% of infertile couples.[33] In both men and women, ASA production are directed against surface antigens on sperm, which can interfere with sperm motility and transport through the female reproductive tract, inhibiting capacitation and acrosome reaction, impaired fertilization, influence on the implantation process, and impaired growth and development of the embryo. The antibodies are classified into different groups: There are IgA, IgG and IgM antibodies. They also differ in the location of the spermatozoon they bind on (head, mid piece, tail). Factors contributing to the formation of antisperm antibodies in women are disturbance of normal immunoregulatory mechanisms, infection, violation of the integrity of the mucous membranes, rape and unprotected oral or anal sex. Risk factors for the formation of antisperm antibodies in men include the breakdown of the blood‑testis barrier, trauma and surgery, orchitis, varicocele, infections, prostatitis, testicular cancer, failure of immunosuppression and unprotected receptive anal or oral sex with men.[33][34]
Infections with the following sexually transmitted pathogens have a negative effect on fertility: Chlamydia trachomatis and Neisseria gonorrhoeae. There is a consistent association of Mycoplasma genitalium infection and female reproductive tract syndromes. M. genitalium infection is associated with increased risk of infertility.[35][36]
Mutations to NR5A1 gene encoding steroidogenic factor 1 (SF-1) have been found in a small subset of men with non-obstructive male factor infertility where the cause is unknown. Results of one study investigating a cohort of 315 men revealed changes within the hinge region of SF-1 and no rare allelic variants in fertile control men. Affected individuals displayed more severe forms of infertility such as azoospermia and severe oligozoospermia.[37]
Small supernumerary marker chromosomes are abnormal extra chromosomes; they are three times more likely to occur in infertile individuals and account for 0.125% of all infertility cases.[38] See Infertility associated with small supernumerary marker chromosomes and Genetics of infertility#Small supernumerary marker chromosomes and infertility.
Factors that can cause male as well as female infertility are:
German scientists have reported that a virus called adeno-associated virus might have a role in male infertility,[61] though it is otherwise not harmful.[62] Other diseases such as chlamydia, and gonorrhea can also cause infertility, due to internal scarring (fallopian tube obstruction).[63][64][65]
The following causes of infertility may only be found in females.
For a woman to conceive, certain things have to happen: vaginal intercourse must take place around the time when an egg is released from her ovary; the system that produces eggs has to be working at optimum levels; and her hormones must be balanced.[67]
For women, problems with fertilization arise mainly from either structural problems in the Fallopian tube or uterus or problems releasing eggs. Infertility may be caused by blockage of the Fallopian tube due to malformations, infections such as chlamydia or scar tissue. For example, endometriosis can cause infertility with the growth of endometrial tissue in the Fallopian tubes or around the ovaries. Endometriosis is usually more common in women in their mid-twenties and older, especially when postponed childbirth has taken place.[68]
Another major cause of infertility in women may be the inability to ovulate. Ovulatory disorders make up 25% of the known causes of female infertility. Oligo-ovulation or anovulation results in infertility because no oocyte will be released monthly. In the absence of an oocyte, there is no opportunity for fertilization and pregnancy. World Health Organization subdivided ovulatory disorders into four classes:
Malformation of the eggs themselves may complicate conception.  For example, polycystic ovarian syndrome (PCOS) is when the eggs only partially develop within the ovary and there is an excess of male hormones. Some women are infertile because their ovaries do not mature and release eggs. In this case, synthetic FSH by injection or Clomid (Clomiphene citrate) via a pill can be given to stimulate follicles to mature in the ovaries.[citation needed]
Other factors that can affect a woman's chances of conceiving include being overweight or underweight, or her age as female fertility declines after the age of 30.[70]
Sometimes it can be a combination of factors, and sometimes a clear cause is never established.
Common causes of infertility of females include:
Male infertility is defined as the inability of a male to make a fertile female pregnant, for a minimum of at least one year of unprotected intercourse. There are multiple causes for male infertility. These include endocrine disorders (usually due to hypogonadism) at an estimated 2% to 5%, sperm transport disorders (such as vasectomy) at 5%, primary testicular defects (which includes abnormal sperm parameters without any identifiable cause) at 65% to 80% and idiopathic (where an infertile male has normal sperm and semen parameters) at 10% to 20%.[72]
The main cause of male infertility is low semen quality. In men who have the necessary reproductive organs to procreate, infertility can be caused by low sperm count due to endocrine problems, drugs, radiation, or infection.  There may be testicular malformations, hormone imbalance, or blockage of the man's duct system. Although many of these can be treated through surgery or hormonal substitutions, some may be indefinite.[73]
Infertility associated with viable, but immotile sperm may be caused by primary ciliary dyskinesia. The sperm must provide the zygote with DNA, centrioles, and activation factor for the embryo to develop. A defect in any of these sperm structures may result in infertility that will not be detected by semen analysis.[74] Antisperm antibodies cause immune infertility.[33][30] Cystic fibrosis can lead to infertility in men.
In some cases, both the man and woman may be infertile or subfertile, and the couple's infertility arises from the combination of these conditions. In other cases, the cause is suspected to be immunological or genetic; it may be that each partner is independently fertile but the couple cannot conceive together without assistance.[citation needed]
In the US, up to 20% of infertile couples have unexplained infertility. In these cases, abnormalities are likely to be present but not detected by current methods. Possible problems could be that the egg is not released at the optimum time for fertilization, that it may not enter the fallopian tube, sperm may not be able to reach the egg, fertilization may fail to occur, transport of the zygote may be disturbed, or implantation fails. It is increasingly recognized that egg quality is of critical importance and women of advanced maternal age have eggs of reduced capacity for normal and successful fertilization.  Also, polymorphisms in folate pathway genes could be one reason for fertility complications in some women with unexplained infertility.[75] However, a growing body of evidence suggests that epigenetic modifications in sperm may be partially responsible.[76][77]
If both partners are young and healthy and have been trying to conceive for one year without success, a visit to a physician or women's health nurse practitioner (WHNP) could help to highlight potential medical problems earlier rather than later. The doctor or WHNP may also be able to suggest lifestyle changes to increase the chances of conceiving.[78]
However, there are instances where couples should seek reproductive counseling after only 6 months of trying for a pregnancy:
A doctor or WHNP takes a medical history and gives a physical examination. They can also carry out some basic tests on both partners to see if there is an identifiable reason for not having achieved a pregnancy. If necessary, they refer patients to a fertility clinic or local hospital for more specialized tests. The results of these tests help determine the best fertility treatment.[citation needed]
Treatment depends on the cause of infertility, but may include counselling, fertility treatments, which include in vitro fertilization. According to ESHRE recommendations, couples with an estimated live birth rate of 40% or higher per year are encouraged to continue aiming for a spontaneous pregnancy.[81] Treatment methods for infertility may be grouped as medical or complementary and alternative treatments. Some methods may be used in concert with other methods. Drugs used for both women and men[82] include clomiphene citrate, human menopausal gonadotropin (hMG), follicle-stimulating hormone (FSH), human chorionic gonadotropin (hCG), gonadotropin-releasing hormone (GnRH) analogues, aromatase inhibitors, and metformin.
Medical treatment of infertility generally involves the use of fertility medication, medical device, surgery, or a combination of the following. If the sperm is of good quality and the mechanics of the woman's reproductive structures are good (patent fallopian tubes, no adhesions or scarring), a course of ovulation induction may be used. The physician or WHNP may also suggest using a conception cap cervical cap, which the patient uses at home by placing the sperm inside the cap and putting the conception device on the cervix, or intrauterine insemination (IUI), in which the doctor or WHNP introduces sperm into the uterus during ovulation, via a catheter. In these methods, fertilization occurs inside the body.[citation needed]
If conservative medical treatments fail to achieve a full-term pregnancy, the physician or WHNP may suggest the patient to undergo in vitro fertilization (IVF). IVF and related techniques (ICSI, ZIFT, GIFT) are called assisted reproductive technology (ART) techniques.[citation needed]
ART techniques generally start with stimulating the ovaries to increase egg production. After stimulation, the physician surgically extracts one or more eggs from the ovary, and unites them with sperm in a laboratory setting, with the intent of producing one or more embryos. Fertilization takes place outside the body, and the fertilized egg is reinserted into the woman's reproductive tract, in a procedure called embryo transfer.[citation needed]
Other medical techniques are e.g. tuboplasty, assisted hatching, and preimplantation genetic diagnosis.[citation needed]
IVF is the most commonly used ART. It has been proven useful in overcoming infertility conditions, such as blocked or damaged tubes, endometriosis, repeated IUI failure, unexplained infertility, poor ovarian reserve, poor or even nil sperm count.[citation needed]
ICSI technique is used in case of poor semen quality, low sperm count or failed fertilization attempts during prior IVF cycles. This technique involves an injection of a single healthy sperm directly injected into mature egg. The fertilized embryo is then transferred to womb.
Fertility tourism is the practice of traveling to another country for fertility treatments.[83] It may be regarded as a form of medical tourism. The main reasons for fertility tourism are legal regulation of the sought procedure in the home country, or lower price. In-vitro fertilization and donor insemination are major procedures involved.[citation needed]
Nowadays, there are several treatments (still in experimentation) related to stem cell therapy. It is a new opportunity, not only for partners with lack of gametes, but also for same-sex couples and single people who want to have offspring. Theoretically, with this therapy, we can get artificial gametes in vitro. There are different studies for both women and men.[84]
Stem cell therapy is really new, and everything is still under investigation. Additionally, it could be the future for the treatment of multiple diseases, including infertility.  It will take time before these studies can be available for clinics and patients.
Prevalence of infertility varies depending on the definition, i.e. on the time span involved in the failure to conceive.
Perhaps except for infertility in science fiction, films and other fiction depicting emotional struggles of assisted reproductive technology have had an upswing first in the latter part of the 2000s decade, although the techniques have been available for decades.[95] Yet, the number of people that can relate to it by personal experience in one way or another is ever-growing, and the variety of trials and struggles is huge.[95]
Pixar's Up contains a depiction of infertility in an extended life montage that lasts the first few minutes of the film.[96]
Other individual examples are referred to individual sub-articles of assisted reproductive technology
There are several ethical issues associated with infertility and its treatment.
Many countries have special frameworks for dealing with the ethical and social issues around fertility treatment.

Asthma is a long-term inflammatory disease of the airways of the lungs.[5] It is characterized by variable and recurring symptoms, reversible airflow obstruction, and easily triggered bronchospasms.[10][11] Symptoms include episodes of wheezing, coughing, chest tightness, and shortness of breath.[4] These may occur a few times a day or a few times per week.[5] Depending on the person, asthma symptoms may become worse at night or with exercise.[5]
Asthma is thought to be caused by a combination of genetic and environmental factors.[4] Environmental factors include exposure to air pollution and allergens.[5] Other potential triggers include medications such as aspirin and beta blockers.[5] Diagnosis is usually based on the pattern of symptoms, response to therapy over time, and spirometry lung function testing.[6] Asthma is classified according to the frequency of symptoms, forced expiratory volume in one second (FEV1), and peak expiratory flow rate.[12] It may also be classified as atopic or non-atopic, where atopy refers to a predisposition toward developing a type 1 hypersensitivity reaction.[13][14]
There is no known cure for asthma, but it can be controlled.[5] Symptoms can be prevented by avoiding triggers, such as allergens and respiratory irritants, and suppressed with the use of inhaled corticosteroids.[7][15] Long-acting beta agonists (LABA) or antileukotriene agents may be used in addition to inhaled corticosteroids if asthma symptoms remain uncontrolled.[16][17] Treatment of rapidly worsening symptoms is usually with an inhaled short-acting beta2 agonist such as salbutamol and corticosteroids taken by mouth.[8] In very severe cases, intravenous corticosteroids, magnesium sulfate, and hospitalization may be required.[18]
In 2019 asthma affected approximately 262 million people and caused approximately 461,000 deaths.[9] Most of the deaths occurred in the developing world.[5] Asthma often begins in childhood,[5] and the rates have increased significantly since the 1960s.[19] Asthma was recognized as early as Ancient Egypt.[20] The word asthma is from the Greek ἆσθμα, âsthma, which means 'panting'.[21]
Asthma is characterized by recurrent episodes of wheezing, shortness of breath, chest tightness, and coughing.[22] Sputum may be produced from the lung by coughing but is often hard to bring up.[23] During recovery from an asthma attack (exacerbation), it may appear pus-like due to high levels of white blood cells called eosinophils.[24] Symptoms are usually worse at night and in the early morning or in response to exercise or cold air.[25] Some people with asthma rarely experience symptoms, usually in response to triggers, whereas others may react frequently and readily and experience persistent symptoms.[26]
A number of other health conditions occur more frequently in people with asthma, including gastroesophageal reflux disease (GERD), rhinosinusitis, and obstructive sleep apnea.[27] Psychological disorders are also more common,[28] with anxiety disorders occurring in between 16 and 52% and mood disorders in 14–41%.[29] It is not known whether asthma causes psychological problems or psychological problems lead to asthma.[30] Current asthma, but not former asthma, is associated with increased all-cause mortality, heart disease mortality, and chronic lower respiratory tract disease mortality.[31] Asthma, particularly severe asthma, is strongly associated with development of chronic obstructive pulmonary disease (COPD).[32][33][34] Those with asthma, especially if it is poorly controlled, are at increased risk for radiocontrast reactions.[35]
Cavities occur more often in people with asthma.[36] This may be related to the effect of beta2 agonists decreasing saliva.[37] These medications may also increase the risk of dental erosions.[37]
Asthma is caused by a combination of complex and incompletely understood environmental and genetic interactions.[38][39] These influence both its severity and its responsiveness to treatment.[40] It is believed that the recent increased rates of asthma are due to changing epigenetics (heritable factors other than those related to the DNA sequence) and a changing living environment.[41] Asthma that starts before the age of 12 years old is more likely due to genetic influence, while onset after age 12 is more likely due to environmental influence.[42]
Many environmental factors have been associated with asthma's development and exacerbation, including allergens, air pollution, and other environmental chemicals.[43] There are some substances that are known to cause asthma in exposed people and they are called asthmagens. Some common asthmagens include ammonia, latex, pesticides, solder and welding fumes, metal or wood dusts, spraying of isocyanate paint in vehicle repair, formaldehyde, glutaraldehyde, anhydrides, glues, dyes, metal working fluids, oil mists, molds.[44][45] Smoking during pregnancy and after delivery is associated with a greater risk of asthma-like symptoms.[46] Low air quality from environmental factors such as traffic pollution or high ozone levels[47] has been associated with both asthma development and increased asthma severity.[48] Over half of cases in children in the United States occur in areas when air quality is below the EPA standards.[49] Low air quality is more common in low-income and minority communities.[50]
Exposure to indoor volatile organic compounds may be a trigger for asthma; formaldehyde exposure, for example, has a positive association.[51] Phthalates in certain types of PVC are associated with asthma in both children and adults.[52][53] While exposure to pesticides is linked to the development of asthma, a cause and effect relationship has yet to be established.[54][55] A meta-analysis concluded gas stoves are a major risk factor for asthma, finding around one in eight cases in the U.S. could be attributed to these.[56]
The majority of the evidence does not support a causal role between paracetamol (acetaminophen) or antibiotic use and asthma.[57][58] A 2014 systematic review found that the association between paracetamol use and asthma disappeared when respiratory infections were taken into account.[59] Maternal psychological stress during pregnancy is a risk factor for the child to develop asthma.[60]
Asthma is associated with exposure to indoor allergens.[61] Common indoor allergens include dust mites, cockroaches, animal dander (fragments of fur or feathers), and mold.[62][63] Efforts to decrease dust mites have been found to be ineffective on symptoms in sensitized subjects.[64][65] Weak evidence suggests that efforts to decrease mold by repairing buildings may help improve asthma symptoms in adults.[66] Certain viral respiratory infections, such as respiratory syncytial virus and rhinovirus,[21] may increase the risk of developing asthma when acquired as young children.[67] Certain other infections, however, may decrease the risk.[21]
The hygiene hypothesis attempts to explain the increased rates of asthma worldwide as a direct and unintended result of reduced exposure, during childhood, to non-pathogenic bacteria and viruses.[68][69] It has been proposed that the reduced exposure to bacteria and viruses is due, in part, to increased cleanliness and decreased family size in modern societies.[70] Exposure to bacterial endotoxin in early childhood may prevent the development of asthma, but exposure at an older age may provoke bronchoconstriction.[71] Evidence supporting the hygiene hypothesis includes lower rates of asthma on farms and in households with pets.[70]
Use of antibiotics in early life has been linked to the development of asthma.[72] Also, delivery via caesarean section is associated with an increased risk (estimated at 20–80%) of asthma – this increased risk is attributed to the lack of healthy bacterial colonization that the newborn would have acquired from passage through the birth canal.[73][74] There is a link between asthma and the degree of affluence which may be related to the hygiene hypothesis as less affluent individuals often have more exposure to bacteria and viruses.[75]
Family history is a risk factor for asthma, with many different genes being implicated.[77] If one identical twin is affected, the probability of the other having the disease is approximately 25%.[77] By the end of 2005, 25 genes had been associated with asthma in six or more separate populations, including GSTM1, IL10, CTLA-4, SPINK5, LTC4S, IL4R and ADAM33, among others.[78] Many of these genes are related to the immune system or modulating inflammation. Even among this list of genes supported by highly replicated studies, results have not been consistent among all populations tested.[78] In 2006 over 100 genes were associated with asthma in one genetic association study alone;[78] more continue to be found.[79]
Some genetic variants may only cause asthma when they are combined with specific environmental exposures.[38] An example is a specific single nucleotide polymorphism in the CD14 region and exposure to endotoxin (a bacterial product). Endotoxin exposure can come from several environmental sources including tobacco smoke, dogs, and farms. Risk for asthma, then, is determined by both a person's genetics and the level of endotoxin exposure.[76]
A triad of atopic eczema, allergic rhinitis and asthma is called atopy.[80] The strongest risk factor for developing asthma is a history of atopic disease;[67] with asthma occurring at a much greater rate in those who have either eczema or hay fever.[81] Asthma has been associated with eosinophilic granulomatosis with polyangiitis (formerly known as Churg–Strauss syndrome), an autoimmune disease and vasculitis.[82] Individuals with certain types of urticaria may also experience symptoms of asthma.[80]
There is a correlation between obesity and the risk of asthma with both having increased in recent years.[83][84] Several factors may be at play including decreased respiratory function due to a buildup of fat and the fact that adipose tissue leads to a pro-inflammatory state.[85]
Beta blocker medications such as propranolol can trigger asthma in those who are susceptible.[86] Cardioselective beta-blockers, however, appear safe in those with mild or moderate disease.[87][88] Other medications that can cause problems in asthmatics are angiotensin-converting enzyme inhibitors, aspirin, and NSAIDs.[89] Use of acid suppressing medication (proton pump inhibitors and H2 blockers) during pregnancy is associated with an increased risk of asthma in the child.[90]
Some individuals will have stable asthma for weeks or months and then suddenly develop an episode of acute asthma. Different individuals react to various factors in different ways.[91] Most individuals can develop severe exacerbation from a number of triggering agents.[91]
Home factors that can lead to exacerbation of asthma include dust, animal dander (especially cat and dog hair), cockroach allergens and mold.[91][92] Perfumes are a common cause of acute attacks in women and children. Both viral and bacterial infections of the upper respiratory tract can worsen the disease.[91] Psychological stress may worsen symptoms – it is thought that stress alters the immune system and thus increases the airway inflammatory response to allergens and irritants.[48][93]
Asthma exacerbations in school‐aged children peak in autumn, shortly after children return to school. This might reflect a combination of factors, including poor treatment adherence, increased allergen and viral exposure, and altered immune tolerance. There is limited evidence to guide possible approaches to reducing autumn exacerbations, but while costly, seasonal omalizumab treatment from four to six weeks before school return may reduce autumn asthma exacerbations.[94]
Asthma is the result of chronic inflammation of the conducting zone of the airways (most especially the bronchi and bronchioles), which subsequently results in increased contractability of the surrounding smooth muscles. This among other factors leads to bouts of narrowing of the airway and the classic symptoms of wheezing. The narrowing is typically reversible with or without treatment. Occasionally the airways themselves change.[22] Typical changes in the airways include an increase in eosinophils and thickening of the lamina reticularis. Chronically the airways' smooth muscle may increase in size along with an increase in the numbers of mucous glands. Other cell types involved include T lymphocytes, macrophages, and neutrophils. There may also be involvement of other components of the immune system, including cytokines, chemokines, histamine, and leukotrienes among others.[21]
While asthma is a well-recognized condition, there is not one universal agreed upon definition.[21] It is defined by the Global Initiative for Asthma as "a chronic inflammatory disorder of the airways in which many cells and cellular elements play a role. The chronic inflammation is associated with airway hyper-responsiveness that leads to recurrent episodes of wheezing, breathlessness, chest tightness and coughing particularly at night or in the early morning. These episodes are usually associated with widespread but variable airflow obstruction within the lung that is often reversible either spontaneously or with treatment".[22]
There is currently no precise test for the diagnosis, which is typically based on the pattern of symptoms and response to therapy over time.[6][21] Asthma may be suspected if there is a history of recurrent wheezing, coughing or difficulty breathing and these symptoms occur or worsen due to exercise, viral infections, allergens or air pollution.[95] Spirometry is then used to confirm the diagnosis.[95] In children under the age of six the diagnosis is more difficult as they are too young for spirometry.[96]
Spirometry is recommended to aid in diagnosis and management.[97][98] It is the single best test for asthma. If the FEV1 measured by this technique improves more than 12% and increases by at least 200 milliliters following administration of a bronchodilator such as salbutamol, this is supportive of the diagnosis. It however may be normal in those with a history of mild asthma, not currently acting up.[21] As caffeine is a bronchodilator in people with asthma, the use of caffeine before a lung function test may interfere with the results.[99] Single-breath diffusing capacity can help differentiate asthma from COPD.[21] It is reasonable to perform spirometry every one or two years to follow how well a person's asthma is controlled.[100]
The methacholine challenge involves the inhalation of increasing concentrations of a substance that causes airway narrowing in those predisposed. If negative it means that a person does not have asthma; if positive, however, it is not specific for the disease.[21]
Other supportive evidence includes: a ≥20% difference in peak expiratory flow rate on at least three days in a week for at least two weeks, a ≥20% improvement of peak flow following treatment with either salbutamol, inhaled corticosteroids or prednisone, or a ≥20% decrease in peak flow following exposure to a trigger.[101] Testing peak expiratory flow is more variable than spirometry, however, and thus not recommended for routine diagnosis. It may be useful for daily self-monitoring in those with moderate to severe disease and for checking the effectiveness of new medications. It may also be helpful in guiding treatment in those with acute exacerbations.[102]
Asthma is clinically classified according to the frequency of symptoms, forced expiratory volume in one second (FEV1), and peak expiratory flow rate.[12] Asthma may also be classified as atopic (extrinsic) or non-atopic (intrinsic), based on whether symptoms are precipitated by allergens (atopic) or not (non-atopic).[13] While asthma is classified based on severity, at the moment there is no clear method for classifying different subgroups of asthma beyond this system.[103] Finding ways to identify subgroups that respond well to different types of treatments is a current critical goal of asthma research.[103] Recently, asthma has been classified based on whether it is associated with type 2 or non–type 2 inflammation. This approach to immunologic classification is driven by a developing understanding of the underlying immune processes and by the development of therapeutic approaches that target type 2 inflammation.[104]
Although asthma is a chronic obstructive condition, it is not considered as a part of chronic obstructive pulmonary disease, as this term refers specifically to combinations of disease that are irreversible such as bronchiectasis and emphysema.[105] Unlike these diseases, the airway obstruction in asthma is usually reversible; however, if left untreated, the chronic inflammation from asthma can lead the lungs to become irreversibly obstructed due to airway remodeling.[106] In contrast to emphysema, asthma affects the bronchi, not the alveoli.[107] The combination of asthma with a component of irreversible airway obstruction has been termed the asthma-chronic obstructive disease (COPD) overlap syndrome (ACOS). Compared to other people with "pure" asthma or COPD, people with ACOS exhibit increased morbidity, mortality and possibly more comorbidities.[108]
An acute asthma exacerbation is commonly referred to as an asthma attack. The classic symptoms are shortness of breath, wheezing, and chest tightness.[21] The wheezing is most often when breathing out.[110] While these are the primary symptoms of asthma,[111] some people present primarily with coughing, and in severe cases, air motion may be significantly impaired such that no wheezing is heard.[109] In children, chest pain is often present.[112]
Signs occurring during an asthma attack include the use of accessory muscles of respiration (sternocleidomastoid and scalene muscles of the neck), there may be a paradoxical pulse (a pulse that is weaker during inhalation and stronger during exhalation), and over-inflation of the chest.[113] A blue color of the skin and nails may occur from lack of oxygen.[114]
In a mild exacerbation the peak expiratory flow rate (PEFR) is ≥200 L/min, or ≥50% of the predicted best.[115] Moderate is defined as between 80 and 200 L/min, or 25% and 50% of the predicted best, while severe is defined as ≤ 80 L/min, or ≤25% of the predicted best.[115]
Acute severe asthma, previously known as status asthmaticus, is an acute exacerbation of asthma that does not respond to standard treatments of bronchodilators and corticosteroids.[116] Half of cases are due to infections with others caused by allergen, air pollution, or insufficient or inappropriate medication use.[116]
Brittle asthma is a kind of asthma distinguishable by recurrent, severe attacks.[109] Type 1 brittle asthma is a disease with wide peak flow variability, despite intense medication. Type 2 brittle asthma is background well-controlled asthma with sudden severe exacerbations.[109]
Exercise can trigger bronchoconstriction both in people with or without asthma.[117] It occurs in most people with asthma and up to 20% of people without asthma.[117] Exercise-induced bronchoconstriction is common in professional athletes. The highest rates are among cyclists (up to 45%), swimmers, and cross-country skiers.[118] While it may occur with any weather conditions, it is more common when it is dry and cold.[119] Inhaled beta2 agonists do not appear to improve athletic performance among those without asthma;[120] however, oral doses may improve endurance and strength.[121][122]
Asthma as a result of (or worsened by) workplace exposures is a commonly reported occupational disease.[123] Many cases, however, are not reported or recognized as such.[124][125] It is estimated that 5–25% of asthma cases in adults are work-related. A few hundred different agents have been implicated, with the most common being isocyanates, grain and wood dust, colophony, soldering flux, latex, animals, and aldehydes. The employment associated with the highest risk of problems include those who spray paint, bakers and those who process food, nurses, chemical workers, those who work with animals, welders, hairdressers and timber workers.[123]
Aspirin-exacerbated respiratory disease (AERD), also known as aspirin-induced asthma, affects up to 9% of asthmatics.[126] AERD consists of asthma, nasal polyps, sinus disease, and respiratory reactions to aspirin and other NSAID medications (such as ibuprofen and naproxen).[127] People often also develop loss of smell and most experience respiratory reactions to alcohol.[128]
Alcohol may worsen asthmatic symptoms in up to a third of people.[129] This may be even more common in some ethnic groups such as the Japanese and those with aspirin-exacerbated respiratory disease.[129] Other studies have found improvement in asthmatic symptoms from alcohol.[129]
Non-atopic asthma, also known as intrinsic or non-allergic, makes up between 10 and 33% of cases. There is negative skin test to common inhalant allergens. Often it starts later in life, and women are more commonly affected than men. Usual treatments may not work as well.[130] The concept that "non-atopic" is synonymous with "non-allergic" is called into question by epidemiological data that the prevalence of asthma is closely related to the serum IgE level standardized for age and sex (P<0.0001), indicating that asthma is almost always associated with some sort of IgE-related reaction and therefore has an allergic basis, although not all the allergic stimuli that cause asthma appear to have been included in the battery of aeroallergens studied (the "missing antigen(s)" hypothesis).[131] For example, an updated systematic review and meta-analysis of population-attributable risk (PAR) of Chlamydia pneumoniae biomarkers in chronic asthma found that the PAR for C. pneumoniae-specific IgE was 47%.[132]
Infectious asthma is an easily identified clinical presentation.[133] When queried, asthma patients may report that their first asthma symptoms began after an acute lower respiratory tract illness. This type of history has been labelled the "infectious asthma" (IA) syndrome,[134] or as "asthma associated with infection" (AAWI)[135] to distinguish infection-associated asthma initiation from the well known association of respiratory infections with asthma exacerbations. Reported clinical prevalences of IA for adults range from around 40% in a primary care practice[134] to 70% in a specialty practice treating mainly severe asthma patients.[136] Additional information on the clinical prevalence  of IA in adult-onset asthma is unavailable because clinicians are not trained to elicit this type of history routinely, and recollection in child-onset asthma is challenging. A population-based incident case-control study in a geographically defined area of Finland reported that 35.8% of new-onset asthma cases had experienced acute bronchitis or pneumonia in the year preceding asthma onset, representing a significantly higher risk compared to randomly selected controls (Odds ratio 7.2, 95% confidence interval 5.2-10).[137]
Many other conditions can cause symptoms similar to those of asthma. In children, symptoms may be due to other upper airway diseases such as allergic rhinitis and sinusitis, as well as other causes of airway obstruction including foreign body aspiration, tracheal stenosis, laryngotracheomalacia, vascular rings, enlarged lymph nodes or neck masses.[138] Bronchiolitis and other viral infections may also produce wheezing.[139] According to European Respiratory Society, it may not be suitable to label wheezing preschool children with the term "asthma" because there is lack of clinical data on inflammation in airways.[140] In adults, COPD, congestive heart failure, airway masses, as well as drug-induced coughing due to ACE inhibitors may cause similar symptoms. In both populations vocal cord dysfunction may present similarly.[138]
Chronic obstructive pulmonary disease can coexist with asthma and can occur as a complication of chronic asthma. After the age of 65, most people with obstructive airway disease will have asthma and COPD. In this setting, COPD can be differentiated by increased airway neutrophils, abnormally increased wall thickness, and increased smooth muscle in the bronchi. However, this level of investigation is not performed due to COPD and asthma sharing similar principles of management: corticosteroids, long-acting beta-agonists, and smoking cessation.[141] It closely resembles asthma in symptoms, is correlated with more exposure to cigarette smoke, an older age, less symptom reversibility after bronchodilator administration, and decreased likelihood of family history of atopy.[142][143]
The evidence for the effectiveness of measures to prevent the development of asthma is weak.[144] The World Health Organization recommends decreasing risk factors such as tobacco smoke, air pollution, chemical irritants including perfume, and the number of lower respiratory infections.[145][146] Other efforts that show promise include: limiting smoke exposure in utero, breastfeeding, and increased exposure to daycare or large families, but none are well supported enough to be recommended for this indication.[144]
Early pet exposure may be useful.[147] Results from exposure to pets at other times are inconclusive[148] and it is only recommended that pets be removed from the home if a person has allergic symptoms to said pet.[149]
Dietary restrictions during pregnancy or when breastfeeding have not been found to be effective at preventing asthma in children and are not recommended.[149] Omega-3 consumption, mediterranean diet and anti-oxidants have been suggested by some studies that might help preventing crisis but the evidence is still inconclusive.[150]
Reducing or eliminating compounds known to sensitive people from the work place may be effective.[123] It is not clear if annual influenza vaccinations affects the risk of exacerbations.[151] Immunization, however, is recommended by the World Health Organization.[152] Smoking bans are effective in decreasing exacerbations of asthma.[153]
While there is no cure for asthma, symptoms can typically be improved.[154] The most effective treatment for asthma is identifying triggers, such as cigarette smoke, pets or other allergens, and eliminating exposure to them. If trigger avoidance is insufficient, the use of medication is recommended. Pharmaceutical drugs are selected based on, among other things, the severity of illness and the frequency of symptoms. Specific medications for asthma are broadly classified into fast-acting and long-acting categories.[155][156] The medications listed below have demonstrated efficacy in improving asthma symptoms; however, real world use-effectiveness is limited as around half of people with asthma worldwide remain sub-optimally controlled, even when treated.[157][158][159] People with asthma may remain sub-optimally controlled either because optimum doses of asthma medications do not work (called "refractory" asthma) or because individuals are either unable (e.g. inability to afford treatment, poor inhaler technique) or unwilling (e.g., wish to avoid side effects of corticosteroids) to take optimum doses of prescribed asthma medications (called "difficult to treat" asthma). In practice, it is not possible to distinguish "refractory" from "difficult to treat" categories for patients who have never taken optimum doses of asthma medications. A related issue is that the asthma efficacy trials upon which the pharmacological treatment guidelines are based have systematically excluded the majority of people with asthma.[160][161] For example, asthma efficacy treatment trials always exclude otherwise eligible people who smoke, and smoking diminishes the efficacy of inhaled corticosteroids, the mainstay of asthma control management.[162][163][164]
Bronchodilators are recommended for short-term relief of symptoms. In those with occasional attacks, no other medication is needed. If mild persistent disease is present (more than two attacks a week), low-dose inhaled corticosteroids or alternatively, a leukotriene antagonist or a mast cell stabilizer by mouth is recommended. For those who have daily attacks, a higher dose of inhaled corticosteroids is used. In a moderate or severe exacerbation, corticosteroids by mouth are added to these treatments.[8]
People with asthma have higher rates of anxiety, psychological stress, and depression.[165][166] This is associated with poorer asthma control.[165] Cognitive behavioral therapy may improve quality of life, asthma control, and anxiety levels in people with asthma.[165]
Improving people's knowledge about asthma and using a written action plan has been identified as an important component of managing asthma.[167] Providing educational sessions that include information specific to a person's culture is likely effective.[168] More research is necessary to determine if increasing preparedness and knowledge of asthma among school staff and families using home-based and school interventions results in long term improvements in safety for children with asthma.[169][170][171] School-based asthma self-management interventions, which attempt to improve knowledge of asthma, its triggers and the importance of regular practitioner review, may reduce hospital admissions and emergency department visits. These interventions may also reduce the number of days children experience asthma symptoms and may lead to small improvements in asthma-related quality of life.[172] More research is necessary to determine if shared-decision-making is helpful for managing adults with asthma[173] or if a personalized asthma action plan is effective and necessary.[174] Some people with asthma use pulse oximeters to monitor their own blood oxygen levels during an asthma attack. However, there is no evidence regarding the use in these instances.[175]
Avoidance of triggers is a key component of improving control and preventing attacks. The most common triggers include allergens, smoke (from tobacco or other sources), air pollution, nonselective beta-blockers, and sulfite-containing foods.[176][177] Cigarette smoking and second-hand smoke (passive smoke) may reduce the effectiveness of medications such as corticosteroids.[178] Laws that limit smoking decrease the number of people hospitalized for asthma.[153] Dust mite control measures, including air filtration, chemicals to kill mites, vacuuming, mattress covers and others methods had no effect on asthma symptoms.[64] There is insufficient evidence to suggest that dehumidifiers are helpful for controlling asthma.[179]
Overall, exercise is beneficial in people with stable asthma.[180] Yoga could provide small improvements in quality of life and symptoms in people with asthma.[181] More research is necessary to determine how effective weight loss is on improving quality of life, the usage of health care services, and adverse effects for people of all ages with asthma.[182][183]
Medications used to treat asthma are divided into two general classes: quick-relief medications used to treat acute symptoms; and long-term control medications used to prevent further exacerbation.[155] Antibiotics are generally not needed for sudden worsening of symptoms or for treating asthma at any time.[184][185]
For children with asthma which is well-controlled on combination therapy of inhaled corticosteroids (ICS) and long-acting beta2-agonists (LABA), the benefits and harms of stopping LABA and stepping down to ICS-only therapy are uncertain.[222] In adults who have stable asthma while they are taking a combination of LABA and inhaled corticosteroids (ICS), stopping LABA may increase the risk of asthma exacerbations that require treatment with corticosteroids by mouth.[223] Stopping LABA probably makes little or no important difference to asthma control or asthma-related quality of life.[223] Whether or not stopping LABA increases the risk of serious adverse events or exacerbations requiring an emergency department visit or hospitalisation is uncertain.[223]
Medications are typically provided as metered-dose inhalers (MDIs) in combination with an asthma spacer or as a dry powder inhaler. The spacer is a plastic cylinder that mixes the medication with air, making it easier to receive a full dose of the drug. A nebulizer may also be used. Nebulizers and spacers are equally effective in those with mild to moderate symptoms. However, insufficient evidence is available to determine whether a difference exists in those with severe disease.[224] For delivering short-acting beta-agonists in acute asthma in children, spacers may have advantages compared to nebulisers, but children with life-threatening asthma have not been studied.[225] There is no strong evidence for the use of intravenous LABA for adults or children who have acute asthma.[226] There is insufficient evidence to directly compare the effectiveness of a metered-dose inhaler attached to a homemade spacer compared to commercially available spacer for treating children with asthma.[227]
Long-term use of inhaled corticosteroids at conventional doses carries a minor risk of adverse effects.[228] Risks include thrush, the development of cataracts, and a slightly slowed rate of growth.[228][229][230] Rinsing the mouth after the use of inhaled steroids can decrease the risk of thrush.[231] Higher doses of inhaled steroids may result in lower bone mineral density.[232]
Inflammation in the lungs can be estimated by the level of exhaled nitric oxide.[233][234] The use of exhaled nitric oxide levels (FeNO) to guide asthma medication dosing may have small benefits for preventing asthma attacks but the potential benefits are not strong enough for this approach to be universally recommended as a method to guide asthma therapy in adults or children.[233][234]
When asthma is unresponsive to usual medications, other options are available for both emergency management and prevention of flareups. Additional options include:
Staying with a treatment approach for preventing asthma exacerbations can be challenging, especially if the person is required to take medicine or treatments daily.[255] Reasons for low adherence range from a conscious decision to not follow the suggested medical treatment regime for various reasons including avoiding potential side effects, misinformation, or other beliefs about the medication.[255] Problems accessing the treatment and problems administering the treatment effectively can also result in lower adherence. Various approaches have been undertaken to try and improve adherence to treatments to help people prevent serious asthma exacerbations including digital interventions.[255]
Many people with asthma, like those with other chronic disorders, use alternative treatments; surveys show that roughly 50% use some form of unconventional therapy.[256][257] There is little data to support the effectiveness of most of these therapies.
Evidence is insufficient to support the usage of vitamin C or vitamin E for controlling asthma.[258][259] There is tentative support for use of vitamin C in exercise induced bronchospasm.[260] Fish oil dietary supplements (marine n-3 fatty acids)[261] and reducing dietary sodium[262] do not appear to help improve asthma control. In people with mild to moderate asthma, treatment with vitamin D supplementation or its hydroxylated metabolites does not reduce acute exacerbations or improve control.[263] There is no strong evidence to suggest that vitamin D supplements improve day-to-day asthma symptoms or a person's lung function.[263] There is no strong evidence to suggest that adults with asthma should avoid foods that contain monosodium glutamate (MSG).[264] There have not been enough high-quality studies performed to determine if children with asthma should avoid eating food that contains MSG.[264]
Acupuncture is not recommended for the treatment as there is insufficient evidence to support its use.[265][266] Air ionisers show no evidence that they improve asthma symptoms or benefit lung function; this applied equally to positive and negative ion generators.[267] Manual therapies, including osteopathic, chiropractic, physiotherapeutic and respiratory therapeutic maneuvers, have insufficient evidence to support their use in treating asthma.[268]  Pulmonary rehabilitation, however, may improve quality of life and functional exercise capacity when compared to usual care for adults with asthma.[269] The Buteyko breathing technique for controlling hyperventilation may result in a reduction in medication use; however, the technique does not have any effect on lung function.[156]  Thus an expert panel felt that evidence was insufficient to support its use.[265] There is no clear evidence that breathing exercises are effective for treating children with asthma.[270]
The prognosis for asthma is generally good, especially for children with mild disease.[271] Mortality has decreased over the last few decades due to better recognition and improvement in care.[272] In 2010 the death rate was 170 per million for males and 90 per million for females.[273] Rates vary between countries by 100-fold.[273]
Globally it causes moderate or severe disability in 19.4 million people as of 2004[update] (16 million of which are in low and middle income countries).[274] Of asthma diagnosed during childhood, half of cases will no longer carry the diagnosis after a decade.[77] Airway remodeling is observed, but it is unknown whether these represent harmful or beneficial changes.[275] Early treatment with corticosteroids seems to prevent or ameliorates a decline in lung function.[276] Asthma in children also has negative effects on quality of life of their parents.[277]
In 2019, approximately 262 million people worldwide were affected by asthma and approximately 461,000 people died from the disease.[9] Rates vary between countries with prevalences between 1 and 18%.[22] It is more common in developed than developing countries.[22] One thus sees lower rates in Asia, Eastern Europe and Africa.[21] Within developed countries it is more common in those who are economically disadvantaged while in contrast in developing countries it is more common in the affluent.[22] The reason for these differences is not well known.[22] Low and middle income countries make up more than 80% of the mortality.[280]
While asthma is twice as common in boys as girls,[22] severe asthma occurs at equal rates.[281] In contrast adult women have a higher rate of asthma than men[22] and it is more common in the young than the old.[21] In 2010, children with asthma experienced over 900,000 emergency department visits, making it the most common reason for admission to the hospital following an emergency department visit in the US in 2011.[282][283]
Global rates of asthma have increased significantly between the 1960s and 2008[19][284] with it being recognized as a major public health problem since the 1970s.[21] Rates of asthma have plateaued in the developed world since the mid-1990s with recent increases primarily in the developing world.[285] Asthma affects approximately 7% of the population of the United States[201] and 5% of people in the United Kingdom.[286] Canada, Australia and New Zealand have rates of about 14–15%.[287]
The average death rate from 2011 to 2015 from asthma in the UK was about 50% higher than the average for the European Union and had increased by about 5% in that time.[288] Children are more likely see a physician due to asthma symptoms after school starts in September.[289]
Population-based epidemiological studies describe temporal associations between acute respiratory illnesses, asthma, and development of severe asthma with irreversible airflow limitation (known as the asthma-chronic obstructive pulmonary disease "overlap" syndrome, or ACOS).[290][291][292] Additional prospective population-based data indicate that ACOS seems to represent a form of severe asthma, characterised by more frequent hospitalisations, and to be the result of early-onset asthma that has progressed to fixed airflow obstruction.[293]
From 2000 to 2010, the average cost per asthma-related hospital stay in the United States for children remained relatively stable at about $3,600, whereas the average cost per asthma-related hospital stay for adults increased from $5,200 to $6,600.[294] In 2010, Medicaid was the most frequent primary payer among children and adults aged 18–44 years in the United States; private insurance was the second most frequent payer.[294] Among both children and adults in the lowest income communities in the United States there is a higher rate of hospital stays for asthma in 2010 than those in the highest income communities.[294]
Asthma was recognized in ancient Egypt and was treated by drinking an incense mixture known as kyphi.[20] It was officially named as a specific respiratory problem by Hippocrates circa 450 BC, with the Greek word for "panting" forming the basis of our modern name.[21] In 200 BC it was believed to be at least partly related to the emotions.[29] In the 12th century the Jewish physician-philosopher Maimonides wrote a treatise on asthma in Arabic, based partly on Arabic sources, in which he discussed the symptoms, proposed various dietary and other means of treatment, and emphasized the importance of climate and clean air.[295] Chinese Traditional Medicine also offered medication for asthma, as indicated by a surviving 14th century manuscript curated by the Wellcome Foundation.[296]
In 1873, one of the first papers in modern medicine on the subject tried to explain the pathophysiology of the disease while one in 1872, concluded that asthma can be cured by rubbing the chest with chloroform liniment.[297][298] Medical treatment in 1880 included the use of intravenous doses of a drug called pilocarpine.[299] In 1886, F. H. Bosworth theorized a connection between asthma and hay fever.[300] Epinephrine was first referred to in the treatment of asthma in 1905.[301] Oral corticosteroids began to be used for this condition in the 1950s while inhaled corticosteroids and selective short acting beta agonist came into wide use in the 1960s.[302][303]
A well-documented case in the 19th century was that of young Theodore Roosevelt (1858–1919). At that time there was no effective treatment. Roosevelt's youth was in large part shaped by his poor health partly related to his asthma. He experienced recurring nighttime asthma attacks that felt as if he was being smothered to death, terrifying the boy and his parents.[304]
During the 1930s to 1950s, asthma was known as one of the "holy seven" psychosomatic illnesses. Its cause was considered to be psychological, with treatment often based on psychoanalysis and other talking cures.[305] As these psychoanalysts interpreted the asthmatic wheeze as the suppressed cry of the child for its mother, they considered the treatment of depression to be especially important for individuals with asthma.[305]
In January 2021, an appeal court in France overturned a deportation order against a 40-year-old Bangladeshi man, who was a patient of asthma. His lawyers had argued that the dangerous levels of pollution in Bangladesh could possibly lead to worsening of his health condition, or even premature death.[306]

Kidney disease, or renal disease, technically referred to as nephropathy, is damage to or disease of a kidney. Nephritis is an inflammatory kidney disease and has several types according to the location of the inflammation. Inflammation can be diagnosed by blood tests. Nephrosis is non-inflammatory kidney disease. Nephritis and nephrosis can give rise to nephritic syndrome and nephrotic syndrome respectively. Kidney disease usually causes a loss of kidney function to some degree and can result in kidney failure, the complete loss of kidney function. Kidney failure is known as the end-stage of kidney disease, where dialysis or a kidney transplant is the only treatment option.
Chronic kidney disease is defined as prolonged kidney abnormalities (functional and/or structural in nature) that last for more than three months.[1] Acute kidney disease is now termed acute kidney injury and is marked by the sudden reduction in kidney function over seven days. In 2007, about one in eight Americans had chronic kidney disease.[2] This rate is increasing over time[1] to where about 1 in 7 Americans are estimated to have CKD as of 2021.[3]
Causes of kidney disease include deposition of the Immunoglobulin A antibodies in the glomerulus, administration of analgesics, xanthine oxidase deficiency, toxicity of chemotherapy agents, and a long-term exposure to lead or its salts. Chronic conditions that can produce nephropathy include systemic lupus erythematosus, diabetes mellitus and high blood pressure (hypertension), which lead to diabetic nephropathy and hypertensive nephropathy, respectively.
One cause of nephropathy is the long term usage of pain medications known as analgesics. The pain medicines which can cause kidney problems include aspirin, acetaminophen, and nonsteroidal anti-inflammatory drugs (NSAIDs).  This form of nephropathy is "chronic analgesic nephritis," a chronic inflammatory change characterized by loss and atrophy of tubules and interstitial fibrosis and inflammation (BRS Pathology, 2nd edition).
Specifically, long-term use of the analgesic phenacetin has been linked to renal papillary necrosis (necrotizing papillitis).
Diabetic nephropathy is a progressive kidney disease caused by angiopathy of the capillaries in the glomeruli. It is characterized by nephrotic syndrome and diffuse scarring of the glomeruli. It is particularly associated with poorly managed diabetes mellitus and is a primary reason for dialysis in many developed countries. It is classified as a small blood vessel complication of diabetes.[4]
Gabow 1990 talks about Autosomal Dominant Polycystic Kidney disease and how this disease is genetic. They go on to say "Autosomal dominant polycystic kidney disease (ADPKD) is the most common genetic disease, affecting a half million Americans. The clinical phenotype can result from at least two different gene defects. One gene that can cause ADPKD has been located on the short arm of chromosome 16."[5] The same article also goes on to say that millions of Americans are effected by this disease and is very common.
Yende & Parikh 2021 talk about the effects that COVID can have on a person that has a pre-existing health issue regarding kidney diseases. "frailty, chronic diseases, disability and immunodeficiency are at increased risk of kidney disease and progression to kidney failure, and infection with SARS-CoV-2 can further increase this risk" (Long COVID and Kidney Disease, 2021).[6]
Higher dietary intake of animal protein, animal fat, and cholesterol may increase risk for microalbuminuria, a sign of kidney function decline,[7] and generally, diets higher in fruits, vegetables, and whole grains but lower in meat and sweets may be protective against kidney function decline.[8] This may be because sources of animal protein, animal fat, and cholesterol, and sweets are more acid-producing, while fruits, vegetables, legumes, and whole grains are more base-producing.[9][10][11][12][13][14][15][16][17][18]
IgA nephropathy is the most common glomerulonephritis throughout the world [19] Primary IgA nephropathy is characterized by deposition of the IgA antibody in the glomerulus. The classic presentation (in 40-50% of the cases) is episodic frank hematuria which usually starts within a day or two of a non-specific upper respiratory tract infection (hence synpharyngitic) as opposed to post-streptococcal glomerulonephritis which occurs some time (weeks) after initial infection. Less commonly gastrointestinal or urinary infection can be the inciting agent. All of these infections have in common the activation of mucosal defenses and hence IgA antibody production.
Kidney disease induced by iodinated contrast media (ICM) is called CIN (= contrast induced nephropathy) or contrast-induced AKI (= acute kidney injury). Currently, the underlying mechanisms are unclear. But there is a body of evidence that several factors including apoptosis-induction seem to play a role.[20]
Lithium, a medication commonly used to treat bipolar disorder and schizoaffective disorders, can cause nephrogenic diabetes insipidus; its long-term use can lead to nephropathy.[21]
Despite expensive treatments, lupus nephritis remains a major cause of morbidity and mortality in people with relapsing or refractory lupus nephritis.[22]
Another possible cause of Kidney disease is due to decreased function of xanthine oxidase in the purine degradation pathway. Xanthine oxidase will degrade hypoxanthine to xanthine and then to uric acid. Xanthine is not very soluble in water; therefore, an increase in xanthine forms crystals (which can lead to kidney stones) and result in damage to the kidney. Xanthine oxidase inhibitors, like allopurinol, can cause nephropathy.
Additional possible cause of nephropathy is due to the formation of cysts or pockets containing fluid within the kidneys. These cysts become enlarged with the progression of aging causing renal failure. Cysts may also form in other organs including the liver, brain, and ovaries. Polycystic Kidney Disease is a genetic disease caused by mutations in the PKD1, PKD2, and PKHD1 genes. This disease affects about half a million people in the US. Polycystic kidneys are susceptible to infections and cancer.
Nephropathy can be associated with some therapies used to treat cancer.  The most common form of kidney disease in cancer patients is Acute Kidney Injury (AKI) which can usually be due to volume depletion from vomiting and diarrhea that occur following chemotherapy or occasionally due to kidney toxicities of chemotherapeutic agents.  Kidney failure from break down of cancer cells, usually after chemotherapy, is unique to onconephrology. Several chemotherapeutic agents, for example Cisplatin, are associated with acute and chronic kidney injuries.[23] Newer agents such as anti Vascular Endothelial Growth Factor (anti VEGF) are also associated with similar injuries, as well as proteinuria, hypertension and thrombotic microangiopathy.[24]
The standard diagnostic workup of suspected kidney disease includes a medical history, physical examination, a urine test, and an ultrasound of the kidneys (renal ultrasonography). An ultrasound is essential in the diagnosis and management of kidney disease.[25]
Treatment approaches for kidney disease focus on managing the symptoms, controlling the progression, and also treating co-morbidities that a person may have.[1]
Millions of people across the world have kidney disease. Of those millions, several thousand will need dialysis or a kidney transplant at its end-stage.[26] In the United States, as of 2008, 16,500 people needed a kidney transplant.[26] Of those, 5,000 died while waiting for a transplant.[26] Currently, there is a shortage of donors, and in 2007 there were only 64,606 kidney transplants in the world.[26] This shortage of donors is causing countries to place monetary value on kidneys.  Countries such as Iran and Singapore are eliminating their lists by paying their citizens to donate.  Also, the black market accounts for 5-10 percent of transplants that occur worldwide.[26]  The act of buying an organ through the black market is illegal in the United States.[27] To be put on the waiting list for a kidney transplant, patients must first be referred by a physician, then they must choose and contact a donor hospital.  Once they choose a donor hospital, patients must then receive an evaluation to make sure they are sustainable to receive a transplant. In order to be a match for a kidney transplant, patients must match blood type and human leukocyte antigen factors with their donors. They must also have no reactions to the antibodies from the donor's kidneys.[28][26]
Kidney disease can have serious consequences if it cannot be controlled effectively. Generally, the progression of kidney disease is from mild to serious. Some kidney diseases can cause kidney failure.


Lung cancer, also known as lung carcinoma, is a malignant tumor that begins in the lung. Lung cancer is caused by genetic damage to the DNA of cells in the airways, often caused by cigarette smoking or inhaling damaging chemicals. Damaged airway cells gain the ability to multiply unchecked, causing the growth of a tumor. Without treatment, tumors spread throughout the lung, damaging lung function. Eventually lung tumors metastasize, spreading to other parts of the body. 
Early lung cancer often has no symptoms and can only be detected by medical imaging. As the cancer progresses, most people experience nonspecific respiratory problems: coughing, shortness of breath, or chest pain. Other symptoms depend on the location and size of the tumor. Those suspected of having lung cancer typically undergo a series of imaging tests to determine the location and extent of any tumors. Definitive diagnosis of lung cancer requires a biopsy of the suspected tumor be examined by a pathologist under a microscope. In addition to recognizing cancerous cells, a pathologist can classify the tumor according to the type of cells it originates from. Around 15% of cases are small-cell lung cancer, and the remaining 85% (the non-small-cell lung cancers) are adenocarcinomas, squamous-cell carcinomas, and large-cell carcinomas. After diagnosis, further imaging and biopsies are done to determine the cancer's stage based on how far it has spread.
Treatment for early stage lung cancer includes surgery to remove the tumor, sometimes followed by radiation therapy and chemotherapy to kill any remaining cancer cells. Later stage cancer is treated with radiation therapy and chemotherapy alongside drug treatments that target specific cancer subtypes. Even with treatment, only around 20% of people survive five years on from their diagnosis.[4] Survival rates are higher in those diagnosed at an earlier stage, diagnosed at a younger age, and in women compared to men.
Most lung cancer cases are caused by tobacco smoking. The remainder are caused by exposure to hazardous substances like asbestos and radon gas, or by genetic mutations that arise by chance. Consequently, lung cancer prevention efforts encourage people to avoid hazardous chemicals and quit smoking. Quitting smoking both reduces one's chance of developing lung cancer and improves treatment outcomes in those already diagnosed with lung cancer.
Lung cancer is the most diagnosed and deadliest cancer worldwide, with 2.2 million cases in 2020 resulting in 1.8 million deaths.[3] Lung cancer is rare in those younger than 40; the average age at diagnosis is 70 years, and the average age at death 72.[2] Incidence and outcomes vary widely across the world, depending on patterns of tobacco use. Prior to the advent of cigarette smoking in the 20th century, lung cancer was a rare disease. In the 1950s and 1960s, increasing evidence linked lung cancer and tobacco use, culminating in declarations by most large national health bodies discouraging tobacco use.
Early lung cancer often has no symptoms. When symptoms do arise they are often nonspecific respiratory problems – coughing, shortness of breath, or chest pain – that can differ from person to person.[5] Those who experience coughing tend to report either a new cough, or an increase in the frequency or strength of a pre-existing cough.[5] Around one in four cough up blood, ranging from small streaks in the sputum to large amounts.[6][5] Around half of those diagnosed with lung cancer experience shortness of breath, while 25–50% experience a dull, persistent chest pain that remains in the same location over time.[5] In addition to respiratory symptoms, some experience systemic symptoms including loss of appetite, weight loss, general weakness, fever, and night sweats.[5][7]
Some less common symptoms suggest tumors in particular locations. Tumors in the thorax can cause breathing problems by obstructing the trachea or disrupting the nerve to the diaphragm; difficulty swallowing by compressing the esophagus; hoarseness by disrupting the nerves of the larynx; and Horner's syndrome by disrupting the sympathetic nervous system.[5][7] Horner's syndrome is also common in tumors at the top of the lung, known as Pancoast tumors, which also cause shoulder pain that radiates down the little-finger side of the arm as well as destruction of the topmost ribs.[7] Swollen lymph nodes above the collarbone can indicate a tumor that has spread within the chest.[5] Tumors obstructing bloodflow to the heart can cause superior vena cava syndrome (swelling of the upper body and shortness of breath), while tumors infiltrating the area around the heart can cause fluid buildup around the heart, arrythmia (irregular heartbeat), and heart failure.[7]
About one in three people diagnosed with lung cancer have symptoms caused by metastases in sites other than the lungs.[7] Lung cancer can metastasize anywhere in the body, with different symptoms depending on the location. Brain metastases can cause headache, nausea, vomiting, seizures, and neurological deficits. Bone metastases can cause pain, bone fractures, and compression of the spinal cord. Metastasis into the bone marrow can deplete blood cells and cause leukoerythroblastosis (immature cells in the blood).[7] Liver metastases can cause liver enlargement, pain in the right upper quadrant of the abdomen, fever, and weight loss.[7]
Lung tumors often cause the release of body-altering hormones, which cause unusual symptoms, called paraneoplastic syndromes.[7] Inappropriate hormone release can cause dramatic shifts in concentrations of blood minerals. Most common is hypercalcemia (high blood calcium) caused by over-production of parathyroid hormone-related protein or parathyroid hormone. Hypercalcemia can manifest as nausea, vomiting, abdominal pain, constipation, increased thirst, frequent urination, and altered mental status.[7] Those with lung cancer also commonly experience hypokalemia (low potassium) due to inappropriate secretion of adrenocorticotropic hormone, as well as hyponatremia (low sodium) due to overproduction of antidiuretic hormone or atrial natriuretic peptide.[7] About one of three people with lung cancer develop nail clubbing, while up to one in ten experience hypertrophic pulmonary osteoarthropathy (nail clubbing, joint soreness, and skin thickening). A variety of autoimmune disorders can arise as paraneoplastic syndromes in those with lung cancer, including Lambert–Eaton myasthenic syndrome (which causes muscle weakness), sensory neuropathies, muscle inflammation, brain swelling, and autoimmune deterioration of cerebellum, limbic system, or brainstem.[7] Up to one in twelve people with lung cancer have paraneoplastic blood clotting, including migratory venous thrombophlebitis, clots in the heart, and disseminated intravascular coagulation (clots throughout the body).[7] Paraneoplastic syndromes involving the skin and kidneys are rare, each occurring in up to 1% of those with lung cancer.[7]
A person suspected of having lung cancer will have imaging tests done to evaluate the presence, extent, and location of tumors. First, many primary care providers perform a chest X-ray to look for a mass inside the lung.[8] The X-ray may reveal an obvious mass, the widening of the mediastinum (suggestive of spread to lymph nodes there), atelectasis (lung collapse), consolidation (pneumonia), or pleural effusion;[9] however, some lung tumors are not visible by X-ray.[5] Next, many undergo computed tomography (CT) scanning, which can reveal the sizes and locations of tumors.[8][10]
A definitive diagnosis of lung cancer requires a biopsy of the suspected tissue be histologically examined for cancer cells.[11] Given the location of lung cancer tumors, biopsies can often be obtained by minimally invasive techniques: a fiberoptic bronchoscope that can retrieve tissue (sometimes guided by endobronchial ultrasound), fine needle aspiration, or other imaging-guided biopsy through the skin.[11] Those who cannot undergo a typical biopsy procedure may instead have a liquid biopsy taken (that is, a sample of some body fluid) which may contain circulating tumor DNA that can be detected.[12]
Imaging is also used to assess the extent of cancer spread. Positron emission tomography (PET) scanning or combined PET-CT scanning is often used to locate metastases in the body. Since PET scanning is less sensitive in the brain, the National Comprehensive Cancer Network recommends magnetic resonance imaging (MRI) – or CT where MRI is unavailable – to scan the brain for metastases in those with NSCLC and large tumors, or tumors that have spread to the nearby lymph nodes.[13] When imaging suggests the tumor has spread, the suspected metastasis is often biopsied to confirm that it is cancerous.[11] Lung cancer most commonly metastasizes to the brain, bones, liver, and adrenal glands.[14]
Lung cancer can often appear as a solitary pulmonary nodule on a chest radiograph or CT scan. In lung cancer screening studies as many as 30% of those screened have a lung nodule, the majority of which turn out to be benign.[15] Besides lung cancer many other diseases can also give this appearance, including hamartomas, and infectious granulomas caused by tuberculosis, histoplasmosis, or coccidioidomycosis.[16]
At diagnosis, lung cancer is classified based on the type of cells the tumor is derived from; tumors derived from different cells progress and respond to treatment differently. There are two main types of lung cancer, categorized by the size and appearance of the malignant cells seen by a histopathologist under a microscope: small cell lung cancer (SCLC; 15% of cases) and non-small-cell lung cancer (NSCLC; 85% of cases).[17] SCLC tumors are often found near the center of the lungs, in the major airways.[18] Their cells appear small with ill-defined boundaries, not much cytoplasm, many mitochondria, and have distinctive nuclei with granular-looking chromatin and no visible nucleoli.[19] NSCLCs comprise a group of three cancer types: adenocarcinoma,  squamous-cell carcinoma, and large-cell carcinoma.[19] Nearly 40% of lung cancers are adenocarcinomas.[20] Their cells grow in three-dimensional clumps, resemble glandular cells, and may produce mucin.[19] About 30% of lung cancers are squamous-cell carcinomas. They typically occur close to large airways.[20] The tumors consist of sheets of cells, with layers of keratin.[19] A hollow cavity and associated cell death are commonly found at the center of the tumor.[20] Less than 10% of lung cancers are large-cell carcinomas,[19] so named because the cells are large, with excess cytoplasm, large nuclei, and conspicuous nucleoli.[20] Around 10% of lung cancers are rarer types.[19] These include mixes of the above subtypes like adenosquamous carcinoma, and rare subtypes such as carcinoid tumors, and sarcomatoid carcinomas.[20]
Several lung cancer types are subclassified based on the growth characteristics of the cancer cells. Adenocarcinomas are classified as lepidic (growing along the surface of intact alveolar walls),[21] acinar and papillary, or micropapillary and solid pattern. Lepidic adenocarcinomas tend to be least aggressive, while micropapillary and solid pattern adenocarcinomas are most aggressive.[22]
In addition to examining cell morphology, biopsies are often stained  by immunohistochemistry to confirm lung cancer classification. SCLCs bear the markers of neuroendocrine cells, such as chromogranin, synaptophysin, and CD56.[23] Adenocarcinomas tend to express Napsin-A and TTF-1; squamous cell carcinomas lack Napsin-A and TTF-1, but express p63 and its cancer-specific isoform p40.[19] CK7 and CK20 are also commonly used to differentiate lung cancers. CK20 is found in several cancers, but typically absent from lung cancer. CK7 is present in many lung cancers, but absent from squamous cell carcinomas.[24]
Lung cancer staging is an assessment of the degree of spread of the cancer from its original source. It is one of the factors affecting both the prognosis and the treatment of lung cancer.[26]
SCLC is typically staged with a relatively simple system: limited stage or extensive stage. Around a third of people are diagnosed at the limited stage, meaning cancer is confined to one side of the chest, within the scope of a single radiotherapy field.[26] The other two thirds are diagnosed at the "extensive stage", with cancer spread to both sides of the chest, or to other parts of the body.[26]
NSCLC – and sometimes SCLC – is typically staged with the American Joint Committee on Cancer's Tumor, Node, Metastasis (TNM) staging system.[27] The size and extent of the tumor (T), spread to regional lymph nodes (N), and distant metastases (M) are scored individually, and combined to form stage groups.[28]
Relatively small tumors are designated T1, which are subdivided by size: tumors ≤ 1 centimeter (cm) across are T1a; 1–2 cm T1b; 2–3 cm T1c. Tumors up to 5 cm across, or those that have spread to the visceral pleura (tissue covering the lung) or main bronchi, are designated T2. T2a designates 3–4 cm tumors; T2b 4–5 cm tumors. T3 tumors are up to 7 cm across, have multiple nodules in the same lobe of the lung, or invade the chest wall, diaphragm (or the nerve that controls it), or area around the heart.[28][29] Tumors that are larger than 7 cm, have nodules spread in different lobes of a lung, or invade the mediastinum (center of the chest cavity), heart, largest blood vessels that supply the heart, trachea, esophagus, or spine are designated T4.[28][29] Lymph node staging depends on the extent of local spread: with the cancer metastasized to no lymph nodes (N0), pulmonary or hilar nodes (along the bronchi) on the same side as the tumor (N1), mediastinal or subcarinal lymph nodes (in the middle of the lungs, N2), or lymph nodes on the opposite side of the lung from the tumor (N3).[29] Metastases are staged as no metastases (M0), nearby metastases (M1a; the space around the lung or the heart, or the opposite lung), a single distant metastasis (M1b), or multiple metastases (M1c).[28]
These T, N, and M scores are combined to designate a stage grouping for the cancer. Cancer limited to smaller tumors is designated stage I. Disease with larger tumors or spread to the nearest lymph nodes is stage II. Cancer with the largest tumors or extensive lymph node spread is stage III. Cancer that has metastasized is stage IV. Each stage is further subdivided based on the combination of T, N, and M scores.[30]
Some countries recommend that people who are at a high risk of developing lung cancer be screened at different intervals using low-dose CT lung scans. Screening programs may result in early detection of lung tumors in people who are not yet experiencing symptoms of lung cancer, ideally, early enough that the tumors can be successfully treated and result in decreased mortality.[32] There is evidence that regular low-dose CT scans in people at high risk of developing lung cancer reduces total lung cancer deaths by as much as 20%.[15] Despite evidence of benefit in these populations, potential harms of screening include the potential for a person to have a 'false positive' screening result that may lead to unnecessary testing, invasive procedures, and distress.[33] Although rare, there is also a risk of radiation-induced cancer.[33] The United States Preventive Services Task Force recommends yearly screening using low-dose CT in people between 55 and 80 who have a smoking history of at least 30 pack-years.[34] The European Commission recommends that cancer screening programs across the European Union be extended to include low-dose CT lung scans for current or previous smokers.[35] Similarly, The Canadian Task Force for Preventative Health recommends that people who are current or former smokers (smoking history of more than 30 pack years) and who are between the ages of 55–74 years be screened for lung cancer.[36]
Treatment for lung cancer depends on the cancer's specific cell type, how far it has spread, and the person's health. Common treatments for early stage cancer includes surgical removal of the tumor, chemotherapy, and radiation therapy. For later-stage cancer, chemotherapy and radiation therapy are combined with newer targeted molecular therapies and immune checkpoint inhibitors.[4] All lung cancer treatment regimens are combined with lifestyle changes and palliative care to improve quality of life.[37]
Limited-stage SCLC is typically treated with a combination of chemotherapy and radiotherapy.[38] For chemotherapy, the National Comprehensive Cancer Network and American College of Chest Physicians guidelines recommend four to six cycles of a platinum-based chemotherapeutic – cisplatin or carboplatin – combined with either etoposide or irinotecan.[39] This is typically combined with thoracic radiation therapy – 45 Gray (Gy) twice-daily – alongside the first two chemotherapy cycles.[38] First-line therapy causes remission in up to 80% of those who receive it; however most people relapse with chemotherapy-resistant disease. Those who relapse are given second-line chemotherapies. Topotecan and lurbinectedin are approved by the US FDA for this purpose.[38] Irinotecan, paclitaxel, docetaxel, vinorelbine, etoposide, and gemcitabine are also sometimes used, and are similarly efficacious.[38] Prophylactic cranial irradiation can reduce the risk of brain metastases and improve survival in those with limited-stage disease.[40][38]
Extensive-stage SCLC is treated first with etoposide along with either cisplatin or carboplatin. Radiotherapy is used only to shrink tumors that are causing particularly severe symptoms. Combining standard chemotherapy with an immune checkpoint inhibitor can improve survival for a minority of those affected, extending the average person's lifespan by around 2 months.[41]
For stage I and stage II NSCLC the first line of treatment is often surgical removal of the affected lobe of the lung.[42] For those not well enough to tolerate full lobe removal, a smaller chunk of lung tissue can be removed by wedge resection or segmentectomy surgery.[42] Those with centrally located tumors and otherwise-healthy respiratory systems may have more extreme surgery to remove an entire lung (pneumonectomy).[42] Experienced thoracic surgeons, and a high-volume surgery clinic improve chances of survival.[42] Those who are unable or unwilling to undergo surgery can instead receive radiation therapy. Stereotactic body radiation therapy is best practice, typically administered several times over 1–2 weeks.[42] Chemotherapy has little effect in those with stage I NSCLC, and may worsen disease outcomes in those with the earliest disease. In those with stage II disease, chemotherapy is usually initiated six to twelve weeks after surgery, with up to four cycles of cisplatin – or carboplatin in those with kidney problems, neuropathy, or hearing impairment – combined with vinorelbine, pemetrexed, gemcitabine, or docetaxel.[42]
Treatment for those with stage III NSCLC depends on the nature of their disease. Those with more limited spread may undergo surgery to have the tumor and affected lymph nodes removed, followed by chemotherapy and potentially radiotherapy. Those with particularly large tumors (T4) and those for whom surgery is impractical are treated with combination chemotherapy and radiotherapy along with the immunotherapy durvalumab.[43] Combined chemotherapy and radiation enhances survival compared to chemotherapy followed by radiation, though the combination therapy comes with harsher side effects.[43]
Those with stage IV disease are treated with combinations of pain medication, radiotherapy, immunotherapy, and chemotherapy.[44] Many cases of advanced disease can be treated with targeted therapies depending on the genetic makeup of the cancerous cells. Up to 30% of tumors have mutations in the EGFR gene that result in an overactive EGFR protein;[45] these can be treated with EGFR inhibitors osimertinib, erlotinib, gefitinib, afatinib, or dacomitinib – with osimertinib known to be superior to erlotinib and gefitinib, and all superior to chemotherapy alone.[44] Up to 7% of those with NSCLC harbor mutations that result in hyperactive ALK protein, which can be treated with ALK inhibitors crizotinib, or its successors alectinib, brigatinib, and ceritinib.[44] Those treated with ALK inhibitors who relapse can then be treated with the third-generation ALK inhibitor lorlatinib.[44] Up to 5% with NSCLC have overactive MET, which can be inhibited with MET inhibitors capmatinib or tepotinib.[44] Targeted therapies are also available for some cancers with rare mutations. Cancers with hyperactive BRAF (around 2% of NSCLC) can be treated by dabrafenib combined with the MEK inhibitor trametinib; those with activated ROS1 (around 1% of NSCLC) can be inhibited by crizotinib, lorlatinib, or entrectinib; overactive NTRK (<1% of NSCLC) by entrectinib or larotrectinib; active RET (around 1% of NSCLC) by selpercatinib.[44]
People whose NSCLC is not targetable by current molecular therapies instead can be treated with combination chemotherapy plus immune checkpoint inhibitors, which prevent cancer cells from inactivating immune T cells. The chemotherapeutic agent of choice depends on the NSCLC subtype: cisplatin plus gemcitabine for squamous cell carcinoma, cisplatin plus pemetrexed for non-squamous cell carcinoma.[46] Immune checkpoint inhibitors are most effective against tumors that express the protein PD-L1, but are sometimes effective in those that do not.[47] Treatment with pembrolizumab, atezolizumab, or combination nivolumab plus ipilimumab are all superior to chemotherapy alone against tumors expressing PD-L1.[47] Those who relapse on the above are treated with second-line chemotherapeutics docetaxel and ramucirumab.[48]
Integrating palliative care (medical care focused on improving symptoms and lessening discomfort) into lung cancer treatment from the time of diagnosis improves the survival time and quality of life of those with lung cancer.[49] Particularly common symptoms of lung cancer are shortness of breath and pain. Supplemental oxygen, improved airflow, re-orienting an affected person in bed, and low-dose morphine can all improve shortness of breath.[50] In around 20 to 30% of those with lung cancer – particularly those with late-stage disease – growth of the tumor can narrow or block the airway, causing coughing and difficulty breathing.[51] Obstructing tumors can be surgically removed where possible, though typically those with airway obstruction are not well enough for surgery. In such cases the American College of Chest Physicians recommends opening the airway by inserting a stent, attempting to shrink the tumor with localized radiation (brachytherapy), or physically removing the blocking tissue by bronchoscopy, sometimes aided by thermal or laser ablation.[52] Other causes of lung cancer-associated shortness of breath can be treated directly, such as antibiotics for a lung infection, diuretics for pulmonary edema, benzodiazepines for anxiety, and steroids for airway obstruction.[50]
Up to 92% of those with lung cancer report pain, either from tissue damage at the tumor site(s) or nerve damage.[53] The World Health Organization (WHO) has developed a three-tiered system for managing cancer pain. For those with mild pain (tier one), the WHO recommends acetominophen or a nonsteroidal anti-inflammatory drug.[53] Around a third of people experience moderate (tier two) or severe (tier three) pain, for which the WHO recommends opioid painkillers.[53] Opioids are typically effective at easing nociceptive pain (pain caused by damage to various body tissues). Opioids are occasionally effective at easing neuropathic pain (pain cauesd by nerve damage). Neuropathic agents such as anticonvulsants, tricyclic antidepressants, and serotonin–norepinephrine reuptake inhibitors, are often used to ease neuropathic pain, either alone or in combination with opioids.[53] In many cases, targeted radiotherapy can be used to shrink tumors, reducing pain and other symptoms caused by tumor growth.[54]
Individuals who have advanced disease and are approaching end-of-life can benefit from dedicated end-of-life care to manage symptoms and ease suffering. As in earlier disease, pain and difficulty breathing are common, and can be managed with opioid pain medications, transitioning from oral medication to injected medication if the affected individual loses the ability to swallow.[55][56] Coughing is also common, and can be managed with opioids or cough suppressants. Some experience terminal delirium – confused behavior, unexplained movements, or a reversal of the sleep-wake cycle – which can be managed by antipsychotic drugs, low-dose sedatives, and investigating other causes of discomfort such as low blood sugar, constipation, and sepsis.[55] In the last few days of life, many develop terminal secretions – pooled fluid in the airways that can cause a rattling sound while breathing. This is thought not to cause respiratory problems, but can distress family members and caregivers. Terminal secretions can be reduced by anticholinergic medications.[55] Even those who are non-communicative or have reduced consciousness may be able to experience cancer-related pain, so pain medications are typically continued until the time of death.[55]
Around 19% of people diagnosed with lung cancer survive five years from diagnosis, though prognosis varies based on the stage of the disease at diagnosis and the type of lung cancer.[4] Prognosis is better for people with lung cancer diagnosed at an earlier stage; those diagnosed at the earliest TNM stage, IA1 (small tumor, no spread), have a two-year survival of 97% and five-year survival of 92%.[57] Those diagnosed at the most-advanced stage, IVB, have a two-year survival of 10% and a five-year survival of 0%.[57] Five-year survival is higher in women (22%) than men (16%).[4] Women tend to be diagnosed with less-advanced disease, and have better outcomes than men diagnosed at the same stage.[58] Average five-year survival also varies across the world, with particularly high five-year survival in Japan (33%), and five-year survival above 20% in 12 other countries: Mauritius, Canada, the US, China, South Korea, Taiwan, Israel, Latvia, Iceland, Sweden, Austria, and Switzerland.[59]
SCLC is particularly aggressive. 10–15% of people survive five years after a SCLC diagnosis.[38] As with other types of lung cancer, the extent of disease at diagnosis also influences prognosis. The average person diagnosed with limited-stage SCLC survives 12–20 months from diagnosis; with extensive-stage SCLC around 12 months.[38] While SCLC often responds initially to treatment, most people eventually relapse with chemotherapy-resistant cancer, surviving an average 3–4 months from the time of relapse.[38] Those with limited stage SCLC that go into complete remission after chemotherapy and radiotherapy have a 50% chance of brain metastases developing within the next two years – a chance reduced by prophylactic cranial irradiation.[39]
Several other personal and disease factors are associated with improved outcomes. Those diagnosed at a younger age tend to have better outcomes. Those who smoke or experience weight loss as a symptom tend to have worse outcomes. Tumor mutations in KRAS are associated with reduced survival.[58]
The uncertainty of lung cancer prognosis often causes stress, and makes future planning difficult, for those with lung cancer and their families.[60] Those whose cancer goes into remission often experience fear of their cancer returning or progressing, associated with poor quality of life, negative mood, and functional impairment. This fear is exacerbated by frequent or prolonged surveillance imaging, and other reminders of cancer risks.[60]
Lung cancer is caused by genetic damage to the DNA of lung cells. These changes are sometimes random, but are typically induced by breathing in toxic substances such as cigarette smoke.[61][62] Cancer-causing genetic changes affect the cell's normal functions, including cell proliferation, programmed cell death (apoptosis), and DNA repair.[63] Eventually, cells gain enough genetic changes to grow uncontrollably, forming a tumor, and eventually spreading within and then beyond the lung. Rampant tumor growth and spread causes the symptoms of lung cancer. If unstopped, the spreading tumor will eventually cause the death of affected individuals.
Tobacco smoking is by far the major contributor to lung cancer, causing 80% to 90% of cases.[64] Lung cancer risk increases with quantity of cigarettes consumed.[65] Tobacco smoking's carcinogenic effect is due to various chemicals in tobacco smoke that cause DNA mutations, increasing the chance of cells becoming cancerous.[66] The International Agency for Research on Cancer identifies at least 50 chemicals in tobacco smoke as carcinogenic, and the most potent is tobacco-specific nitrosamines.[65] Exposure to these chemicals causes several kinds of DNA damage: DNA adducts, oxidative stress, and breaks in the DNA strands.[67] Being around tobacco smoke – called passive smoking – can also cause lung cancer. Living with a tobacco smoker increases one's risk of developing lung cancer by 24%. An estimated 17% of lung cancer cases in those who do not smoke are caused by high levels of environmental tobacco smoke.[68]
Vaping may be a risk factor for lung cancer, but less than that of cigarettes, and further research as of 2021 is necessary due to the length of time it can take for lung cancer to develop following an exposure to carcinogens.[69]
The smoking of non-tobacco products is not known to be associated with lung cancer development. Marijuana smoking does not seem to independently cause lung cancer – despite the relatively high levels of tar and known carcinogens in marijuana smoke. The relationship between smoking cocaine and developing lung cancer has not been studied as of 2020.[70]
Exposure to a variety of other toxic chemicals – typically encountered in certain occupations – is associated with an increased risk of lung cancer.[71] Occupational exposures to carcinogens cause 9–15% of lung cancer.[71] A prominent example is asbestos, which causes lung cancer either directly or indirectly by inflaming the lung.[71] Exposure to all commercially available forms of asbestos increases cancer risk, and cancer risk increases with time of exposure.[71] Asbestos and cigarette smoking increase risk synergistically – that is, the risk of someone who smokes and has asbestos exposure dying from lung cancer is much higher than would be expected from adding the two risks together.[71] Similarly, exposure to radon, a naturally occurring breakdown product of the Earth's radioactive elements, is associated with increased lung cancer risk. Radon levels vary with geography.[72] Underground miners have the greatest exposure; however even the lower levels of radon that seep into residential spaces can increase occupants' risk of lung cancer. Like asbestos, cigarette smoking and radon exposure increase risk synergistically.[71] Radon exposure is responsible for between 3% and 14% of lung cancer cases.[72]
Several other chemicals encountered in various occupations are also associated with increased lung cancer risk including arsenic used in wood preservation, pesticide application, and some ore smelting; ionizing radiation encountered during uranium mining; vinyl chloride in papermaking; beryllium in jewelers, ceramics workers, missile technicians, and nuclear reactor workers; chromium in stainless steel production, welding, and hide tanning; nickel in electroplaters, glass workers, metal workers, welders, and those who make batteries, ceramics, and jewelry; and diesel exhaust encountered by miners.[71]
Exposure to air pollution, especially particulate matter released by motor vehicle exhaust and fossil fuel-burning power plants, increases the risk of lung cancer.[73][74] Indoor air pollution from burning wood, charcoal, or crop residue for cooking and heating has also been linked to an increased risk of developing lung cancer.[75] The International Agency for Research on Cancer has classified emission from household burning of coal and biomass as "carcinogenic" and "probably carcinogenic" respectively.[75]
Several other diseases that cause inflammation of the lung increase one's risk of lung cancer. This association is strongest for chronic obstructive pulmonary disorder – the risk is highest in those with the most inflammation, and reduced in those whose inflammation is treated with inhaled corticosteroids.[76] Other inflammatory lung and immune system diseases such as alpha-1 antitrypsin deficiency, interstitial fibrosis, scleroderma, Chlamydia pneumoniae infection, tuberculosis, and HIV infection are associated with increased risk of developing lung cancer.[76] Epstein–Barr virus is associated with the development of the rare lung cancer lymphoepithelioma-like carcinoma in people from Asia, but not in people from Western nations.[77] A role for several other infectious agents – namely human papillomaviruses, BK virus, JC virus, human cytomegalovirus, SV40, measles virus, and Torque teno virus – in lung cancer development has been studied but remains inconclusive as of 2020.[77]
Particular gene combinations may make some people more susceptible to lung cancer. Close family members of those with lung cancer have around twice the risk of developing lung cancer as an average person, even after controlling for occupational exposure and smoking habits.[78] Genome-wide association studies have identified many gene variants associated with lung cancer risk, each of which contributes a small risk increase.[79] Many of these genes participate in pathways known to be involved in carcinogenesis, namely DNA repair, inflammation, the cell division cycle, cellular stress responses, and chromatin remodeling.[79] Some rare genetic disorders that increase the risk of various cancers also increase the risk of lung cancer, namely retinoblastoma and Li–Fraumeni syndrome.[80]
As with all cancers, lung cancer is triggered by mutations that allow tumor cells to endlessly multiply, stimulate blood vessel growth, avoid apoptosis (programmed cell death), generate pro-growth signalling molecules, ignore anti-growth signalling molecules, and eventually spread into surrounding tissue or metastasize throughout the body.[81] Different tumors can acquire these abilities through different mutations, though generally cancer-contributing mutations activate oncogenes and inactivate tumor suppressors.[81] Some mutations – called "driver mutations" – are particularly common in adenocarcinomas, and contribute disproportionately to tumor development. These typically occur in the receptor tyrosine kinases EGFR, BRAF, MET, KRAS, and PIK3CA.[81] Similarly, some adenocarcinomas are driven by chromosomal rearrangements that result in overexpression of tyrosine kinases ALK, ROS1, NTRK, and RET. A given tumor will typically have just one driver mutation.[81] In contrast, SCLCs rarely have these driver mutations, and instead often have mutations that have inactivated the tumor suppressors p53 and RB.[82] A cluster of tumor suppressor genes on the short arm of chromosome 3 are often lost early in the development of all lung cancers.[81]
Those who smoke can reduce their lung cancer risk by quitting smoking – the risk reduction is greater the longer a person goes without smoking.[83] Self-help programs tend to have little influence on success of smoking cessation, whereas combined counseling and pharmacotherapy improve cessation rates.[83] The US FDA has approved antidepressant therapies and the nicotine replacement varenicline as first-line therapies to aid in smoking cessation. Clonidine and nortriptyline are recommended second-line therapies.[83] The majority of those diagnosed with lung cancer attempt to quit smoking; around half succeed.[84] Even after lung cancer diagnosis, smoking cessation improves treatment outcomes, reducing cancer treatment toxicity and failure rates, and lengthening survival time.[85]
At a societal level, smoking cessation can be promoted by tobacco control policies that make tobacco products more difficult to obtain or use. Many such policies are mandated or recommended by the WHO Framework Convention on Tobacco Control, ratified by 182 countries, representing over 90% of the world's population.[86] The WHO groups these policies into six intervention categories, each of which has been shown to be effective in reducing the cost of tobacco-induced disease burden on a population: 
Policies implementing each intervention are associated with decreases in tobacco smoking prevalence. The more policies implemented, the greater the reduction.[88] Reducing access to tobacco for adolescents is particularly effective at decreasing uptake of habitual smoking, and adolescent demand for tobacco products is particularly sensitive to increases in cost.[89]
Several foods and dietary supplements have been associated with lung cancer risk. High consumption of some animal products – red meat (but not other meats or fish), saturated fats, as well as nitrosamines and nitrites (found in salted and smoked meats) – is associated with an increased risk of developing lung cancer.[90] In contrast, high consumption of fruits and vegetables is associated with a reduced risk of lung cancer, particularly consumption of cruciferous vegetables and raw fruits and vegetables.[90] Based on the beneficial effects of fruits and vegetables, supplementation of several individual vitamins have been studied. Supplementation with vitamin A or beta-carotene had no effect on lung cancer, and instead slightly increased mortality.[90] Dietary supplementation with vitamin E or retinoids similarly had no effect.[91] Consumption of polyunsaturated fats, tea, alcoholic beverages, and coffee are all associated with reduced risk of developing lung cancer.[90]
Along with diet, body weight and exercise habits are also associated with lung cancer risk. Being overweight is associated with a lower risk of developing lung cancer, possibly due to the tendency of those who smoke cigarettes to have a lower body weight.[92] However, being underweight is also associated with a reduced lung cancer risk.[92] Some studies have shown those who exercise regularly or have better cardiovascular fitness to have a lower risk of developing lung cancer.[92]
Worldwide, lung cancer is the most diagnosed type of cancer, and the leading cause of cancer death.[94][95] In 2020, 2.2 million new cases were diagnosed, and 1.8 million people died from lung cancer, representing 18% of all cancer deaths.[3] Lung cancer deaths are expected to rise globally to nearly 3 million annual deaths by 2035, due to high rates of tobacco use and aging populations.[95] Lung cancer is rare in those younger than 40; after that, cancer rates increase with age, stabilizing around age 80.[1] The median age of a person diagnosed with lung cancer is 70; the median age of death is 72.[2]
Lung cancer incidence varies by geography and sex, with the highest rates in Micronesia, Polynesia, Europe, Asia, and North America; and lowest rates in Africa and Central America.[96] Globally, around 8% of men and 6% of women develop lung cancer in their lifetimes.[1] The ratio of lung cancer cases in men to women varies considerably by geography, from as high as nearly 12:1 in Belarus, to 1:1 in Brazil, likely due to differences in smoking patterns.[97]
Lung cancer risk is influenced by environmental exposure, namely cigarette smoking, as well as occupational risks in mining, shipbuilding, petroleum refining, and occupations that involve asbestos exposure.[97] People who have smoked cigarettes account for 85–90% of lung cancer cases, and 15% of smokers develop lung cancer.[97] Non-smokers' risk of developing lung cancer is also influenced by tobacco smoking; secondhand smoke (that is, being around tobacco smoke) increases risk of developing lung cancer around 30%, with risk correlated to duration of exposure.[97]
Lung cancer was uncommon before the advent of cigarette smoking. Surgeon Alton Ochsner recalled that as a Washington University medical student in 1919, his entire medical school class was summoned to witness an autopsy of a man who had died from lung cancer, and told they may never see such a case again.[98][99] In Isaac Adler's 1912 Primary Malignant Growths of the Lungs and Bronchi, he called lung cancer "among the rarest forms of disease";[100] Adler tabulated the 374 cases of lung cancer that had been published to that time, concluding the disease was increasing in incidence.[101] By the 1920s, several theories had been put forward linking the increase in lung cancer to various chemical exposures that had increased including tobacco smoke, asphalt dust, industrial air pollution, and poisonous gasses from World War I.[101]
Over the following decades, growing scientific evidence linked lung cancer to cigarette consumption. Through the 1940s and early 1950s, several case-control studies showed that those with lung cancer were more likely to have smoked cigarettes compared to those without lung cancer.[102] These were followed by several prospective cohort studies in the 1950s – including the first report of the British Doctors Study in 1954 – all of which showed that those who smoked tobacco were at dramatically increased risk of developing lung cancer.[102]
A 1953 study showing that tar from cigarette smoke could cause tumors in mice attracted attention in the popular press, with features in Life and Time magazines. Facing public concern and falling stock prices, the CEOs of six of the largest American tobacco companies gathered in December 1953.[103] They enlisted the help of public relations firm Hill & Knowlton to craft a multi-pronged strategy aiming to distract from accumulating evidence by funding tobacco-friendly research, declaring the link to lung cancer "controversial", and demanding ever-more research to settle this purported controversy.[103][104] At the same time, internal research at the major tobacco companies supported the link between tobacco and lung cancer; though these results were kept secret from the public.[105]
As evidence linking tobacco use with lung cancer mounted, various health bodies announced official positions linking the two. In 1962, the United Kingdom's Royal College of Physicians officially concluded that cigarette smoking causes lung cancer, prompting the United States Surgeon General to empanel an advisory committee, which deliberated in secret over nine sessions between November 1962 and December 1963.[106] The committee's report, published in January 1964, firmly concluded that cigarette smoking "far outweighs all other factors" in causing lung cancer.[107] The report received substantial coverage in the popular press, and is widely seen as a turning point for public recognition that tobacco smoking causes lung cancer.[106][108]
The connection with radon gas was first recognized among miners in Germany's Ore Mountains. As early as 1500, miners were noted to develop a deadly disease called "mountain sickness" ("Bergkrankheit"), identified as lung cancer by the late 19th century.[109][110] By 1938, up to 80% of miners in affected regions died from the disease.[109] In the 1950s radon and its breakdown products became established as causes of lung cancer in miners. Based largely on studies of miners, the International Agency for Research on Cancer classified radon as "carcinogenic to humans" in 1988.[110] In 1956, a study revealed radon in Swedish residences. Over the following decades, high radon concentrations were found in residences across the world; by the 1980s many countries had established national radon programs to catalog and mitigate residential radon.[111]
The first successful pneumonectomy for lung cancer was performed in 1933 by Evarts Graham at Barnes Hospital in St. Louis, Missouri.[112] Over the following decades, surgical development focused on sparing as much healthy lung tissue as possible, with the lobectomy surpassing the pneumectomy in frequency by the 1960s, and the wedge resection appearing in the early 1970s.[113][114] This trend continud with the development of video-assisted thoracoscopic surgery in the 1980s, now widely performed for many lung cancer surgeries.[115]
While lung cancer is the deadliest type of cancer, it receives the third-most funding from the US National Cancer Institute (NCI, the world's largest cancer research funder) behind brain cancers and breast cancer.[116] Despite high levels of gross research funding, lung cancer funding per death lags behind many other cancers, with around $3,200 spent on lung cancer research in 2022 per US death, considerably lower than that for brain cancer ($22,000 per death), breast cancer ($14,000 per death), and cancer as a whole ($11,000 per death).[117] A similar trend holds for private nonprofit organizations. Annual revenues of lung cancer-focused nonprofits rank fifth among cancer types, but lung cancer nonprofits have lower revenue than would be expected for the number of lung cancer cases, deaths, and potential years of life lost.[118]
Despite this, many investigational lung cancer treatments are undergoing clinical trials – with nearly 2,250 active clinical trials registered as of 2021.[119] Of these, a large plurality are testing radiotherapy regimens (26% of trials) and surgical techniques (22%). Many others are testing targeted anticancer drugs, with targets including EGFR (17% of trials), microtubules (12%), VEGF (12%), immune pathways (10%), mTOR (1%), and histone deacetylases (<1%).[120]
Books
Journal articles

Type 2 diabetes (T2D), formerly known as adult-onset diabetes, is a form of diabetes mellitus that is characterized by high blood sugar, insulin resistance, and relative lack of insulin.[6] Common symptoms include increased thirst, frequent urination, fatigue and unexplained weight loss.[3] Symptoms may also include increased hunger,  having a sensation of pins and needles, and sores (wounds) that do not heal.[3] Often symptoms come on slowly.[6] Long-term complications from high blood sugar include heart disease, strokes, diabetic retinopathy which can result in blindness, kidney failure, and poor blood flow in the limbs which may lead to amputations.[1] The sudden onset of hyperosmolar hyperglycemic state may occur; however, ketoacidosis is uncommon.[4][5]
Type 2 diabetes primarily occurs as a result of obesity and lack of exercise.[1] Some people are genetically more at risk than others.[6]
Type 2 diabetes makes up about 90% of cases of diabetes, with the other 10% due primarily to type 1 diabetes and gestational diabetes.[1] In type 1 diabetes there is a lower total level of insulin to control blood glucose, due to an autoimmune induced loss of insulin-producing beta cells in the pancreas.[12][13] Diagnosis of diabetes is by blood tests such as fasting plasma glucose, oral glucose tolerance test, or glycated hemoglobin (A1C).[3]
Type 2 diabetes is largely preventable by staying a normal weight, exercising regularly, and eating a healthy diet (high in fruits and vegetables and low in sugar and saturated fats).[1] Treatment involves exercise and dietary changes.[1] If blood sugar levels are not adequately lowered, the medication metformin is typically recommended.[7][14] Many people may eventually also require insulin injections.[9] In those on insulin, routinely checking blood sugar levels (such as through a continuous glucose monitor) is advised; however, this may not be needed in those who are not on insulin therapy.[15] Bariatric surgery often improves diabetes in those who are obese.[8][16]
Rates of type 2 diabetes have increased markedly since 1960 in parallel with obesity.[17] As of 2015 there were approximately 392 million people diagnosed with the disease compared to around 30 million in 1985.[11][18] Typically it begins in middle or older age,[6] although rates of type 2 diabetes are increasing in young people.[19][20] Type 2 diabetes is associated with a ten-year-shorter life expectancy.[10] Diabetes was one of the first diseases ever described, dating back to an Egyptian manuscript from c. 1500 BCE.[21] The importance of insulin in the disease was determined in the 1920s.[22]
The classic symptoms of diabetes are frequent urination (polyuria), increased thirst (polydipsia), increased hunger (polyphagia), and weight loss.[23] Other symptoms that are commonly present at diagnosis include a history of blurred vision, itchiness, peripheral neuropathy, recurrent vaginal infections, and fatigue.[13] Other symptoms may include loss of taste.[24]  Many people, however, have no symptoms during the first few years and are diagnosed on routine testing.[13] A small number of people with type 2 diabetes can develop a hyperosmolar hyperglycemic state (a condition of very high blood sugar associated with a decreased level of consciousness and low blood pressure).[13]
Type 2 diabetes is typically a chronic disease associated with a ten-year-shorter life expectancy.[10] This is partly due to a number of complications with which it is associated, including: two to four times the risk of cardiovascular disease, including ischemic heart disease and stroke; a 20-fold increase in lower limb amputations, and increased rates of hospitalizations.[10] In the developed world, and increasingly elsewhere, type 2 diabetes is the largest cause of nontraumatic blindness and kidney failure.[25] It has also been associated with an increased risk of cognitive dysfunction and dementia through disease processes such as Alzheimer's disease and vascular dementia.[26] Other complications include hyperpigmentation of skin (acanthosis nigricans), sexual dysfunction, and frequent infections.[23] There is also an association between type 2 diabetes and mild hearing loss.[27]
The development of type 2 diabetes is caused by a combination of lifestyle and genetic factors.[25][28] While some of these factors are under personal control, such as diet and obesity, other factors are not, such as increasing age, female sex, and genetics.[10] Obesity is more common in women than men in many parts of Africa.[29] The nutritional status of a mother during fetal development may also play a role, with one proposed mechanism being that of DNA methylation.[30] The intestinal bacteria Prevotella copri and Bacteroides vulgatus have been connected with type 2 diabetes.[31]
Lifestyle factors are important to the development of type 2 diabetes, including obesity and being overweight (defined by a body mass index of greater than 25), lack of physical activity, poor diet, psychological stress, and urbanization.[10][32] Excess body fat is associated with 30% of cases in those of Chinese and Japanese descent, 60–80% of cases in those of European and African descent, and 100% of cases in Pima Indians and Pacific Islanders.[13] Among those who are not obese, a high waist–hip ratio is often present.[13] Smoking appears to increase the risk of type 2 diabetes.[33]  A lack of sleep has also been linked to type 2 diabetes.[34] Laboratory studies have linked short-term sleep deprivations to changes in glucose metabolism, nervous system activity, or hormonal factors that may lead to diabetes.[34]
Dietary factors also influence the risk of developing type 2 diabetes. Consumption of sugar-sweetened drinks in excess is associated with an increased risk.[35][36] The type of fats in the diet are important, with saturated fats and trans fatty acids increasing the risk, and polyunsaturated and monounsaturated fat decreasing the risk.[28] Eating a lot of white rice appears to play a role in increasing risk.[37] A lack of exercise is believed to cause 7% of cases.[38] Persistent organic pollutants may also play a role.[39]
Most cases of diabetes involve many genes, with each being a small contributor to an increased probability of becoming a type 2 diabetic.[10] The proportion of diabetes that is inherited is estimated at 72%.[40] More than 36 genes and 80 single nucleotide polymorphisms (SNPs) had been found that contribute to the risk of type 2 diabetes.[41][42] All of these genes together still only account for 10% of the total heritable component of the disease.[41] The TCF7L2 allele, for example, increases the risk of developing diabetes by 1.5 times and is the greatest risk of the common genetic variants.[13] Most of the genes linked to diabetes are involved in pancreatic beta cell functions.[13]
There are a number of rare cases of diabetes that arise due to an abnormality in a single gene (known as monogenic forms of diabetes or "other specific types of diabetes").[10][13] These include maturity onset diabetes of the young (MODY), Donohue syndrome, and Rabson–Mendenhall syndrome, among others.[10] Maturity onset diabetes of the young constitute 1–5% of all cases of diabetes in young people.[43]
Epigenetic regulation occurs at multiple levels including (1) direct methylation of cytosine and adenine residues in DNA, (2) covalent modification of histone proteins in chromatin, and (3) action of non coding microRNAs (for other examples, see Wikipedia article "Epigenetics").  On November 17-19, 2017, the American Diabetes Association held a research symposium entitled "Epigenetics and Epigenomics: Implications for Diabetes and Obesity."  As a result of this symposium, an overview of the state of the field was presented in which it was noted that over 1,000 research articles have been published that address the intersection of diabetes and epigenetics or epigenomics.[44]  The current state of knowledge in this field is addressed the Wikipedia article "Epigenetics of diabetes Type 2."
There are a number of medications and other health problems that can predispose to diabetes.[45] Some of the medications include: glucocorticoids, thiazides, beta blockers, atypical antipsychotics,[46] and statins.[47] Those who have previously had gestational diabetes are at a higher risk of developing type 2 diabetes.[23] Other health problems that are associated include: acromegaly, Cushing's syndrome, hyperthyroidism, pheochromocytoma, and certain cancers such as glucagonomas.[45] Individuals with cancer may be at a higher risk of mortality if they also have diabetes.[48] Testosterone deficiency is also associated with type 2 diabetes.[49][50] Eating disorders may also interact with type 2 diabetes, with bulimia nervosa increasing the risk and anorexia nervosa decreasing it.[51]
Type 2 diabetes is due to insufficient insulin production from beta cells in the setting of insulin resistance.[13] Insulin resistance, which is the inability of cells to respond adequately to normal levels of insulin, occurs primarily within the muscles, liver, and fat tissue.[52] In the liver, insulin normally suppresses glucose release. However, in the setting of insulin resistance, the liver inappropriately releases glucose into the blood.[10] The proportion of insulin resistance versus beta cell dysfunction differs among individuals, with some having primarily insulin resistance and only a minor defect in insulin secretion and others with slight insulin resistance and primarily a lack of insulin secretion.[13]
Other potentially important mechanisms associated with type 2 diabetes and insulin resistance include: increased breakdown of lipids within fat cells, resistance to and lack of incretin, high glucagon levels in the blood, increased retention of salt and water by the kidneys, and inappropriate regulation of metabolism by the central nervous system.[10] However, not all people with insulin resistance develop diabetes since an impairment of insulin secretion by pancreatic beta cells is also required.[13]
In the early stages of insulin resistance, the mass of beta cells expands, increasing the output of insulin to compensate for the insulin insensitivity.[53] But when type 2 diabetes has become manifest, a type 2 diabetic will have lost about half of their beta cells.[53]
Fatty acids in the beta cells activate FOXO1, resulting in apoptosis of the beta cells.[53]
The causes of the aging-related insulin resistance seen in obesity and in type 2 diabetes are uncertain. Effects of intracellular lipid metabolism and ATP production in liver and muscle cells may contribute to insulin resistance.[54]  New evidence also points to a role of a brain region called the hypothalamus in the development of insulin resistance. For one thing, a gene called Dusp8 is linked with an increased risk for diabetes.[55] This gene codes for a protein that regulates neuronal signaling in the hypothalamus. Also, infusions into the hypothalamus of a hormone called leptin normalize blood glucose and diminish insulin resistance in diabetic animals.[56] Activation of hypothalamic cells by leptin has an important role in maintaining normal levels of blood glucose. Thus, both the endocrine cells of the pancreas AND cells in the hypothalamus may have a role in the etiology of type 2 diabetes.
Hypothalamic cells regulate blood glucose via projections to the autonomic nervous system. Autonomic innervation of liver and muscle cells stimulates an increased uptake of glucose. In diabetic humans, the control of blood glucose by the autonomic nervous system is abnormal.[57] Leptin-sensitive, glucose regulating neurons become resistant to leptin during aging or during exposure to a high-fat diet. These leptin resistant neurons fail to restrain food intake, obesity, and blood glucose. The reasons for this lowered responsiveness to leptin are uncertain and are part of the puzzle of the causes of type 2 diabetes.[58]
Blood glucose levels can also be normalized in diabetic rodents by a single intrahypothalamic infusion of Fibroblast Growth Factor 1 (FGF1), an effect that persists for months even in severely diabetic animals. This remarkable cure of diabetes is accomplished by a stimulation of accessory brain cells called astrocytes.[59][60]  Hypothalamic astrocytes that produce Fatty Acid Binding Protein 7 (FABP7) are targets of FGF1; these cells are also in close contact with leptin-sensitive neurons, influence their function, and regulate leptin sensitivity.[61][62] An abnormal function of FABP7+ astrocytes thus may contribute to the resistance to leptin and insulin that appear during aging and during exposure to high-fat diets.
During aging, FABP7+ astrocytes develop cytoplasmic granules derived from degenerating mitochondria. This mitochondrial degeneration is partly due to the oxidative stress of the heightened amounts of fatty acids that are taken up by these cells and oxidized within mitochondria.[63][64]  A pathological degeneration of mitochondria in these cells may compromise their normal functions and contribute to abnormalities in the control of blood glucose by the hypothalamus.
The World Health Organization definition of diabetes (both type 1 and type 2) is for a single raised glucose reading with symptoms, otherwise raised values on two occasions, of either:[67]
A random blood sugar of greater than 11.1 mmol/L (200 mg/dL) in association with typical symptoms[23] or a glycated hemoglobin (HbA1c) of ≥ 48 mmol/mol (≥ 6.5 DCCT %) is another method of diagnosing diabetes.[10] In 2009 an International Expert Committee that included representatives of the American Diabetes Association (ADA), the International Diabetes Federation (IDF), and the European Association for the Study of Diabetes (EASD) recommended that a threshold of ≥ 48 mmol/mol (≥ 6.5 DCCT %) should be used to diagnose diabetes.[68] This recommendation was adopted by the American Diabetes Association in 2010.[69] Positive tests should be repeated unless the person presents with typical symptoms and blood sugars >11.1 mmol/L (>200 mg/dL).[68]
Threshold for diagnosis of diabetes is based on the relationship between results of glucose tolerance tests, fasting glucose or HbA1c and complications such as retinal problems.[10] A fasting or random blood sugar is preferred over the glucose tolerance test, as they are more convenient for people.[10] HbA1c has the advantages that fasting is not required and results are more stable but has the disadvantage that the test is more costly than measurement of blood glucose.[71] It is estimated that 20% of people with diabetes in the United States do not realize that they have the disease.[10]
Type 2 diabetes is characterized by high blood glucose in the context of insulin resistance and relative insulin deficiency.[72] This is in contrast to type 1 diabetes in which there is an absolute insulin deficiency due to destruction of islet cells in the pancreas and gestational diabetes that is a new onset of high blood sugars associated with pregnancy.[13] Type 1 and type 2 diabetes can typically be distinguished based on the presenting circumstances.[68] If the diagnosis is in doubt antibody testing may be useful to confirm type 1 diabetes and C-peptide levels may be useful to confirm type 2 diabetes,[73] with C-peptide levels normal or high in type 2 diabetes, but low in type 1 diabetes.[74]
Universal screening for diabetes in people without risk factors or symptoms is not recommended.[75][76] Screening is recommended by the World Health Organization, the United States Preventive Services Task Force (USPSTF), and the American Diabetes Association for high-risk adults.[77][78][79]  Risk factors considered by the USPSTF include adults over 35 years old who are overweight or have obesity and adults without symptoms whose blood pressure is greater than 135/80 mmHg.[80][needs update][81] For those whose blood pressure is less, the evidence is insufficient to recommend for or against screening.[80][needs update] The American Diabetes Society recommends screening for adults with a body mass index (BMI) over 25.[79] For people of Asian descent, screening is recommended if they have a BMI over 23.[79] Other high risk groups include people with a first degree relative with diabetes; some ethnic groups, including Hispanics, African-Americans, and Native-Americans; a history of gestational diabetes; polycystic ovary syndrome; excess weight; and conditions associated with metabolic syndrome.[23] There is no evidence that screening changes the risk of death and any benefit of screening on adverse effects, incidence of type 2 diabetes, HbA1c or socioeconomic effects are not clear.[76][82]
In the UK, NICE guidelines suggest taking action to prevent diabetes for people with a body mass index (BMI) of 30 or more.[83] For people of Black African, African-Caribbean, South Asian and Chinese descent the recommendation to start prevention starts at the BMI of 27,5.[83] A study based on a large sample of people in England suggest even lower BMIs for certain ethnic groups for the start of prevention, for example 24 in South Asian and 21 in Bangladeshi populations.[84][85]
Onset of type 2 diabetes can be delayed or prevented through proper nutrition and regular exercise.[86][87] Intensive lifestyle measures may reduce the risk by over half.[25][88] The benefit of exercise occurs regardless of the person's initial weight or subsequent weight loss.[89] High levels of physical activity reduce the risk of diabetes by about 28%.[90] Evidence for the benefit of dietary changes alone, however, is limited,[91] with some evidence for a diet high in green leafy vegetables[92] and some for limiting the intake of sugary drinks.[93] There is an association between higher intake of sugar-sweetened fruit juice and diabetes, but no evidence of an association with 100% fruit juice.[94] A 2019 review found evidence of benefit from dietary fiber.[95]
In those with impaired glucose tolerance, a 2019 systematic review found moderate-quality evidence that Metformin, when compared to diet and exercise or a placebo intervention, appeared to delay or reduce the risk of developing type 2 diabetes.[96] This same review found moderate-quality evidence that when compared to intensive diet and exercise, Metformin did not reduce risk of developing type 2 diabetes, as well as very low-quality evidence that combining Metformin with intensive diet and exercise does not appear to have any effect on risk of developing type 2 diabetes when compared to intensive diet and exercise alone.[96] This systematic review only found one suitable trial comparing Metformin with Sulphonylurea in reducing risk of type 2 diabetes but it did not report any patient-relevant outcomes.[96]
A Cochrane systematic review assessed the effect of alpha-glucosidase inhibitors in people with impaired glucose tolerance, impaired fasting blood glucose, elevated glycated hemoglobin A1c (HbA1c).[97]It was found that Acarbose appeared to reduce incidence of diabetes mellitus type 2 when compared to placebo, however there was no conclusive evidence that acarbose compare to diet and exercise, metformin, placebo, no intervention improved all-cause mortality, reduced or increased risk of cardiovascular mortality, serious or non-serious adverse events, non-fatal stroke, congestive heart failure, or non-fatal myocardial infarction.[97]The same review found that there was no conclusive evidence that voglibose compared to diet and exercise or placebo reduced incidence of diabetes mellitus type 2, or any of the other measured outcomes.[97]
A 2017 review found that, long term, lifestyle changes decreased the risk by 28%, while medication does not reduce risk after withdrawal.[98] While low vitamin D levels are associated with an increased risk of diabetes, correcting the levels by supplementing vitamin D3 does not improve that risk.[99]
Management of type 2 diabetes focuses on lifestyle interventions, lowering other cardiovascular risk factors, and maintaining blood glucose levels in the normal range.[25] Self-monitoring of blood glucose for people with newly diagnosed type 2 diabetes may be used in combination with education,[100] although the benefit of self-monitoring in those not using multi-dose insulin is questionable.[25] In those who do not want to measure blood levels, measuring urine levels may be done.[101] Managing other cardiovascular risk factors, such as hypertension, high cholesterol, and microalbuminuria, improves a person's life expectancy.[25] Decreasing the systolic blood pressure to less than 140 mmHg is associated with a lower risk of death and better outcomes.[102] Intensive blood pressure management (less than 130/80 mmHg) as opposed to standard blood pressure management (less than 140-160 mmHg systolic to 85–100 mmHg diastolic) results in a slight decrease in stroke risk but no effect on overall risk of death.[103]
Intensive blood sugar lowering (HbA1c<6%) as opposed to standard blood sugar lowering (HbA1c of 7–7.9%) does not appear to change mortality.[104][105] The goal of treatment is typically an HbA1c of 7 to 8% or a fasting glucose of less than 7.2 mmol/L (130 mg/dL); however these goals may be changed after professional clinical consultation, taking into account particular risks of hypoglycemia and life expectancy.[79][106][107] Hypoglycemia is associated with adverse outcomes in older people with type 2 diabetes.[108] Despite guidelines recommending that intensive blood sugar control be based on balancing immediate harms with long-term benefits, many people – for example people with a life expectancy of less than nine years who will not benefit, are over-treated.[109]
It is recommended that all people with type 2 diabetes get regular eye examinations.[13] There is moderate evidence suggesting that treating gum disease by scaling and root planing results in an improvement in blood sugar levels for people with diabetes.[110]
A proper diet and regular exercise are foundations of diabetic care,[23] with one review indicating that a greater amount of exercise improved outcomes.[111] Regular exercise may improve blood sugar control, decrease body fat content, and decrease blood lipid levels.[112]
Calorie restriction to promote weight loss is generally recommended.[113][70] Around 80 percent of obese people with type 2 diabetes achieve complete remission with no need for medication if they sustain a weight loss of at least 15 kilograms (33 lb),[114][115] but most patients are not able to achieve or sustain significant weight loss.[116] Even modest weight loss can produce significant improvements in glycemic control and reduce the need for medication.[117]
Several diets may be effective such as the Dietary Approaches to Stop Hypertension (DASH), Mediterranean diet, low-fat diet, or monitored carbohydrate diets such as a low carbohydrate diet.[70][118][119] Other recommendations include emphasizing intake of fruits, vegetables, reduced saturated fat and low-fat dairy products, and with a macronutrient intake tailored to the individual, to distribute calories and carbohydrates throughout the day.[70][120] A 2021 review showed that consumption of tree nuts (walnuts, almonds, and hazelnuts) reduced fasting blood glucose in diabetic people.[121] As of 2015[update], there is insufficient data to recommend nonnutritive sweeteners, which may help reduce caloric intake.[122] An elevated intake of microbiota-accessible carbohydrates can help reducing the effects of T2D.[123] Viscous fiber supplements may be useful in those with diabetes.[124]
Culturally appropriate education may help people with type 2 diabetes control their blood sugar levels for up to 24 months.[125] There is not enough evidence to determine if lifestyle interventions affect mortality in those who already have type 2 diabetes.[88]
Although psychological stress is recognized as a risk factor for type 2 diabetes,[10] the effect of stress management interventions on disease progression are not established.[126] A Cochrane review is under way to assess the effects of mindfulness‐based interventions for adults with type 2 diabetes.[127]
There are several classes of anti-diabetic medications available. Metformin is generally recommended as a first line treatment as there is some evidence that it decreases mortality;[7][25][128] however, this conclusion is questioned.[129] Metformin should not be used in those with severe kidney or liver problems.[23] TheAmerican Diabetes Association and European Association for the Study of Diabetes recommend using a GLP-1 agonist or SGLT2 inhibitor as the first-line treatment in patients who have or are at high risk for atherosclerotic cardiovascular disease, heart failure, or kidney disease.[130][131] The higher cost of these drugs compared to metformin has limited their use.[116][132][133]
A second oral agent of another class or insulin may be added if metformin is not sufficient after three months.[106] Other classes of medications include: sulfonylureas, thiazolidinediones, dipeptidyl peptidase-4 inhibitors, SGLT2 inhibitors, and GLP-1 receptor agonists.[106] A 2018 review found that SGLT2 inhibitors and GLP-1 agonists, but not DPP-4 inhibitors, were associated with lower mortality than placebo or no treatment.[134] Rosiglitazone, a thiazolidinedione, has not been found to improve long-term outcomes even though it improves blood sugar levels.[135] Additionally it is associated with increased rates of heart disease and death.[136]
Injections of insulin may either be added to oral medication or used alone.[25] Most people do not initially need insulin.[13] When it is used, a long-acting formulation is typically added at night, with oral medications being continued.[23][25] Doses are then increased to effect (blood sugar levels being well controlled).[25] When nightly insulin is insufficient, twice daily insulin may achieve better control.[23] The long acting insulins glargine and detemir are equally safe and effective,[137] and do not appear much better than neutral protamine Hagedorn (NPH) insulin, but as they are significantly more expensive, they are not cost effective as of 2010.[138] In those who are pregnant, insulin is generally the treatment of choice.[23]
Many international guidelines recommend blood pressure treatment targets that are lower than 140/90 mmHg for people with diabetes.[139] However, there is only limited evidence regarding what the lower targets should be. A 2016 systematic review found potential harm to treating to targets lower than 140 mmHg,[140] and a subsequent review in 2019 found no evidence of additional benefit from blood pressure lowering to between 130–140mmHg, although there was an increased risk of adverse events.[141]
2015 American Diabetes Association recommendations are that people with diabetes and albuminuria should receive an inhibitor of the renin-angiotensin system to reduce the risks of progression to end-stage renal disease, cardiovascular events, and death.[70] There is some evidence that angiotensin converting enzyme inhibitors (ACEIs) are superior to other inhibitors of the renin-angiotensin system such as angiotensin receptor blockers (ARBs),[142] or aliskiren in preventing cardiovascular disease.[143] Although a more recent review found similar effects of ACEIs and ARBs on major cardiovascular and renal outcomes.[144] There is no evidence that combining ACEIs and ARBs provides additional benefits.[144]
The use of aspirin to prevent cardiovascular disease in diabetes is controversial.[70] Aspirin is recommended in people at high risk of cardiovascular disease, however routine use of aspirin has not been found to improve outcomes in uncomplicated diabetes.[145] 2015 American Diabetes Association recommendations for aspirin use (based on expert consensus or clinical experience) are that low-dose aspirin use is reasonable in adults with diabetes who are at intermediate risk of cardiovascular disease (10-year cardiovascular disease risk, 5–10%).[70]
Vitamin D supplementation to people with type 2 diabetes may improve markers of insulin resistance and HbA1c.[146]
Sharing their electronic health records with people who have type 2 diabetes helps them to reduce their blood sugar levels. It is a way of helping people understand their own health condition and involving them actively in its management.[147][148]
Weight loss surgery in those who are obese is an effective measure to treat diabetes.[149] Many are able to maintain normal blood sugar levels with little or no medication following surgery[150] and long-term mortality is decreased.[151] There however is some short-term mortality risk of less than 1% from the surgery.[152] The body mass index cutoffs for when surgery is appropriate are not yet clear.[151] It is recommended that this option be considered in those who are unable to get both their weight and blood sugar under control.[153][154]
The International Diabetes Federation estimates nearly 537 million people lived with diabetes worldwide in 2021,[155] 90–95% of whom have type 2 diabetes.[156] Diabetes is common both in the developed and the developing world.[10]
Some ethnic groups such as South Asians, Pacific Islanders, Latinos, and Native Americans are at particularly high risk of developing type 2 diabetes.[23] Type 2 diabetes in normal weight individuals represents 60 to 80 percent of all cases in some Asian countries. The mechanism causing diabetes in non-obese individuals is poorly understood.[157][158][159]
Rates of diabetes in 1985 were estimated at 30 million, increasing to 135 million in 1995 and 217 million in 2005.[18] This increase is believed to be primarily due to the global population aging, a decrease in exercise, and increasing rates of obesity.[18] Traditionally considered a disease of adults, type 2 diabetes is increasingly diagnosed in children in parallel with rising obesity rates.[10] The five countries with the greatest number of people with diabetes as of 2000 are India having 31.7 million, China 20.8 million, the United States 17.7 million, Indonesia 8.4 million, and Japan 6.8 million.[160] It is recognized as a global epidemic by the World Health Organization.[1]
Diabetes is one of the first diseases described[21] with an Egyptian manuscript from c. 1500 BCE mentioning "too great emptying of the urine."[161] The first described cases are believed to be of type 1 diabetes.[161] Indian physicians around the same time identified the disease and classified it as madhumeha or honey urine noting that the urine would attract ants.[161] The term "diabetes" or "to pass through"  was first used in 230 BCE by the Greek Apollonius Memphites.[161] The disease was rare during the time of the Roman empire with Galen commenting that he had only seen two cases during his career.[161]
Type 1 and type 2 diabetes were identified as separate conditions for the first time by the Indian physicians Sushruta and Charaka in 400–500 AD with type 1 associated with youth and type 2 with being overweight.[161] Effective treatment was not developed until the early part of the 20th century when the Canadians Frederick Banting and Charles Best discovered insulin in 1921 and 1922.[161] This was followed by the development of the long acting NPH insulin in the 1940s.[161]
In 1916, Elliot Joslin proposed that in people with diabetes, periods of fasting are helpful.[162] Subsequent research has supported this, and weight loss is a first line treatment in type 2 diabetes.[162]
Researchers developed the Diabetes Severity Score (DISSCO), a tool that might better than the standard blood test at identify if a person's condition is declining. It uses a computer algorithm to analyse data from anonymised electronic patient records and produces a score based on 34 indicators.[163][164]
